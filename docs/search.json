[
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "Manuscript",
    "section": "",
    "text": "HTML\n   \n  \n  \n    HTML (new window)\n  \n\n      \n    HTML (code included)\n  \n\n  \n    PDF\n   \n  \n  \n  \n\n  \n    docx (download)",
    "crumbs": [
      "Manuscript"
    ]
  },
  {
    "objectID": "Simulations/simParameterRecovery.html",
    "href": "Simulations/simParameterRecovery.html",
    "title": "Parameter Recovery Simulations",
    "section": "",
    "text": "Parameter Recovery Simulations\nAssessing the parameter recovery of ALM and EXAM on synthetic data.\n\nCodepacman::p_load(tidyverse,data.table)\noptions(dplyr.summarise.inform=FALSE)\n\ninput.activation&lt;-function(x.target, c) { return(exp((-1*c)*(x.target-inputNodes)^2))}\n\noutput.activation&lt;-function(x.target, weights, c){\n  return(weights%*%input.activation(x.target, c))\n}\nmean.prediction&lt;-function(x.target, weights, c){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\n\nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){t\n  yt.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  alm.train\n}\n\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, c,trainVec){\n  #trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, c)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, c)\n  mOver = mean.prediction(xOver, weights, c)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n\n# Modify the sim_data function to accept the dataset as an argument\nsim_data &lt;- function(dat, c=0.5, lr=0.2, inNodes=7, outNodes=32, trainVec=c(5,6,7)) {\n  inputNodes &lt;&lt;- seq(1,7,length.out=inNodes*1)  \n  outputNodes &lt;&lt;- seq(50,1600,length.out=outNodes*1) \n  wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n  tt&lt;-trainTest.alm(dat, c, lr, wm, trainVec)\n}\n\ngen_train &lt;- function(trainVec=c(5,6,7),trainRep=10,noise=0){\n   bandVec=c(0,100,350,600,800,1000,1200)\n   ts &lt;- rep(seq(1,length(trainVec)),trainRep)\n   noiseVec=rnorm(length(ts),mean=0)*noise\n   if(noise==0) {noiseVec=noiseVec*0}\n   tibble(input=trainVec[ts],vx=bandVec[trainVec[ts]]+noiseVec)\n}\n\n\ntrainTest.alm&lt;-function(dat, c=0.05, lr=0.5, weights,testVec){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  almPred &lt;- sapply(testVec,mean.prediction,weights,c)\n  examPred &lt;- sapply(testVec,exam.prediction,weights,c,trainVec=c(1,sort(unique(dat$input))))\n  list(almTrain=alm.train,almPred=almPred,examPred=examPred)\n}\n\nwrap_alm &lt;- function(par,dat, weights,lossFun){\n    c=par[1]; lr=par[2]\n   pred=train.alm(dat, c=c, lr=lr, weights=weights)\n   #sqrt(mean((dat$vx -pred)^2))\n   lossFun(dat$vx,pred)\n}\n\nwrap_optim &lt;- function(dat,lossFun=RMSE){\n  if(class(lossFun)==\"character\"){lossFun=get(lossFun)}\n  inputNodes = seq(1,7,1)  # \n  outputNodes = seq(50,1600,50)\n  wm=matrix(.00001,nrow=length(outputNodes),ncol=length(inputNodes))\n  testVec=seq(2,7)\n  \n  bounds_lower &lt;- c(.0000001, .00001)\n  bounds_upper &lt;- c(10, 10)\n  parmsLab &lt;- c(\"c\",\"lr\")\n  \n fit=optim(par=c(.1, .2),\n   fn = wrap_alm,\n   dat = dat, weights = wm,lossFun=lossFun,\n   method = \"L-BFGS-B\",\n   lower = bounds_lower,\n   upper = bounds_upper,\n   control = list(maxit = 1e5, pgtol = 0, factr = 0)\n )\n\n l=reduce(list(list(fit),fit$par,fit$value),append)\n names(l)=c(\"Fit\",parmsLab,\"Value\")\n return(l)\n}\n\n# implement wrap_optim using grid search\nwrap_grid &lt;- function(dat,lossFun=RMSE){\n  if(class(lossFun)==\"character\"){lossFun=get(lossFun)}  \n  inputNodes = seq(1,7,1)  # \n  outputNodes = seq(50,1600,50)\n  wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n  testVec=seq(2,7)\n  # define grid boundaries\n  cRange &lt;- seq(.000001, 5, length.out = 30)\n  lrRange &lt;- seq(.05, 5, length.out = 20)\n  # create grid\n  grid &lt;- expand.grid(c = cRange, lr = lrRange)\n  grid$Value &lt;- NA\n  # loop through grid\n  for (i in 1:nrow(grid)) {\n    grid$Value[i] &lt;- wrap_alm(par=c(grid[i,c(\"c\") ],grid[i,c(\"lr\") ]), dat=dat, weights=wm,lossFun=lossFun)\n  }\n\n  # find best fit\n  bestFit &lt;- grid[which.min(grid$Value), ]\n  bestFit$Value &lt;- min(grid$Value)\n\n  # return best fit, and best c and lr\n  return(list(Fit = bestFit, c = bestFit$c, lr = bestFit$lr, Value = bestFit$Value))\n \n}\n\n# sim_data &lt;- function(c=0.5, lr=0.2,noise=0,inNodes=7,outNodes=32,\n#                      trainVec=c(5,6,7),trainRep=10,testVec=seq(2,7)) {\n#   \n#   inputNodes &lt;&lt;- seq(1,7,length.out=inNodes*1)  \n#   outputNodes &lt;&lt;- seq(50,1600,length.out=outNodes*1) \n#   wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n#   dat&lt;-gen_train(trainVec,trainRep,noise)\n#   #trainDat &lt;- train.alm(dat,c,lr,wm)\n#   tt&lt;-trainTest.alm(dat,c,lr,wm,testVec)\n# }\n\n\nLoss Functions\n\nCodeRMSE &lt;- function(x,y){\n # print(\"rmseTrial\")\n  sqrt(mean((x-y)^2))\n}\n\nRMSE.blocked &lt;- function(x,y,blocks=6){\n  #print(\"rmseBlocked\")\n  data.table(x=x,y=y,t=seq(1,length(x))) %&gt;% \n    .[, `:=`(fitBins = cut(t, breaks = ..blocks, labels = c(1:..blocks)))] %&gt;%\n    .[, .(predMean = mean(x), obsMean = mean(y)), keyby = .(fitBins)] %&gt;%\n    .[, RMSE(predMean,obsMean)] %&gt;% as.numeric()\n}\n\nMAE &lt;- function(x, y) {\n  mean(abs(x - y))\n}\n\nMAPE &lt;- function(x, y) {\n  mean(abs((x - y) / y)) * 100\n}\n\nMedAE &lt;- function(x, y) {\n  median(abs(x - y))\n}\n\nHuberLoss &lt;- function(x, y, delta = 1) {\n  error &lt;- x - y\n  abs_error &lt;- abs(error)\n  loss &lt;- ifelse(abs_error &lt;= delta, 0.5 * error^2, delta * (abs_error - 0.5 * delta))\n  mean(loss)\n}\n\n\n\nCodek= parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise))),\n                                                 o=map(td,head,1),vx1=map(o,\"vx\"))\n\nparmVec &lt;- tibble(crossing(c = c(0.1), lr = c(0.1,0.4,1), noise = c(10), trainRep = c(20), lossFun = list(\"RMSE\", \"MAE\",\"MAPE\", \"MedAE\", \"HuberLoss\"), simNum = 1:10))\n\nsdp &lt;- parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;% ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr), ~sim_data(dat = .x, c = ..2, lr = ..3)),\n         almTrainDat = map(d, \"almTrain\"),\n         almTestDat = map(d, \"almPred\"),\n         examTestDat = map(d, \"examPred\"),\n         fitO = map2(td, lossFun, ~wrap_optim(.x, .y)),\n         cFitO = map_dbl(fitO, \"c\"),\n         lrFitO = map_dbl(fitO, \"lr\"),\n         optimValO = map_dbl(fitO, \"Value\")) \n\nsdpResults &lt;- sdp %&gt;% \n  mutate(lossFun=as.character(lossFun),cDiff=cFitO-c,lrDiff=lrFitO-lr) %&gt;%\n  relocate(simNum,lossFun, c,lr,cFitO,lrFitO,optimValO,cDiff,lrDiff) %&gt;% \n  arrange(lossFun,lr,c)\n\naveraged_sdp &lt;- sdpResults %&gt;%\n  group_by(lossFun, c, lr) %&gt;%\n  summarise(\n    avg_cFitO = mean(cFitO),\n    avg_lrFitO = mean(lrFitO),\n    avg_optimValO = mean(optimValO),\n    avg_cDiff = mean(cDiff),\n    avg_lrDiff = mean(lrDiff),\n    .groups = \"drop\"\n  )\n\n\naveraged_sdp &lt;- sdpResults %&gt;%\n  group_by(lossFun, c, lr) %&gt;%\n  summarise(\n    avg_cFitO = mean(cFitO),\n    var_cFitO = var(cFitO),\n    avg_lrFitO = mean(lrFitO),\n    var_lrFitO = var(lrFitO),\n    avg_optimValO = mean(optimValO),\n    var_optimValO = var(optimValO),\n    avg_cDiff = mean(cDiff),\n    var_cDiff = var(cDiff),\n    avg_lrDiff = mean(lrDiff),\n    var_lrDiff = var(lrDiff),\n    .groups = \"drop\"\n  )\n\n# averaged_sdp &lt;- sdp %&gt;%\n#   group_by(c, lr, noise, trainRep, lossFun) %&gt;%\n#   summarise(across(starts_with(\"cFit\") | starts_with(\"lrFit\") | starts_with(\"optimVal\"), list(mean = mean), .names = \"mean_{.col}\")) %&gt;% \n#   mutate(lossFun=as.character(lossFun),\n#          diff_cFitO = abs(c - mean_cFitO),\n#          diff_lrFitO = abs(lr - mean_lrFitO)) %&gt;%\n#   relocate(noise, trainRep, lossFun, c, diff_cFitO, lr, diff_lrFitO) %&gt;%\n#   dplyr::arrange(c,lr)\n\n\n#k=parmVec %&gt;% group_by(simNum) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise))))\n#sdp &lt;- parmVec %&gt;% mutate(d = pmap(list(c, lr, noise, trainRep), ~sim_data(c = ..1, lr = ..2, noise = ..3, trainRep = ..4)),\n\n\n\nCodelibrary(plotly)\n\ng2= grid %&gt;% filter(Value&lt;160) %&gt;% arrange(Value)\n#plot_ly() %&gt;% add_trace(data=g2,x=grid$c,y=grid$lr,z=grid$Value,type=\"mesh3d\")\n \nplot_ly(data=g2,x=~c,y=~lr,z=~Value,type = 'mesh3d')\nplot_ly(g2,type = 'surface')\n\n\n\n\n\nfig &lt;- plot_ly(x = grid2$lr, y = grid2$c, z = grid2$Value) %&gt;% add_surface()\n\np &lt;- ggplot(g2, aes(c, lr, z= Value)) +\n  stat_contour(geom=\"polygon\",aes(fill=stat(level))) +\n  scale_fill_distiller(palette = \"Spectral\", direction = 1)\nggplotly(p)\n\n\np &lt;- ggplot(grid, aes(c, lr, z= Value,colour=stat(level))) +\n  geom_contour() \nggplotly(p)\n\n\nplot_ly(g2, x = ~c, y = ~lr, z = ~Value, type = 'scatter3d', mode = 'lines+markers',\n        opacity = 7, \n        line = list(width = 6, colorscale = 'Viridis', reverscale = FALSE)\n        )\n\n\n#install.packages(\"echarts4r\")\nlibrary(echarts4r)\ng2 |&gt; \n  e_charts(c) |&gt; \n  e_surface(lr, Value, wireframe = list(show = FALSE)) |&gt; \n  e_visual_map(Value)\n\n\nc lr noise trainRep lossFun mean_cFitO mean_cFitG mean_lrFitO mean_lrFitG mean_optimValO mean_optimValG  1 0.1 0.4 500 20 RMSE 4.94 5 1.00 1.09 3.30e- 5 0.115\n2 0.1 0.4 500 20 MAE 0.100 5 0.268 1.09 2.56e+ 1 0.0921 3 0.1 0.4 500 20 MAPE 0.101 5 0.268 1.09 2.31e+ 0 0.00954 4 0.1 0.4 500 20 MedAE 0.288 5 0.429 1.09 3.65e- 1 0.124\n5 0.1 0.4 500 20 HuberLoss 8.09 5 1.00 1.09 1.98e-17 0.00662"
  },
  {
    "objectID": "Simulations/alm_learning.html",
    "href": "Simulations/alm_learning.html",
    "title": "ALM Learning",
    "section": "",
    "text": "pacman::p_load(tidyverse,data.table,patchwork,glue,knitr,kableExtra,here)\noptions(dplyr.summarise.inform=FALSE)\npurrr::walk(here(c(\"Functions/alm_functions.R\",\"Functions/Display_Functions.R\")),source)\n\n\nupdate.weights.with_noise &lt;- function(x.new, y.new, weights, c, lr, noise_sd){\n  y.feedback.activation &lt;- exp(-1 * c * (y.new - outputNodes)^2)\n  x.feedback.activation &lt;- output.activation(x.new, weights, c)\n  weight_updates &lt;- lr * (y.feedback.activation - x.feedback.activation) %*% t(input.activation(x.new, c))\n  noise &lt;- matrix(rnorm(nrow(weight_updates) * ncol(weight_updates), sd = noise_sd), \n                  nrow = nrow(weight_updates), ncol = ncol(weight_updates))\n  updated_weights &lt;- weights + weight_updates + noise\n  return(updated_weights)\n}\n\n\nupdate.weights&lt;-function(x.new, y.new, weights, c, lr, noise_sd = NULL){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\nsim_data &lt;- function(dat, c=0.5, lr=0.2, inNodes=7, outNodes=32, trainVec=c(5,6,7),noise_sd=0,update_func=\"update.weights\" ) {\n  inputNodes &lt;&lt;- seq(1,7,length.out=inNodes*1)  \n  outputNodes &lt;&lt;- seq(50,1600,length.out=outNodes*1) \n  #print(length(outputNodes))\n  wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n  # wm=matrix(rnorm(length(outputNodes)*length(inputNodes),.1,5),nrow=length(outputNodes),ncol=length(inputNodes))\n  tt&lt;-trainTest.alm(dat, c, lr, wm, trainVec, update_func, noise_sd)\n}   \n\ntrainTest.alm &lt;- function(dat, c=0.05, lr=0.5, weights, testVec, update_func, noise_sd) {\n   alm.train &lt;- rep(NA, nrow(dat))  \n   update_func=get(update_func)\n   decay_factor = 0.79\n  for (i in 1:nrow(dat)) {\n    #lr = lr * decay_factor^i\n    resp = mean.prediction(dat$input[i], weights, c)\n    weights &lt;- update_func(dat$input[i], dat$vx[i], weights, c, lr, noise_sd)\n    alm.train[i] = resp\n    weights[weights&lt;0] = 0\n  }\n  almPred &lt;- sapply(testVec, mean.prediction, weights, c)\n  examPred &lt;- sapply(testVec, exam.prediction, weights, c, trainVec=c(1,sort(unique(dat$input))))\n  list(almTrain=alm.train, almPred=almPred, examPred=examPred,weights=weights)\n}\n\nModel learning, and resulting weights, across range of parameter values\n\ntibble(crossing(\n  c = c(.5,5),lr = c(.05,1),noise = c(0),\n  inNodes=c(7),outNodes=c(32),\n  trainVec=list(list(5,6,7)),trainRep = c(9),\n  lossFun = list(\"MAE\"),\n  simNum = 1:1,\n  update_func = list(\"update.weights\"),update_func_name = c(\"uW\"),\n  noise_sd = c(0)\n)) %&gt;%   mutate(id=seq(1,nrow(.)),td = pmap(list(trainVec,trainRep,noise),~gen_train(trainVec=.x,trainRep=..2,noise=..3) )) %&gt;% \n  ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr, update_func,noise_sd,inNodes,outNodes), \n                  ~sim_data(dat = .x, c = ..2, lr = ..3, update_func = ..4, noise_sd = ..5,inNodes=..6,outNodes=..7)),\n         almTrainDat = map(d, \"almTrain\"),weights=map(d,\"weights\"))%&gt;%\n  unnest(c(almTrainDat, td)) %&gt;% select(-d) %&gt;% mutate(input=as.factor(input)) %T&gt;%\n  {pf(.) } %&gt;% trainTab\n\ncleaner imlementation of above\n\n# Define parameters\nparams &lt;- tibble(crossing(\n  c = c(.5,5),\n  lr = c(.05,1),\n  noise = c(0),\n  inNodes = c(7),\n  outNodes = c(32),\n  trainVec = list(list(5,6,7)),\n  trainRep = c(9),\n  lossFun = list(\"MAE\"),\n  simNum = 1:1,\n  update_func = list(\"update.weights\"),\n  update_func_name = c(\"uW\"),\n  noise_sd = c(0)\n))\n\n# Generate training data\nparams &lt;- params %&gt;% \n  mutate(\n    id = seq(1, nrow(.)),\n    td = pmap(list(trainVec, trainRep, noise), ~gen_train(trainVec = .x, trainRep = ..2, noise = ..3))\n  )\n\n# Run simulations\nparams &lt;- params %&gt;% \n  mutate(\n    d = pmap(list(td, c, lr, update_func, noise_sd, inNodes, outNodes), \n              ~sim_data(dat = .x, c = ..2, lr = ..3, update_func = ..4, noise_sd = ..5, inNodes = ..6, outNodes = ..7)),\n    almTrainDat = map(d, \"almTrain\"),\n    weights = map(d, \"weights\")\n  )\n\n# Unnest and select relevant columns\nparams &lt;- params %&gt;% \n  unnest(c(almTrainDat, td)) %&gt;% \n  select(-d) %&gt;% \n  mutate(input = as.factor(input))\n\n# Apply pf function and trainTab\nresult &lt;- params %T&gt;% \n  {pf(.) } %&gt;% \n  trainTab\n\n\n# Define a function to fit the model and return the parameters\nfit_model &lt;- function(data, initial_c, initial_lr) {\n  # Fit the model here and extract the parameters\n  # This is a placeholder and should be replaced with your actual model fitting code\n  fit_c = initial_c #+ rnorm(1, 0, 0.1)\n  fit_lr = initial_lr #+ rnorm(1, 0, 0.1)\n  \n  tibble(\n    gen_c = initial_c,\n    gen_lr = initial_lr,\n    fit_c = fit_c,\n    fit_lr = fit_lr,\n    error_c = fit_c - initial_c,\n    error_lr = fit_lr - initial_lr\n  )\n}\n\nparams &lt;- tibble(crossing(\n  c = seq(.01,2,length.out=10),\n  lr = seq(.01,2,length.out=10)\n))\n# Run the simulations\nresults &lt;- params %&gt;%\n  mutate(simulation = map2(c, lr, ~fit_model(td, .x, .y))) %&gt;%\n  unnest(simulation)\n\n\nfit_model &lt;- function(data, initial_c, initial_lr) {\n  # Simulate data from the ALM model\n  sim_data &lt;- sim_data(dat = data, c = initial_c, lr = initial_lr, \n                       update_func = \"update.weights\", noise_sd = 0, \n                       inNodes = 7, outNodes = 32)\n  \n  # Extract the fitted parameters\n  fit_c = sim_data$c\n  fit_lr = sim_data$lr\n  \n  tibble(\n    gen_c = initial_c,\n    gen_lr = initial_lr,\n    fit_c = fit_c,\n    fit_lr = fit_lr,\n    error_c = fit_c - initial_c,\n    error_lr = fit_lr - initial_lr\n  )\n}\n\n# Run the simulations\nresults &lt;- params %&gt;%\n  mutate(simulation = map2(c, lr, ~fit_model(td, .x, .y))) %&gt;%\n  unnest(simulation)\n\n# Print the results\nprint(results)\n\n# Print the results\nprint(results %&gt;% select(c,lr,gen_c,gen_lr,fit_c,fit_lr,error_c,error_lr))\n\nggplot(results, aes(x = gen_c, y = fit_c)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(x = \"Generating c\", y = \"Fitted c\", title = \"Parameter Recovery for c\")\n\n# Plot for lr parameter\nggplot(results, aes(x = gen_lr, y = fit_lr)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(x = \"Generating lr\", y = \"Fitted lr\", title = \"Parameter Recovery for lr\")\n\nSelf contained\n\npacman::p_load(tidyverse,data.table,knitr,kableExtra,glue)\ninput.activation&lt;-function(x.target, c){return(exp((-1*c)*(x.target-inputNodes)^2))}\noutput.activation&lt;-function(x.target, weights, c){return(weights%*%input.activation(x.target, c))}\nmean.prediction&lt;-function(x.target, weights, c){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  } \n  list(almTrain = alm.train, weights = weights)\n  }\nsim_train &lt;- function(dat, c=0.5, lr=0.2, inNodes=7, outNodes=32, trainVec=c(5,6,7),noise_sd=0,update_func=\"update.weights\" ) {\n  inputNodes &lt;&lt;- seq(1,7,length.out=inNodes*1)  \n  outputNodes &lt;&lt;- seq(50,1600,length.out=outNodes*1) \n  wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n  tt&lt;-train.alm(dat, c, lr, wm)\n} \ngen_train &lt;- function(trainVec = c(5, 6, 7), trainRep = 3, noise = 0) {\n    bandVec &lt;- c(0, 100, 350, 600, 800, 1000, 1200)\n    if (class(trainVec) == \"list\") {trainVec &lt;- unlist(trainVec)}\n    ts &lt;- rep(seq(1, length(trainVec)), trainRep)\n    noiseVec &lt;- rnorm(length(ts), mean = 0) * noise\n    if (noise == 0) {noiseVec &lt;- noiseVec * 0}\n    tibble(trial = seq(1, length(ts)), input = trainVec[ts], vx = bandVec[trainVec[ts]] + noiseVec)\n}\n\ntibble(crossing(\n    c = c(.5, 5), lr = c(.05, 1), noise = c(0),\n    inNodes = c(7), outNodes = c(32),\n    trainVec = list(list(5, 6, 7)), trainRep = c(9),\n    lossFun = list(\"MAE\"),\n    simNum = 1:1,\n)) %&gt;%\n    mutate(id = seq(1, nrow(.)), td = pmap(list(trainVec, trainRep, noise), ~ gen_train(trainVec = .x, trainRep = ..2, noise = ..3))) %&gt;%\n    ungroup() %&gt;%\n    mutate(\n        d = pmap(\n            list(td, c, lr,inNodes, outNodes),\n            ~ sim_train(dat = .x, c = ..2, lr = ..3, inNodes = ..4, outNodes = ..5)\n        ),\n        almTrainDat = map(d, \"almTrain\"), weights = map(d, \"weights\")\n    ) %&gt;%\n    unnest(c(almTrainDat, td)) %&gt;%\n    select(-d) %&gt;%\n    mutate(input = as.factor(input)) %&gt;%\n    trainTab()\n\ntibble(crossing(\n    c = c(.5, 5), lr = c(.05, 1), noise = c(0),\n    inNodes = c(7), outNodes = c(32),\n    trainVec = list(list(5, 6, 7)), trainRep = c(9),\n    lossFun = list(\"MAE\"),\n    simNum = 1:1,\n)) %&gt;%\n    mutate(id = seq(1, nrow(.)), td = pmap(list(trainVec, trainRep, noise), ~ gen_train(trainVec = .x, trainRep = ..2, noise = ..3))) %&gt;%\n    ungroup() %&gt;%\n    mutate(\n        d = pmap(\n            list(td, c, lr,inNodes, outNodes),\n            ~ sim_train(dat = .x, c = ..2, lr = ..3, inNodes = ..4, outNodes = ..5)\n        ),\n        almTrainDat = map(d, \"almTrain\"), weights = map(d, \"weights\")\n    ) %&gt;%\n    unnest(c(almTrainDat, td)) %&gt;%\n    select(-d) %&gt;%\n    mutate(input = as.factor(input)) %&gt;%\n    trainTab()\n\nOptimize for single decay curve\ngenerate data that follows an exponetial decay function of error over trials, inspect ability of model to fit that data.\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=18) %&gt;% mutate(cor=vx,err=(400-0)*exp(-.1*seq(1,n()))+0,vx=cor-err)\n\nbias &lt;- 1000; \ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=228,noise=0) %&gt;% mutate(\n  cor = vx,\n  err = (bias - 0) * exp(-.005 * seq(1, n())) + 0,\n  en = map2_dbl(err,cor, ~rnorm(n = 1, mean = .y, sd = .x/2)),\n  enAvg = map2_dbl(err,cor, ~mean(rnorm(n = 1, mean = .y, sd = .x))),\n  weight = (seq(1, n()) - 1) / (n() - 1),\n  vx = (weight*en)+bias*(1-weight),\n  vx=en\n)\ngt %&gt;% ggplot(aes(x = trial, y = vx, color = as.factor(input))) +\n  geom_line() + ylim(c(-10,1600))\n\n\nk=wrap_optim(gt,lossFun = \"MAE\")\ns=sim_data(dat=mutate(gt,vx=cor),c=k %&gt;% pluck(\"c\"),lr= k %&gt;% pluck(\"lr\"))\nggp &lt;- gt %&gt;% mutate(pred=s %&gt;% pluck(\"almTrain\"),c=k %&gt;% pluck(\"c\"),lr= k %&gt;% pluck(\"lr\"),input=as.factor(input)) %&gt;%  \n  ggplot(aes(x = trial, y = pred, color = input)) +\n  geom_line() + ylim(c(0,1600))\nggo &lt;-  gt %&gt;% ggplot(aes(x = trial, y = vx, color = as.factor(input))) +\n  geom_line() + ylim(c(-400,1600))\n\nggo+ggp\n\n\n#k = t[1,]\n#image(matrix(unlist(k$weights),nrow=7))\n# mutate(md=map(weights,~matrix(unlist(.),nrow=7)))\n\nwms &lt;- t %&gt;% filter(trial==1) %&gt;% \n  mutate(k=map(weights,~ as.data.frame(matrix(unlist(.),nrow=7)) %&gt;% \n                 rownames_to_column(\"Input\") %&gt;%\n                 pivot_longer(-c(Input), names_to = \"Output\",\n                              names_prefix = \"V\", values_to = \"weight\") %&gt;%  mutate(Output= fct_relevel(Output,unique(.$Output)))))\n\nwms %&gt;% unnest(k) %&gt;% ggplot(.,aes(x=Input, y=Output, fill=weight)) + \n  geom_raster() + \n  scale_fill_viridis_c()+facet_wrap(~id+c)\n\n\n\nmd=matrix(unlist(k$weights),nrow=7)\nkd=as.data.frame(matrix(unlist(k$weights),nrow=7))%&gt;%\n  rownames_to_column(\"Input\") %&gt;%\n  pivot_longer(-c(Input), names_to = \"Output\",names_prefix = \"V\", values_to = \"weight\")\n\nkd %&gt;% \n  mutate(Output= fct_relevel(Output,unique(kd$Output))) %&gt;%\n  ggplot(aes(x=Input, y=Output, fill=weight)) + \n  geom_raster() + \n  scale_fill_viridis_c()\n\n\npv &lt;- t &lt;- parmVec &lt;- tibble(crossing(\n  c = c(0.00003),\n  lr = c(0.051),\n  noise = c(0),\n  inNodes=c(7),\n  outNodes=c(32),\n  trainVec=list(list(1,5,6,7),list(5,6,7)),\n  trainRep = c(4),\n  lossFun = list(\"MAE\"),\n  simNum = 1:1,\n  update_func = list(\"update.weights\"),\n  update_func_name = c(\"update.weights\"),\n  noise_sd = c(0)\n)) %&gt;% mutate(id=seq(1,nrow(.)))\n\ninspect learning\n\nparmVec &lt;- tibble(crossing(\n  c = c(0.1),\n  lr = c(0.1, 0.4, 1),\n  noise = c(10),\n  trainRep = c(20),\n  lossFun = list(\"MAE\"),\n  simNum = 1:10,\n  update_func = list(\"update.weights\", \"update.weights.with_noise\"),\n  update_func_name = c(\"update.weights\", \"update.weights.with_noise\"),\n  noise_sd = c(0.1, 0.5)\n))\n\n\n\nt &lt;- parmVec %&gt;%\n  group_by(simNum, c, lr, update_func,noise_sd) %&gt;%\n  mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;%\n  ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr, update_func,noise_sd), \n                  ~sim_data(dat = .x, c = ..2, lr = ..3, update_func = ..4, noise_sd = ..5)),\n         almTrainDat = map(d, \"almTrain\"))%&gt;%\n  unnest(almTrainDat, td) %&gt;%\n  select(-d) %&gt;% \n  mutate(trial = rep(seq(1, nrow(.)/10), 10),input=as.factor(input))\n\nt %&gt;% group_by(lr, update_func_name, simNum, trial, input) %&gt;%\n  summarize(almTrainDat = mean(almTrainDat), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = trial, y = almTrainDat, color = input)) +\n  geom_line() + ylim(c(0,1300))+\n  facet_grid(lr ~ update_func_name)\n\n\nparmVec &lt;- tibble(crossing(c = c(0.1), lr = c(0.1,0.4,1), \n                           noise = c(10), trainRep = c(20), lossFun = list(\"MAE\"), simNum = 1))\n\nt &lt;- parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;% ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr), ~sim_data(dat = .x, c = ..2, lr = ..3)),\n         almTrainDat = map(d, \"almTrain\"))\n\n# extract and plot each almTrainDat \nalmTrainDat &lt;- parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;% ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr), ~sim_data(dat = .x, c = ..2, lr = ..3)),\n         almTrainDat = map(d, \"almTrain\")) %&gt;% unnest(almTrainDat) %&gt;% select(-d)\n\n# For each unique value of lr, plot the learning curve, showing almTrainDat as a function of trial number, color by input, and facet by lr. Convert input to factor first. \n\nalmTrainDat %&gt;% group_by(lr) %&gt;% mutate(trial = seq(1,n()),input= as.factor(input)) %&gt;% ggplot(aes(x=trial,y=almTrainDat,color=input)) + geom_line() + facet_wrap(~lr)\n\n\n\nparmVec &lt;- tibble(crossing(c = c(0.1), lr = c(.01,.05,0.1), noise = c(5), trainRep = c(20), lossFun = list(\"MAE\"), simNum = 1:30))\n\n# The rest of the code remains the same\nalmTrainDat &lt;- parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;% ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr), ~sim_data(dat = .x, c = ..2, lr = ..3)),\n         almTrainDat = map(d, \"almTrain\")) %&gt;% unnest(almTrainDat,td) %&gt;% select(-d) %&gt;%\n  mutate(trial = rep(seq(1, nrow(.)/30), 30),input=as.factor(input))\n\n\nmean_sd_almTrainDat &lt;- almTrainDat %&gt;%\n  group_by(lr,input, trial) %&gt;%\n  summarise(avg_almTrainDat = mean(almTrainDat), sd_almTrainDat = sd(almTrainDat), .groups = 'drop')\n\n# Check the results\nhead(mean_sd_almTrainDat)\n\n\nggplot(mean_sd_almTrainDat, aes(x = trial, y = avg_almTrainDat,color=input)) +\n   geom_line() +\n  geom_errorbar(aes(ymin = avg_almTrainDat - sd_almTrainDat, ymax = avg_almTrainDat + sd_almTrainDat), width = 0.2) +\n  facet_wrap(~lr) +\n  labs(title = \"ALM Train Data: Mean and Variance Over Trials\",\n       x = \"Trial\",\n       y = \"Average ALM Train Data\") + ylim(c(0,1300))+\n  theme_minimal()\n\n\n#plot(seq(1,90), (200-.50)*exp(-.1*seq(1,90))+.50)\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=18) %&gt;% mutate(cor=vx,err=(400-0)*exp(-.1*seq(1,n()))+0,vx=cor-err)\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=10) %&gt;% mutate(cor=vx,err=(100-0)*exp(-.03*seq(1,n()))+0,en=map_dbl(err,~rnorm(n=1,mean=0,sd=.x)),vx1=cor-err,vx2=cor-en)\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=0) %&gt;% group_by(input) %&gt;% mutate(cor=vx,err=(200--10)*exp(-.01*seq(1,n()))+(-10),en=map(err,~rnorm(n=1,mean=0,sd=.x)),vx=cor-err)\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=10) %&gt;% mutate(cor=vx,\n            err=ifelse(seq(1, n()) &lt;= n()/2, (700-0)*exp(-.01*seq(1,n()))+0, (700-0)*exp(-.06*(seq(1,n())-n()/2))+0),\n            en=map(err,~rnorm(n=1,mean=0,sd=.x)),\n            vx=cor-err)\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=10) %&gt;%\n  mutate(\n    cor = vx,\n    err = (700 - 0) * exp(-0.03 * seq(1, n()) * (input / max(input))) + 0,\n    en = map(err, ~rnorm(n = 1, mean = 0, sd = .x)),\n    vx = cor - err\n  )\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=10) %&gt;% mutate(\n  cor = vx,\n  err = (700 - 1) * exp(-.03 * seq(1, n())) + 1,\n  en = map(err, ~rnorm(n = 1, mean = 0, sd = .x)),\n  weight = (seq(1, n()) - 1) / (n() - 1),\n  vx = cor * weight - err + abs(min(cor * weight - err)) + 1\n)"
  },
  {
    "objectID": "Simulations/DeLosh97_Sim.html",
    "href": "Simulations/DeLosh97_Sim.html",
    "title": "Simulating DeLosh 1997",
    "section": "",
    "text": "Code#lapply(c('tidyverse','data.table','igraph','ggraph','kableExtra'),library,character.only=TRUE))\npacman::p_load(tidyverse,data.table,igraph,ggraph,kableExtra, patchwork) \npurrr::walk(here::here(c(\"Functions/Display_Functions.R\")),source)\nsource(here::here(\"Functions\",\"deLosh_data.R\"))\nCode#https://nrennie.rbind.io/blog/2022-06-06-creating-flowcharts-with-ggplot2/\ninNodes &lt;- seq(1,6,1) %&gt;% as.integer()\noutNodes &lt;- seq(300,1000,50)%&gt;% as.integer()\n\nstim &lt;- \"Stim\"\nresp &lt;- \"Response\"\ninFlow &lt;- tibble(expand.grid(from=stim,to=inNodes)) %&gt;% mutate_all(as.character)\noutFlow &lt;- tibble(expand.grid(from=outNodes,to=resp)) %&gt;% mutate_all(as.character)\n\ngd &lt;- tibble(expand.grid(from=inNodes,to=outNodes)) %&gt;% mutate_all(as.character) %&gt;%\n  rbind(inFlow,.) %&gt;% rbind(.,outFlow)\n\ng = graph_from_data_frame(gd,directed=TRUE)\ncoords2=layout_as_tree(g)\ncolnames(coords2)=c(\"y\",\"x\")\nodf &lt;- as_tibble(coords2) %&gt;% \n  mutate(label=vertex_attr(g,\"name\"),\n         type=c(\"stim\",rep(\"Input\",length(inNodes)),rep(\"Output\",length(outNodes)),\"Resp\"),\n         x=x*-1) %&gt;%\n  mutate(y=ifelse(type==\"Resp\",0,y),xmin=x-.05,xmax=x+.05,ymin=y-.35,ymax=y+.35)\n\nplot_edges = gd %&gt;% mutate(id=row_number()) %&gt;%\n  pivot_longer(cols=c(\"from\",\"to\"),names_to=\"s_e\",values_to=(\"label\")) %&gt;%\n                 mutate(label=as.character(label)) %&gt;% \n  group_by(id) %&gt;%\n  mutate(weight=sqrt(rnorm(1,mean=0,sd=10)^2)/10) %&gt;%\n  left_join(odf,by=\"label\") %&gt;%\n  mutate(xmin=xmin+.02,xmax=xmax-.02)\n\nggplot() + geom_rect(data = odf,\n            mapping = aes(xmin = xmin, ymin = ymin, \n                          xmax = xmax, ymax = ymax, \n                          fill = type, colour = type),alpha = 0.5) +\n  geom_text(data=odf,aes(x=x,y=y,label=label,size=3)) +\n  geom_path(data=plot_edges,mapping=aes(x=x,y=y,group=id,alpha=weight)) +\n  theme_void() + theme(legend.position = \"none\")\nCodelinear_function &lt;- function(x) 2.2 * x + 30\nexponential_function &lt;- function(x) 200 * (1 - exp(-x/25))\nquadratic_function &lt;- function(x) 210 - (x - 50)^2 / 12\n\nextrapLines &lt;- list(geom_vline(xintercept=30,color=\"black\",alpha=.4,linetype=\"dashed\"),\n                    geom_vline(xintercept=70,color=\"black\",alpha=.4,linetype=\"dashed\"))\n \nlinear_plot &lt;- ggplot(deLosh_data$human_data_linear, aes(x, y)) +\n    geom_point(shape=1) + stat_function(fun = linear_function, color = \"black\") +\n  labs(y=\"Response Magnitude\", title=\"Linear Function\",x=\"\") + extrapLines\n\nexponential_plot &lt;- ggplot(deLosh_data$human_data_exp, aes(x, y)) +\n  geom_point(aes(shape = \"Observed\", color = \"Observed\"),shape=1) + \n  stat_function(aes(color = \"True Function\"),fun = exponential_function, geom=\"line\")+\n  labs(x=\"Stimulus Magnitude\", title=\"Exponential Function\",y=\"\")  +\n  extrapLines +\n  scale_shape_manual(values = c(1)) +\n  scale_color_manual(values = c(\"Observed\" = \"black\", \"True Function\" = \"black\")) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +\n  guides(color = guide_legend(override.aes = list(shape = c(1, NA), \n                                                  linetype = c(0, 1))))\n\nquadratic_plot &lt;- ggplot(deLosh_data$human_data_quad, aes(x = x, y = y)) +\n  geom_point( shape = 1) +\n  stat_function( fun = quadratic_function, geom = \"line\") +\n  labs(title=\"Quadratic Function\",x=\"\",y=\"\") + extrapLines\n\nlinear_plot + exponential_plot + quadratic_plot",
    "crumbs": [
      "Simulations",
      "Simulating DeLosh 1997"
    ]
  },
  {
    "objectID": "Simulations/DeLosh97_Sim.html#alm-definition",
    "href": "Simulations/DeLosh97_Sim.html#alm-definition",
    "title": "Simulating DeLosh 1997",
    "section": "ALM Definition",
    "text": "ALM Definition\n\n\nInput Activation\n\\[\na_i(X)=\\exp \\left|-\\gamma \\cdot\\left[X-X_i\\right]^2\\right|\n\\]\nOutput activation\n\\[\no_j(X)=\\Sigma_{i=1, M} w_{j i} \\cdot a_i(X)\n\\]\nOutput Probability\n\\[\nP\\left[Y_j \\mid X\\right]=o_j(X) / \\Sigma_{k=1, L} o_k(X)\n\\]\nMean Response\n\\[\nm(X)=\\Sigma_{j=1, L} Y_j \\cdot P\\left[Y_j \\mid X\\right]\n\\]\n\n\nFeedback Signal\n\\[\nf_j(Z)=e^{-c\\cdot(Z-Y_j)^2}\n\\]\nWeight Updates\n\\[\nw_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t))\n\\]\n\n\nInput node actvation\n\\[\nP[X_i|X] = \\frac{a_i(X)}{\\\\sum_{k=1}^Ma_k(X)}\n\\]\nSlope Computation\n\\[\nE[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg]\\cdot[X-X_i]\n\\]\n\n\n\nGenerate Response\n\nCode##| code-fold: show\n#| code-summary: \"Toggle Code\"\nalm.response &lt;- function(input = 1, c, input.layer, output.layer,weight.mat) {\n  input.activation &lt;- exp(-c * (input.layer - input)^2) / sum(exp(-c * (input.layer - input)^2))\n  output.activation &lt;- weight.mat %*% input.activation\n  output.probability &lt;- output.activation / sum(output.activation)\n  mean.response &lt;- sum(output.layer * output.probability)\n  list(mean.response = mean.response, input.activation = input.activation, output.activation = output.activation)\n}\n\n\nUpdate Weights Based on Feedback\n\nToggle Codealm.update &lt;- function(corResp, c, lr, output.layer, input.activation, output.activation, weight.mat) {\n  fz &lt;- exp(-c * (output.layer - corResp)^2)\n  teacherSignal &lt;- (fz - output.activation) * lr\n  wChange &lt;- teacherSignal %*% t(input.activation)\n  weight.mat &lt;- weight.mat + wChange\n  weight.mat[weight.mat &lt; 0] = 0\n  return(weight.mat)\n}\n\nalm.trial &lt;- function(input, corResp, c, lr, input.layer, output.layer, weight.mat) {\n  alm_resp &lt;- alm.response(input, c, input.layer,output.layer, weight.mat)\n  updated_weight.mat &lt;- alm.update(corResp, c, lr, output.layer, alm_resp$input.activation, alm_resp$output.activation, weight.mat)\n  return(list(mean.response = alm_resp$mean.response, weight.mat = updated_weight.mat))\n}\n\n\nExam Generalization\n\nToggle Codeexam.response &lt;- function(input, c, trainVec, input.layer = INPUT_LAYER_DEFAULT,output.layer = OUTPUT_LAYER_DEFAULT, weight.mat) {\n  nearestTrain &lt;- trainVec[which.min(abs(input - trainVec))]\n  aresp &lt;- alm.response(nearestTrain, c, input.layer = input.layer,output.layer = OUTPUT_LAYER_DEFAULT,weight.mat)$mean.response\n  \n  xUnder &lt;- ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver &lt;- ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  \n  mUnder &lt;- alm.response(xUnder, c, input.layer = input.layer, output.layer, weight.mat)$mean.response\n  mOver &lt;- alm.response(xOver, c, input.layer = input.layer,output.layer, weight.mat)$mean.response\n  \n  exam.output &lt;- round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (input - nearestTrain), 3)\n  exam.output\n}\n\n\nSimulation Functions\n\nCode# simulation function\nalm.sim &lt;- function(dat, c, lr, input.layer = INPUT_LAYER_DEFAULT, output.layer = OUTPUT_LAYER_DEFAULT) {\n  weight.mat &lt;- matrix(0.00, nrow = length(output.layer), ncol = length(input.layer))\n  xt &lt;- dat$x\n  n &lt;- nrow(dat)\n  st &lt;- numeric(n) # Initialize the vector to store mean responses\n  for(i in 1:n) {\n    trial &lt;- alm.trial(dat$x[i], dat$y[i], c, lr, input.layer, output.layer, weight.mat)\n    weight.mat &lt;- trial$weight.mat\n    st[i] &lt;- trial$mean.response\n  }\n  dat &lt;- dat %&gt;% mutate(almResp = st)\n  return(list(d = dat, wm = weight.mat, c = c, lr = lr))\n}\n\n\nsimOrganize &lt;- function(simOut) {\n  dat &lt;- simOut$d\n  weight.mat &lt;- simOut$wm\n  c &lt;- simOut$c\n  lr &lt;- simOut$lr\n  trainX &lt;- unique(dat$x)\n  \n  almResp &lt;- generate.data(seq(0,100,.5), type = first(dat$type)) %&gt;% rowwise() %&gt;% \n    mutate(model = \"ALM\", resp = alm.response(x, c, input.layer = INPUT_LAYER_DEFAULT,output.layer = OUTPUT_LAYER_DEFAULT, weight.mat = weight.mat)$mean.response)\n  \n  examResp &lt;- generate.data(seq(0,100,.5), type = first(dat$type)) %&gt;% rowwise() %&gt;% \n    mutate(model = \"EXAM\", resp = exam.response(x, c, trainVec = trainX, input.layer = INPUT_LAYER_DEFAULT,output.layer = OUTPUT_LAYER_DEFAULT, weight.mat))\n  \n  organized_data &lt;- bind_rows(almResp, examResp) %&gt;% \n    mutate(type = first(dat$type),\n           error = abs(resp - y),\n           c = c,\n           lr = lr,\n           type = factor(type, levels = c(\"linear\", \"exponential\", \"quadratic\")),\n           test_region = ifelse(x %in% trainX, \"train\", \n                                ifelse(x &gt; min(trainX) & x &lt; max(trainX), \"interpolate\", \"extrapolate\")))\n  organized_data\n}\n\n\ngenerateSimData &lt;- function(density, envTypes, noise) {\n  reps &lt;- 200 / length(trainingBlocks[[density]])\n  map_dfr(envTypes, ~ \n            generate.data(rep(trainingBlocks[[density]], reps), type = .x, noise)) |&gt;\n    group_by(type) |&gt;\n    mutate(block = rep(1:reps, each = length(trainingBlocks[[density]])),\n           trial=seq(1,200))\n}\n\nsimulateAll &lt;- function(density,envTypes, noise, c = .2, lr = .2) {\n  trainMat &lt;- generateSimData(density, envTypes, noise)\n  trainData &lt;- map(envTypes, ~ alm.sim(trainMat %&gt;% filter(type == .x), c = c, lr = lr))\n  assign(paste(density),list(train=trainData, test=map_dfr(trainData, simOrganize) %&gt;% mutate(density = density)))\n}\n\n\nSimulate Training and Testing\n\nCodeenvTypes &lt;- c(\"linear\", \"exponential\", \"quadratic\")\ndensities &lt;- c(\"low\", \"med\", \"high\")\nnoise=0\nINPUT_LAYER_DEFAULT &lt;- seq(0, 100, 0.5)\nOUTPUT_LAYER_DEFAULT &lt;- seq(0, 250, 1)\n\nc = 1.4\nlr=.8\n\nresults &lt;- map(densities, ~ simulateAll(.x, envTypes, noise, c, lr)) |&gt;\n  set_names(densities) \n\ntrainAll &lt;- results %&gt;%\n  map_df(~ map_df(.x$train, pluck, \"d\"), .id = \"density\") |&gt;\n  mutate(stage=as.numeric(cut(trial,breaks=20,labels=seq(1,20))),\n         dev=sqrt((y-almResp)^2),\n         density=factor(density,levels=c(\"low\",\"med\",\"high\")),\n         type=factor(type,levels=c(\"linear\",\"exponential\",\"quadratic\"))) |&gt;\n  dplyr::relocate(density,type,stage)\n\nsimTestAll &lt;- results |&gt; map(\"test\") |&gt; bind_rows() |&gt;\n  group_by(type,density,model) %&gt;%\n  mutate(type=factor(type,levels=c(\"linear\",\"exponential\",\"quadratic\")),\n         density=factor(density,levels=c(\"low\",\"med\",\"high\"))) %&gt;%\n  dplyr::relocate(density,type,test_region)\n\n\nTraining Data\n\nCodetrainAll %&gt;% ggplot(aes(x=block,y=dev,color=type)) + stat_summary(geom=\"line\",fun=mean,alpha=.4)+\n  stat_summary(geom=\"point\",fun=mean,alpha=.4)+\n  stat_summary(geom=\"errorbar\",fun.data=mean_cl_normal,alpha=.4)+facet_wrap(~density, scales=\"free_x\")\n\n\n\n\n\n\n\nPredictions for Generalization\n\nCodesimTestAll %&gt;% ggplot(aes(x=x,y=y)) + \n  geom_point(aes(x=x,y=resp,shape=model,color=model),alpha=.7,size=1) + \n  geom_line(aes(x=x,y=y),alpha=.4)+ \n  geom_point(data=simTestAll %&gt;% filter(test_region==\"train\"),aes(x=x,y=y),color=\"black\",size=1,alpha=1) +\n  facet_grid(density~type) + \n  theme_bw() + theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\nCollpasing Across Density Levels gives us:\n\nCodesimTestAll %&gt;% group_by(type,model,x,y) %&gt;% summarise(resp=mean(resp))  %&gt;% ggplot(aes(x=x,y=y)) + \n  geom_point(aes(x=x,y=resp,shape=model,color=model),alpha=.7,size=1) + \n  geom_line(aes(x=x,y=y),alpha=.4)+ \n  facet_grid(~type) + \n  theme_bw() + theme(legend.position=\"bottom\")\n\n\nTable\n\n\n\n\n\n\nModel & Definition\nR Code\n\n\n\n\\(a_i(X)=\\exp \\left|-\\gamma \\cdot\\left[X-X_i\\right]^2\\right|\\)\nexp(-c * (input.layer - input)^2)\n\n\n\\(o_j(X)=\\Sigma_{i=1, M} w_{j i} \\cdot a_i(X)\\)\nweight.mat %*% input.activation\n\n\n\\(P\\left[Y_j \\mid X\\right]=o_j(X) / \\Sigma_{k=1, L} o_k(X)\\)\noutput.activation / sum(output.activation)\n\n\n\\(m(X)=\\Sigma_{j=1, L} Y_j \\cdot P\\left[Y_j \\mid X\\right]\\)\nsum(output.layer * output.probability)\n\n\n\\(f_j(Z)=e^{-c\\cdot(Z-Y_j)^2}\\)\nexp(-c * (output.layer - corResp)^2)\n\n\n\\(w_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t)\\)\nlr *(fz - output.activation) %*% t(input.activation)\n\n\n\n\n\n\n\\(E[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg]\\cdot[X-X_i]\\)\ntrainVec[which.min(abs(input - trainVec))]; xUnder &lt;- ...; xOver &lt;- ...; mUnder &lt;- ...; mOver &lt;- ...; exam.output &lt;- round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (input - nearestTrain), 3)\n\n\nMaster Function for full simulation\nSimulations with noise\nPrimary Functions",
    "crumbs": [
      "Simulations",
      "Simulating DeLosh 1997"
    ]
  },
  {
    "objectID": "Sections/combo1.html#function-learning-and-extrapolation",
    "href": "Sections/combo1.html#function-learning-and-extrapolation",
    "title": "HTW Modeling",
    "section": "\n1.1 Function Learning and Extrapolation",
    "text": "1.1 Function Learning and Extrapolation\nThe study of human function learning investigates how people learn relationships between continuous input and output values. Function learning is studied both in tasks where individuals are exposed to a sequence of input/output pairs (DeLosh et al., 1997; McDaniel et al., 2013), or situations where observers are presented with an incomplete scatterplot or line graph and make predictions about regions of the plot that don’t contain data (Ciccione & Dehaene, 2021; Courrieu, 2012; Said & Fischer, 2021; Schulz et al., 2020).\nCarroll (1963) conducted the earliest work on function learning. Input stimuli and output responses were both lines of varying length. The correct output response was related to the length of the input line by a linear, quadratic, or random function. Participants in the linear and quadratic performed above chance levels during extrapolation testing, with those in the linear condition performing the best overall. Carroll argued that these results were best explained by a ruled based model wherein learners form an abstract representation of the underlying function. Subsequent work by Brehmer (1974),testing a wider array of functional forms, provided further evidence for superior extrapolation in tasks with linear functions. Brehmer argued that individuals start out with an assumption of a linear function, but given sufficient error will progressively test alternative hypothesis with polynomials of greater degree. Koh & Meyer (1991) employed a visuomotor function learning task, wherein participants were trained on examples from an unknown function relating the length of an input line to the duration of a response (time between keystrokes). In this domain, participants performed best when the relation between line length and response duration was determined by a power, as opposed to linear function. Koh & Meyer developed the log-polynomial adaptive-regression model to account for their results.\nThe first significant challenge to the rule-based accounts of function learning was put forth by DeLosh et al. (1997) . In their task, participants learned to associate stimulus magnitudes with response magnitudes that were related via either linear, exponential, or quadratic function. Participants approached ceiling performance by the end of training in each function condition, and were able to correctly respond in interpolation testing trials. All three conditions demonstrated some capacity for extrapolation, however participants in the linear condition tended to underestimate the true function, while exponential and quadratic participants reliably overestimated the true function on extrapolation trials. Extrapolation and interpolation performance are depicted in Figure 1.\nThe authors evaluated both of the rule-based models introduced in earlier research (with some modifications enabling trial-by-trial learning). The polynomial hypothesis testing model (Brehmer, 1974; Carroll, 1963) tended to mimic the true function closely in extrapolation, and thus offered a poor account of the human data. The log-polynomial adaptive regression model (Koh & Meyer, 1991) was able to mimic some of the systematic deviations produced by human subjects, but also predicted overestimation in cases where underestimation occurred.\nThe authors also introduced two new function-learning models. The Associative Learning Model (ALM) and the extrapolation-association model (EXAM). ALM is a two layer connectionist model adapted from the ALCOVE model in the category learning literature (Kruschke, 1992). ALM belongs to the general class of radial-basis function neural networks, and can be considered a similarity-based model in the sense that the nodes in the input layer of the network are activated as a function of distance. The EXAM model retains the same similarity based activation and associative learning mechanisms as ALM, while being augmented with a linear rule response mechanism. When presented with novel stimuli, EXAM will retrieve the most similar input-output examples encountered during training, and from those examples compute a local slope. ALM was able to provide a good account of participant training and interpolation data in all three function conditions, however it was unable to extrapolate. EXAM, on the other hand, was able to reproduce both the extrapolation underestimation, as well as the quadratic and exponential overestimation patterns exhibited by the human participants. Subsequent research identified some limitations in EXAM’s ability to account for cases where human participants learn and extrapolate sinusoidal function Bott & Heit (2004) or to scenarios where different functions apply to different regions of the input space Kalish et al. (2004), though EXAM has been shown to provide a good account of human learning and extrapolation in tasks with bi-linear, V shaped input spaces Mcdaniel et al. (2009).\n\n1.1.1 Variability and Function Learning\nThe influence of variability on function learning tasks has received relatively little attention. The study by DeLosh et al. (1997) (described in detail above) did include a variability manipulation (referred to as density in their paper), wherein participants were trained with either either 8, 20, or 50 unique input-output pairs, with the total number of training trials held constant. They found a minimal influence of variability on training performance, and no difference between groups in interpolation or extrapolation, with all three variability conditions displaying accurate interpolation, and linearly biased extrapolation that was well accounted for by the EXAM model.\nIn the domain of visuomotor learning, van Dam & Ernst (2015) employed a task which required participants to learn a linear function between the spikiness of shape stimuli and the correct horizontal position to make a rapid pointing response. The shapes ranged from very spiky to completely circular at the extreme ends of the space. Participants trained with intermediate shapes from a lower variation (2 shapes) or higher variation (5 shapes) condition, with the 2 items of the lower varied condition matching the items used on the extreme ends of the higher variation training space. Learning was significantly slower in the higher variation group. However, the two conditions did not differ when tested with novel shapes, with both groups producing extrapolation responses of comparable magnitudes to the most similar training item, rather than in accordance with the true linear function. The authors accounted for both learning and extrapolation performance with a Bayesian learning model. Similar to ALM, the bayesian model assumes that generalization occurs as a Gaussian function of the distance between stimuli. However unlike ALM, the bayesian learning model utilizes more elaborate probabilistic stimulus representations, with a separate Kalman Filter for each shape stimulus.\n\n\n\n\n\n\n\nFigure 1: Generalization reproduced patterns from DeLosh et al. (1997) Figure 3. Stimulii that fall within the dashed lines are interpolations of the training examples."
  },
  {
    "objectID": "Sections/combo1.html#overview-of-present-study",
    "href": "Sections/combo1.html#overview-of-present-study",
    "title": "HTW Modeling",
    "section": "\n1.2 Overview Of Present Study",
    "text": "1.2 Overview Of Present Study\nThe present study investigates the influence of training variability on learning, generalization, and extrapolation in a uni-dimensional visuomotor function learning task. To the best of our knowledge, this research is the first to employ the classic constant vs. varied training manipulation, commonly used in the literature on the benefits of variability, in the context of a uni-dimensional function learning task. Across three experiments, we compare constant and varied training conditions in terms of learning performance, extrapolation accuracy, and the ability to reliably discriminate between stimuli.\nTo account for the empirical results, we will apply a series of computational models, including the Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM). Notably, this study is the first to employ approximate Bayesian computation (ABC) to fit these models to individual subject data, enabling us to thoroughly investigate the full range of posterior predictions of each model, and to examine the ability of these influential models of function learning to account for both the group level and individual level data."
  },
  {
    "objectID": "Sections/combo1.html#methods",
    "href": "Sections/combo1.html#methods",
    "title": "HTW Modeling",
    "section": "\n1.3 Methods",
    "text": "1.3 Methods"
  },
  {
    "objectID": "Sections/combo1.html#methods-1",
    "href": "Sections/combo1.html#methods-1",
    "title": "HTW Modeling",
    "section": "\n1.4 Methods",
    "text": "1.4 Methods\nParticipants A total of 156 participants were recruited from the Indiana University Introductory Psychology Course. Participants were randomly assigned to one of two training conditions: varied training or constant training.\nTask. The “Hit The Wall” (HTW) visuomotor extrapolation task task was programmed in Javascript, making heavy use of the phaser.io game library. The HTW task involved launching a projectile such that it would strike the “wall” at target speed indicated at the top of the screen (see Figure 2). The target velocities were given as a range, or band, of acceptable velocity values (e.g. band 800-1000). During the training stage, participants received feedback indicating whether they had hit the wall within the target velocity band, or how many units their throw was above or below from the target band. Participants were instructed that only the x velocity component of the ball was relevant to the task. The y velocity, or the location at which the ball struck the wall, had no influence on the task feedback.\n/\n\n\n\n\n\n\n\nFigure 2: The Hit the wall task. Participants launch the blue ball to hit the red wall at the target velocity band indicated at the top of the screen. The ball must be released from within the orange square - but the location of release, and the location at which the ball strikes the wall are both irrelevant to the task feedback.\n\n\n\n\n\n1.4.1 Procedure\nProcedure. All participants completed the task online. Participants were provided with a description of the experiment and indicated informed consent. Figure 3 illustrates the general procedure. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). Participants in the constant training condition trained on only one velocity band (800-1000) - the closest band to what would be the novel extrapolation bands in the testing stage.\nFollowing the training stage, participants proceeded immediately to the testing stage. Participants were tested from all six velocity bands, in two separate stages. In the novel extrapolation testing stage, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials. Participants were also tested from the three velocity bands that were trained by the varied condition (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training. The order in which participants completed the novel-extrapolation and testing-from-3-varied bands was counterbalanced across participants. A final training stage presented participants with “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 800-10001000-12001200-1400Test1\nTest  Novel Bands 100-300350-550600-800data1-&gt;Test1\ndata2\n Constant Training 800-1000data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  100-300350-550600-800Test2\n  Test   Varied Training Bands  800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 3: Experiment 1 Design. Constant and Varied participants complete different training conditions."
  },
  {
    "objectID": "Sections/combo1.html#htw-task",
    "href": "Sections/combo1.html#htw-task",
    "title": "HTW Modeling",
    "section": "\n1.5 HTW Task",
    "text": "1.5 HTW Task\n\nneed to create a demo version without consent form. And maybe separate windows for the different versions.\n\nExperimental Task for the HTW Project. Programmed in Javascript, and making use of phaser.js.",
    "crumbs": [
      "Sections",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Sections/combo1.html#experiment-1",
    "href": "Sections/combo1.html#experiment-1",
    "title": "HTW Modeling",
    "section": "\n1.5 Experiment 1",
    "text": "1.5 Experiment 1\n\n1.5.1 Analyses Strategy\nAll data processing and statistical analyses were performed in R version 4.32 Team (2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R Bürkner (2017), and descriptive stats and tables were extracted with the BayestestR package Makowski et al. (2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as avoiding convergence issues common to the frequentist analogues of our mixed models.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 discarded as warmup chains. Rhat values were within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for for the random effects. For each model, we report 1) the mean values of the posterior distribution for the parameters of interest, 2) the lower and upper credible intervals (CrI), and the probability of direction value (pd).\n\n\n\n\n\n\n\nGroup Comparison\nCode\nData\n\n\n\nEnd of Training Accuracy\nbrm(dist ~ condit)\nFinal Training Block\n\n\nTest Accuracy\nbrm(dist ~ condit * bandType + (1|id) + (1|bandInt)\nAll Testing trials\n\n\nBand Discrimination\nbrm(vx ~ condit * band +(1 + bandInt|id)\nAll Testing Trials\n\n\n\n\nIn each experiment we compare varied and constant conditions in terms of 1) accuracy in the final training block; 2) testing accuracy as a function of band type (trained vs. extrapolation bands); 3) extent of discrimination between all six testing bands. We quantified accuracy as the absolute deviation between the response velocity and the nearest boundary of the target band. Thus, when the target band was velocity 600-800, throws of 400, 650, and 900 would result in deviation values of 200, 0, and 100, respectively. The degree of discrimination between bands was index by fitting a linear model predicting the response velocity as a function of the target velocity. Participants who reliably discriminated between velocity bands tended to haves slope values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.\n\n1.5.2 Results\n\n\n\n\n\n\n\nFigure 4: Experiment 1 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\nTable 1: Experiment 1 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive estimates indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n106.34\n95.46\n117.25\n1\n\n\nconditVaried\n79.64\n57.92\n101.63\n1\n\n\n\n\n\n\n\nTraining. Figure 4 displays the average deviations across training blocks for the varied group, which trained on three velocity bands, and the constant group, which trained on one velocity band. To compare the training conditions at the end of training, we analyzed performance on the 800-1000 velocity band, which both groups trained on. The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, (\\(\\beta\\) = 79.64, 95% CrI [57.92, 101.63]; pd = 100%).\n\n\nTable 2: Experiment 1 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. Larger coefficients indicate larger deviations from the baselines (Condition=constant & bandType=Trained) - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n152.55\n70.63\n229.85\n1.0\n\n\nconditVaried\n39.00\n-21.10\n100.81\n0.9\n\n\nbandTypeExtrapolation\n71.51\n33.24\n109.60\n1.0\n\n\nconditVaried:bandTypeExtrapolation\n66.46\n32.76\n99.36\n1.0\n\n\n\n\n\n\nTesting. To compare accuracy between groups in the testing stage, we fit a Bayesian mixed effects model predicting deviation from the target band as a function of training condition (varied vs. constant) and band type (trained vs. extrapolation), with random intercepts for participants and bands. The model results are shown in Table 2. The main effect of training condition was not significant (\\(\\beta\\) = 39, 95% CrI [-21.1, 100.81]; pd = 89.93%). The extrapolation testing items had a significantly greater deviation than the training bands (\\(\\beta\\) = 71.51, 95% CrI [33.24, 109.6]; pd = 99.99%). Most importantly, the interaction between training condition and band type was significant (\\(\\beta\\) = 66.46, 95% CrI [32.76, 99.36]; pd = 99.99%), As shown in Figure 5, the varied group had disproportionately larger deviations compared to the constant group in the extrapolation bands.\n\n\n\n\n\n\n\nFigure 5: A) Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (training vs. extrapolation) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nTable 3: Experiment 1. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients on Band represent greater sensitivity/discrimination.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\nFinally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. See Table 4 for the full model results. The estimated coefficient for training condition (\\(\\beta\\) = 164.05, 95% CrI [45.5, 278.85], pd = 99.61%) suggests that the varied group tends to produce harder throws than the constant group, but is not in and of itself useful for assessing discrimination. Most relevant to the issue of discrimination is the coefficient on the Band predictor (\\(\\beta\\) = 0.71 95% CrI [0.62, 0.8], pd = 100%). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The estimate for the interaction between slope and condition (\\(\\beta\\) = -0.14, 95% CrI [-0.26, -0.01], pd = 98.39%), suggests that the discrimination was somewhat modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition. This difference is depicted visually in Figure 6.\n\n\n\n\n\n\n\nFigure 6: Empirical distribution of velocities producing in testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nTable 4\n\n\n\n\nExperiment 1. Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands."
  },
  {
    "objectID": "Sections/combo1.html#e1-discussion",
    "href": "Sections/combo1.html#e1-discussion",
    "title": "HTW Modeling",
    "section": "\n1.7 E1 Discussion",
    "text": "1.7 E1 Discussion\nIn Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions rresulted in lower final training performance is consistent with much of the prior researchon the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "Sections",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Sections/combo1.html#experiment-2",
    "href": "Sections/combo1.html#experiment-2",
    "title": "HTW Modeling",
    "section": "\n1.7 Experiment 2",
    "text": "1.7 Experiment 2\n\n1.7.1 Methods & Procedure\nThe task and procedure of Experiment 2 was identical to Experiment 1, with the exception that the training and testing bands were reversed (see Figure 7). The Varied group trained on bands 100-300, 350-550, 600-800, and the constant group trained on band 600-800. Both groups were tested from all six bands. A total of 110 participants completed the experiment (Varied: 55, Constant: 55).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 100-300350-550600-800Test1\nTest  Novel Bands  800-10001000-12001200-1400data1-&gt;Test1\ndata2\n Constant Training 600-800data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  800-10001000-12001200-1400Test2\n  Test   Varied Training Bands  100-300350-550600-800Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 7: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.\n\n\n\n\n\n1.7.2 Results\n\n\n\n\n\n\n\nFigure 8: Experiment 2 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\nTable 5: Experiment 2 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n91.01\n80.67\n101.26\n1\n\n\nconditVaried\n36.15\n16.35\n55.67\n1\n\n\n\n\n\n\n\nTraining. Figure 8 presents the deviations across training blocks for both constant and varied training groups. We again compared training performance on the band common to both groups (600-800). The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, ( \\(\\beta\\) = 36.15, 95% CrI [16.35, 55.67]; pd = 99.95%).\n\n\nTable 6: Experiment 2 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. Larger coefficient estimates indicate larger deviations from the baselines (constant & trained bands) - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\n \nTesting Accuracy. The analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 6, reveal no significant main effect of training condition (\\(\\beta\\) = -20.58, 95% CrI [-72.94, 33.08]; pd = 77.81%). However, the interaction between training condition and band type was significant (\\(\\beta\\) = 82, 95% CrI [41.89, 121.31]; pd = 100%), with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands (see Figure 9).\n\n\n\n\n\n\n\nFigure 9: A) Deviations from target band during testing without feedback stage. B) Estimated marginal means for the interaction between training condition and band type. Error bars represent 95% confidence intervals.\n\n\n\n\n\n\nTable 7: Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\nTesting Discrimination. Finally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. The full model results are shown in Table 8. The overall slope on target velocity band predictor was significantly positive, (\\(\\beta\\) = 0.71, 95% CrI [0.58, 0.84]; pd= 100%), indicating that participants exhibited discrimination between bands. The interaction between slope and condition was not significant, (\\(\\beta\\) = -0.06, 95% CrI [-0.24, 0.13]; pd= 72.67%), suggesting that the two conditions did not differ in their ability to discriminate between bands (see Figure 10).\n\n\n\n\n\n\n\nFigure 10: E2 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nTable 8\n\n\n\n\nConditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\n\n\n1.7.3 Experiment 2 Summary\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied group did not show a significant difference in discrimination between bands."
  },
  {
    "objectID": "Sections/combo1.html#e2-discussion",
    "href": "Sections/combo1.html#e2-discussion",
    "title": "HTW Modeling",
    "section": "\n1.9 E2 Discussion",
    "text": "1.9 E2 Discussion\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied group did not show a significant difference in discrimination between bands.",
    "crumbs": [
      "Sections",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Sections/combo1.html#experiment-3",
    "href": "Sections/combo1.html#experiment-3",
    "title": "HTW Modeling",
    "section": "\n1.8 Experiment 3",
    "text": "1.8 Experiment 3\n\n1.8.1 Methods & Procedure\nThe major adjustment of Experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the prior experiments. After each training throw, participants are informed whether a throw was too soft, too hard, or correct (i.e. within the target velocity range). All other aspects of the task and design are identical to Experiments 1 and 2. We utilized the order of training and testing bands from both of the prior experiments, thus assigning participants to both an order condition (Original or Reverse) and a training condition (Constant or Varied). Participants were once again recruited from the online Indiana University Introductory Psychology Course pool. Following exclusions, 195 participants were included in the final analysis, n=51 in the Constant-Original condition, n=59 in the Constant-Reverse condition, n=39 in the Varied-Original condition, and n=46 in the Varied-Reverse condition.\n\n1.8.2 Results\n\n\nTable 9: Experiment 3 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n121.86\n109.24\n134.60\n1.00\n\n\nconditVaried\n64.93\n36.99\n90.80\n1.00\n\n\nbandOrderReverse\n1.11\n-16.02\n18.16\n0.55\n\n\nconditVaried:bandOrderReverse\n-77.02\n-114.16\n-39.61\n1.00\n\n\n\n\n\n\nTraining. Figure 11 displays the average deviations from the target band across training blocks, and Table 9 shows the results of the Bayesian regression model predicting the deviation from the common band at the end of training (600-800 for reversed order, and 800-1000 for original order conditions). The main effect of training condition is significant, with the varied condition showing larger deviations ( \\(\\beta\\) = 64.93, 95% CrI [36.99, 90.8]; pd = 100%). The main effect of band order is not significant \\(\\beta\\) = 1.11, 95% CrI [-16.02, 18.16]; pd = 55.4%, however the interaction between training condition and band order is significant, with the varied condition showing greater accuracy in the reverse order condition ( \\(\\beta\\) = -77.02, 95% CrI [-114.16, -39.61]; pd = 100%).\n\n\n\n\n\n\n\nFigure 11: E3. Deviations from target band during testing without feedback stage.\n\n\n\n\n\n\nTable 10: Experiment 3 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition, (constant training; trained bands & original order), and the remaining coefficients reflect the deviation from that baseline. Positive coefficients thus represent worse performance relative to the baseline, - and a positive interaction coefficient indicates disproportionate deviation for the varied condition or reverse order condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n288.65\n199.45\n374.07\n1.00\n\n\nconditVaried\n-40.19\n-104.68\n23.13\n0.89\n\n\nbandTypeExtrapolation\n-23.35\n-57.28\n10.35\n0.92\n\n\nbandOrderReverse\n-73.72\n-136.69\n-11.07\n0.99\n\n\nconditVaried:bandTypeExtrapolation\n52.66\n14.16\n90.23\n1.00\n\n\nconditVaried:bandOrderReverse\n-37.48\n-123.28\n49.37\n0.80\n\n\nbandTypeExtrapolation:bandOrderReverse\n80.69\n30.01\n130.93\n1.00\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n30.42\n-21.00\n81.65\n0.87\n\n\n\n\n\n\nTesting Accuracy. Table 10 presents the results of the Bayesian mixed efects model predicting absolute deviation from the target band during the testing stage. There was no significant main effect of training condition,\\(\\beta\\) = -40.19, 95% CrI [-104.68, 23.13]; pd = 89.31%, or band type,\\(\\beta\\) = -23.35, 95% CrI [-57.28, 10.35]; pd = 91.52%. However the effect of band order was significant, with the reverse order condition showing lower deviations, \\(\\beta\\) = -73.72, 95% CrI [-136.69, -11.07]; pd = 98.89%. The interaction between training condition and band type was also significant \\(\\beta\\) = 52.66, 95% CrI [14.16, 90.23]; pd = 99.59%, with the varied condition showing disproprionately large deviations on the extrapolation bands compared to the constant group. There was also a significant interaction between band type and band order, \\(\\beta\\) = 80.69, 95% CrI [30.01, 130.93]; pd = 99.89%, such that the reverse order condition showed larger deviations on the extrapolation bands. No other interactions were significant.\n\n\n\n\n\n\n\nFigure 12: Experiment 3 Testing Accuracy. A) Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (training vs. extrapolation) on testing accuracy. Error bars represent 95% confidence intervals.\n\n\n\n\n\n\nTable 11: Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band. The Intercept represents the baseline condition (constant training & original order), and the Band coefficient represents the linear slope for the baseline condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nconditVaried:Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:bandInt\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:bandInt\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\nTesting Discrimination. The full results of the discrimination model are presented in Table 10. For the purposes of assessing group differences in discrimination, only the coefficients including the band variable are of interest. The baseline effect of band represents the slope cofficient for the constant training - original order condition, this effect was significant \\(\\beta\\) = 0.49, 95% CrI [0.36, 0.62]; pd = 100%. Neither of the two way interactions reached significance, \\(\\beta\\) = -0.04, 95% CrI [-0.23, 0.15]; pd = 66.63%, \\(\\beta\\) = -0.1, 95% CrI [-0.27, 0.08]; pd = 86.35%. However, the three way interaction between training condition, band order, and target band was significant, \\(\\beta\\) = 0.42, 95% CrI [0.17, 0.7]; pd = 99.96% - indicating that the varied condition showed a greater slope coefficient on the reverse order bands, compared to the constant condition - this is clearly shown in Figure 13, where the steepness of the best fitting line for the varied-reversed condition is noticably steeper than the other conditions.\n\n\n\n\n\n\n\nFigure 13: e3 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\n1.8.3 Experiment 3 Summary\nIn Experiment 3, we investigated the effects of training condition (constant vs. varied) and band type (training vs. extrapolation) on participants’ accuracy and discrimination during the testing phase. Unlike the previous experiments, participants received ordinal feedback during the training phase. Additionally, Experiment 3 included both the original order condition from Experiment 1 and the reverse order condition from Experiment 2. The results revealed no significant main effects of training condition on testing accuracy, nor was there a significant difference between groups in band discrimination. However, we observed a significant three-way interaction for the discrimination analysis, indicating that the varied condition showed a steeper slope coefficient on the reverse order bands compared to the constant condition. This result suggests that varied training enhanced participants’ ability to discriminate between velocity bands, but only when the band order was reversed during testing."
  },
  {
    "objectID": "Sections/combo1.html#results-summary",
    "href": "Sections/combo1.html#results-summary",
    "title": "HTW Modeling",
    "section": "\n1.9 Results Summary",
    "text": "1.9 Results Summary",
    "crumbs": [
      "Sections",
      "Combo_all"
    ]
  },
  {
    "objectID": "Sections/combo1.html#overall-summary-of-experiments-1-3",
    "href": "Sections/combo1.html#overall-summary-of-experiments-1-3",
    "title": "HTW Modeling",
    "section": "\n3.1 Overall Summary of Experiments 1-3",
    "text": "3.1 Overall Summary of Experiments 1-3\nAcross three experiments, we investigated the impact of training variability on learning and extrapolation in a visuomotor function learning task. In Experiment 1, participants in the varied training condition, who experienced a wider range of velocity bands during training, showed lower accuracy at the end of training compared to those in the constant training condition.\nCrucially, during the testing phase, the varied group exhibited significantly larger deviations from the target velocity bands, particularly for the extrapolation bands that were not encountered during training. The varied group also showed less discrimination between velocity bands, as evidenced by shallower slopes when predicting response velocity from target velocity band.\nExperiment 2 extended these findings by reversing the order of the training and testing bands. Similar to Experiment 1, the varied group demonstrated poorer performance during both training and testing phases. However, unlike Experiment 1, the varied group did not show a significant difference in discrimination between bands compared to the constant group.\nIn Experiment 3, we provided only ordinal feedback during training, in contrast to the continuous feedback provided in the previous experiments. Participants were assigned to both an order condition (original or reverse) and a training condition (constant or varied). The varied condition showed larger deviations at the end of training, consistent with the previous experiments. Interestingly, there was a significant interaction between training condition and band order, with the varied condition showing greater accuracy in the reverse order condition. During testing, the varied group once again exhibited larger deviations, particularly for the extrapolation bands. The reverse order conditions showed smaller deviations compared to the original order conditions. Discrimination between velocity bands was poorer for the varied group in the original order condition, but not in the reverse order condition.\nAll three of our experiments yielded evidence that varied training conditions produced less learning by the end of training, a pattern consistent with much of the previous research on the influence of training variability (Catalano & Kleiner, 1984; Soderstrom & Bjork, 2015; Wrisberg et al., 1987). The sole exception to this pattern was the reverse order condition in Experiment 3, where the varied group was not significantly worse than the constant group. Neither the varied condition trained with the same reverse-order items in Experiment 2, nor the original-order varied condition trained with ordinal feedback in Experiment 3 were able to match the performance of their complementary constant groups by the end of training, suggesting that the relative success of the ordinal-reverse ordered varied group cannot be attributed to item or feedback effects alone.\nOur findings also diverge from the two previous studies to cleanly manipulate the variability of training items in a function learning task (DeLosh et al., 1997; van Dam & Ernst, 2015), although the varied training condition of van Dam & Ernst (2015) also exhibited less learning, neither of these previous studies observed any difference between training conditions in extrapolation to novel items. Like DeLosh et al. (1997) , our participants exhibited above chance extrapolation/discrimination of novel items, however they observed no difference between any of their three training conditions. A noteworthy difference difference between our studies is that DeLosh et al. (1997) trained participants with either 8, 20, or 50 unique items (all receiving the same total number of training trials). These larger sets of unique items, combined with the fact that participants achieved near ceiling level performance by the end of training - may have made it more difficult to observe any between-group differences of training variation in their study. van Dam & Ernst (2015) ’s variability manipulation was more similar to our own, as they trained participants with either 2 or 5 unique items. However, although the mapping between their input stimuli and motor responses was technically linear, the input dimension was more complex than our own, as it was defined by the degree of “spikiness” of the input shape. This entirely arbitrary mapping also would have preculded any sense of a “0” point, which may partially explain why neither of their training conditions were able to extrapolate linearly in the manner observed in the current study or in DeLosh et al. (1997).",
    "crumbs": [
      "Sections",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Sections/combo1.html#computational-model",
    "href": "Sections/combo1.html#computational-model",
    "title": "HTW Modeling",
    "section": "\n1.9 Computational Model",
    "text": "1.9 Computational Model\n\n\n\n\n\n\n\nFigure 15: The Associative Learning Model (ALM). The diagram illustrates the basic structure of the ALM model as used in the present work. Input nodes are activated as a function of their similarity to the lower-boundary of the target band. The generalization parameter, \\(c\\), determines the degree to which nearby input nodes are activated. The output nodes are activated as a function of the weighted sum of the input nodes - weights are updated via the delta rule."
  },
  {
    "objectID": "Sections/combo1.html#alm-exam-description",
    "href": "Sections/combo1.html#alm-exam-description",
    "title": "HTW Modeling",
    "section": "\n2.1 ALM & Exam Description",
    "text": "2.1 ALM & Exam Description\nALM is a localist neural network model (Page, 2000), with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on the value of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter. The EXAM model is an extension of ALM, with the same learning rule and representational scheme for input and output units. EXAM differs from ALM only in its response rule, as it includes a linear extrapolation mechanism for generating novel responses. Although this extrapolation rule departs from a strictly similarity-based generalization mechanism, EXAM is distinct from pure rule-based models in that it remains constrained by the weights learned during training. EXAM retrieves the two nearest training inputs, and the ALM responses associated with those inputs, and computes the slope between these two points. The slope is then used to extrapolate the response to the novel test stimulus. Because EXAM requires at least two input-output pairs to generate a response, additional assumptions were required in order for it to generate resposnes for the constant group. We assumed that participants come to the task with prior knowledge of the origin point (0,0), which can serve as a reference point necessary for the model to generate responses for the constant group. This assumption is motivated by previous function learning research (Brown & Lacroix (2017)), which through a series of manipulations of the y intercept of the underlying function, found that participants consistently demonstrated knowledge of, or a bias towards, the origin point (see Kwantes & Neal (2006) for additional evidence of such a bias in function learning tasks).\nSee Table 12 for a full specification of the equations that define ALM and EXAM, and Figure 15 for a visual representation of the ALM model.\n\n\n\nTable 12: ALM & EXAM Equations\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).\n\n\n\nEXAM Extrapolation\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\n\nSlope Computation\n\n\\(S =\\) \\(\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\n\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nALM response \\(m(X_i)\\) adjusted by slope."
  },
  {
    "objectID": "Sections/combo1.html#model-fitting-strategy",
    "href": "Sections/combo1.html#model-fitting-strategy",
    "title": "HTW Modeling",
    "section": "\n2.2 Model Fitting Strategy",
    "text": "2.2 Model Fitting Strategy\nTo fit ALM and EXAM to our participant data, we employ a similar method to Mcdaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data in with separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase.Models to were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate parameters, we used approximate bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g. many neural network and evidence accumulation models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individuals velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.\n\n2.2.1 Modelling Results\n\n2.2.1.1 Group level Patterns\n\n\n\nTable 13: Mean model errors predicting empirical data from the testing and training stage, aggregated over all participants and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on\n\n\n\n\n\n\nstage\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nTest\nFit to Test Data\nALM\n\n199.9\n\n103.4\n\n\nEXAM\n\n104.0\n\n85.7\n\n\nFit to Test & Training Data\nALM\n\n217.0\n\n170.3\n\n\nEXAM\n\n127.9\n\n144.9\n\n\nFit to Training Data\nALM\n\n467.7\n\n291.4\n\n\nEXAM\n\n273.3\n\n297.9\n\n\nTrain\nFit to Test Data\nALM\n\n297.8\n\n2,016.0\n\n\nEXAM\n\n53.9\n\n184.0\n\n\nFit to Test & Training Data\nALM\n\n57.4\n\n132.3\n\n\nEXAM\n\n42.9\n\n127.9\n\n\nFit to Training Data\nALM\n\n51.8\n\n103.5\n\n\nEXAM\n\n51.4\n\n107.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y axes are scaled differently between facets.\n\n\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 14, and model predictions are shown alongside the empirical data in Figure 16 and Figure 17 (i.e. these plots combine all the posterior samples from all of the subjects). There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Aggregated residuals are displayed in Table 13. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences in training performance between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 15) also show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that y axes are scaled different between plots). Unsurprisingly, the advantage of EXAM is strongest for extrapolation positions (the three smallest bands for both groups - as well as the two highest bands for the Constant group). Although EXAM tends to perform better for both Constant and Varied participants (see also Table 13), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods.\nPanel B of Figure 15 directly compares the aggregated observed data to the posterior predictive distributions for the testing stage. Of interest are a) the extent to which the median estimates of the ALM and EXAM posteriors deviate from the observed medians for each velocity band; b) the ability of ALM and EXAM to discriminate between velocity bands; c) the relative performance of models that are constrained by the training data (i.e. the ‘fit to train only’ and ‘fit to both’ models) compared to the ‘fit to test only’ models;\nConsidering first the models fit to only the testing data, which reflect the best possible performance of ALM and EXAM at capturing the group-aggregated testing patterns. For the varied group, both ALM and EXAM are able to capture the median values of the observed data within the 66% credible intervals, and the spread of model predictions generally matches that of the observed data. For the constant group, only EXAM is able to capture the median range of values across the velocity bands, with ALM generally underestimating human velocoties in the upper bands, and overestimating in the lower bands. In the case of band 100, the median ALM prediction appears to match that of our participants - however this is due to a large subset of participants have ALM predictions near 0 for band 100, a pattern we will explore further in our considertation of individual patterns below. Models fit to both training and testing data show a similar pattern to only the testing data display the same basic pattern as those fit to only the testing data, albeit with slightly larger residuals. However models fit to only the training data display markedly worse performance at accounting for the key testing patterns.\n\n** explain how the constant group ALM predictions for band 100 look deceptively good due to aggregation of a large subset of subjects having ALM predictions of 0 for vb100, and a large subset with ALM predictions close to their position 800 value. This is relected by much greater variance of the ALM esimates in the posterior predictive plot\n** comment on how much constrained by the training data has a worse impact on the EXAM predictions for varied than for constant - perhaps due to the varied training data being much noisier than the constant training data.\n** comment on EXAM doing a better job mimicing the within-condition variance of the observed data\n** comment on the % of Constant subjects being best accounted for by EXAM being higher.\n** does EXAM do better for the Constant group because the constant group performs better? Or does training with a single example encourage an exam sort of strategy?\n\n\n\n\n\n\n\n\nFigure 16\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: Empirical data and Model predictions for absolute deviation from target (accuracy). Observed data is shown on top, and predictions from each combination of model and fitting method are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 Accounting for individual patterns\nTo more accurately assess the relative abilities of ALM and EXAM to capture important empirical patterns - we will now examine the predictions of both models for the subset of individual participants shown in Figure 18. Panel A presents three varied and constant participants who demonstrated a reasonable degree of discrimination between the 6 velocity bands during testing.\n\n** comment on the different ways ALM can completely fail to mimic discrimination patterns (sbj. 35; sbj. 137),and on how it can sometimes partially succeed (sbj. 11; 14,74)\n** comment on how EXAM can somtimes mimic non-monotonic spacing between bands due to associative stregth from training (i.e. subject 47)\n** compare c values to slope parameters from the statistical models earlier in paper\n\n\n\n\n\n\n\n\n\n\n\nFigure 18: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data.",
    "crumbs": [
      "Sections",
      "combo1"
    ]
  },
  {
    "objectID": "Sections/combo1.html#references",
    "href": "Sections/combo1.html#references",
    "title": "HTW Modeling",
    "section": "\n3.1 References",
    "text": "3.1 References\n\n\nBengtsson, H. (2021). A Unifying Framework for Parallel and Distributed Processing in R using Futures. The R Journal, 13(2), 208. https://doi.org/10.32614/RJ-2021-048\n\n\nBerniker, M., Mirzaei, H., & Kording, K. P. (2014). The effects of training breadth on motor generalization. Journal of Neurophysiology, 112(11), 2791–2798. https://doi.org/10.1152/jn.00615.2013\n\n\nBott, L., & Heit, E. (2004). Nonmonotonic Extrapolation in Function Learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 30(1), 38–50. https://doi.org/10.1037/0278-7393.30.1.38\n\n\nBraithwaite, D. W., & Goldstone, R. L. (2015). Effects of Variation and Prior Knowledge on Abstract Concept Learning. Cognition and Instruction, 33(3), 226–256. https://doi.org/10.1080/07370008.2015.1067215\n\n\nBraun, D. A., Aertsen, A., Wolpert, D. M., & Mehring, C. (2009). Motor Task Variation Induces Structural Learning. Current Biology, 19(4), 352–357. https://doi.org/10.1016/j.cub.2009.01.036\n\n\nBrehmer, B. (1974). Hypotheses about relations between scaled variables in the learning of probabilistic inference tasks. Organizational Behavior and Human Performance, 11(1), 1–27. https://doi.org/10.1016/0030-5073(74)90002-6\n\n\nBrekelmans, G., Lavan, N., Saito, H., Clayards, M., & Wonnacott, E. (2022). Does high variability training improve the learning of non-native phoneme contrasts over low variability training? A replication. Journal of Memory and Language, 126, 104352. https://doi.org/10.1016/j.jml.2022.104352\n\n\nBrown, M. A., & Lacroix, G. (2017). Underestimation in linear function learning: Anchoring to zero or x-y similarity? Canadian Journal of Experimental Psychology/Revue Canadienne de Psychologie Expérimentale, 71(4), 274–282. https://doi.org/10.1037/cep0000129\n\n\nBürkner, P.-C. (2017). Brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80, 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBusemeyer, J. R., Byun, E., DeLosh, E. L., & McDaniel, M. A. (1997). Learning Functional Relations Based on Experience with Input-output Pairs by Humans and Artificial Neural Networks. In Knowledge Concepts and Categories (pp. 405–437). Psychology Press.\n\n\nCarroll, J. D. (1963). Functional Learning: The Learning of Continuous Functional Mappings Relating Stimulus and Response Continua. ETS Research Bulletin Series, 1963(2), i–144. https://doi.org/10.1002/j.2333-8504.1963.tb00958.x\n\n\nCatalano, J. F., & Kleiner, B. M. (1984). Distant Transfer in Coincident Timing as a Function of Variability of Practice. Perceptual and Motor Skills, 58(3), 851–856. https://doi.org/10.2466/pms.1984.58.3.851\n\n\nCiccione, L., & Dehaene, S. (2021). Can humans perform mental regression on a graph? Accuracy and bias in the perception of scatterplots. Cognitive Psychology, 128, 101406. https://doi.org/10.1016/j.cogpsych.2021.101406\n\n\nCohen, A. L., Nosofsky, R. M., & Zaki, S. R. (2001). Category variability, exemplar similarity, and perceptual classification. Memory & Cognition, 29(8), 1165–1175. https://doi.org/10.3758/BF03206386\n\n\nCourrieu, P. (2012). Quick approximation of bivariate functions. British Journal of Mathematical and Statistical Psychology, 65(1), 89–121. https://doi.org/10.1111/j.2044-8317.2011.02016.x\n\n\nCranmer, K., Brehmer, J., & Louppe, G. (2020). The frontier of simulation-based inference. Proceedings of the National Academy of Sciences, 117(48), 30055–30062. https://doi.org/10.1073/pnas.1912789117\n\n\nDeLosh, E. L., McDaniel, M. A., & Busemeyer, J. R. (1997). Extrapolation: The Sine Qua Non for Abstraction in Function Learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 23(4), 19. https://doi.org/10.1037/0278-7393.23.4.968\n\n\nFarrell, S., & Lewandowsky, S. (2018). Computational Modeling of Cognition and Behavior: (1st ed.). Cambridge University Press. https://doi.org/10.1017/CBO9781316272503\n\n\nGuo, J.-P., Yang, L.-Y., & Ding, Y. (2014). Effects of example variability and prior knowledge in how students learn to solve equations. European Journal of Psychology of Education, 29(1), 21–42. https://www.jstor.org/stable/43551124\n\n\nHu, M., & Nosofsky, R. M. (2024). High-variability training does not enhance generalization in the prototype-distortion paradigm. Memory & Cognition, 1–16. https://doi.org/10.3758/s13421-023-01516-1\n\n\nKalish, M. L. (2013). Learning and extrapolating a periodic function. Memory & Cognition, 41(6), 886–896. https://doi.org/10.3758/s13421-013-0306-9\n\n\nKalish, M. L., Lewandowsky, S., & Kruschke, J. K. (2004). Population of Linear Experts: Knowledge Partitioning and Function Learning. Psychological Review, 111(4), 1072–1099. https://doi.org/10.1037/0033-295X.111.4.1072\n\n\nKangasrääsiö, A., Jokinen, J. P. P., Oulasvirta, A., Howes, A., & Kaski, S. (2019). Parameter Inference for Computational Cognitive Models with Approximate Bayesian Computation. Cognitive Science, 43(6), e12738. https://doi.org/10.1111/cogs.12738\n\n\nKoh, K., & Meyer, D. E. (1991). Function learning: Induction of continuous stimulus-response relations. Journal of Experimental Psychology: Learning, Memory, and Cognition, 17(5), 811. https://doi.org/10.1037/0278-7393.17.5.811\n\n\nKruschke, J. K. (1992). ALCOVE: An exemplar-based connectionist model of Category Learning. Psychological Review, 99(1). https://doi.org/10.1037/0033-295X.99.1.22\n\n\nKwantes, P. J., & Neal, A. (2006). Why people underestimate y when extrapolating in linear functions. Journal of Experimental Psychology: Learning, Memory, and Cognition, 32(5), 1019–1030. https://doi.org/10.1037/0278-7393.32.5.1019\n\n\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541\n\n\nMcdaniel, M. A., Dimperio, E., Griego, J. A., & Busemeyer, J. R. (2009). Predicting transfer performance: A comparison of competing function learning models. Journal of Experimental Psychology. Learning, Memory, and Cognition, 35, 173–195. https://doi.org/10.1037/a0013982\n\n\nMcDaniel, M. A., Fadler, C. L., & Pashler, H. (2013). Effects of spaced versus massed training in function learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 39(5), 1417–1432. https://doi.org/10.1037/a0032184\n\n\nPage, M. (2000). Connectionist modelling in psychology: A localist manifesto. Behavioral and Brain Sciences, 23(4), 443–467. https://doi.org/10.1017/S0140525X00003356\n\n\nPerry, L. K., Samuelson, L. K., Malloy, L. M., & Schiffer, R. N. (2010). Learn Locally, Think Globally: Exemplar Variability Supports Higher-Order Generalization and Word Learning. Psychological Science, 21(12), 1894–1902. https://doi.org/10.1177/0956797610389189\n\n\nPosner, M. I., & Keele, S. W. (1968). On the genesis of abstract ideas. Journal of Experimental Psychology, 77(3), 353–363. https://doi.org/10.1037/h0025953\n\n\nRaviv, L., Lupyan, G., & Green, S. C. (2022). How variability shapes learning and generalization. Trends in Cognitive Sciences, S1364661322000651. https://doi.org/10.1016/j.tics.2022.03.007\n\n\nRoller, C. A., Cohen, H. S., Kimball, K. T., & Bloomberg, J. J. (2001). Variable practice with lenses improves visuo-motor plasticity. Cognitive Brain Research, 12(2), 341–352. https://doi.org/10.1016/S0926-6410(01)00077-5\n\n\nSaid, N., & Fischer, H. (2021). Extrapolation accuracy underestimates rule learning: Evidence from the function-learning paradigm. Acta Psychologica, 218, 103356. https://doi.org/10.1016/j.actpsy.2021.103356\n\n\nSchmidt, R. A. (1975). A schema theory of discrete motor skill learning. Psychological Review, 82(4), 225–260. https://doi.org/10.1037/h0076770\n\n\nSchulz, E., Quiroga, F., & Gershman, S. J. (2020). Communicating Compositional Patterns. Open Mind, 4, 25–39. https://doi.org/10.1162/opmi_a_00032\n\n\nSoderstrom, N. C., & Bjork, R. A. (2015). Learning versus performance: An integrative review. Perspectives on Psychological Science, 10(2), 176–199. https://doi.org/10.1177/1745691615569000\n\n\nTeam, R. C. (2020). R: A Language and Environment for Statistical Computing. R: A Language and Environment for Statistical Computing.\n\n\nTurner, B. M., Sederberg, P. B., & McClelland, J. L. (2016). Bayesian analysis of simulation-based models. Journal of Mathematical Psychology, 72, 191–199. https://doi.org/10.1016/j.jmp.2014.10.001\n\n\nTurner, B. M., & Van Zandt, T. (2012). A tutorial on approximate Bayesian computation. Journal of Mathematical Psychology, 56(2), 69–85. https://doi.org/10.1016/j.jmp.2012.02.005\n\n\nvan Dam, L. C. J., & Ernst, M. O. (2015). Mapping Shape to Visuomotor Mapping: Learning and Generalisation of Sensorimotor Behaviour Based on Contextual Information. PLOS Computational Biology, 11(3), e1004172. https://doi.org/10.1371/journal.pcbi.1004172\n\n\nVan Rossum, J. H. A. (1990). Schmidt’s schema theory: The empirical base of the variability of practice hypothesis. Human Movement Science, 9(3-5), 387–435. https://doi.org/10.1016/0167-9457(90)90010-B\n\n\nWrisberg, C. A., Winter, T. P., & Kuhlman, J. S. (1987). The Variability of Practice Hypothesis: Further Tests and Methodological Discussion. Research Quarterly for Exercise and Sport, 58(4), 369–374. https://doi.org/10.1080/02701367.1987.10608114"
  },
  {
    "objectID": "Sections/Results.html",
    "href": "Sections/Results.html",
    "title": "Experiment 3",
    "section": "",
    "text": "All data processing and statistical analyses were performed in R version 4.32 Team (2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R Bürkner (2017), and descriptive stats and tables were extracted with the BayestestR package Makowski et al. (2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as avoiding convergence issues common to the frequentist analogues of our mixed models.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 discarded as warmup chains. Rhat values were within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for for the random effects. For each model, we report 1) the mean values of the posterior distribution for the parameters of interest, 2) the lower and upper credible intervals (CrI), and the probability of direction value (pd).\n\n\n\n\n\n\n\nGroup Comparison\nCode\nData\n\n\n\nEnd of Training Accuracy\nbrm(dist ~ condit)\nFinal Training Block\n\n\nTest Accuracy\nbrm(dist ~ condit * bandType + (1|id) + (1|bandInt)\nAll Testing trials\n\n\nBand Discrimination\nbrm(vx ~ condit * band +(1 + bandInt|id)\nAll Testing Trials\n\n\n\n\nIn each experiment we compare varied and constant conditions in terms of 1) accuracy in the final training block; 2) testing accuracy as a function of band type (trained vs. extrapolation bands); 3) extent of discrimination between all six testing bands. We quantified accuracy as the absolute deviation between the response velocity and the nearest boundary of the target band. Thus, when the target band was velocity 600-800, throws of 400, 650, and 900 would result in deviation values of 200, 0, and 100, respectively. The degree of discrimination between bands was index by fitting a linear model predicting the response velocity as a function of the target velocity. Participants who reliably discriminated between velocity bands tended to haves slope values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.\n\n\n\n\n\n\n\n\nFigure 1: Experiment 1 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\nTable 1: Experiment 1 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive estimates indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n106.34\n95.46\n117.25\n1\n\n\nconditVaried\n79.64\n57.92\n101.63\n1\n\n\n\n\n\n\n\nTraining. Figure 1 displays the average deviations across training blocks for the varied group, which trained on three velocity bands, and the constant group, which trained on one velocity band. To compare the training conditions at the end of training, we analyzed performance on the 800-1000 velocity band, which both groups trained on. The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, (\\(\\beta\\) = 79.64, 95% CrI [57.92, 101.63]; pd = 100%).\n\n\nTable 2: Experiment 1 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. Larger coefficients indicate larger deviations from the baselines (Condition=constant & bandType=Trained) - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n152.55\n70.63\n229.85\n1.0\n\n\nconditVaried\n39.00\n-21.10\n100.81\n0.9\n\n\nbandTypeExtrapolation\n71.51\n33.24\n109.60\n1.0\n\n\nconditVaried:bandTypeExtrapolation\n66.46\n32.76\n99.36\n1.0\n\n\n\n\n\n\nTesting. To compare accuracy between groups in the testing stage, we fit a Bayesian mixed effects model predicting deviation from the target band as a function of training condition (varied vs. constant) and band type (trained vs. extrapolation), with random intercepts for participants and bands. The model results are shown in Table 2. The main effect of training condition was not significant (\\(\\beta\\) = 39, 95% CrI [-21.1, 100.81]; pd = 89.93%). The extrapolation testing items had a significantly greater deviation than the training bands (\\(\\beta\\) = 71.51, 95% CrI [33.24, 109.6]; pd = 99.99%). Most importantly, the interaction between training condition and band type was significant (\\(\\beta\\) = 66.46, 95% CrI [32.76, 99.36]; pd = 99.99%), As shown in Figure 2, the varied group had disproportionately larger deviations compared to the constant group in the extrapolation bands.\n\n\n\n\n\n\n\nFigure 2: A) Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (training vs. extrapolation) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nTable 3: Experiment 1. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients on Band represent greater sensitivity/discrimination.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\nFinally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. See Table 4 for the full model results. The estimated coefficient for training condition (\\(\\beta\\) = 164.05, 95% CrI [45.5, 278.85], pd = 99.61%) suggests that the varied group tends to produce harder throws than the constant group, but is not in and of itself useful for assessing discrimination. Most relevant to the issue of discrimination is the coefficient on the Band predictor (\\(\\beta\\) = 0.71 95% CrI [0.62, 0.8], pd = 100%). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The estimate for the interaction between slope and condition (\\(\\beta\\) = -0.14, 95% CrI [-0.26, -0.01], pd = 98.39%), suggests that the discrimination was somewhat modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition. This difference is depicted visually in Figure 3.\n\n\n\n\n\n\n\nFigure 3: Empirical distribution of velocities producing in testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nTable 4\n\n\n\n\nExperiment 1. Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\n\n\nIn Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions rresulted in lower final training performance is consistent with much of the prior researchon the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.\n\n\nThe task and procedure of Experiment 2 was identical to Experiment 1, with the exception that the training and testing bands were reversed (see Figure 4). The Varied group trained on bands 100-300, 350-550, 600-800, and the constant group trained on band 600-800. Both groups were tested from all six bands. A total of 110 participants completed the experiment (Varied: 55, Constant: 55).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 100-300350-550600-800Test1\nTest  Novel Bands  800-10001000-12001200-1400data1-&gt;Test1\ndata2\n Constant Training 600-800data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  800-10001000-12001200-1400Test2\n  Test   Varied Training Bands  100-300350-550600-800Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 4: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Experiment 2 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\nTable 5: Experiment 2 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n91.01\n80.67\n101.26\n1\n\n\nconditVaried\n36.15\n16.35\n55.67\n1\n\n\n\n\n\n\n\nTraining. Figure 5 presents the deviations across training blocks for both constant and varied training groups. We again compared training performance on the band common to both groups (600-800). The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, ( \\(\\beta\\) = 36.15, 95% CrI [16.35, 55.67]; pd = 99.95%).\n\n\nTable 6: Experiment 2 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. Larger coefficient estimates indicate larger deviations from the baselines (constant & trained bands) - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\n \nTesting Accuracy. The analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 6, reveal no significant main effect of training condition (\\(\\beta\\) = -20.58, 95% CrI [-72.94, 33.08]; pd = 77.81%). However, the interaction between training condition and band type was significant (\\(\\beta\\) = 82, 95% CrI [41.89, 121.31]; pd = 100%), with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands (see Figure 6).\n\n\n\n\n\n\n\nFigure 6: A) Deviations from target band during testing without feedback stage. B) Estimated marginal means for the interaction between training condition and band type. Error bars represent 95% confidence intervals.\n\n\n\n\n\n\nTable 7: Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\nTesting Discrimination. Finally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. The full model results are shown in Table 8. The overall slope on target velocity band predictor was significantly positive, (\\(\\beta\\) = 0.71, 95% CrI [0.58, 0.84]; pd= 100%), indicating that participants exhibited discrimination between bands. The interaction between slope and condition was not significant, (\\(\\beta\\) = -0.06, 95% CrI [-0.24, 0.13]; pd= 72.67%), suggesting that the two conditions did not differ in their ability to discriminate between bands (see Figure 7).\n\n\n\n\n\n\n\nFigure 7: E2 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nTable 8\n\n\n\n\nConditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\n\n\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied group did not show a significant difference in discrimination between bands.\n\n\nThe major adjustment of Experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the prior experiments. After each training throw, participants are informed whether a throw was too soft, too hard, or correct (i.e. within the target velocity range). All other aspects of the task and design are identical to Experiments 1 and 2. We utilized the order of training and testing bands from both of the prior experiments, thus assigning participants to both an order condition (Original or Reverse) and a training condition (Constant or Varied). Participants were once again recruited from the online Indiana University Introductory Psychology Course pool. Following exclusions, 195 participants were included in the final analysis, n=51 in the Constant-Original condition, n=59 in the Constant-Reverse condition, n=39 in the Varied-Original condition, and n=46 in the Varied-Reverse condition.\n\n\n\nTable 9: Experiment 3 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n121.86\n109.24\n134.60\n1.00\n\n\nconditVaried\n64.93\n36.99\n90.80\n1.00\n\n\nbandOrderReverse\n1.11\n-16.02\n18.16\n0.55\n\n\nconditVaried:bandOrderReverse\n-77.02\n-114.16\n-39.61\n1.00\n\n\n\n\n\n\nTraining. Figure 8 displays the average deviations from the target band across training blocks, and Table 9 shows the results of the Bayesian regression model predicting the deviation from the common band at the end of training (600-800 for reversed order, and 800-1000 for original order conditions). The main effect of training condition is significant, with the varied condition showing larger deviations ( \\(\\beta\\) = 64.93, 95% CrI [36.99, 90.8]; pd = 100%). The main effect of band order is not significant \\(\\beta\\) = 1.11, 95% CrI [-16.02, 18.16]; pd = 55.4%, however the interaction between training condition and band order is significant, with the varied condition showing greater accuracy in the reverse order condition ( \\(\\beta\\) = -77.02, 95% CrI [-114.16, -39.61]; pd = 100%).\n\n\n\n\n\n\n\nFigure 8: E3. Deviations from target band during testing without feedback stage.\n\n\n\n\n\n\nTable 10: Experiment 3 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition, (constant training; trained bands & original order), and the remaining coefficients reflect the deviation from that baseline. Positive coefficients thus represent worse performance relative to the baseline, - and a positive interaction coefficient indicates disproportionate deviation for the varied condition or reverse order condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n288.65\n199.45\n374.07\n1.00\n\n\nconditVaried\n-40.19\n-104.68\n23.13\n0.89\n\n\nbandTypeExtrapolation\n-23.35\n-57.28\n10.35\n0.92\n\n\nbandOrderReverse\n-73.72\n-136.69\n-11.07\n0.99\n\n\nconditVaried:bandTypeExtrapolation\n52.66\n14.16\n90.23\n1.00\n\n\nconditVaried:bandOrderReverse\n-37.48\n-123.28\n49.37\n0.80\n\n\nbandTypeExtrapolation:bandOrderReverse\n80.69\n30.01\n130.93\n1.00\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n30.42\n-21.00\n81.65\n0.87\n\n\n\n\n\n\nTesting Accuracy. Table 10 presents the results of the Bayesian mixed efects model predicting absolute deviation from the target band during the testing stage. There was no significant main effect of training condition,\\(\\beta\\) = -40.19, 95% CrI [-104.68, 23.13]; pd = 89.31%, or band type,\\(\\beta\\) = -23.35, 95% CrI [-57.28, 10.35]; pd = 91.52%. However the effect of band order was significant, with the reverse order condition showing lower deviations, \\(\\beta\\) = -73.72, 95% CrI [-136.69, -11.07]; pd = 98.89%. The interaction between training condition and band type was also significant \\(\\beta\\) = 52.66, 95% CrI [14.16, 90.23]; pd = 99.59%, with the varied condition showing disproprionately large deviations on the extrapolation bands compared to the constant group. There was also a significant interaction between band type and band order, \\(\\beta\\) = 80.69, 95% CrI [30.01, 130.93]; pd = 99.89%, such that the reverse order condition showed larger deviations on the extrapolation bands. No other interactions were significant.\n\n\n\n\n\n\n\n\nFigure 9: Experiment 3 Testing Accuracy. A) Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (training vs. extrapolation) on testing accuracy. Error bars represent 95% confidence intervals.\n\n\n\n\n\n\nTable 11: Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band. The Intercept represents the baseline condition (constant training & original order), and the Band coefficient represents the linear slope for the baseline condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nconditVaried:Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:bandInt\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:bandInt\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\nTesting Discrimination. The full results of the discrimination model are presented in Table 10. For the purposes of assessing group differences in discrimination, only the coefficients including the band variable are of interest. The baseline effect of band represents the slope cofficient for the constant training - original order condition, this effect was significant \\(\\beta\\) = 0.49, 95% CrI [0.36, 0.62]; pd = 100%. Neither of the two way interactions reached significance, \\(\\beta\\) = -0.04, 95% CrI [-0.23, 0.15]; pd = 66.63%, \\(\\beta\\) = -0.1, 95% CrI [-0.27, 0.08]; pd = 86.35%. However, the three way interaction between training condition, band order, and target band was significant, \\(\\beta\\) = 0.42, 95% CrI [0.17, 0.7]; pd = 99.96% - indicating that the varied condition showed a greater slope coefficient on the reverse order bands, compared to the constant condition - this is clearly shown in Figure 10, where the steepness of the best fitting line for the varied-reversed condition is noticably steeper than the other conditions.\n\n\n\n\n\n\n\nFigure 10: e3 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\nIn Experiment 3, we investigated the effects of training condition (constant vs. varied) and band type (training vs. extrapolation) on participants’ accuracy and discrimination during the testing phase. Unlike the previous experiments, participants received ordinal feedback during the training phase. Additionally, Experiment 3 included both the original order condition from Experiment 1 and the reverse order condition from Experiment 2. The results revealed no significant main effects of training condition on testing accuracy, nor was there a significant difference between groups in band discrimination. However, we observed a significant three-way interaction for the discrimination analysis, indicating that the varied condition showed a steeper slope coefficient on the reverse order bands compared to the constant condition. This result suggests that varied training enhanced participants’ ability to discriminate between velocity bands, but only when the band order was reversed during testing.\n\n\nAcross three experiments, we investigated the impact of training variability on learning and extrapolation in a visuomotor function learning task.\nIn Experiment 1, participants in the varied training condition, who experienced a wider range of velocity bands during training, showed lower accuracy at the end of training compared to those in the constant training condition. Crucially, during the testing phase, the varied group exhibited significantly larger deviations from the target velocity bands, particularly for the extrapolation bands that were not encountered during training. The varied group also showed less discrimination between velocity bands, as evidenced by shallower slopes when predicting response velocity from target velocity band.\nExperiment 2 extended these findings by reversing the order of the training and testing bands. Similar to Experiment 1, the varied group demonstrated poorer performance during both training and testing phases. However, unlike Experiment 1, the varied group did not show a significant difference in discrimination between bands compared to the constant group.\nIn Experiment 3, we provided only ordinal feedback during training, in contrast to the continuous feedback provided in the previous experiments. Participants were assigned to both an order condition (original or reverse) and a training condition (constant or varied). The varied condition showed larger deviations at the end of training, consistent with the previous experiments. Interestingly, there was a significant interaction between training condition and band order, with the varied condition showing greater accuracy in the reverse order condition. During testing, the varied group once again exhibited larger deviations, particularly for the extrapolation bands. The reverse order conditions showed smaller deviations compared to the original order conditions. Discrimination between velocity bands was poorer for the varied group in the original order condition, but not in the reverse order condition.\n\n\n\nBürkner, P.-C. (2017). Brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80, 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541\n\n\nRaviv, L., Lupyan, G., & Green, S. C. (2022). How variability shapes learning and generalization. Trends in Cognitive Sciences, S1364661322000651. https://doi.org/10.1016/j.tics.2022.03.007\n\n\nSoderstrom, N. C., & Bjork, R. A. (2015). Learning versus performance: An integrative review. Perspectives on Psychological Science, 10(2), 176–199. https://doi.org/10.1177/1745691615569000\n\n\nTeam, R. C. (2020). R: A Language and Environment for Statistical Computing. R: A Language and Environment for Statistical Computing.",
    "crumbs": [
      "Sections",
      "Results"
    ]
  },
  {
    "objectID": "Sections/Results.html#experiment-1",
    "href": "Sections/Results.html#experiment-1",
    "title": "Experiment 3",
    "section": "",
    "text": "All data processing and statistical analyses were performed in R version 4.32 Team (2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R Bürkner (2017), and descriptive stats and tables were extracted with the BayestestR package Makowski et al. (2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as avoiding convergence issues common to the frequentist analogues of our mixed models.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 discarded as warmup chains. Rhat values were within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for for the random effects. For each model, we report 1) the mean values of the posterior distribution for the parameters of interest, 2) the lower and upper credible intervals (CrI), and the probability of direction value (pd).\n\n\n\n\n\n\n\nGroup Comparison\nCode\nData\n\n\n\nEnd of Training Accuracy\nbrm(dist ~ condit)\nFinal Training Block\n\n\nTest Accuracy\nbrm(dist ~ condit * bandType + (1|id) + (1|bandInt)\nAll Testing trials\n\n\nBand Discrimination\nbrm(vx ~ condit * band +(1 + bandInt|id)\nAll Testing Trials\n\n\n\n\nIn each experiment we compare varied and constant conditions in terms of 1) accuracy in the final training block; 2) testing accuracy as a function of band type (trained vs. extrapolation bands); 3) extent of discrimination between all six testing bands. We quantified accuracy as the absolute deviation between the response velocity and the nearest boundary of the target band. Thus, when the target band was velocity 600-800, throws of 400, 650, and 900 would result in deviation values of 200, 0, and 100, respectively. The degree of discrimination between bands was index by fitting a linear model predicting the response velocity as a function of the target velocity. Participants who reliably discriminated between velocity bands tended to haves slope values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.\n\n\n\n\n\n\n\n\nFigure 1: Experiment 1 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\nTable 1: Experiment 1 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive estimates indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n106.34\n95.46\n117.25\n1\n\n\nconditVaried\n79.64\n57.92\n101.63\n1\n\n\n\n\n\n\n\nTraining. Figure 1 displays the average deviations across training blocks for the varied group, which trained on three velocity bands, and the constant group, which trained on one velocity band. To compare the training conditions at the end of training, we analyzed performance on the 800-1000 velocity band, which both groups trained on. The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, (\\(\\beta\\) = 79.64, 95% CrI [57.92, 101.63]; pd = 100%).\n\n\nTable 2: Experiment 1 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. Larger coefficients indicate larger deviations from the baselines (Condition=constant & bandType=Trained) - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n152.55\n70.63\n229.85\n1.0\n\n\nconditVaried\n39.00\n-21.10\n100.81\n0.9\n\n\nbandTypeExtrapolation\n71.51\n33.24\n109.60\n1.0\n\n\nconditVaried:bandTypeExtrapolation\n66.46\n32.76\n99.36\n1.0\n\n\n\n\n\n\nTesting. To compare accuracy between groups in the testing stage, we fit a Bayesian mixed effects model predicting deviation from the target band as a function of training condition (varied vs. constant) and band type (trained vs. extrapolation), with random intercepts for participants and bands. The model results are shown in Table 2. The main effect of training condition was not significant (\\(\\beta\\) = 39, 95% CrI [-21.1, 100.81]; pd = 89.93%). The extrapolation testing items had a significantly greater deviation than the training bands (\\(\\beta\\) = 71.51, 95% CrI [33.24, 109.6]; pd = 99.99%). Most importantly, the interaction between training condition and band type was significant (\\(\\beta\\) = 66.46, 95% CrI [32.76, 99.36]; pd = 99.99%), As shown in Figure 2, the varied group had disproportionately larger deviations compared to the constant group in the extrapolation bands.\n\n\n\n\n\n\n\nFigure 2: A) Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (training vs. extrapolation) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nTable 3: Experiment 1. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients on Band represent greater sensitivity/discrimination.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\nFinally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. See Table 4 for the full model results. The estimated coefficient for training condition (\\(\\beta\\) = 164.05, 95% CrI [45.5, 278.85], pd = 99.61%) suggests that the varied group tends to produce harder throws than the constant group, but is not in and of itself useful for assessing discrimination. Most relevant to the issue of discrimination is the coefficient on the Band predictor (\\(\\beta\\) = 0.71 95% CrI [0.62, 0.8], pd = 100%). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The estimate for the interaction between slope and condition (\\(\\beta\\) = -0.14, 95% CrI [-0.26, -0.01], pd = 98.39%), suggests that the discrimination was somewhat modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition. This difference is depicted visually in Figure 3.\n\n\n\n\n\n\n\nFigure 3: Empirical distribution of velocities producing in testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nTable 4\n\n\n\n\nExperiment 1. Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.",
    "crumbs": [
      "Sections",
      "Results"
    ]
  },
  {
    "objectID": "Sections/Results.html#e1-discussion",
    "href": "Sections/Results.html#e1-discussion",
    "title": "Experiment 3",
    "section": "\n2 E1 Discussion",
    "text": "2 E1 Discussion\nIn Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions rresulted in lower final training performance is consistent with much of the prior researchon the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "Sections",
      "Experiment 3"
    ]
  },
  {
    "objectID": "Sections/Results.html#experiment-2",
    "href": "Sections/Results.html#experiment-2",
    "title": "Experiment 3",
    "section": "",
    "text": "The task and procedure of Experiment 2 was identical to Experiment 1, with the exception that the training and testing bands were reversed (see Figure 4). The Varied group trained on bands 100-300, 350-550, 600-800, and the constant group trained on band 600-800. Both groups were tested from all six bands. A total of 110 participants completed the experiment (Varied: 55, Constant: 55).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 100-300350-550600-800Test1\nTest  Novel Bands  800-10001000-12001200-1400data1-&gt;Test1\ndata2\n Constant Training 600-800data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  800-10001000-12001200-1400Test2\n  Test   Varied Training Bands  100-300350-550600-800Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 4: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Experiment 2 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\nTable 5: Experiment 2 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n91.01\n80.67\n101.26\n1\n\n\nconditVaried\n36.15\n16.35\n55.67\n1\n\n\n\n\n\n\n\nTraining. Figure 5 presents the deviations across training blocks for both constant and varied training groups. We again compared training performance on the band common to both groups (600-800). The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, ( \\(\\beta\\) = 36.15, 95% CrI [16.35, 55.67]; pd = 99.95%).\n\n\nTable 6: Experiment 2 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. Larger coefficient estimates indicate larger deviations from the baselines (constant & trained bands) - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\n \nTesting Accuracy. The analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 6, reveal no significant main effect of training condition (\\(\\beta\\) = -20.58, 95% CrI [-72.94, 33.08]; pd = 77.81%). However, the interaction between training condition and band type was significant (\\(\\beta\\) = 82, 95% CrI [41.89, 121.31]; pd = 100%), with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands (see Figure 6).\n\n\n\n\n\n\n\nFigure 6: A) Deviations from target band during testing without feedback stage. B) Estimated marginal means for the interaction between training condition and band type. Error bars represent 95% confidence intervals.\n\n\n\n\n\n\nTable 7: Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\nTesting Discrimination. Finally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. The full model results are shown in Table 8. The overall slope on target velocity band predictor was significantly positive, (\\(\\beta\\) = 0.71, 95% CrI [0.58, 0.84]; pd= 100%), indicating that participants exhibited discrimination between bands. The interaction between slope and condition was not significant, (\\(\\beta\\) = -0.06, 95% CrI [-0.24, 0.13]; pd= 72.67%), suggesting that the two conditions did not differ in their ability to discriminate between bands (see Figure 7).\n\n\n\n\n\n\n\nFigure 7: E2 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nTable 8\n\n\n\n\nConditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\n\n\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied group did not show a significant difference in discrimination between bands.",
    "crumbs": [
      "Sections",
      "Results"
    ]
  },
  {
    "objectID": "Sections/Results.html#e2-discussion",
    "href": "Sections/Results.html#e2-discussion",
    "title": "Experiment 3",
    "section": "\n4 E2 Discussion",
    "text": "4 E2 Discussion\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied group did not show a significant difference in discrimination between bands.",
    "crumbs": [
      "Sections",
      "Experiment 3"
    ]
  },
  {
    "objectID": "Sections/Results.html#experiment-3",
    "href": "Sections/Results.html#experiment-3",
    "title": "Experiment 3",
    "section": "",
    "text": "The major adjustment of Experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the prior experiments. After each training throw, participants are informed whether a throw was too soft, too hard, or correct (i.e. within the target velocity range). All other aspects of the task and design are identical to Experiments 1 and 2. We utilized the order of training and testing bands from both of the prior experiments, thus assigning participants to both an order condition (Original or Reverse) and a training condition (Constant or Varied). Participants were once again recruited from the online Indiana University Introductory Psychology Course pool. Following exclusions, 195 participants were included in the final analysis, n=51 in the Constant-Original condition, n=59 in the Constant-Reverse condition, n=39 in the Varied-Original condition, and n=46 in the Varied-Reverse condition.\n\n\n\nTable 9: Experiment 3 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n121.86\n109.24\n134.60\n1.00\n\n\nconditVaried\n64.93\n36.99\n90.80\n1.00\n\n\nbandOrderReverse\n1.11\n-16.02\n18.16\n0.55\n\n\nconditVaried:bandOrderReverse\n-77.02\n-114.16\n-39.61\n1.00\n\n\n\n\n\n\nTraining. Figure 8 displays the average deviations from the target band across training blocks, and Table 9 shows the results of the Bayesian regression model predicting the deviation from the common band at the end of training (600-800 for reversed order, and 800-1000 for original order conditions). The main effect of training condition is significant, with the varied condition showing larger deviations ( \\(\\beta\\) = 64.93, 95% CrI [36.99, 90.8]; pd = 100%). The main effect of band order is not significant \\(\\beta\\) = 1.11, 95% CrI [-16.02, 18.16]; pd = 55.4%, however the interaction between training condition and band order is significant, with the varied condition showing greater accuracy in the reverse order condition ( \\(\\beta\\) = -77.02, 95% CrI [-114.16, -39.61]; pd = 100%).\n\n\n\n\n\n\n\nFigure 8: E3. Deviations from target band during testing without feedback stage.\n\n\n\n\n\n\nTable 10: Experiment 3 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition, (constant training; trained bands & original order), and the remaining coefficients reflect the deviation from that baseline. Positive coefficients thus represent worse performance relative to the baseline, - and a positive interaction coefficient indicates disproportionate deviation for the varied condition or reverse order condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n288.65\n199.45\n374.07\n1.00\n\n\nconditVaried\n-40.19\n-104.68\n23.13\n0.89\n\n\nbandTypeExtrapolation\n-23.35\n-57.28\n10.35\n0.92\n\n\nbandOrderReverse\n-73.72\n-136.69\n-11.07\n0.99\n\n\nconditVaried:bandTypeExtrapolation\n52.66\n14.16\n90.23\n1.00\n\n\nconditVaried:bandOrderReverse\n-37.48\n-123.28\n49.37\n0.80\n\n\nbandTypeExtrapolation:bandOrderReverse\n80.69\n30.01\n130.93\n1.00\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n30.42\n-21.00\n81.65\n0.87\n\n\n\n\n\n\nTesting Accuracy. Table 10 presents the results of the Bayesian mixed efects model predicting absolute deviation from the target band during the testing stage. There was no significant main effect of training condition,\\(\\beta\\) = -40.19, 95% CrI [-104.68, 23.13]; pd = 89.31%, or band type,\\(\\beta\\) = -23.35, 95% CrI [-57.28, 10.35]; pd = 91.52%. However the effect of band order was significant, with the reverse order condition showing lower deviations, \\(\\beta\\) = -73.72, 95% CrI [-136.69, -11.07]; pd = 98.89%. The interaction between training condition and band type was also significant \\(\\beta\\) = 52.66, 95% CrI [14.16, 90.23]; pd = 99.59%, with the varied condition showing disproprionately large deviations on the extrapolation bands compared to the constant group. There was also a significant interaction between band type and band order, \\(\\beta\\) = 80.69, 95% CrI [30.01, 130.93]; pd = 99.89%, such that the reverse order condition showed larger deviations on the extrapolation bands. No other interactions were significant.\n\n\n\n\n\n\n\n\nFigure 9: Experiment 3 Testing Accuracy. A) Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (training vs. extrapolation) on testing accuracy. Error bars represent 95% confidence intervals.\n\n\n\n\n\n\nTable 11: Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band. The Intercept represents the baseline condition (constant training & original order), and the Band coefficient represents the linear slope for the baseline condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nconditVaried:Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:bandInt\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:bandInt\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\nTesting Discrimination. The full results of the discrimination model are presented in Table 10. For the purposes of assessing group differences in discrimination, only the coefficients including the band variable are of interest. The baseline effect of band represents the slope cofficient for the constant training - original order condition, this effect was significant \\(\\beta\\) = 0.49, 95% CrI [0.36, 0.62]; pd = 100%. Neither of the two way interactions reached significance, \\(\\beta\\) = -0.04, 95% CrI [-0.23, 0.15]; pd = 66.63%, \\(\\beta\\) = -0.1, 95% CrI [-0.27, 0.08]; pd = 86.35%. However, the three way interaction between training condition, band order, and target band was significant, \\(\\beta\\) = 0.42, 95% CrI [0.17, 0.7]; pd = 99.96% - indicating that the varied condition showed a greater slope coefficient on the reverse order bands, compared to the constant condition - this is clearly shown in Figure 10, where the steepness of the best fitting line for the varied-reversed condition is noticably steeper than the other conditions.\n\n\n\n\n\n\n\nFigure 10: e3 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\nIn Experiment 3, we investigated the effects of training condition (constant vs. varied) and band type (training vs. extrapolation) on participants’ accuracy and discrimination during the testing phase. Unlike the previous experiments, participants received ordinal feedback during the training phase. Additionally, Experiment 3 included both the original order condition from Experiment 1 and the reverse order condition from Experiment 2. The results revealed no significant main effects of training condition on testing accuracy, nor was there a significant difference between groups in band discrimination. However, we observed a significant three-way interaction for the discrimination analysis, indicating that the varied condition showed a steeper slope coefficient on the reverse order bands compared to the constant condition. This result suggests that varied training enhanced participants’ ability to discriminate between velocity bands, but only when the band order was reversed during testing.",
    "crumbs": [
      "Sections",
      "Results"
    ]
  },
  {
    "objectID": "Sections/Results.html#results-summary",
    "href": "Sections/Results.html#results-summary",
    "title": "Experiment 3",
    "section": "\n5 Results Summary",
    "text": "5 Results Summary",
    "crumbs": [
      "Sections",
      "Results"
    ]
  },
  {
    "objectID": "Sections/Results.html#overall-summary-of-experiments-1-3",
    "href": "Sections/Results.html#overall-summary-of-experiments-1-3",
    "title": "Experiment 3",
    "section": "",
    "text": "Across three experiments, we investigated the impact of training variability on learning and extrapolation in a visuomotor function learning task.\nIn Experiment 1, participants in the varied training condition, who experienced a wider range of velocity bands during training, showed lower accuracy at the end of training compared to those in the constant training condition. Crucially, during the testing phase, the varied group exhibited significantly larger deviations from the target velocity bands, particularly for the extrapolation bands that were not encountered during training. The varied group also showed less discrimination between velocity bands, as evidenced by shallower slopes when predicting response velocity from target velocity band.\nExperiment 2 extended these findings by reversing the order of the training and testing bands. Similar to Experiment 1, the varied group demonstrated poorer performance during both training and testing phases. However, unlike Experiment 1, the varied group did not show a significant difference in discrimination between bands compared to the constant group.\nIn Experiment 3, we provided only ordinal feedback during training, in contrast to the continuous feedback provided in the previous experiments. Participants were assigned to both an order condition (original or reverse) and a training condition (constant or varied). The varied condition showed larger deviations at the end of training, consistent with the previous experiments. Interestingly, there was a significant interaction between training condition and band order, with the varied condition showing greater accuracy in the reverse order condition. During testing, the varied group once again exhibited larger deviations, particularly for the extrapolation bands. The reverse order conditions showed smaller deviations compared to the original order conditions. Discrimination between velocity bands was poorer for the varied group in the original order condition, but not in the reverse order condition.",
    "crumbs": [
      "Sections",
      "Results"
    ]
  },
  {
    "objectID": "Sections/Results.html#references",
    "href": "Sections/Results.html#references",
    "title": "Experiment 3",
    "section": "",
    "text": "Bürkner, P.-C. (2017). Brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80, 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541\n\n\nRaviv, L., Lupyan, G., & Green, S. C. (2022). How variability shapes learning and generalization. Trends in Cognitive Sciences, S1364661322000651. https://doi.org/10.1016/j.tics.2022.03.007\n\n\nSoderstrom, N. C., & Bjork, R. A. (2015). Learning versus performance: An integrative review. Perspectives on Psychological Science, 10(2), 176–199. https://doi.org/10.1177/1745691615569000\n\n\nTeam, R. C. (2020). R: A Language and Environment for Statistical Computing. R: A Language and Environment for Statistical Computing.",
    "crumbs": [
      "Sections",
      "Results"
    ]
  },
  {
    "objectID": "Sections/Intro.html",
    "href": "Sections/Intro.html",
    "title": "",
    "section": "",
    "text": "SectionsIntroduction CodeShow All CodeHide All CodeView Source\nLink to project page\nWorking Draft of Manuscript\nrepo",
    "crumbs": [
      "Sections",
      "Introduction"
    ]
  },
  {
    "objectID": "Sections/Intro.html#function-learning-and-extrapolation",
    "href": "Sections/Intro.html#function-learning-and-extrapolation",
    "title": "",
    "section": "Function Learning and Extrapolation",
    "text": "Function Learning and Extrapolation\nThe study of human function learning investigates how people learn relationships between continuous input and output values. Function learning is studied both in tasks where individuals are exposed to a sequence of input/output pairs (DeLosh et al., 1997; McDaniel et al., 2013), or situations where observers are presented with an incomplete scatterplot or line graph and make predictions about regions of the plot that do not contain data (Ciccione & Dehaene, 2021; Courrieu, 2012; Said & Fischer, 2021; Schulz et al., 2020). Studies of function learning often compare the difficulty of learning functions of different underlying forms (e.g. linear, bi-linear, power, sinusoidal), and the extent to which participants can accurately respond to novel inputs that fall in-between previously experienced inputs (interpolation testing), or that fall outside the range of previously experienced inputs (extrapolation).\nCarroll (1963) conducted the earliest work on function learning. Input stimuli and output responses were both lines of varying length. The correct output response was related to the length of the input line by a linear, quadratic, or random function. Participants in the linear and quadratic performed above chance levels during extrapolation testing, with those in the linear condition performing the best overall. Carroll argued that these results were best explained by a rule-based model wherein learners form an abstract representation of the underlying function. Subsequent work by Brehmer (1974), testing a wider array of functional forms, provided further evidence for superior extrapolation in tasks with linear functions. Brehmer argued that individuals start out assuming a linear function, but given sufficient error will progressively test alternative hypotheses with polynomials of greater degree. Koh & Meyer (1991) employed a visuomotor function learning task, wherein participants were trained on examples from an unknown function relating the length of an input line to the duration of a response (time between keystrokes). In this domain, participants performed best when the relation between line length and response duration was determined by a power law, as opposed to linear function. Koh and Meyer developed the log-polynomial adaptive-regression model to account for their results.\nThe first significant challenge to rule-based accounts of function learning was put forth by DeLosh et al. (1997) . In their task, participants learned to associate stimulus magnitudes with response magnitudes that were related via either linear, exponential, or quadratic function. Participants approached ceiling performance by the end of training in each function condition, and were able to accurately respond on interpolation testing trials. All three conditions demonstrated some capacity for extrapolation, however participants in the linear condition tended to underestimate the true function, while exponential and quadratic participants reliably overestimated the true function on extrapolation trials. Extrapolation and interpolation performances are depicted in Figure 1.\nThe authors evaluated the rule-based models introduced in earlier research (with some modifications enabling trial-by-trial learning). The polynomial hypothesis testing model (Brehmer, 1974; Carroll, 1963) tended to mimic the true function closely in extrapolation, and thus offered a poor account of the under and over-estimation biases shown in the human data. The log-polynomial adaptive regression model (Koh & Meyer, 1991) was able to mimic some of the systematic deviations produced by human subjects, but also predicted overestimation in cases where underestimation occurred.\nThe authors also introduced two new function-learning models. The Associative Learning Model (ALM) and the extrapolation-association model (EXAM). ALM is a two layer connectionist model adapted from the ALCOVE model in the category learning literature (Kruschke, 1992). ALM belongs to the general class of radial-basis function neural networks, and can be considered a similarity-based model in the sense that the nodes in the input layer of the network are activated as a function of distance (see ?@fig-alm-diagram). The EXAM model retains the same similarity-based activation and associative learning mechanisms as ALM, while being augmented with a linear rule response mechanism. When presented with novel stimuli, EXAM will retrieve the most similar input-output examples encountered during training, and from those examples compute a local slope. ALM was able to provide a good account of participants’ training and interpolation data in all three function conditions, however it was unable to extrapolate. EXAM, by contrast, was able to reproduce both the extrapolation underestimation, as well as the quadratic and exponential overestimation patterns exhibited by the human participants. Subsequent research identified some limitations in EXAM’s ability to account for cases where human participants learn and extrapolate a sinusoidal function (Bott & Heit, 2004) or to scenarios where different functions apply to different regions of the input space (Kalish et al., 2004), though EXAM has been shown to provide a good account of human learning and extrapolation in tasks with bi-linear, V-shaped input spaces (Mcdaniel et al., 2009).\n\nDisplay codepacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n  brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n  data.table, stringr, here,conflicted, gt, ggh4x, patchwork, knitr)\n#options(brms.backend=\"cmdstanr\",mc.cores=4)\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\",\"here\"), conflict_prefer_all, quiet = TRUE)\noptions(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\n\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\",\"here\"), conflict_prefer_all, quiet = TRUE)\nwalk(c(\"Display_Functions\",\"deLosh_data\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\", \"prep_model_data\",\"org_functions\"), ~source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n\n# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n# source(here::here(\"Functions\",\"deLosh_data.R\"))\n# source(here::here(\"Functions\",\"Display_Functions.R\"))\n\n\n\n\n\n\n\n\n\nFigure 1: The generalization patterns of human particpiants observed in DeLosh et al. (1997) (reproduced from Figure 3 in their manuscript). Dots represent the average responses of human participants, and solid lines represent the true functions. The dashed vertical lines indicate the lower and upper bounds of the trained examples. Stimulii that fall within the dashed lines are interpolations of the training examples, while those that fall outside the dashed lines are extrapolations.",
    "crumbs": [
      "Sections",
      "Introduction"
    ]
  },
  {
    "objectID": "Sections/Intro.html#overview-of-present-study",
    "href": "Sections/Intro.html#overview-of-present-study",
    "title": "",
    "section": "Overview Of Present Study",
    "text": "Overview Of Present Study\nThe present study investigates the influence of training variability on learning, generalization, and extrapolation in a uni-dimensional visuomotor function learning task. To the best of our knowledge, this research is the first to employ the classic constant vs. varied training manipulation, commonly used in the literature studying the benefits of variability, in the context of a uni-dimensional function learning task. Across three experiments, we compare constant and varied training conditions in terms of learning performance, extrapolation accuracy, and the ability to reliably discriminate between stimuli.\nTo account for the empirical results, we will apply a series of computational models, including the Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM). Notably, this study is the first to employ approximate Bayesian computation (ABC) to fit these models to individual subject data, enabling us to thoroughly investigate the full range of posterior predictions of each model, and to examine the ability of these influential models of function learning to account for both the group level and individual level data.",
    "crumbs": [
      "Sections",
      "Introduction"
    ]
  },
  {
    "objectID": "Model/indv_model_fits.html",
    "href": "Model/indv_model_fits.html",
    "title": "indv fits",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,patchwork,here, pander, latex2exp, flextable)\npurrr::walk(here::here(c(\"Functions/Display_Functions.R\", \"Functions/alm_core.R\",\"Functions/misc_model_funs.R\")),source)\nselect &lt;- dplyr::select; mutate &lt;- dplyr::mutate \n\n\n\n\nind_ex_te[[1]]$test\nind_ex_te[[1]]$Fit\nind_ex_te[[1]]$id\n\n\n\nind_ex_te=as.list(readRDS((here::here(paste0(\"data/model_cache/indv_nll_de2_ex_te15_26_16.rds\")))))\n\n\n\n# full combination of files, Model_Names, and Fit_Method - lined by by index. \nfile_list &lt;- list(\"indv_nll_de2_ex_te15_26_16\", \"indv_nll_de2_ex_tetr15_31_48\", \"indv_exam_de2_ex_tr15_37_27\", \"indv_nll_de2_alm_te15_42_48\",\"indv_nll_de2_alm_tetr15_48_08\", \"indv_nll_de2_alm_tr15_53_32\")\nModel_Names &lt;- list(\"EXAM\", \"EXAM\", \"EXAM\",\"ALM\",\"ALM\",\"ALM\")\nFit_Method &lt;- list(\"Test Only\", \"Test & Train\", \"Train Only\", \"Test Only\", \"Test & Train\", \"Train Only\")\n\n\n\ncompile_fit_info &lt;- function(file_name, model_name, fit_method) {\n  file_path &lt;- here::here(paste0(\"data/model_cache/\", file_name, \".rds\"))\n  data_list &lt;- as.list(readRDS(file_path))\n\n  map_dfr(data_list, ~ {\n    test_df &lt;- .x$test |&gt; slice(1)\n    fit_info &lt;- .x$Fit |&gt; slice(1)\n\n    tibble(\n      id = test_df$sbj,  \n      condit = test_df$condit,  \n      Model = model_name,\n      Fit_Method = fit_method,\n      fit_info,  \n      train_error = .x$train_error,\n      test_error = .x$test_error\n    )\n  })\n}\n\nindv_fit_info &lt;- map2_dfr(file_list, Model_Names, ~ compile_fit_info(.x, .y, Fit_Method[[which(file_list == .x)]])) |&gt; arrange(id,Fit_Method,Model)\n\nindv_fit_info &lt;- indv_fit_info %&gt;% \n  group_by(id, Fit_Method) %&gt;% \n  mutate(best_model = Model[which.min(value)]) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nmap_dfr(ind_ex_te,~tibble( pluck(.x$\"test\"))) |&gt; select(id=sbj,condit)\nmap_dfr(ind_ex_te,~tibble( pluck(.x$\"test\"))) |&gt; pull(sbj)\n\n\n# example creating one\n# ind_ex_te &lt;- map_dfr(as.list(readRDS((here::here(paste0(\"data/model_cache/indv_nll_de2_ex_te15_26_16.rds\"))))), ~ tibble(id = .x$id,condist=.x$condit, pluck(.x$\"Fit\"), train_error=.x$train_error, test_error=.x$test_error))\n\n\n# params &lt;- map_dfr(ind_ex_te,~pluck(.x$\"Fit\"))\n# ids &lt;- map_dfr(ind_ex_te, ~ tibble(id = .x$id))\n\ni47 &lt;- ind_ex_te[[47]]\ni47$\n\n\n\nCodeindv_fit_info |&gt; ggplot(aes(x=Model,y=value,group=condit,fill=condit)) +geom_bar(position=\"dodge\",stat=\"identity\") + facet_wrap(~Fit_Method, scales=\"free_y\")\n\n\nsummary_data &lt;- indv_fit_info %&gt;%\n  group_by(Fit_Method, condit, best_model) %&gt;%\n  summarize(Count = n(), .groups = 'drop')\n\n# Create the plot\nggplot(summary_data, aes(x = Fit_Method, y = Count, fill = best_model)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  facet_wrap(~condit) +\n  labs(title = \"Best Model Fit by Condition and Fit Method\",\n       x = \"Fit Method\",\n       y = \"Number of Subjects\",\n       fill = \"Best Model\") +\n  theme_minimal()\n\n\n\nCodecompile_test_info &lt;- function(file_name, model_name, fit_method) {\n  file_path &lt;- here::here(paste0(\"data/model_cache/\", file_name, \".rds\"))\n  data_list &lt;- as.list(readRDS(file_path))\n   test &lt;- map_dfr(data_list,~tibble( pluck(.x$\"test\"), Model = model_name,Fit_Method = fit_method))\n\n}\n\n\n\n\nindv_test &lt;- map2_dfr(file_list, Model_Names, ~ compile_test_info(.x, .y, Fit_Method[[which(file_list == .x)]])) |&gt; rename(id=sbj)\n\nhead(indv_test)\n\nindv_test |&gt; pivot_longer(c(y,pred), names_to=\"Resp\", values_to = \"vx\") |&gt;\nggplot(aes(x=x, y=vx,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(), fun.data = mean_se, alpha = .7) +  \n    facet_wrap(Model~Resp~Fit_Method, scales=\"free_y\")\n\n\n indv_test |&gt; pivot_longer(c(y,pred), names_to=\"Resp\", values_to = \"vx\")   |&gt; filter(Fit_Method==\"Test & Train\") |&gt;\n ggplot(aes(x=x, y=vx,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(), fun.data = mean_se, alpha = .7) +  \n    facet_wrap(Model~Resp, scales=\"free_y\")\n\n indv_test |&gt; pivot_longer(c(y,pred), names_to=\"Resp\", values_to = \"vx\")   |&gt; filter(Fit_Method==\"Test & Train\") |&gt;\n ggplot(aes(x=x, y=vx,fill=Resp)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(), fun.data = mean_se, alpha = .7) +  \n    facet_wrap(Model~condit, scales=\"free_y\")\n\n\n indv_test |&gt; pivot_longer(c(y,pred), names_to=\"Resp\", values_to = \"vx\")   |&gt; filter(Fit_Method==\"Train Only\") |&gt;\n ggplot(aes(x=x, y=vx,fill=Resp)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(), fun.data = mean_se, alpha = .7) +  \n    facet_wrap(Model~condit, scales=\"free_y\")\n\n indv_test |&gt; pivot_longer(c(y,pred), names_to=\"Resp\", values_to = \"vx\")   |&gt; filter(Fit_Method==\"Test Only\") |&gt;\n ggplot(aes(x=x, y=vx,fill=Resp)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(), fun.data = mean_se, alpha = .7) +  \n    facet_wrap(Model~condit, scales=\"free_y\")\n\n\n\n#test &lt;- map_dfr(ind_ex_te, ~pluck(.x$\"test\"))\n#ind_ex_te[[1]]$test\n\n\n\nCodeindv_fit_info |&gt; filter(condit==\"Constant\",Fit_Method==\"Test & Train\") |&gt; arrange(value)\n\nidC &lt;- indv_fit_info |&gt; filter(condit==\"Constant\",Fit_Method==\"Test & Train\", Model==\"EXAM\") |&gt; \n  arrange(test_error) |&gt; slice(1:3)\n\n indv_test |&gt; pivot_longer(c(y,pred), names_to=\"Resp\", values_to = \"vx\") |&gt; \n   filter(id %in% idC$id, Model==\"EXAM\",Fit_Method==\"Test & Train\") |&gt;  \n    ggplot(aes(x=x, y=vx,fill=Resp)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(), fun.data = mean_se, alpha = .7) +  \n    facet_wrap(Fit_Method~id, scales=\"free_y\")\n\nidV &lt;- indv_fit_info |&gt; filter(condit==\"Varied\",Fit_Method==\"Test Only\", Model==\"EXAM\") |&gt; \n  arrange(value) |&gt; slice(1:3)\n\n indv_test |&gt; pivot_longer(c(y,pred), names_to=\"Resp\", values_to = \"vx\") |&gt; \n   filter(id %in% idV$id, Model==\"EXAM\",Fit_Method==\"Test Only\") |&gt;  \n    ggplot(aes(x=x, y=vx,fill=Resp)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(), fun.data = mean_se, alpha = .7) +  \n    facet_wrap(Fit_Method~id, scales=\"free_y\")\n\n\n\nCode# find all files in model_cache dir that contain strings \"_nm_\" AND \"_nll_\", save paths in \"model_files\" list \n\ncompile_fit_info &lt;- function(file_name) {\n  file_path &lt;- here::here(\"data/model_cache\", file_name)\n  map_dfr(readRDS(file_path), ~ tibble(\n    id = .x$test$sbj[1],  \n    condit = .x$test$condit[1],  \n    Fit_Method = .x$loss_dat,\n    Model = .x$pred_fun,\n     .x$Fit,  \n    train_error = .x$train_error,\n    test_error = .x$test_error\n  )) %&gt;% \n  mutate(\n    Fit_Method = case_when(\n      Fit_Method == \"train_error\"            ~ \"Train Only\",\n      Fit_Method == \"test_error\"             ~ \"Test Only\",\n      Fit_Method == \"test_error+train_error\" ~ \"Test & Train\"\n    ),\n    Model = case_when(\n      Model == \"exam.response\"    ~ \"EXAM\",\n      Model == \"alm.responseOnly\" ~ \"ALM\"\n    )\n  )\n}\n\nmodel_files &lt;- list.files(here::here(\"data/model_cache\"), pattern = \"_nm_\")\nindv_fit_info_nm &lt;- map_dfr(model_files, compile_fit_info) |&gt; arrange(id)\n\nmodel_files &lt;- list.files(here::here(\"data/model_cache\"), pattern = \"_bfgs_\")\nindv_fit_info_bfgs &lt;- map_dfr(model_files, compile_fit_info) |&gt; arrange(id)\n\nmodel_files &lt;- list.files(here::here(\"data/model_cache\"), pattern = \"_sann_\")\nindv_fit_info_sann &lt;- map_dfr(model_files, compile_fit_info) |&gt; arrange(id)\n\nmodel_files &lt;- list.files(here::here(\"data/model_cache\"), pattern = \"_de2_\", full.names = TRUE) %&gt;%\n    file.info() %&gt;%\n    arrange(desc(mtime)) %&gt;%\n    rownames() %&gt;%\n    head(6) |&gt; basename()\n\nindv_fit_info_de2 &lt;- map_dfr(model_files, compile_fit_info) |&gt; rename(Value=value) |&gt; arrange(id)\n\n\n\n\ncombined_data &lt;- bind_rows(\n  indv_fit_info_nm %&gt;% mutate(Optimization = \"Nelder-Mead\"),\n  indv_fit_info_bfgs %&gt;% mutate(Optimization = \"BFGS\"),\n  indv_fit_info_sann %&gt;% mutate(Optimization = \"SANN\"),\n  indv_fit_info_de2 %&gt;% mutate(Optimization = \"DEoptim\")\n)\n\n\n\nhead(indv_fit_info_bfgs)\nhead(indv_fit_info_sann)\nhead(indv_fit_info_de2)\n\n\n\nCode# Grouped Bar Chart for Train and Test Errors\nggplot(combined_data, aes(x = Optimization, y = train_error, fill = Optimization)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", position = position_dodge()) +\n  geom_bar(aes(y = test_error), stat = \"summary\", fun = \"mean\", position = position_dodge(), color = \"blue\") +\n  labs(title = \"Comparison of Train and Test Errors Across Optimization Methods\", \n       y = \"Error\", \n       x = \"Optimization Method\") +\n  theme_minimal()\n\n\nlong_data &lt;- combined_data %&gt;%\n  pivot_longer(cols = c(Value, test_error, train_error), names_to = \"Metric\", values_to = \"Value\")\n\nlong_data |&gt; pandoc.table()\n\n\n# Create the plot\nggplot(long_data, aes(x = Optimization, y = Value, fill = Optimization)) +\n  geom_bar(stat = \"summary\", fun = \"median\", position = position_dodge()) +\n  facet_wrap(Fit_Method~Metric, scales = \"free_y\") +\n  labs(title = \"Comparison of Optimization Methods Across Different Metrics\",\n       y = \"Value\",\n       x = \"Optimization Method\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nlong_data &lt;- combined_data %&gt;%\n  pivot_longer(cols = c(Value, test_error, train_error), names_to = \"Metric\", values_to = \"Value\") %&gt;%\n  group_by(Optimization, Metric) %&gt;%\n  summarize(\n    Mean = mean(Value, na.rm = TRUE),\n    Median = median(Value, na.rm = TRUE)\n  ) %&gt;%\n  pivot_longer(cols = c(Mean, Median), names_to = \"Statistic\", values_to = \"Value\") %&gt;%\n  ungroup()\n\n# Create the plot\nggplot(long_data, aes(x = Optimization, y = Value, fill = Optimization)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  facet_grid(Statistic ~ Metric, scales = \"free_y\") +\n  labs(title = \"Comparison of Optimization Methods Across Different Metrics\",\n       y = \"Value\",\n       x = \"Optimization Method\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nreshaped_df &lt;- combined_data %&gt;%\n  select(Optimization, Fit_Method, Model, train_error, test_error) %&gt;%\n  pivot_longer(cols = c(train_error, test_error), names_to = \"Parameter\") %&gt;%\n  unite(\"Group\", Optimization, Parameter, sep = \"_\", na.rm = TRUE) %&gt;%  # Ensure no NAs in combined names\n  pivot_wider(\n    names_from = Group, \n    values_from = value, \n    values_fn = list(value = ~ mean(.x, na.rm = TRUE))  # Handle NAs in mean calculation\n  ) %&gt;%\n  filter(complete.cases(.))  # Optional: Remove rows with any NA values\n\nft &lt;- flextable(reshaped_df)\n\nreshaped_df &lt;- combined_data %&gt;%\n  select(Optimization, Fit_Method, Model, train_error, test_error) %&gt;%\n  pivot_longer(cols = c(train_error, test_error), names_to = \"Parameter\") %&gt;%\n  group_by(Optimization, Fit_Method, Model, Parameter) %&gt;%\n  summarize(Value = mean(value, na.rm = TRUE), .groups = 'drop') %&gt;%\n  arrange(Fit_Method, Model, Parameter, Optimization)\n\n# Check the structure of reshaped_df\nstr(reshaped_df)\n\n# Create a flextable from the reshaped data\nft &lt;- flextable(reshaped_df)\nft"
  },
  {
    "objectID": "Model/htw_model_e3.html",
    "href": "Model/htw_model_e3.html",
    "title": "HTW Model e3",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable, flextable,ggstance, htmltools,ggdist)\n#conflict_prefer_all(\"dplyr\", quiet = TRUE)\noptions(scipen = 999)\nwalk(c(\"Display_Functions\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))",
    "crumbs": [
      "Model",
      "HTW Model e3"
    ]
  },
  {
    "objectID": "Model/htw_model_e3.html#deviation-predictions",
    "href": "Model/htw_model_e3.html#deviation-predictions",
    "title": "HTW Model e3",
    "section": "Deviation Predictions",
    "text": "Deviation Predictions\n\nCode post_dat_l |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvg$vb))) |&gt;\n  ggplot(aes(x=condit,y=dist,fill=vbLab)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~Resp, axes = \"all\",ncol=3,scale=\"free\")\n\n\n\n\n\n\n\n\nCode po &lt;- post_dat_l |&gt; \n  filter(bandOrder==\"Original\") |&gt;\n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvg$vb))) |&gt;\n  ggplot(aes(x=condit,y=dist,fill=vbLab)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~Resp, axes = \"all\",ncol=3,scale=\"free\")+\n  labs(title=\"Original Order\")\n\n  pr &lt;- post_dat_l |&gt; \n  filter(bandOrder==\"Reverse\") |&gt;\n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvg$vb))) |&gt;\n  ggplot(aes(x=condit,y=dist,fill=vbLab)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~Resp, axes = \"all\",ncol=3,scale=\"free\")+\n  labs(title=\"Reverse Order\")\n\n  po/pr\n\n\n\n\n\n\n\n\nCodec_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit,bandOrder, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model+bandOrder) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit,bandOrder, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model+bandOrder) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n\n\n\nFigure 2: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\nAccounting for individual patterns\n\nCode# could compute best model for each posterior parameter - examine consistency\n# then I'd have an error bar for each subject in the model error diff. figure\n\ntid1 &lt;- post_dat  |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt; \n  mutate(e2=abs(y-pred)) |&gt; \n  summarise(y1=mean(y), pred1=mean(pred),mean_error=abs(y1-pred1)) |&gt;\n  group_by(id,condit,Model,Fit_Method) |&gt; \n  summarise(mean_error=mean(mean_error)) |&gt; \n  arrange(id,condit,Fit_Method) |&gt;\n  round_tibble(1) \n\nbest_id &lt;- tid1 |&gt; \n  group_by(id,condit,Fit_Method) |&gt; mutate(best=ifelse(mean_error==min(mean_error),1,0)) \n\nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\n\nerror_difference&lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(mean_error)) %&gt;%\n  mutate(Error_difference = (ALM - EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\nfull_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  left_join(group_ids, by = join_by(id,condit)) |&gt;\n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n  geom_col() +\n  ggh4x::facet_grid2(~condit+bandOrder,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  labs(fill=\"Best Model\",x=\"Mean Model Error Difference (ALM - EXAM)\",y=\"Participant\")\n\n\n\n# full_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n#   ungroup() |&gt;\n#   mutate(id = reorder(id, Error_difference)) |&gt;\n#   left_join(post_dat_avg |&gt; filter(x==100) |&gt; select(-x) |&gt; ungroup(), by=c(\"id\",\"condit\")) |&gt;\n#   ggplot(aes(y=id,x=c,fill=Best_Model))+\n#   stat_pointinterval(position=position_dodge(.1))\n\n\n\n\n\n\nFigure 3: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\nSubjects with biggest differential favoring ALM\n\nCodevAlm &lt;- c(245,264,280,408,370,413); cAlm &lt;- c(219,202,225,456,463,407)\n\npost_dat_l |&gt; filter(id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit+bandOrder~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Subjects with biggest differential favoring ALM\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\n\n\n\n\n\n\nSubjects with biggest differential favoring EXAM\n\nCodevAlm &lt;- c(278,215,274,401,375,425); cAlm &lt;- c(260,223,249,376,440,418)\n\npost_dat_l |&gt; filter(id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,bandOrder,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit+bandOrder~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Subjects with biggest differential favoring EXAM\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())",
    "crumbs": [
      "Model",
      "HTW Model e3"
    ]
  },
  {
    "objectID": "Model/htw_model_e1.html",
    "href": "Model/htw_model_e1.html",
    "title": "HTW Model e1",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable, flextable,ggstance, htmltools,ggdist)\n#conflict_prefer_all(\"dplyr\", quiet = TRUE)\nwalk(c(\"flextable\",\"dplyr\"), conflict_prefer_all, quiet = TRUE)\n\noptions(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\nwalk(c(\"Display_Functions\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\nCodealm_plot()\n\n\n\n\n\n\nFigure 1: The basic structure of the ALM model.",
    "crumbs": [
      "Model",
      "HTW Model e1"
    ]
  },
  {
    "objectID": "Model/htw_model_e1.html#alm-exam-description",
    "href": "Model/htw_model_e1.html#alm-exam-description",
    "title": "HTW Model e1",
    "section": "ALM & Exam Description",
    "text": "ALM & Exam Description\nALM is a localist neural network model (Page, 2000), with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on the value of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter. The EXAM model is an extension of ALM, with the same learning rule and representational scheme for input and output units. The primary difference is that EXAM includes a linear extrapolation mechanism for generating novel responses during testing, a modification necessary to account for human extrapolation patterns in past research Brown & Lacroix (2017). Although this extrapolation rule departs from a strictly similarity-based generalization mechanism, EXAM is distinct from pure rule-based models in that it remains constrained by the weights learned during training.\nSee Table 1 for a full specification of the equations that define ALM and EXAM.\n\n\n\nTable 1: ALM & EXAM Equations\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).\n\n\n\nEXAM Extrapolation\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\n\nSlope Computation\n\n\\(S =\\) \\(\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\n\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nALM response \\(m(X_i)\\) adjusted by slope.",
    "crumbs": [
      "Model",
      "HTW Model e1"
    ]
  },
  {
    "objectID": "Model/htw_model_e1.html#model-fitting-strategy",
    "href": "Model/htw_model_e1.html#model-fitting-strategy",
    "title": "HTW Model e1",
    "section": "Model Fitting Strategy",
    "text": "Model Fitting Strategy\nTo fit ALM and EXAM to our participant data, we employ a similar method to Mcdaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data in with separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase.Models to were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate parameters, we used approximate bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g. many neural network and evidence accumulation models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individuals velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.\nModelling Results\n\nCodeds &lt;- readRDS(here::here(\"data/e1_md_11-06-23.rds\"))  |&gt; as.data.table()\nnbins &lt;- 3\ne1Sbjs &lt;- ds |&gt; group_by(id,condit) |&gt; summarise(n=n())\nfd &lt;- readRDS(here(\"data/e1_08-21-23.rds\"))\ntest &lt;- testE1 &lt;-  fd |&gt; filter(expMode2 == \"Test\") \ntestAvgE1 &lt;- testE1 %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(nHits=sum(dist==0),vxAvg=mean(vx),distAvg=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n\ntrainAvg &lt;- fd |&gt; filter(expMode2 == \"Train\") |&gt; group_by(id) |&gt; \n  mutate(tr=trial,x=vb,Block=case_when(expMode2==\"Train\" ~ cut(tr,breaks=seq(1,max(tr), length.out=nbins+1),include.lowest=TRUE,labels=FALSE),\n                                         expMode2==\"Test\" ~ 4)) |&gt; \n  group_by(id,condit,vb,x,Block) |&gt; \n  summarise(dist=mean(dist),y=mean(vx))\n\ninput_layer &lt;&lt;- output_layer &lt;&lt;-  c(100,350,600,800,1000,1200)\nids2 &lt;- c(1,66,36)\nfile_name &lt;- \"n_iter_200_ntry_300_5354\"\n#file_name &lt;- \"n_iter_400_ntry_100_2944\"\n\n\nind_fits &lt;- map(list.files(here(paste0('data/abc_reject/'),file_name),full.names=TRUE), readRDS)\nind_fits_df &lt;- ind_fits |&gt; map(~list(dat=.x[[1]], Model = .x[[\"Model\"]], Fit_Method=.x[[\"Fit_Method\"]]))\nind_fits_df &lt;- ind_fits_df |&gt; map(~rbindlist(.x$dat) |&gt; mutate(Model = .x$Model, Fit_Method = .x$Fit_Method)) |&gt; rbindlist() \n\n\n\nCodegenerate_data &lt;- function(Model, post_samples, data, num_samples = 1, return_dat = \"train_data, test_data\") {\n  # Filter data for the specific id without invalidating selfref\n  sbj_data &lt;- copy(data[id == post_samples$id[1]])\n  simulation_function &lt;- ifelse(Model == \"EXAM\", full_sim_exam, full_sim_alm)\n\n  target_data &lt;- switch(return_dat,\n                        \"test_data\" = copy(sbj_data[expMode2 == \"Test\"]),\n                        \"train_data\" = copy(sbj_data[expMode2 == \"Train\"]),\n                        \"train_data, test_data\" = copy(sbj_data[expMode2 %in% c(\"Test\", \"Train\")]))\n  \n  post_samples &lt;- post_samples[order(mean_error)][1:num_samples, .(c, lr, mean_error, rank = .I)]\n\n  simulated_data_list &lt;- lapply(1:nrow(post_samples), function(i) {\n    params &lt;- post_samples[i]\n    sim_data &lt;- simulation_function(sbj_data, params$c, params$lr, input_layer = input_layer, \n                                    output_layer = output_layer, return_dat = return_dat)\n    sim_data_dt &lt;- data.table(id = sbj_data$id[1], condit = sbj_data$condit[1], \n                              expMode2 = target_data$expMode2, Model = Model,tr=target_data$tr,\n                              y = target_data$y, x = target_data$x, c = params$c, \n                              lr = params$lr, mean_error = params$mean_error, rank = i,\n                              pred = sim_data)\n    return(sim_data_dt)\n  })\n  \n  result_dt &lt;- rbindlist(simulated_data_list)\n  setcolorder(result_dt, c(\"id\", \"condit\", \"expMode2\",\"tr\", \"c\", \"lr\", \"x\", \"y\", \"pred\"))\n  return(result_dt)\n}\n\n#future::plan(multisession)\n\nnestSbjModelFit &lt;- ind_fits_df %&gt;% nest(.by=c(id,Model,Fit_Method))\n\n# organize test data predictions\n# post_dat &lt;- nestSbjModelFit |&gt; mutate(pp=furrr::future_pmap(list(id,Model,Fit_Method,data), ~{\n#    generate_data(..2, ..4 |&gt; mutate(id=..1), ds, num_samples = 50, return_dat=\"test_data\")\n#    })) |&gt; \n#   select(Fit_Method,pp,-data) |&gt;  \n#   unnest(pp) |&gt;  filter(expMode2==\"Test\") |&gt; as.data.table()\n# \n# saveRDS(post_dat, here(\"data/model_cache/post_dat.rds\"))\n\npost_dat &lt;- readRDS(here(\"data/model_cache/post_dat.rds\"))\n\npost_dat_avg &lt;- post_dat |&gt; group_by(id, condit, Model, Fit_Method, x, c, lr, rank) |&gt; \n  mutate(error2 = y - pred) |&gt;\n  summarise(y = mean(y), pred = mean(pred), error = y - pred, error2=mean(error2)) |&gt; as.data.table()\n\nsetorder(post_dat_avg, id, x, rank)\npost_dat_l &lt;- melt(post_dat_avg, id.vars = c(\"id\", \"condit\", \"Model\", \"Fit_Method\", \"x\", \"c\", \"lr\", \"rank\",\"error\"),\n                   measure.vars = c(\"pred\", \"y\"), variable.name = \"Resp\", value.name = \"val\")\npost_dat_l[, Resp := fifelse(Resp == \"y\", \"Observed\",\n                             fifelse(Model == \"ALM\", \"ALM\", \"EXAM\"))]\nsetorder(post_dat_l, id, Resp)\n#rm(post_dat_avg)\n\npost_dat_l &lt;- post_dat_l |&gt; mutate(dist = case_when(\n    val &gt;= x & val &lt;= x + 200 ~ 0,                 \n    val &lt; x ~ abs(x - val),                       \n    val &gt; x + 200 ~ abs(val - (x + 200)),           \n    TRUE ~ NA_real_                                 \n  ), signed_dist = case_when(\n    val &gt;= x & val &lt;= x + 200 ~ 0,                 \n    val &lt; x ~ x - val,                             \n    val &gt; x + 200 ~ val - (x + 200),               \n    TRUE ~ NA_real_                                 \n  ))\n\npost_dat &lt;- post_dat |&gt;  mutate(dist = case_when(\n    y &gt;= x & y &lt;= x + 200 ~ 0,                 \n    y &lt; x ~ abs(x - y),                       \n    y &gt; x + 200 ~ abs(y - (x + 200)),           \n    TRUE ~ NA_real_                                 \n  ), pred_dist = case_when(\n    pred &gt;= x & pred &lt;= x + 200 ~ 0,                 \n    pred &lt; x ~ abs(x - pred),                       \n    pred &gt; x + 200 ~ abs(pred - (x + 200)),           \n    TRUE ~ NA_real_                                 \n  ))\n\n\npost_dat &lt;- post_dat |&gt; \n left_join(testAvgE1 |&gt; \n             select(id,condit,bandInt,bandType,vb,bandInt), \n           by=join_by(id,condit,x==bandInt))\n\npost_dat_l &lt;- post_dat_l |&gt; \n left_join(testAvgE1 |&gt; \n             select(id,condit,bandInt,bandType,vb,bandInt), \n           by=join_by(id,condit,x==bandInt))\n\n\n\n# organize training data predictions\n# pd_train &lt;- nestSbjModelFit |&gt; mutate(pp=furrr::future_pmap(list(id,Model,Fit_Method,data), ~{\n#    generate_data(..2, ..4 |&gt; mutate(id=..1), ds, num_samples = 20, return_dat=\"train_data\")\n#    })) |&gt;\n#   select(Fit_Method,pp,-data) |&gt;\n#   unnest(pp) |&gt; as.data.table() |&gt; filter(expMode2==\"Train\")\n\n#saveRDS(pd_train, here(\"data/model_cache/pd_train.rds\"))\n\npd_train &lt;- readRDS(here(\"data/model_cache/pd_train.rds\"))\n\nnbins &lt;- 3\npd_train &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method) |&gt;\n  mutate(Block=cut(tr,breaks=seq(1,max(tr), length.out=nbins+1),include.lowest=TRUE,labels=FALSE))\nsetorder(pd_train, id, x,Block, rank)\n\npd_train_l &lt;- reshape2::melt(pd_train, id.vars = c(\"id\", \"condit\", \"Model\",\"Block\", \"Fit_Method\", \"x\", \"c\", \"lr\", \"rank\"),\n                   measure.vars = c(\"pred\", \"y\"), variable.name = \"Resp\", value.name = \"val\") |&gt; as.data.table()\npd_train_l[, Resp := fifelse(Resp == \"y\", \"Observed\",\n                             fifelse(Model == \"ALM\", \"ALM\", \"EXAM\"))] \nsetorder(pd_train_l, id,Block, Resp) \n\npd_train_l &lt;- pd_train_l  |&gt;\n  mutate(dist = case_when(\n    val &gt;= x & val &lt;= x + 200 ~ 0,                 \n    val &lt; x ~ abs(x - val),                       \n    val &gt; x + 200 ~ abs(val - (x + 200)),           \n    TRUE ~ NA_real_                                 \n  ))\n\n#plan(sequential)\n\n\nGroup level aggregations\n\nCodepost_tabs &lt;- abc_tables(post_dat,post_dat_l)\ntrain_tab &lt;- abc_train_tables(pd_train,pd_train_l)\n# post_tabs$agg_pred_full |&gt; \n#   mutate(Fit_Method=rename_fm(Fit_Method)) |&gt;\n#   flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"),\n#                        `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\n\n#post_tabs$agg_pred_full |&gt; pander::pandoc.table()\n\n# train_tab$agg_pred_full |&gt; \n#   mutate(Fit_Method=rename_fm(Fit_Method)) |&gt;\n#   flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"),\n#                        `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\n# \n\nrbind(post_tabs$agg_pred_full |&gt; mutate(stage=\"Test\"), train_tab$agg_pred_full |&gt; mutate(stage=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) |&gt;\n  flextable::tabulator(rows=c(\"stage\",\"Fit_Method\",\"Model\"), columns=c(\"condit\"),\n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\n post_dat  |&gt; group_by(condit,Model,Fit_Method,x) |&gt; \n    mutate(e2=abs(dist-pred_dist)) |&gt; \n    summarise(dist=mean(dist), pred=mean(pred_dist), mean_error=mean(e2)) |&gt;\n    group_by(condit,Model,Fit_Method) |&gt; \n    summarise(mean_error=mean(mean_error)) |&gt; \n    round_tibble(1) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) |&gt;\n  flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"),\n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\nagg_x_full_bt &lt;-  post_dat  |&gt; group_by(condit,Model,Fit_Method,x,bandType) |&gt; \n summarise(y=mean(y), pred=mean(pred), mean_error=abs(y-pred)) |&gt;\n    group_by(condit,Model,Fit_Method,x,bandType) |&gt; \n    summarise(mean_error=mean(mean_error)) |&gt; \n    round_tibble(1) \n\nagg_x_full_bt |&gt; \n  filter(!(Fit_Method==\"Train\")) |&gt;\n  mutate(Fit_Method=rename_fm(Fit_Method)) |&gt;\n  rename(\"Band\"=x) |&gt;\n  flextable::tabulator(rows=c(\"Fit_Method\",\"condit\",\"Model\",\"bandType\"), columns=c(\"Band\"),\n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\nagg_x_full_bt |&gt; \n  filter((Fit_Method==\"Test_Train\")) |&gt;\n  mutate(Fit_Method=(Fit_Method)) |&gt;\n  rename(\"Band\"=x, \"Condition\"=condit) |&gt;\n  pander::pandoc.table(style=\"rmarkdown\", split.table=Inf)\n\n\n\n| Condition | Model | Fit_Method | Band |   bandType    | mean_error |\n|:---------:|:-----:|:----------:|:----:|:-------------:|:----------:|\n| Constant  |  ALM  | Test_Train | 100  | Extrapolation |    67.9    |\n| Constant  |  ALM  | Test_Train | 350  | Extrapolation |   114.5    |\n| Constant  |  ALM  | Test_Train | 600  | Extrapolation |   138.5    |\n| Constant  |  ALM  | Test_Train | 800  |    Trained    |    93.9    |\n| Constant  |  ALM  | Test_Train | 1000 | Extrapolation |   259.4    |\n| Constant  |  ALM  | Test_Train | 1200 | Extrapolation |   369.6    |\n| Constant  | EXAM  | Test_Train | 100  | Extrapolation |    116     |\n| Constant  | EXAM  | Test_Train | 350  | Extrapolation |    64.1    |\n| Constant  | EXAM  | Test_Train | 600  | Extrapolation |    14.4    |\n| Constant  | EXAM  | Test_Train | 800  |    Trained    |    68.4    |\n| Constant  | EXAM  | Test_Train | 1000 | Extrapolation |    83.8    |\n| Constant  | EXAM  | Test_Train | 1200 | Extrapolation |    57.8    |\n|  Varied   |  ALM  | Test_Train | 100  | Extrapolation |    32.4    |\n|  Varied   |  ALM  | Test_Train | 350  | Extrapolation |    73.4    |\n|  Varied   |  ALM  | Test_Train | 600  | Extrapolation |    61.4    |\n|  Varied   |  ALM  | Test_Train | 800  |    Trained    |   103.1    |\n|  Varied   |  ALM  | Test_Train | 1000 |    Trained    |   143.5    |\n|  Varied   |  ALM  | Test_Train | 1200 |    Trained    |   183.8    |\n|  Varied   | EXAM  | Test_Train | 100  | Extrapolation |    18.2    |\n|  Varied   | EXAM  | Test_Train | 350  | Extrapolation |    13.2    |\n|  Varied   | EXAM  | Test_Train | 600  | Extrapolation |    7.3     |\n|  Varied   | EXAM  | Test_Train | 800  |    Trained    |   101.1    |\n|  Varied   | EXAM  | Test_Train | 1000 |    Trained    |   129.9    |\n|  Varied   | EXAM  | Test_Train | 1200 |    Trained    |    192     |\n\nCodepost_tabs$agg_pred_full_bt |&gt; \n   pander::pandoc.table(style=\"rmarkdown\", split.table=Inf)\n\n\n\n| Fit_Method | Model |   bandType    |  condit  | mean_error |\n|:----------:|:-----:|:-------------:|:--------:|:----------:|\n|    Test    |  ALM  |    Trained    | Constant |   112.3    |\n|    Test    |  ALM  |    Trained    |  Varied  |   115.4    |\n|    Test    |  ALM  | Extrapolation | Constant |   217.5    |\n|    Test    |  ALM  | Extrapolation |  Varied  |   91.38    |\n|    Test    | EXAM  |    Trained    | Constant |   112.7    |\n|    Test    | EXAM  |    Trained    |  Varied  |   99.44    |\n|    Test    | EXAM  | Extrapolation | Constant |   102.3    |\n|    Test    | EXAM  | Extrapolation |  Varied  |    71.9    |\n| Test_Train |  ALM  |    Trained    | Constant |   165.5    |\n| Test_Train |  ALM  |    Trained    |  Varied  |   200.8    |\n| Test_Train |  ALM  | Extrapolation | Constant |   227.3    |\n| Test_Train |  ALM  | Extrapolation |  Varied  |   139.7    |\n| Test_Train | EXAM  |    Trained    | Constant |    130     |\n| Test_Train | EXAM  |    Trained    |  Varied  |   195.1    |\n| Test_Train | EXAM  | Extrapolation | Constant |   127.5    |\n| Test_Train | EXAM  | Extrapolation |  Varied  |   94.59    |\n|   Train    |  ALM  |    Trained    | Constant |    191     |\n|   Train    |  ALM  |    Trained    |  Varied  |   248.4    |\n|   Train    |  ALM  | Extrapolation | Constant |   523.1    |\n|   Train    |  ALM  | Extrapolation |  Varied  |   334.4    |\n|   Train    | EXAM  |    Trained    | Constant |   191.2    |\n|   Train    | EXAM  |    Trained    |  Varied  |   248.3    |\n|   Train    | EXAM  | Extrapolation | Constant |   289.7    |\n|   Train    | EXAM  | Extrapolation |  Varied  |   347.5    |\n\nCodepost_tabs$agg_pred_full |&gt; \n   pander::pandoc.table(style=\"rmarkdown\", split.table=Inf)\n\n\n\n| Fit_Method | Model |  condit  | mean_error |\n|:----------:|:-----:|:--------:|:----------:|\n|    Test    |  ALM  | Constant |   199.9    |\n|    Test    |  ALM  |  Varied  |   103.4    |\n|    Test    | EXAM  | Constant |    104     |\n|    Test    | EXAM  |  Varied  |   85.68    |\n| Test_Train |  ALM  | Constant |    217     |\n| Test_Train |  ALM  |  Varied  |   170.3    |\n| Test_Train | EXAM  | Constant |   127.9    |\n| Test_Train | EXAM  |  Varied  |   144.9    |\n|   Train    |  ALM  | Constant |   467.7    |\n|   Train    |  ALM  |  Varied  |   291.4    |\n|   Train    | EXAM  | Constant |   273.3    |\n|   Train    | EXAM  |  Varied  |   297.9    |\n\n\n\nTable 2: Mean model errors predicting empirical data from the testing and training stage, aggregated over all participants and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on\n\n\n\n\n\n\nstage\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nTest\nFit to Test Data\nALM\n\n199.9\n\n103.4\n\n\nEXAM\n\n104.0\n\n85.7\n\n\nFit to Test & Training Data\nALM\n\n217.0\n\n170.3\n\n\nEXAM\n\n127.9\n\n144.9\n\n\nFit to Training Data\nALM\n\n467.7\n\n291.4\n\n\nEXAM\n\n273.3\n\n297.9\n\n\nTrain\nFit to Test Data\nALM\n\n297.8\n\n2,016.0\n\n\nEXAM\n\n53.9\n\n184.0\n\n\nFit to Test & Training Data\nALM\n\n57.4\n\n132.3\n\n\nEXAM\n\n42.9\n\n127.9\n\n\nFit to Training Data\nALM\n\n51.8\n\n103.5\n\n\nEXAM\n\n51.4\n\n107.0\n\n\n\n\n\n\n\n\n\n\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nFit to Test Data\nALM\n\n181.4\n\n172.2\n\n\nEXAM\n\n158.6\n\n163.3\n\n\nFit to Test & Training Data\nALM\n\n180.9\n\n185.5\n\n\nEXAM\n\n169.6\n\n179.5\n\n\nFit to Training Data\nALM\n\n326.1\n\n263.3\n\n\nEXAM\n\n223.6\n\n265.1\n\n\n\n\n\n\n\n\n\n\nFit_Method\ncondit\nModel\nbandType\n\n100\n\n350\n\n600\n\n800\n\n1000\n\n1200\n\n\n\nFit to Test Data\nConstant\nALM\nTrained\n\n\n\n\n\n\n\n61.8\n\n\n\n\n\n\nExtrapolation\n\n75.5\n\n136.9\n\n172.7\n\n\n\n226.3\n\n337.3\n\n\nEXAM\nTrained\n\n\n\n\n\n\n\n33.1\n\n\n\n\n\n\nExtrapolation\n\n83.1\n\n29.5\n\n50.4\n\n\n\n45.8\n\n25.3\n\n\nVaried\nALM\nTrained\n\n\n\n\n\n\n\n88.5\n\n48.4\n\n66.7\n\n\nExtrapolation\n\n10.6\n\n60.0\n\n23.4\n\n\n\n\n\n\n\n\nEXAM\nTrained\n\n\n\n\n\n\n\n48.6\n\n52.2\n\n48.3\n\n\nExtrapolation\n\n22.9\n\n5.6\n\n33.7\n\n\n\n\n\n\n\n\nFit to Test & Training Data\nConstant\nALM\nTrained\n\n\n\n\n\n\n\n93.9\n\n\n\n\n\n\nExtrapolation\n\n67.9\n\n114.5\n\n138.5\n\n\n\n259.4\n\n369.6\n\n\nEXAM\nTrained\n\n\n\n\n\n\n\n68.4\n\n\n\n\n\n\nExtrapolation\n\n116.0\n\n64.1\n\n14.4\n\n\n\n83.8\n\n57.8\n\n\nVaried\nALM\nTrained\n\n\n\n\n\n\n\n103.1\n\n143.5\n\n183.8\n\n\nExtrapolation\n\n32.4\n\n73.4\n\n61.4\n\n\n\n\n\n\n\n\nEXAM\nTrained\n\n\n\n\n\n\n\n101.1\n\n129.9\n\n192.0\n\n\nExtrapolation\n\n18.2\n\n13.2\n\n7.3",
    "crumbs": [
      "Model",
      "HTW Model e1"
    ]
  },
  {
    "objectID": "Model/htw_model_e1.html#deviation-predictions",
    "href": "Model/htw_model_e1.html#deviation-predictions",
    "title": "HTW Model e1",
    "section": "Deviation Predictions",
    "text": "Deviation Predictions\n\nCode post_dat_l |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=condit,y=dist,fill=vbLab)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~Resp, axes = \"all\",ncol=3,scale=\"free\")\n\n\n\n\n\n\nCodepost_dat_l |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=Resp,y=dist,fill=vbLab)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\")\n\n\n\n\n\n\nCodep1 &lt;- post_dat_l |&gt; \n  filter(Fit_Method==\"Test_Train\") |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=Resp,y=dist,fill=vbLab)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(~condit, axes = \"all\",ncol=2,scale=\"free\")\n \n\np2 &lt;- post_dat_l |&gt; \n  filter(Fit_Method==\"Test_Train\") |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=Resp,y=val,fill=vbLab)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(~condit, axes = \"all\",ncol=2,scale=\"free\")\n \np1/p2\n\n\n\n\n\n\nCode post_dat_l |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=Resp,y=dist,fill=condit)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(~vbLab, axes = \"all\",ncol=2,scale=\"free\")\n\n\n\n\n\n\nCode post_dat_l |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=Resp,y=val,fill=condit)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(~vbLab, axes = \"all\",ncol=2,scale=\"free\")\n\n\n\n\n\n\nCode post_dat_l |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=vbLab,y=dist,fill=Resp)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(~condit, axes = \"all\",ncol=2,scale=\"free\")\n\n\n\n\n\n\nCode post_dat_l |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=bandType,y=dist,fill=Resp)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(~condit, axes = \"all\",ncol=2,scale=\"free\")\n\n\n\n\n\n\nCode post_dat_l |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=Resp,y=dist,fill=bandType)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(~condit, axes = \"all\",ncol=2,scale=\"free\")\n\n\n\n\n\n\nCode post_dat_l |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=bandType,y=dist,fill=condit)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(~Resp, axes = \"all\",ncol=3,scale=\"free\")\n\n\n\n\n\n\n\n\nCodec_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n\n\n\nFigure 3: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\nAccounting for individual patterns\nTo more accurately assess the relative abilities of ALM and EXAM to capture important empirical patterns - we will now examine the predictions of both models for the subset of individual participants shown in Figure 4. Panel A presents three varied and constant participants who demonstrated a reasonable degree of discrimination between the 6 velocity bands during testing.\n\n** comment on the different ways ALM can completely fail to mimic discrimination patterns (sbj. 35; sbj. 137),and on how it can sometimes partially succeed (sbj. 11; 14,74)\n** comment on how EXAM can somtimes mimic non-monotonic spacing between bands due to associative stregth from training (i.e. subject 47)\n** compare c values to slope parameters from the statistical models earlier in paper\n\n\nCodecId_tr &lt;- c(137, 181, 11)\nvId_tr &lt;- c(14, 193, 47)\ncId_tt &lt;- c(11, 93, 35)\nvId_tt &lt;- c(1,14,74)\n# filter(id %in% (filter(bestTestEXAM,group_rank&lt;=9, Fit_Method==\"Test\")\n\ntestIndv &lt;- post_dat_l |&gt; filter(id %in% c(cId_tt,vId_tt), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Individual Participant fits from Test & Train Fitting Method\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\n\ntrainIndv &lt;- post_dat_l |&gt; filter(id %in% c(cId_tr,vId_tr), Fit_Method==\"Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp), flab=paste0(\"Subject: \",id)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar + \n  ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Individual Participant fits from Train Only Fitting Method\", y=\"X Velocity\",\n       fill=\"Target Velocity\") +\n     guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\n\n(testIndv  / trainIndv) +\n  plot_annotation(tag_levels = list(c('A','B')),tag_suffix = ') ') & \n  theme(plot.tag.position = c(0, 1))\n\n\n\n\n\n\nFigure 4: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data.\n\n\n\n\n\nCode# could compute best model for each posterior parameter - examine consistency\n# then I'd have an error bar for each subject in the model error diff. figure\n\ntid1 &lt;- post_dat  |&gt; filter(Fit_Method==\"Test_Train\") |&gt; group_by(id,condit,Model,Fit_Method) |&gt; \n  mutate(e2=abs(y-pred)) |&gt; \n  summarise(y1=median(y), pred1=median(pred),mean_error=abs(y1-pred1)) |&gt;\n  group_by(id,condit,Model,Fit_Method) |&gt; \n  summarise(mean_error=mean(mean_error)) |&gt; \n  arrange(id,condit,Fit_Method) |&gt;\n  round_tibble(1) \n\nbest_id &lt;- tid1 |&gt; \n  group_by(id,condit,Fit_Method) |&gt; mutate(best=ifelse(mean_error==min(mean_error),1,0)) \n\nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\n\nerror_difference&lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(mean_error)) %&gt;%\n  mutate(Error_difference = (ALM - EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\nfull_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n  geom_col() +\n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  labs(fill=\"Best Model\",x=\"Mean Model Error Difference (ALM - EXAM)\",y=\"Participant\")\n\n\n\n# full_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n#   ungroup() |&gt;\n#   mutate(id = reorder(id, Error_difference)) |&gt;\n#   left_join(post_dat_avg |&gt; filter(x==100) |&gt; select(-x) |&gt; ungroup(), by=c(\"id\",\"condit\")) |&gt;\n#   ggplot(aes(y=id,x=c,fill=Best_Model))+\n#   stat_pointinterval(position=position_dodge(.1))\n\n\n\n\n\n\nFigure 5: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\nSubjects with biggest differential favoring ALM\n\nCodevAlm &lt;- c(29,70,68); cAlm &lt;- c(73,49,128)\n\npost_dat_l |&gt; filter(id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Subjects with biggest differential favoring ALM\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\n\n\n\n\n\n\nSubjects with biggest differential favoring EXAM\n\nCodevAlm &lt;- c(23,155,184); cAlm &lt;- c(119,85,175)\n\npost_dat_l |&gt; filter(id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Subjects with biggest differential favoring EXAM\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\n\n\n\n\n\n\nSubjects with no clear best model\n\nCodevAlm &lt;- c(129,192,105); cAlm &lt;- c(101, 109,134)\n\npost_dat_l |&gt; filter(id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Subjects with no clear best model\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\n\n\n\n\n\n\n\nCodevAlm &lt;- c(29,70,68); cAlm &lt;- c(73,49,128)\n\ndifAlm &lt;-  post_dat_avg  |&gt; filter(x==100,id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt;\n    group_by(id, condit, Model, rank) %&gt;%\n      mutate(flab=paste0(\"Subject: \",id)) |&gt;\n    ggplot(aes(y=log(c), x = id,col=Model)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n   # ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",scales=\"free_y\",ncol=3) + \n      labs(title=\"c parameter - sbjs. with biggest diff. favoring ALM\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4)) +ylim(c(-11,-4))\n\n\nvAlm &lt;- c(23,155,184); cAlm &lt;- c(119,85,175)\n\ndifExam &lt;- post_dat_avg  |&gt; filter(x==100,id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt;\n    group_by(id, condit, Model, rank) %&gt;%\n      mutate(flab=paste0(\"Subject: \",id)) |&gt;\n    ggplot(aes(y=log(c), x = id,col=Model)) + \n  stat_pointinterval(position=position_dodge(.1)) +\n   # ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",scales=\"free_y\",ncol=3) + \n      labs(title=\"c parameter - sbjs. with biggest diff. favoring EXAM\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4)) +ylim(c(-11,-4))\n\ndifAlm/difExam\n\n\n\n\n\n\nCodevAlm &lt;- c(29,70,68); cAlm &lt;- c(73,49,128)\n\ndifAlm &lt;-  post_dat_avg  |&gt; filter(x==100,id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt;\n    group_by(id, condit, Model, rank) %&gt;%\n      mutate(flab=paste0(\"Subject: \",id)) |&gt;\n    ggplot(aes(y=lr, x = id,col=Model)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n   # ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",scales=\"free_y\",ncol=3) + \n      labs(title=\"lr parameter - sbjs. with biggest diff. favoring ALM\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))+ylim(c(0,12))\n\n\nvAlm &lt;- c(23,155,184); cAlm &lt;- c(119,85,175)\n\ndifExam &lt;- post_dat_avg  |&gt; filter(x==100,id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt;\n    group_by(id, condit, Model, rank) %&gt;%\n      mutate(flab=paste0(\"Subject: \",id)) |&gt;\n    ggplot(aes(y=lr, x = id,col=Model)) + \n  stat_pointinterval(position=position_dodge(.1)) +\n   # ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",scales=\"free_y\",ncol=3) + \n      labs(title=\"lr parameter - sbjs. with biggest diff. favoring EXAM\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4)) +ylim(c(0,12))\n\ndifAlm/difExam\n\n\n\n\n\n\n\n\nCode# \n# \n# full_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt;\n#   ungroup() |&gt;\n#   mutate(id = reorder(id, Error_difference)) %&gt;%\n#   ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n#   geom_col()+\n#   ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")\n# \n\n# d &lt;- testAvgE1 |&gt; left_join(full_comparison, by=c(\"id\",\"condit\")) |&gt; filter(Fit_Method==\"Test_Train\")\n# \n# \n# \n# d |&gt; ggplot(aes(x=vb,y=vx,fill=condit)) + stat_bar + facet_wrap(Fit_Method~Best_Model2,ncol=2)\n# \n# d |&gt; \n#   group_by(condit,Fit_Method,Best_Model) |&gt; \n#   mutate(nGrp2=n()) |&gt;\n#   filter(abs(Error_difference)&gt;15) |&gt; \n#   ggplot(aes(x=vb,y=dist,fill=condit)) + \n#   stat_bar + facet_wrap(Fit_Method~Best_Model,ncol=2)\n# \n# d |&gt; group_by(condit,Fit_Method,Best_Model) %&gt;% tally() |&gt; mutate(n=n/6)\n# \n# d |&gt; group_by(condit,Fit_Method,Best_Model) |&gt; filter(abs(Error_difference)&gt;15) |&gt; tally() |&gt; mutate(n=n/6)\n# \n# d |&gt; group_by(condit,Fit_Method) |&gt; mutate(m=mean(Error_difference), \n#                                               sd=sd(Error_difference), \n#                                               n=n()/6,se=sd/sqrt(n)) |&gt;\n#    group_by(condit,Fit_Method,Best_Model) |&gt; \n#  # filter(abs(Error_difference)&gt;(2.5*se)) |&gt; \n#   ggplot(aes(x=vb,y=dist,fill=condit)) + \n#   stat_bar + facet_wrap(Fit_Method~Best_Model,ncol=2)\n#   \n\n\nTo add to appendix\n\nCodepost_tabs$agg_x_full |&gt; flextable::tabulator(rows=c(\"Fit_Method\",\"x\"), columns=c(\"condit\",\"Model\"),\n                       `X` = as_paragraph(mean_error)) |&gt; as_flextable()\n\n\n\n\n\n\nFit_Method\nx\n\nConstant\n\nVaried\n\n\n\nALM\n\nEXAM\n\nALM\n\nEXAM\n\n\n\n\nTest\n100\n\n203.3\n\n191.4\n\n233.5\n\n194.8\n\n\n350\n\n249.8\n\n169.0\n\n213.2\n\n193.5\n\n\n600\n\n264.1\n\n199.5\n\n222.4\n\n219.2\n\n\n800\n\n218.2\n\n214.3\n\n243.9\n\n222.9\n\n\n1,000\n\n315.9\n\n245.3\n\n224.4\n\n222.3\n\n\n1,200\n\n409.1\n\n275.9\n\n249.8\n\n237.2\n\n\nTest_Train\n100\n\n195.0\n\n213.2\n\n238.1\n\n217.2\n\n\n350\n\n241.4\n\n183.9\n\n241.0\n\n207.1\n\n\n600\n\n255.3\n\n190.5\n\n270.5\n\n230.0\n\n\n800\n\n244.9\n\n222.0\n\n270.3\n\n257.9\n\n\n1,000\n\n355.3\n\n265.1\n\n276.0\n\n272.2\n\n\n1,200\n\n437.3\n\n297.0\n\n313.8\n\n319.9\n\n\nTrain\n100\n\n519.3\n\n430.2\n\n495.7\n\n498.8\n\n\n350\n\n466.6\n\n310.9\n\n398.6\n\n405.2\n\n\n600\n\n445.4\n\n243.0\n\n347.3\n\n349.0\n\n\n800\n\n260.9\n\n261.2\n\n298.5\n\n300.0\n\n\n1,000\n\n667.3\n\n352.9\n\n311.0\n\n311.0\n\n\n1,200\n\n809.3\n\n443.5\n\n361.3\n\n361.3\n\n\n\n\n\nCode# post_dat  |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt; \n#   mutate(e2=abs(y-pred)) |&gt; \n#   summarise(y1=mean(y), pred1=mean(pred)) |&gt;\n#   group_by(condit,Model,Fit_Method,x) |&gt; \n#   summarise(y=mean(y1), pred=mean(pred1),mean_error=abs(y-pred)) |&gt; \n#   round_tibble(1) |&gt; pander::pandoc.table()\n\n\n# post_dat  |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt;\n#   mutate(e2=abs(y-pred)) |&gt;\n#   summarise(y1=mean(y), pred1=mean(pred)) |&gt;\n#   group_by(condit,Model,Fit_Method) |&gt;\n#   summarise(y=mean(y1), pred=mean(pred1),mean_error=abs(y-pred)) |&gt;\n#   round_tibble(1) |&gt; pander::pandoc.table()\n# \n# post_dat  |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt; \n#   mutate(e2=abs(y-pred),Fit_Method=rename_fm(Fit_Method)) |&gt; \n#   summarise(y1=mean(y), pred1=mean(pred)) |&gt;\n#   group_by(condit,Model,Fit_Method) |&gt; \n#   summarise(y=mean(y1), pred=mean(pred1),mean_error=abs(y-pred)) |&gt; \n#   select(-y,-pred) |&gt;\n#   arrange(condit,Fit_Method) |&gt;\n#   round_tibble(1) |&gt; pander::pandoc.table()\n# \n# \n# \n# post_dat  |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt; \n#   mutate(e2=abs(y-pred),Fit_Method=rename_fm(Fit_Method)) |&gt; \n#   summarise(y1=mean(y), pred1=mean(pred)) |&gt;\n#   group_by(condit,Model,Fit_Method,x) |&gt; \n#   summarise(y2=mean(y1), pred2=mean(pred1)) |&gt; \n#   group_by(condit,Model,Fit_Method) |&gt; \n#   summarise(y=mean(y2), pred=mean(pred2),mean_error=abs(y-pred)) |&gt; \n#   select(-y,-pred) |&gt;\n#   arrange(condit,Fit_Method) |&gt;\n#   round_tibble(1) |&gt; pander::pandoc.table()\n# \n# \n# post_dat  |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt; \n#   mutate(e2=abs(y-pred),Fit_Method=rename_fm(Fit_Method)) |&gt; \n#   summarise(y1=mean(y), pred1=mean(pred),mean_error=abs(y1-pred1)) |&gt;\n#   group_by(id,condit,Model,Fit_Method) |&gt; \n#   summarise(mean_error=mean(mean_error)) |&gt; \n#   group_by(condit,Model,Fit_Method) |&gt;\n#   summarise(mean_error=mean(mean_error)) |&gt; \n#   arrange(condit,Fit_Method) |&gt;\n#   round_tibble(1) |&gt; pander::pandoc.table()",
    "crumbs": [
      "Model",
      "HTW Model e1"
    ]
  },
  {
    "objectID": "Model/htw_model_e1.html#comparison-to-project-1",
    "href": "Model/htw_model_e1.html#comparison-to-project-1",
    "title": "HTW Model e1",
    "section": "Comparison to Project 1",
    "text": "Comparison to Project 1\nDifferences between the tasks\nThere are a number of differences between Project 1’s Hit The Target (HTT), and Project 2’s Hit The Wall (HTW) tasks.\n\nTask Space Complexity: In HTW, the task space is also almost perfectly smooth, at least for the continuous feedback subjects, if they throw 100 units too hard, they’ll be told that they were 100 units too hard. Whereas in HTT,  it was possible to produce xy velocity combinations that were technically closer to the empirical solution space than other throws, but which resulted in worse feedback due to striking the barrier.\nPerceptual Distinctiveness: HTT offers perceptually distinct varied conditions that directly relate to the task’s demands, which may increase the sallience between training positions encounted by the varied group. In contrast, HTW’s varied conditions differ only in the numerical values displayed, lacking the same level of perceptual differentiation. Conversely in HTW, the only difference between conditions for the varied group are the numbers displayed at the top of the screen which indicate the current target band(e.g. 800-1000, or 1000-1200)\nIn HTW, our primary testing stage of interest has no feedback, whereas in HTT testing always included feedback (the intermittent testing in HTT expt 1 being the only exception). Of course, we do collect testing with feedback data at the end of HTW, but we haven’t focused on that data at all in our modelling work thus far. It’s also interesting to recall that the gap between varied and constant in HTW does seem to close substantially in the testing-with-feedback stage. The difference between no-feedback and feedback testing might be relevant if the benefits of variation have anything to do with improving subsequent learning (as opposed to subsequent immediate performance), OR if the benefits of constant training rely on having the most useful anchor, having the most useful anchor might be a lot less helpful if you’re getting feedback from novel positions and can thus immediately begin to form position-specific anchors for the novelties, rather than relying on a training anchor. \nHTW and HTT both have a similar amount of training trials (~200), and thus the constant groups acquire a similar amount of experience with their single position/velocity in both experiments. However, the varied conditions in both HTT experiments train on 2 positions, whereas the varied group in HTW trains on 3 velocity bands. This means that in HTT the varied group gets half as much experience on any one position as the constant group, and in HTW they only get 1/3 as much experience in any one position. There are likely myriad ways in which this might impact the success of the varied group regardless of how you think the benefits of variation might be occurring, e.g. maybe they also need to develop a coherent anchor, maybe they need more experience in order to extract a function, or more experience in order to properly learn to tune their c parameter.",
    "crumbs": [
      "Model",
      "HTW Model e1"
    ]
  },
  {
    "objectID": "Model/htw_exam.html",
    "href": "Model/htw_exam.html",
    "title": "EXAM Group Fits",
    "section": "",
    "text": "Code# load and view data\npacman::p_load(dplyr,purrr,tidyr,patchwork,here, pander, latex2exp, flextable)\npurrr::walk(here::here(c(\"Functions/Display_Functions.R\", \"Functions/alm_core.R\",\"Functions/fun_model.R\")),source)\nselect &lt;- dplyr::select; mutate &lt;- dplyr::mutate \n\nds &lt;- readRDS(here::here(\"data/e1_md_11-06-23.rds\"))\ndsAvg &lt;- ds |&gt; group_by(condit,expMode2,tr, x) |&gt; \n  summarise(y=mean(y),.groups=\"keep\") \n\nvAvg &lt;- dsAvg |&gt; filter(condit==\"Varied\")\ncAvg &lt;- dsAvg |&gt; filter(condit==\"Constant\")\n\n#i1 &lt;- ds |&gt; filter(id==\"3\")\n\ninput.layer &lt;- c(100,350,600,800,1000,1200)\noutput.layer &lt;- c(100,350,600,800,1000,1200)\n\n\npurrr::walk(c(\"con_group_exam_fits\", \"var_group_exam_fits\", \"hybrid_group_exam_fits\"), \n            ~ list2env(readRDS(here::here(paste0(\"data/model_cache/\", .x, \".rds\"))), \n            envir = .GlobalEnv))\n\n# pluck(ex_te_v, \"Fit\") |&gt; mutate(w= ifelse(exists(\"w\"), round(w,2),NA))\n# pluck(hybrid_te_v, \"Fit\") |&gt; mutate(w= ifelse(exists(\"w\"), round(w,2), NA))\nCode alm_plot()\n\n\n\n\n\n\nFigure 1: The basic structure of the ALM model.",
    "crumbs": [
      "Model",
      "EXAM Group Fits"
    ]
  },
  {
    "objectID": "Model/htw_exam.html#alm-exam-description",
    "href": "Model/htw_exam.html#alm-exam-description",
    "title": "EXAM Group Fits",
    "section": "ALM & Exam Description",
    "text": "ALM & Exam Description\nDeLosh et al. (1997) introduced the associative learning model (ALM), a connectionist model within the popular class of radial-basis networks. ALM was inspired by, and closely resembles Kruschke’s influential ALCOVE model of categorization (Kruschke, 1992).\nALM is a localist neural network model, with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on thevalue of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nSee Table 1 for a full specification of the equations that define ALM and EXAM.",
    "crumbs": [
      "Model",
      "EXAM Group Fits"
    ]
  },
  {
    "objectID": "Model/htw_exam.html#model-fitting-and-comparison",
    "href": "Model/htw_exam.html#model-fitting-and-comparison",
    "title": "EXAM Group Fits",
    "section": "Model Fitting and Comparison",
    "text": "Model Fitting and Comparison\nFollowing the procedure used by Mcdaniel et al. (2009), we will assess the ability of both ALM and EXAM to account for the empirical data when fitting the models to 1) only the training data, and 2) both training and testing data. Models were fit to the aggregated participant data by minimizing the root-mean squared deviation (RMSE). Because ALM has been shown to do poorly at accounting for human patterns extrapolation (DeLosh et al., 1997), we will also generate predictions from the EXAM model for the testing stage. EXAM which operates identically to ALM during training, but includes a linear extrapolation mechanism for generating novel responses during testing.\nFor the hybrid model, predictions are computed by first generating separate predictions from ALM and EXAM, and then combining them using the following equation: \\(\\hat{y} = (1 - w) \\cdot alm.pred + w \\cdot exam.pred\\). For the grid search, the weight parameter is varied from 0 to 1, and the resulting RMSE is recorded.\nEach model was fit to the data in 3 different ways. 1) To just the testing data, 2) Both the training and testing data, 3) Only the training data. In all cases, the model only updates its weights during the training phase, and the weights are frozen during the testing phase. In all cases, only the ALM model generates predictions during the training phase. For the testing phase, all 3 models are used to generate predictions.\n\n\nCode##| column: body-outset-right\n\n\nreshaped_df &lt;- all_combined_params %&gt;%\n  select(-Value,-Test_RMSE) |&gt;\n  rename(\"Fit Method\" = Fit_Method) |&gt;\n  pivot_longer(cols=c(c,lr,w),names_to=\"Parameter\") %&gt;%\n  unite(Group, Group, Parameter) %&gt;%\n  pivot_wider(names_from = Group, values_from = value)\n\nheader_df &lt;- data.frame(\n  col_keys = c(\"Model\", \"Fit Method\",\"Constant_c\", \"Constant_lr\", \"Constant_w\", \"Varied_c\", \"Varied_lr\", \"Varied_w\"),\n  line1 = c(\"\", \"\", \"Constant\", \"\", \"\", \"Varied\", \"\",\"\"),\n  line2 = c(\"Model\", \"Fit Method\", \"c\", \"lr\", \"w\", \"c\", \"lr\", \"w\")\n)\n\nft &lt;- flextable(reshaped_df) %&gt;% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %&gt;% add_header_lines(values = \" \") %&gt;%\n  theme_booktabs() %&gt;% \n  merge_v(part = \"header\") %&gt;% \n  merge_h(part = \"header\") %&gt;%\n  merge_h(part = \"header\") %&gt;%\n  align(align = \"center\", part = \"all\") %&gt;% \n  #autofit() %&gt;% \n  empty_blanks() %&gt;% \n  fix_border_issues() %&gt;% \n  hline(part = \"header\", i = 2, j=3:5) %&gt;% \n  hline(part = \"header\", i = 2, j=6:8)\n\nft\n\n\nTable 2: Fit Parameters and Model RMSE. The Test_RMSE column is the main performance indicator of interest, and represents the RMSE for just the testing data. The Fit_Method column indicates the data used to fit the model. The \\(w\\) parameter determines the balance between the ALM and EXAM response generation processes, and is only included for the hybrid model. A weight of .5 would indicate equal contribution from both models. \\(w\\) values approaching 1 indicate stronger weight for EXAM.\n\n\n\n\n\n\n \n\n\nConstant\n\nVaried\n\n\n\nModel\nFit Method\nc\nlr\nw\nc\nlr\nw\n\n\n\n\nALM\nTest Only\n0.000\n0.100\n\n0.134\n2.030\n\n\n\nALM\nTest & Train\n0.047\n0.080\n\n0.067\n0.100\n\n\n\nALM\nTrain Only\n0.060\n0.100\n\n0.047\n0.080\n\n\n\nEXAM\nTest Only\n0.007\n1.327\n\n0.409\n1.910\n\n\n\nEXAM\nTest & Train\n0.081\n0.161\n\n0.074\n0.100\n\n\n\nEXAM\nTrain Only\n0.060\n0.100\n\n0.047\n0.080\n\n\n\nHybrid\nTest Only\n0.008\n1.580\n1\n0.395\n2.017\n0.64\n\n\nHybrid\nTest & Train\n0.067\n0.134\n1\n0.134\n2.017\n0.79\n\n\nHybrid\nTrain Only\n0.042\n0.067\n0\n0.042\n0.067\n0.00\n\n\n\n\n\n\n\n\n\nTesting Observations vs. Predictions\n\nCodetvte&lt;- pluck(a_te_v, \"test\") |&gt; \n  mutate(Fit_Method=\"Test Only\") |&gt;\n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_te_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_te_v, \"test\") |&gt; pull(pred))\n\ntvtetr&lt;-pluck(a_tetr_v, \"test\") |&gt; \n  mutate(Fit_Method=\"Test & Train\") |&gt; \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_tetr_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tetr_v, \"test\") |&gt; pull(pred))\n\ntvtr&lt;- pluck(a_tr_v, \"test\")|&gt; \n  mutate(Fit_Method=\"Train Only\") |&gt; \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_tr_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tr_v, \"test\") |&gt; pull(pred))\n\ntcte&lt;- pluck(a_te_c, \"test\") |&gt; \n  mutate(Fit_Method=\"Test Only\") |&gt; \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_te_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_te_c, \"test\") |&gt; pull(pred))\n\ntctetr&lt;-pluck(a_tetr_c, \"test\") |&gt; \n  mutate(Fit_Method=\"Test & Train\") |&gt;  \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_tetr_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tetr_c, \"test\") |&gt; pull(pred))\n\ntctr&lt;- pluck(a_tr_c, \"test\")|&gt; \n  mutate(Fit_Method=\"Train Only\") |&gt;  \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_tr_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tr_c, \"test\") |&gt; pull(pred))\n\nvPreds &lt;- rbind(tvte,tvtetr, tvtr) |&gt; relocate(Fit_Method,.before=x) |&gt; \n   mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\ncPreds &lt;- rbind(tcte,tctetr, tctr) |&gt; relocate(Fit_Method,.before=x) |&gt; \n   mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\nallPreds &lt;- rbind(vPreds |&gt; mutate(Group=\"Varied\"), cPreds |&gt; mutate(Group=\"Constant\")) |&gt;\n  pivot_longer(cols=c(\"ALM\",\"EXAM\",\"Hybrid\"), names_to=\"Model\",values_to = \"Prediction\") |&gt; \n  mutate(Error=Observed-Prediction, Abs_Error=((Error)^2)) |&gt; \n  group_by(Group,Fit_Method, Model) #|&gt; summarise(Mean_Error=mean(Error), Abs_Error=mean(Abs_Error))\n\n\n\nCodeallPreds |&gt; summarise(Error=mean(Error), Abs_Error=sqrt(mean(Abs_Error))) |&gt; \n  mutate(Fit_Method=factor(Fit_Method, levels=c(\"Test Only\", \"Test & Train\", \"Train Only\"))) |&gt;\n  tabulator(rows=c(\"Fit_Method\", \"Model\"), columns=c(\"Group\"), \n             `ME` = as_paragraph(Error), \n            `RMSE` = as_paragraph(Abs_Error)) |&gt; as_flextable()\n\n\nTable 3: Model Perforamnce - averaged over all X values/Bands. ME=Mean Average Error, RMSE = Root mean squared error.\n\n\n\n\n\n\n\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nME\nRMSE\n\nME\nRMSE\n\n\n\n\nTest Only\nALM\n\n223.8\n348.0\n\n56.3\n95.4\n\n\nEXAM\n\n-59.2\n127.5\n\n-6.0\n45.9\n\n\nHybrid\n\n-58.2\n127.4\n\n-3.0\n33.8\n\n\nTest & Train\nALM\n\n193.2\n328.7\n\n82.3\n106.6\n\n\nEXAM\n\n-28.8\n132.1\n\n13.2\n60.2\n\n\nHybrid\n\n-16.7\n136.7\n\n16.7\n46.5\n\n\nTrain Only\nALM\n\n194.5\n329.2\n\n86.3\n109.1\n\n\nEXAM\n\n75.3\n199.9\n\n17.5\n65.4\n\n\nHybrid\n\n197.5\n330.4\n\n88.3\n110.3",
    "crumbs": [
      "Model",
      "EXAM Group Fits"
    ]
  },
  {
    "objectID": "Model/htw_exam.html#varied-testing-predictions",
    "href": "Model/htw_exam.html#varied-testing-predictions",
    "title": "EXAM Group Fits",
    "section": "Varied Testing Predictions",
    "text": "Varied Testing Predictions\n\nCode##| column: screen-inset-right\n\n####\n\nvte &lt;-  pluck(a_te_v, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_te_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_te_v, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test Only\")\n\nvtetr &lt;-  pluck(a_tetr_v, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_tetr_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tetr_v, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") + \n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test and Train\")\n\nvtr &lt;-  pluck(a_tr_v, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_tr_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tr_v, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Train Only\")\n\n vte/vtetr/vtr\n\n\n\n\n\n\nFigure 2: Varied Group - Mean Model predictions vs. observations\n\n\n\n\n\nCode##| column: screen-inset-right\n\n\n# Create a custom header dataframe\nheader_df &lt;- data.frame(\n  col_keys = c(\"Fit_Method\", \"x\",\"Observed\" ,\"ALM_Predicted\", \"ALM_Residual\", \"EXAM_Predicted\",\"EXAM_Residual\", \"Hybrid_Predicted\",\"Hybrid_Residual\"),\n  line1 = c(\"\",\"\",\"\", \"ALM\", \"\", \"EXAM\", \"\", \"Hybrid\",\"\"),\n  line2 = c(\"Fit Method\", \"X\", \"Observed\", \"Predicted\",\"Residual\", \"Predicted\",\"Residual\", \"Predicted\",\"Residual\")\n)\n\n\nbest_vPreds &lt;- vPreds %&gt;%\n  pivot_longer(cols = c(ALM, EXAM, Hybrid), names_to = \"Model\", values_to = \"Predicted\") |&gt;\n  mutate(Residual=(Observed-Predicted), abs_res =abs(Residual)) |&gt; group_by(Fit_Method,x) |&gt;\n  mutate(best=if_else(abs_res==min(abs_res),1,0)) |&gt; select(-abs_res)\n\nlong_vPreds &lt;- best_vPreds |&gt; select(-best) |&gt;\n  pivot_longer(cols=c(Predicted,Residual), names_to=\"Model_Perf\") |&gt;\n  relocate(Model, .after=Fit_Method) |&gt; \n  unite(Model,Model,Model_Perf) |&gt;\n  pivot_wider(names_from=Model,values_from=value)\n\nbest_wide &lt;- best_vPreds |&gt; select(-Residual,-Predicted,-Observed) |&gt; ungroup() |&gt;\n  pivot_wider(names_from=Model,values_from=best) |&gt; select(ALM,EXAM,Hybrid)\n\nbest_indexV &lt;- row_indices &lt;- apply(best_wide, 1, function(row) {\n which(row == 1)\n})\n\n\napply_best_formatting &lt;- function(ft, best_index) {\n  for (i in 1:length(best_index)) {\n      #ft &lt;- ft %&gt;% surround(i=i,j=best_index[i],border=fp_border_default(color=\"red\",width=1))\n      ind = best_index[[i]]\n      ind &lt;- ind  %&gt;% map_dbl(~ .x*2+3)\n      ft &lt;- ft %&gt;% highlight(i=i,j=ind,color=\"wheat\")\n      }\n  return(ft)\n}\n\nft &lt;- flextable(long_vPreds) %&gt;% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %&gt;% \n  theme_booktabs() %&gt;% \n  merge_v(part = \"header\") %&gt;% \n  merge_h(part = \"header\") %&gt;%\n  align(align = \"center\", part = \"all\") %&gt;% \n  #autofit() %&gt;% \n  empty_blanks() %&gt;% \n  fix_border_issues() %&gt;%\n  hline(part = \"header\", i = 1, j=4:9) %&gt;%\n  vline(j=c(\"Observed\",\"ALM_Residual\",\"EXAM_Residual\")) %&gt;%\n  hline(part = \"body\", i=c(6,12)) |&gt; \n  bold(i=long_vPreds$x %in% c(100,350,600), j=2) \n\n  # bold the cell with the lowest residual, based on best_wide df\n  # for each row, the cell that should be bolded matches which column in best_wide==1 at that row\nft &lt;- apply_best_formatting(ft, best_indexV)\nft\n\n\nTable 4: Varied group - mean model predictions vs. observations. Extrapolation Bands are bolded. For each Modelling fitting and band combination, the model with the smallest residual is highlighted. Only the lower bound of each velocity band is shown (bands are all 200 units).\n\n\n\n\n\n\n\n\nALM\n\nEXAM\n\nHybrid\n\n\n\nFit Method\nX\nObserved\nPredicted\nResidual\nPredicted\nResidual\nPredicted\nResidual\n\n\n\n\nTest Only\n100\n663\n675\n-12\n716\n-53\n708\n-45\n\n\nTest Only\n350\n764\n675\n89\n817\n-53\n792\n-28\n\n\nTest Only\n600\n884\n675\n209\n895\n-11\n875\n9\n\n\nTest Only\n800\n1,083\n1,078\n5\n1,000\n83\n1,091\n-8\n\n\nTest Only\n1,000\n1,196\n1,202\n-6\n1,199\n-3\n1,204\n-8\n\n\nTest Only\n1,200\n1,283\n1,230\n53\n1,282\n1\n1,221\n62\n\n\nTest & Train\n100\n663\n675\n-12\n716\n-53\n707\n-44\n\n\nTest & Train\n350\n764\n675\n89\n817\n-53\n788\n-24\n\n\nTest & Train\n600\n884\n675\n209\n902\n-18\n851\n33\n\n\nTest & Train\n800\n1,083\n1,000\n83\n1,000\n83\n1,004\n79\n\n\nTest & Train\n1,000\n1,196\n1,163\n33\n1,165\n31\n1,196\n0\n\n\nTest & Train\n1,200\n1,283\n1,191\n92\n1,194\n89\n1,227\n56\n\n\nTrain Only\n100\n663\n675\n-12\n716\n-53\n675\n-12\n\n\nTrain Only\n350\n764\n675\n89\n817\n-53\n675\n89\n\n\nTrain Only\n600\n884\n675\n209\n905\n-21\n675\n209\n\n\nTrain Only\n800\n1,083\n1,000\n83\n1,000\n83\n999\n84\n\n\nTrain Only\n1,000\n1,196\n1,150\n46\n1,150\n46\n1,143\n53\n\n\nTrain Only\n1,200\n1,283\n1,180\n103\n1,180\n103\n1,176\n107\n\n\n\n\n\n\n\n\n\n\nCodepander(tvte, caption=\"Varied fit to test only\")\npander(tvtetr,caption=\"Varied fit to train and test\")\npander(tvtr,caption=\"Varied fit to train only\")",
    "crumbs": [
      "Model",
      "EXAM Group Fits"
    ]
  },
  {
    "objectID": "Model/htw_exam.html#constant-testing-predictions",
    "href": "Model/htw_exam.html#constant-testing-predictions",
    "title": "EXAM Group Fits",
    "section": "Constant Testing Predictions",
    "text": "Constant Testing Predictions\n\nCode##| column: screen-inset-right\n\n####\n\ncte &lt;-  pluck(a_te_c, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_te_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_te_c, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test Only\")\n\nctetr &lt;-  pluck(a_tetr_c, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_tetr_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tetr_c, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") + \n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test and Train\")\n\nctr &lt;-  pluck(a_tr_c, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_tr_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tr_c, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Train Only\")\n  \ncte/ctetr/ctr\n\n\n\n\n\n\nFigure 3: Constant Group - Mean Model predictions vs. observations\n\n\n\n\n\nCode##| column: screen-inset-right\n\n\n\nbest_cPreds &lt;- cPreds %&gt;%\n  pivot_longer(cols = c(ALM, EXAM, Hybrid), names_to = \"Model\", values_to = \"Predicted\") |&gt;\n  mutate(Residual=(Observed-Predicted), abs_res =abs(Residual)) |&gt; group_by(Fit_Method,x) |&gt;\n  mutate(best=if_else(abs_res==min(abs_res),1,0)) |&gt; select(-abs_res)\n\nlong_cPreds &lt;- best_cPreds |&gt; select(-best) |&gt;\n  pivot_longer(cols=c(Predicted,Residual), names_to=\"Model_Perf\") |&gt;\n  relocate(Model, .after=Fit_Method) |&gt; \n  unite(Model,Model,Model_Perf) |&gt;\n  pivot_wider(names_from=Model,values_from=value)\n\nbest_wideC &lt;- best_cPreds |&gt; select(-Residual,-Predicted,-Observed) |&gt; ungroup() |&gt;\n  pivot_wider(names_from=Model,values_from=best) |&gt; select(ALM,EXAM,Hybrid)\n\nbest_indexC &lt;- row_indices &lt;- apply(best_wideC, 1, function(row) {\n which(row == 1)\n})\n\n\nft &lt;- flextable(long_cPreds) %&gt;% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %&gt;% \n  theme_booktabs() %&gt;% \n  merge_v(part = \"header\") %&gt;% \n  merge_h(part = \"header\") %&gt;%\n  align(align = \"center\", part = \"all\") %&gt;% \n  #autofit() %&gt;% \n  empty_blanks() %&gt;% \n  fix_border_issues() %&gt;%\n  hline(part = \"header\", i = 1, j=4:9) %&gt;%\n  vline(j=c(\"Observed\",\"ALM_Residual\",\"EXAM_Residual\")) %&gt;%\n  hline(part = \"body\", i=c(6,12)) |&gt; \n  bold(i=long_cPreds$x %in% c(100,350,600, 1000,1200), j=2) \n\n  # bold the cell with the lowest residual, based on best_wide df\n  # for each row, the cell that should be bolded matches which column in best_wide==1 at that row\n\nft &lt;- apply_best_formatting(ft, best_indexC)\nft\n\n\nTable 5: Constant group - mean model predictions vs. observations. The X values of Extrapolation Bands are bolded. For each Modelling fitting and band combination, the model with the smallest residual is highlighted. Only the lower bound of each velocity band is shown (bands are all 200 units).\n\n\n\n\n\n\n\n\nALM\n\nEXAM\n\nHybrid\n\n\n\nFit Method\nX\nObserved\nPredicted\nResidual\nPredicted\nResidual\nPredicted\nResidual\n\n\n\n\nTest Only\n100\n527\n675\n-148\n717\n-190\n717\n-190\n\n\nTest Only\n350\n666\n675\n-9\n822\n-156\n821\n-155\n\n\nTest Only\n600\n780\n675\n105\n927\n-147\n926\n-146\n\n\nTest Only\n800\n980\n675\n305\n1,010\n-30\n1,009\n-29\n\n\nTest Only\n1,000\n1,163\n675\n488\n1,094\n69\n1,093\n70\n\n\nTest Only\n1,200\n1,277\n675\n602\n1,178\n99\n1,176\n101\n\n\nTest & Train\n100\n527\n675\n-148\n712\n-185\n711\n-184\n\n\nTest & Train\n350\n666\n675\n-9\n806\n-140\n800\n-134\n\n\nTest & Train\n600\n780\n675\n105\n900\n-120\n889\n-109\n\n\nTest & Train\n800\n980\n859\n121\n975\n5\n960\n20\n\n\nTest & Train\n1,000\n1,163\n675\n488\n1,049\n114\n1,031\n132\n\n\nTest & Train\n1,200\n1,277\n675\n602\n1,124\n153\n1,102\n175\n\n\nTrain Only\n100\n527\n675\n-148\n697\n-170\n675\n-148\n\n\nTrain Only\n350\n666\n675\n-9\n752\n-86\n675\n-9\n\n\nTrain Only\n600\n780\n675\n105\n807\n-27\n675\n105\n\n\nTrain Only\n800\n980\n851\n129\n851\n129\n833\n147\n\n\nTrain Only\n1,000\n1,163\n675\n488\n895\n268\n675\n488\n\n\nTrain Only\n1,200\n1,277\n675\n602\n939\n338\n675\n602",
    "crumbs": [
      "Model",
      "EXAM Group Fits"
    ]
  },
  {
    "objectID": "Misc/quartile_splits.html",
    "href": "Misc/quartile_splits.html",
    "title": "Comparing, High/Low performers",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,lme4,emmeans,here,knitr,kableExtra,gt,gghalves)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\n\ntrain &lt;- e1 |&gt; filter(expMode %in% c(\"train\"))\ntrainAvg &lt;- train %&gt;% group_by(id, condit, vb, bandInt,trainStage) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist))\ntest &lt;- e1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\"))\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist),sdist=mean(sdist))\n\n\n\nCodetestAvg %&gt;% \n  group_by(condit,vb) %&gt;%  \n  reframe(enframe(quantile(dist, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"dist\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=dist,names_prefix=\"Q_\") |&gt;\n  group_by(vb,condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean)))\n\n# A tibble: 12 × 7\n# Groups:   vb [6]\n   vb    condit `Q_0%_mean` `Q_25%_mean` `Q_50%_mean` `Q_75%_mean` `Q_100%_mean`\n   &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 100-… Const…       0            110.          205.         393.          849.\n 2 100-… Varied       1.81          82.4         249.         634.         1260.\n 3 350-… Const…       7.34          91.5         149.         254.          757.\n 4 350-… Varied       4.21          81.2         180.         433.          998.\n 5 600-… Const…       9.52          78.9         117.         196.          523.\n 6 600-… Varied      28.4          105.          182.         284.          787.\n 7 800-… Const…      12.9           86.3         145.         252.          896.\n 8 800-… Varied       0.670        128.          195.         308.          634.\n 9 1000… Const…       2.99         122.          200.         317.          824.\n10 1000… Varied      14.5          111.          182.         298.          540.\n11 1200… Const…      26.9          158.          240.         398.          909.\n12 1200… Varied       3.65         141.          209.         320.          594.\n\nCodetestAvg %&gt;% \n  group_by(condit,vb) %&gt;%  \n  reframe(enframe(quantile(dist, seq(0,1,1/8)), \"quantile\", \"dist\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=dist,names_prefix=\"Q_\") |&gt;\n  group_by(vb,condit) |&gt;\n  summarise(across(starts_with(\"Q\"),round,1)) %&gt;% kable(format=\"html\",escape=F) %&gt;% kable_styling() \n\n\n\n\nvb\ncondit\nQ_0%\nQ_12.5%\nQ_25%\nQ_37.5%\nQ_50%\nQ_62.5%\nQ_75%\nQ_87.5%\nQ_100%\n\n\n\n100-300\nConstant\n0.0\n46.1\n109.7\n141.1\n205.0\n261.5\n393.2\n603.3\n849.4\n\n\n100-300\nVaried\n1.8\n32.1\n82.4\n170.8\n248.9\n520.7\n633.9\n852.8\n1259.7\n\n\n350-550\nConstant\n7.3\n50.9\n91.5\n111.8\n149.5\n187.3\n253.9\n440.1\n757.5\n\n\n350-550\nVaried\n4.2\n41.4\n81.2\n130.9\n180.1\n279.5\n433.1\n699.3\n997.8\n\n\n600-800\nConstant\n9.5\n59.2\n78.9\n98.7\n117.0\n148.5\n195.6\n305.8\n522.6\n\n\n600-800\nVaried\n28.4\n68.8\n104.9\n139.1\n181.7\n211.6\n284.2\n478.9\n786.6\n\n\n800-1000\nConstant\n12.9\n45.2\n86.3\n111.7\n145.0\n197.9\n251.9\n364.7\n896.1\n\n\n800-1000\nVaried\n0.7\n70.6\n127.8\n156.8\n195.1\n226.9\n308.2\n385.5\n633.8\n\n\n1000-1200\nConstant\n3.0\n83.0\n122.2\n156.1\n200.1\n236.3\n317.2\n441.5\n823.7\n\n\n1000-1200\nVaried\n14.5\n69.4\n110.7\n147.3\n181.7\n217.6\n298.3\n368.9\n539.9\n\n\n1200-1400\nConstant\n26.9\n125.5\n157.9\n193.0\n240.5\n308.4\n397.7\n503.8\n908.9\n\n\n1200-1400\nVaried\n3.6\n96.7\n141.3\n170.6\n208.7\n287.1\n320.2\n389.0\n593.9\n\n\n\n\n\n\n\nCoderaw_table &lt;- testAvg %&gt;% \n  group_by(condit,vb) %&gt;%  \n  reframe(enframe(quantile(dist, seq(0,1,1/8)), \"quantile\", \"dist\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=dist,names_prefix=\"Q_\") |&gt;\n  group_by(vb,condit) |&gt;\n  summarise(across(starts_with(\"Q\"), round,1))\n\n\nlong_data &lt;- raw_table %&gt;% \n  pivot_longer(\n    cols = starts_with(\"Q\"),\n    names_to = \"Quartile\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(Quart = str_remove_all(Quartile, \"Q_|%\"), # Remove \"Q_\" and \"%\"\n         Quart = as.numeric(Quart), # Convert Quart to numeric\n         Quart = factor(Quart, levels = sort(unique(Quart)))) \n\n\n\n\n\n\nbold_lower &lt;- function(data, by_group) {\n  ifelse(data &lt; by_group, cell_spec(data, \"html\", bold = T), as.character(data))\n}\n\n# Separate data by condition\nconstant_data &lt;- raw_table %&gt;% filter(condit == \"Constant\")\nvaried_data &lt;- raw_table %&gt;% filter(condit == \"Varied\")\n\n# Apply function to varied data\nvaried_data &lt;- varied_data %&gt;%\n  group_by(vb) %&gt;%\n  mutate(across(starts_with(\"Q\"), function(.x) {\n    col_name &lt;- cur_column()\n    by_group &lt;- constant_data[[col_name]][constant_data$vb == first(vb)]\n    bold_lower(.x, by_group)\n  }, .names = \"{.col}\"))\n\n# Format the constant_data to match the varied_data\nconstant_data &lt;- constant_data %&gt;%\n  group_by(vb) %&gt;%\n  mutate(across(starts_with(\"Q\"), ~as.character(.x), .names = \"{.col}\"))\n\n# Join the constant and varied data frames back together\nfinal_table &lt;- bind_rows(constant_data, varied_data) %&gt;%\n  select(vb, condit, ends_with(\"%\")) %&gt;%\n  arrange(vb, condit)\n\n# Print the table\nfinal_table %&gt;% kable(\"html\", escape = F) %&gt;% kable_styling() %&gt;%\n  pack_rows(index = table(final_table$vb))\n\n\n\n\nvb\ncondit\nQ_0%\nQ_12.5%\nQ_25%\nQ_37.5%\nQ_50%\nQ_62.5%\nQ_75%\nQ_87.5%\nQ_100%\n\n\n\n100-300\n\n\n100-300\nConstant\n0\n46.1\n109.7\n141.1\n205\n261.5\n393.2\n603.3\n849.4\n\n\n100-300\nVaried\n1.8\n32.1\n82.4\n170.8\n248.9\n520.7\n633.9\n852.8\n1259.7\n\n\n350-550\n\n\n350-550\nConstant\n7.3\n50.9\n91.5\n111.8\n149.5\n187.3\n253.9\n440.1\n757.5\n\n\n350-550\nVaried\n4.2\n41.4\n81.2\n130.9\n180.1\n279.5\n433.1\n699.3\n997.8\n\n\n600-800\n\n\n600-800\nConstant\n9.5\n59.2\n78.9\n98.7\n117\n148.5\n195.6\n305.8\n522.6\n\n\n600-800\nVaried\n28.4\n68.8\n104.9\n139.1\n181.7\n211.6\n284.2\n478.9\n786.6\n\n\n800-1000\n\n\n800-1000\nConstant\n12.9\n45.2\n86.3\n111.7\n145\n197.9\n251.9\n364.7\n896.1\n\n\n800-1000\nVaried\n0.7\n70.6\n127.8\n156.8\n195.1\n226.9\n308.2\n385.5\n633.8\n\n\n1000-1200\n\n\n1000-1200\nConstant\n3\n83\n122.2\n156.1\n200.1\n236.3\n317.2\n441.5\n823.7\n\n\n1000-1200\nVaried\n14.5\n69.4\n110.7\n147.3\n181.7\n217.6\n298.3\n368.9\n539.9\n\n\n1200-1400\n\n\n1200-1400\nConstant\n26.9\n125.5\n157.9\n193\n240.5\n308.4\n397.7\n503.8\n908.9\n\n\n1200-1400\nVaried\n3.6\n96.7\n141.3\n170.6\n208.7\n287.1\n320.2\n389\n593.9\n\n\n\n\n\n\n\nCodeggplot(long_data, aes(x = Quart, y = Value, color = condit, group = condit)) +\n  facet_wrap(~ vb) +\n  geom_line() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  labs(title = \"Comparison of Varied and Constant Conditions across Octiles\", \n       x = \"Octile\", \n       y = \"Distance from target\")\n\n\n\n\n\n\n\n\nCodemodel &lt;- lmer(Value ~ condit * Quart + (1|vb), data = long_data)\nsummary(model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Value ~ condit * Quart + (1 | vb)\n   Data: long_data\n\nREML criterion at convergence: 1115.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5486 -0.4061  0.0253  0.3715  4.0652 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n vb       (Intercept) 2096     45.79   \n Residual             8970     94.71   \nNumber of obs: 108, groups:  vb, 6\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)               9.933     42.946   0.231\nconditVaried             -1.067     54.680  -0.020\nQuart12.5                58.383     54.680   1.068\nQuart25                  97.817     54.680   1.789\nQuart37.5               125.467     54.680   2.295\nQuart50                 166.250     54.680   3.040\nQuart62.5               213.383     54.680   3.902\nQuart75                 291.650     54.680   5.334\nQuart87.5               433.267     54.680   7.924\nQuart100                783.100     54.680  14.321\nconditVaried:Quart12.5   -4.083     77.330  -0.053\nconditVaried:Quart25      1.367     77.330   0.018\nconditVaried:Quart37.5   18.250     77.330   0.236\nconditVaried:Quart50     24.250     77.330   0.314\nconditVaried:Quart62.5   68.317     77.330   0.883\nconditVaried:Quart75     79.133     77.330   1.023\nconditVaried:Quart87.5   86.933     77.330   1.124\nconditVaried:Quart100     9.983     77.330   0.129\n\nCodelibrary(multcomp)\ncomparison &lt;- glht(model, linfct = mcp(condit = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = Value ~ condit * Quart + (1 | vb), data = long_data)\n\nLinear Hypotheses:\n                       Estimate Std. Error z value Pr(&gt;|z|)\nVaried - Constant == 0   -1.067     54.680   -0.02    0.984\n(Adjusted p values reported -- single-step method)\n\n\n\nCoderaw_table &lt;- test %&gt;% \n  group_by(id,condit,vb) %&gt;%  \n  reframe(enframe(quantile(dist, seq(0,1,1/6)), \"quantile\", \"dist\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=dist,names_prefix=\"Q_\") |&gt;\n  group_by(id,vb,condit) |&gt;\n  summarise(across(starts_with(\"Q\"), round,1))\n\n\nlong_data &lt;- raw_table %&gt;% \n  pivot_longer(\n    cols = starts_with(\"Q\"),\n    names_to = \"Quartile\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(Quart = str_remove_all(Quartile, \"Q_|%\"), # Remove \"Q_\" and \"%\"\n         Quart = as.numeric(Quart), # Convert Quart to numeric\n         Quart = factor(Quart, levels = sort(unique(Quart)))) \n\n\nggplot(long_data, aes(x = Quart, y = Value, fill = condit, color=condit,group = condit)) +\n  facet_wrap(~ vb) +\n  stat_summary(geom=\"line\",fun=mean)+\n  stat_summary(geom=\"errorbar\",fun.data=mean_se)+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  labs(title = \"Comparison of Varied and Constant Conditions across Octiles\", \n       x = \"Octile\", \n       y = \"Distance from target\")\n\n\n\n\n\n\nCodemodel &lt;- lmer(Value ~ condit * Quart + (1|vb)+(1|id), data = long_data)\nsummary(model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Value ~ condit * Quart + (1 | vb) + (1 | id)\n   Data: long_data\n\nREML criterion at convergence: 91516.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5591 -0.5577 -0.0522  0.4545  7.2509 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 16898    129.99  \n vb       (Intercept)  2466     49.66  \n Residual             36196    190.25  \nNumber of obs: 6832, groups:  id, 166; vb, 6\n\nFixed effects:\n                           Estimate Std. Error t value\n(Intercept)                   32.90      25.86   1.273\nconditVaried                  11.29      23.65   0.477\nQuart16.66667                 44.20      11.80   3.746\nQuart33.33333                 96.86      11.80   8.209\nQuart50                      158.69      11.80  13.449\nQuart66.66667                234.59      11.80  19.882\nQuart83.33333                334.62      11.80  28.360\nQuart100                     521.83      11.80  44.226\nconditVaried:Quart16.66667    17.30      17.26   1.002\nconditVaried:Quart33.33333    26.67      17.26   1.545\nconditVaried:Quart50          28.93      17.26   1.676\nconditVaried:Quart66.66667    30.17      17.26   1.748\nconditVaried:Quart83.33333    39.94      17.26   2.314\nconditVaried:Quart100         40.22      17.26   2.330\n\nCodecomparison &lt;- glht(model, linfct = mcp(condit = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = Value ~ condit * Quart + (1 | vb) + (1 | id), \n    data = long_data)\n\nLinear Hypotheses:\n                       Estimate Std. Error z value Pr(&gt;|z|)\nVaried - Constant == 0    11.29      23.65   0.477    0.633\n(Adjusted p values reported -- single-step method)"
  },
  {
    "objectID": "Misc/htw_dp.html",
    "href": "Misc/htw_dp.html",
    "title": "Project 2",
    "section": "",
    "text": "In project 1, we applied model-based techniques to quantify and control for the similarity between training and testing experience, which in turn enabled us to account for the difference between varied and constant training via an extended version of a similarity based generalization model. In project 2, we will go a step further, implementing a full process model capable of both 1) producing novel responses and 2) modeling behavior in both the learning and testing stages of the experiment. Project 2 also places a greater emphasis on extrapolation performance following training - as varied training has often been purported to be particularly beneficial in such situations. Extrapolation has long been a focus of the literature on function learning (Brehmer, 1974; Carroll, 1963). Central questions of the function learning literature have included the relative difficulties of learning various functional forms (e.g. linear vs.bilinear vs. quadratic), and the relative effectiveness of rule-based vs. association-based exemplar models vs. various hybrid models (Bott & Heit, 2004; DeLosh et al., 1997; Jones et al., 2018; Kalish et al., 2004; M. Mcdaniel et al., 2009; M. A. Mcdaniel & Busemeyer, 2005). However the issue of training variation has received surprisingly little attention in this area.",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/htw_dp.html#participants",
    "href": "Misc/htw_dp.html#participants",
    "title": "Project 2",
    "section": "Participants",
    "text": "Participants\nData was collected from 647 participants (after exclusions). The results shown below consider data from subjects in our initial experiment, which consisted of 196 participants (106 constant, 90 varied). The follow-up experiments entailed minor manipulations: 1) reversing the velocity bands that were trained on vs. novel during testing; 2) providing ordinal rather than numerical feedback during training (e.g. correct, too low, too high). The data from these subsequent experiments are largely consistently with our initial results shown below.",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/htw_dp.html#task",
    "href": "Misc/htw_dp.html#task",
    "title": "Project 2",
    "section": "Task",
    "text": "Task\nWe developed a novel visuomotor extrapolation task, termed the “Hit The Wall” (HTW) task, wherein participants learned to launch a projectile such that it hit a rectangle at the far end of the screen with an appropriate amount of force. Although the projectile had both x and y velocity components, only the x-dimension was relevant for the task.  Link to task demo",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/htw_dp.html#design",
    "href": "Misc/htw_dp.html#design",
    "title": "Project 2",
    "section": "Design",
    "text": "Design\n\n90 training trials split evenly divided between velocity bands. Varied training with 3 velocity bands and Constant training with 1 band.\nNo-feedback testing from 3 novel extrapolation bands. 15 trials each.  \nNo-feedbacd testing from the 3 bands used during the training phase (2 of which were novel for the constant group). 9 trials each.\nFeedback testing for each of the 3 extrapolation bands. 10 trials each.\n\n\n\n\nExperiment Procedure",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/htw_dp.html#training",
    "href": "Misc/htw_dp.html#training",
    "title": "Project 2",
    "section": "Training",
    "text": "Training\nTraining performance is shown in Results Figure 2A. All groups show improvement from each of their training velocity-bands (i.e. decreasing average distance from target). In the velocity band trained at by both groups (800-1000), the constant group maintains a superior level of performance from the early through the final stages of training. This difference is unsurprising given that the constant group had 3x more practice trials from that band.\n\nCode#fig.cap=\"\\\\label{fig:figs}training performance\"\n\n#title=paste0(\"HTW Training\")\n#figpatch::fig(here(\"Assets/Training-1.png\"))\nknitr::include_graphics(here(\"Assets/Training-1.png\"))\n\n\n\n\n\n\nCode  #fig_lab(.,title,pos=\"top\",fontface='bold',size=12,hjust=.01)",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/htw_dp.html#testing",
    "href": "Misc/htw_dp.html#testing",
    "title": "Project 2",
    "section": "Testing",
    "text": "Testing\nFor evaluating testing performance, we consider 3 separate metrics. 1) The average absolute deviation from the correct velocity, 2) The % of throws in which the wall was hit with the correct velocity and 3) The average x velocity produced.\nResults Figure 2B shows the average velocity produced for all 6 bands that were tested. At least at the aggregate level, both conditions were able to differentiate all 6 bands in the correct order, despite only having received training feedback for 1/6 (constant) or 3/6 (varied) bands during training. Participants in both groups also had a bias towards greatly overestimating the correct velocity for band 100-300, for which both groups had an average of greater than 500.\n\nCode# fig2aCap=str_wrap(\"Figure 2B: Bands 100-300, 350-550 and 600-800 are novel extrapolations for both groups. Band 800-1000 was a training band for both groups. Bands 1000-1200, and 1200-1400 were trained for the varied group, and novel for the constant group.  Top figure displays mean deviation from correct velocity. Bottom figure displays the average % of trials where participants hit the wall with the correct velocity. Error bars indicate standard error of the mean. \" ,width=170)\n\n\n#figpatch::fig(here(\"Assets/Testing_Vx-1.png\"))\nknitr::include_graphics(here(\"Assets/Testing_Vx-1.png\"))\n\n\n\n\n\n\n\nAs is reflected in Results Figure 2C, the constant group performed significantly better than the varied group at the 3 testing bands of greatest interest. Both groups tended to perform worse for testing bands further away from their training conditions. The varied group had a slight advantage for bands 1000-1200 and 1200-1400, which were repeats from training for the varied participants, but novel for the constant participants.\n\nCode#figpatch::fig(here(\"Assets/Test_Performance-1.png\"))\nknitr::include_graphics(here(\"Assets/Test_Performance-1.png\"))\n\n\n\n\n\n\nCode# \n# gtitle=\"2C. Testing Performance\"\n# title = ggdraw()+draw_label(gtitle,fontface = 'bold',x=0,hjust=0)+theme(plot.margin = margin(0, 0, 0, 7))\n# captionText=str_wrap(\"Figure 2C: Bands 100-300, 350-550 and 600-800 are novel extrapolations for both groups. Band 800-1000 was a training band for both groups. Bands 1000-1200, and 1200-1400 were trained for the varied group, and novel for the constant group.  Right side figure displays mean deviation from correct velocity band (lower values correspond to better performance). Bottom Left displays the average % of trials where participants hit the wall with the correct velocity (higher values correspond got better performance). Error bars indicate standard error of the mean. \",150)\n# capt=ggdraw()+draw_label(captionText,fontface = 'italic',x=0,hjust=0,size=11)+theme(plot.margin = margin(0, 0, 0, 1))\n# \n# plot_grid(title,NULL,leg,NULL,gbDev,gbHit,capt,NULL,ncol=2,rel_heights=c(.1,.1,1,.1),rel_widths=c(1,1))",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/htw_dp.html#alm-exam-description",
    "href": "Misc/htw_dp.html#alm-exam-description",
    "title": "Project 2",
    "section": "ALM & Exam Description",
    "text": "ALM & Exam Description\nDelosh et al. (1997) introduced the associative learning model (ALM), a connectionist model within the popular class of radial-basis networks. ALM was inspired by, and closely resembles Kruschke’s influential ALCOVE model of categorization (Kruscke 1992).\nALM is a localist neural network model, with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on thevalue of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nSee Table 2A for a full specification of the equations that define ALM and EXAM.",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/htw_dp.html#model-equations",
    "href": "Misc/htw_dp.html#model-equations",
    "title": "Project 2",
    "section": "Model Equations",
    "text": "Model Equations\n\nCodetext_tbl &lt;- data.frame(\n    'Step'=c(\"Input Activation\",\"Output Activation\",\"Output Probability\",\"Mean Output\",\"Feedback Activation\",\"Update Weights\",\"Extrapolation\",\"\"),\n    'Equation' = c(\"$a_i(X) = \\\\frac{e^{-c \\\\cdot (X-X_i)^2}}{ \\\\sum_{k=1}^Me^{-c \\\\cdot (X-X_i)^2}}$\", \n                   '$O_j(X) = \\\\sum_{k=1}^Mw_{ji} \\\\cdot a_i(X)$',\n                   '$P[Y_j | X] = \\\\frac{O_i(X)}{\\\\sum_{k=1}^Mo_k(X)}$',\n                   \"$m(x) = \\\\sum_{j=1}^LY_j \\\\cdot \\\\bigg[\\\\frac{O_j(X)}{\\\\sum_{k=1}^Lo_k(X)}\\\\bigg]$\",\n                   \"$f_j(Z)=e^{-c\\\\cdot(Z-Y_j)^2}$\",\n                   \"$w_{ji}(t+1)=w_{ji}(t)+\\\\alpha \\\\cdot {f_i(Z(t))-O_j(X(t))} \\\\cdot a_i(X(t))$\",\n                   \"$P[X_i|X] = \\\\frac{a_i(X)}{\\\\sum_{k=1}^Ma_k(X)}$\",\n                   \"$E[Y|X_i]=m(X_i) + \\\\bigg[\\\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\\\bigg] \\\\cdot[X-X_i]$\"),\n    \n    'Description'= c(\n            \"Activation of each input node, $X_i$, is a function of the Gaussian similarity between the node value and stimulus X. \",\n            \"Activation of each Output unit $O_j$ is the weighted sum of the input activations and association weights\",\n            \"Each output node has associated response, $Y_j$. The probability of response $Y_j$ is determined by the ratio of output activations\",\n            \"The response to stimulus x is the weighted average of the response probabilities\",\n            \"After responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response  \",\n            \"Delta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\",\n            \"Novel test stimulus X activates input nodes associated with trained stimuli\",\n            \"Slope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)\")\n)\ntext_tbl$Step=cell_spec(text_tbl$Step,font_size=12)\ntext_tbl$Equation=cell_spec(text_tbl$Equation,font_size=18)\nalmTable=kable(text_tbl, 'html', \n  booktabs=T, escape = F, align='l',\n  caption = '&lt;span style = \"color:black;\"&gt;&lt;center&gt;&lt;strong&gt;Table 1: ALM & EXAM Equations&lt;/strong&gt;&lt;/center&gt;&lt;/span&gt;',\n  col.names=c(\"\",\"Equation\",\"Description\")) %&gt;%\n  kable_styling(position=\"left\",bootstrap_options = c(\"hover\")) %&gt;%\n  column_spec(1, bold = F,border_right=T) %&gt;%\n  column_spec(2, width = '10cm')%&gt;%\n  column_spec(3, width = '15cm') %&gt;%\n  pack_rows(\"ALM Activation & Response\",1,4,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"ALM Learning\",5,6,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"EXAM\",7,8,bold=FALSE,italic=TRUE)\n  #save_kable(file=\"almTable.html\",self_contained=T)\n#almTable\n\n\ncat(almTable)\n\n\n\n\nTable 1: ALM & EXAM Equations\n\n\n\n\n\n\n\nEquation\n\n\nDescription\n\n\n\n\n\nALM Activation & Response\n\n\n\n\nInput Activation\n\n\n\\(a_i(X) = \\frac{e^{-c \\cdot (X-X_i)^2}}{ \\sum_{k=1}^Me^{-c \\cdot (X-X_i)^2}}\\)\n\n\nActivation of each input node, \\(X_i\\), is a function of the Gaussian similarity between the node value and stimulus X.\n\n\n\n\nOutput Activation\n\n\n\\(O_j(X) = \\sum_{k=1}^Mw_{ji} \\cdot a_i(X)\\)\n\n\nActivation of each Output unit \\(O_j\\) is the weighted sum of the input activations and association weights\n\n\n\n\nOutput Probability\n\n\n\\(P[Y_j | X] = \\frac{O_i(X)}{\\sum_{k=1}^Mo_k(X)}\\)\n\n\nEach output node has associated response, \\(Y_j\\). The probability of response \\(Y_j\\) is determined by the ratio of output activations\n\n\n\n\nMean Output\n\n\n\\(m(x) = \\sum_{j=1}^LY_j \\cdot \\bigg[\\frac{O_j(X)}{\\sum_{k=1}^Lo_k(X)}\\bigg]\\)\n\n\nThe response to stimulus x is the weighted average of the response probabilities\n\n\n\n\nALM Learning\n\n\n\n\nFeedback Activation\n\n\n\\(f_j(Z)=e^{-c\\cdot(Z-Y_j)^2}\\)\n\n\nAfter responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response\n\n\n\n\nUpdate Weights\n\n\n\\(w_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t))\\)\n\n\nDelta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\n\n\n\n\nEXAM\n\n\n\n\nExtrapolation\n\n\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^Ma_k(X)}\\)\n\n\nNovel test stimulus X activates input nodes associated with trained stimuli\n\n\n\n\n\n\n\n\\(E[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg] \\cdot[X-X_i]\\)\n\n\nSlope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/htw_dp.html#model-fitting-and-comparison",
    "href": "Misc/htw_dp.html#model-fitting-and-comparison",
    "title": "Project 2",
    "section": "Model Fitting and Comparison",
    "text": "Model Fitting and Comparison\nFollowing the procedure used by McDaniel & Busemeyer (2009), we will assess the ability of both ALM and EXAM to account for the empirical data when fitting the models to 1) only the training data, and 2) both training and testing data. Models will be fit directly to the trial by trial data of each individual participants, both by minimizing the root-mean squared deviation (RMSE), and by maximizing log likelihood. Because ALM has been shown to do poorly at accounting for human patterns extrapolation (DeLosh et al., 1997), we will also fit the extended EXAM version of the model, which operates identically to ALM during training, but includes a linear extrapolation mechanism for generating novel responses during testing.",
    "crumbs": [
      "Misc",
      "Project 2"
    ]
  },
  {
    "objectID": "Misc/e1_brms_test.html",
    "href": "Misc/e1_brms_test.html",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes,brms,bayesplot,bayestestR,\n               broom.mixed,lme4,emmeans,here,knitr,kableExtra,gt)\nwalk(c(here(\"Functions/Display_Functions.R\"), here(\"Functions/org_functions.R\")), source)\n\ntest &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) |&gt; \n  filter(expMode2 == \"Test\") |&gt; \n  mutate(distS2 = custom_scale(dist))\n\noptions(brms.backend=\"cmdstanr\",mc.cores=4)"
  },
  {
    "objectID": "Misc/e1_brms_test.html#random-effects---interaction-condit-x-vb",
    "href": "Misc/e1_brms_test.html#random-effects---interaction-condit-x-vb",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Random Effects - Interaction condit x vb",
    "text": "Random Effects - Interaction condit x vb\n\nCodemodelName &lt;- \"e1_testDistBT_RF\"\ne1_testDistBT_RF &lt;- brm(dist ~ (condit * vb) + bandType + (1 + vb +bandType|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=2000,chains=4,silent=0,\n                      control=list(adapt_delta=0.92, max_treedepth=13))"
  },
  {
    "objectID": "Misc/e1_brms_test.html#random-effects---interaction-condit-x-vb-1",
    "href": "Misc/e1_brms_test.html#random-effects---interaction-condit-x-vb-1",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Random Effects - Interaction condit x vb",
    "text": "Random Effects - Interaction condit x vb\n\nCodemodelName &lt;- \"e1_testDistRF2\"\ne1_testDistRF2 &lt;- brm(dist ~ condit * vb + (1 + vb|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\nGetModelStats(e1_testDistRF2)\n\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDistRF2, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nbayes_R2(e1_testDistRF2)\nsjPlot::tab_model(e1_testDistRF2)\n\n\nfixef(e1_testDistRF2)\nnewdat &lt;-data.frame(crossing(condit=c(\"Constant\",\"Varied\"), vb = unique(test$vb)))\npreds &lt;- fitted(e1_testDistRF2, re_formula = NA, newdata = newdat, probs = c(0.025, 0.975))\n\n\n#shinystan::launch_shinystan(e1_testDistRF2)"
  },
  {
    "objectID": "Misc/e1_brms_test.html#intercept-random-effects---interaction-condit-x-vb",
    "href": "Misc/e1_brms_test.html#intercept-random-effects---interaction-condit-x-vb",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "0 intercept; Random Effects - Interaction condit x vb",
    "text": "0 intercept; Random Effects - Interaction condit x vb\n\nCodemodelName &lt;- \"e1_testDistRF2_0\"\ne1_testDistRF2_0 &lt;- brm(dist ~ 0 + (condit * vb) + (0 + vb|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\",modelName)), \n                        iter=5000,chains=4)\nGetModelStats(e1_testDistRF2_0)\n\nkable(fixef(e1_testDistRF2_0))\nnewdat &lt;-data.frame(crossing(condit=c(\"Constant\",\"Varied\"), vb = unique(test$vb)))\npreds &lt;- fitted(e1_testDistRF2_0, re_formula = NA, newdata = newdat, probs = c(0.025, 0.975))\n\n\nkable(tidy(e1_testDistRF2_0, effects=\"fixed\",ess=TRUE))\nconditional_effects(e1_testDistRF2_0,\"condit:vb\",method=\"pp_expect\",points=TRUE)\n\n\ndraws_fit &lt;- as_draws_df(e1_testDistRF2_0, variable = \"^b_\", regex = TRUE)\n\n\nmcmc_plot(e1_testDistRF2_0, type=\"trace\",variable=\"^b_\",regex=TRUE)\nmcmc_plot(e1_testDistRF2_0, type=\"intervals\",variable=\"^b_\",regex=TRUE)\nmcmc_plot(e1_testDistRF2_0, type=\"areas\",variable=\"^b_\",regex=TRUE)\n\nmcmc_hist(e1_testDistRF2_0,pars=c(\"b_conditConstant\",\"b_conditVaried\"))\nplot(e1_testDistRF2_0,variable=c(\"b_conditConstant\",\"b_conditVaried\"))\n\n\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(e1_testDistRF2_0,ndraws=200),group=test$vb)\n\n\nmcmc_hist(e1_testDistRF2_0,prob=.5,regex_pars=c(\"^r_id\\\\[1,.*\\\\]\"))\n\n\nmodel_parameters(e1_testDistRF2_0,effects=\"random\",keep=\"^r_id\\\\[3,.*\\\\]\")\nplot_subject_fits(e1_testDistRF2_0,3)\n\n\nindvFit &lt;- GetIndvFits(e1_testDistRF2_0)\n\nindvFit |&gt; ggplot(aes(x=condit,y=Median,fill=condit))+stat_halfeye()+facet_wrap(~vb)"
  },
  {
    "objectID": "Misc/e1_brms_test.html#random-effects",
    "href": "Misc/e1_brms_test.html#random-effects",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Random Effects",
    "text": "Random Effects\nCodemodelName &lt;- \"e1_testDistRF\"\ne1_testDistRF &lt;- brm(dist ~ condit + vb + (1 + vb|id),\n                     data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\n\n\npt &lt;- posterior_table(e1_testDistRF) |&gt; select(-CI)\nkable(pt)\nbrms_posterior_checks(e1_testDistRF,dist,vb)\n\nmap_estimate(e1_testDistRF)\n\n\nhdi(e1_testDistRF$fit, ci = c(0.5, 0.75, 0.89, 0.95))\n\n\nplot(pt)\n\nintPlot &lt;- plot(conditional_effects(e1_testDistRF,effects=\"vb:condit\"))\n\n\nmd &lt;- tidy_draws(e1_testDistRF) |&gt; select(b_Intercept:`b_vb1200M1400`)\nplot(bayestestR::hdi(md, ci = c(.89, .95)))\n\nplot(bayestestR::bayesfactor_parameters(e1_testDistRF, null = c(-.5, .5)))\n\np1 &lt;- GetModelStats(e1_testDistRF)\n\nkable(p1) |&gt; column_spec(1:9,width=\"5em\")\n\n#GetBrmsModelStats(e1_testDistRF)\n\n\n\ne1_testDistRF %&gt;%\n  spread_draws(b_Intercept, r_condition[condition,])\n\n\ndraws1 &lt;- e1_testDistRF |&gt; spread_draws()"
  },
  {
    "objectID": "Misc/e1_brms_test.html#interaction---grouped-random-effects",
    "href": "Misc/e1_brms_test.html#interaction---grouped-random-effects",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Interaction - Grouped Random Effects",
    "text": "Interaction - Grouped Random Effects\n\nCodemodelName &lt;- \"e1_testConditVb_Dist_Gr\"\ne1_testConditVb_Dist_Gr &lt;- brm(dist ~ condit * vb + (1 + vb|gr(id,by=condit)),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nfixef(e1_testConditVb_Dist_Gr)\nhead(coef(e1_testConditVb_Dist_Gr)$id)\nbayes_R2(e1_testConditVb_Dist_Gr)\n\nGetModelStats(e1_testConditVb_Dist_Gr)\n\n# coef(e1_testConditVb_Dist_Gr)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id, starts_with(\"Est\")) |&gt; print(n=10)\n\nindividual_coefs &lt;- coef(e1_testConditVb_Dist_Gr)$id %&gt;%\n    as_tibble(rownames = \"id\") %&gt;%\n    select(id, starts_with(\"Estim\")) %&gt;%\n    pivot_longer(cols = -id, names_to = \"variable\", values_to = \"value\") %&gt;%\n    separate(variable, c(\"type\", \"effect\"), sep = \"\\\\.\")\n\nmerged_data &lt;- test |&gt; group_by(id, condit) |&gt; summarise(n=n()) %&gt;%\n               left_join(individual_coefs, by = \"id\") |&gt;  filter(effect == \"Intercept\" | grepl(\"^vb\", effect))\n\n\nhead(merged_data)\n\n\n\nggplot(merged_data, aes(x = effect, y = value, color = condit)) +\n  geom_point(position = position_jitterdodge()) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    title = \"Individual Differences in Velocity Production Task (Intercept & vb Effects)\",\n    x = \"Velocity Band Effect\",\n    y = \"Estimated Value\",\n    color = \"Condition\"\n  )\n\n\n\n epred_draws(e1_testConditVb_Dist_Gr, newdata = test,ndraws=50) |&gt;\nggplot(aes(x = vb, y = .epred, color = condit)) +\n    gghalves::geom_half_violin(position=position_dodge(),alpha=.7) +\n    gghalves::geom_half_boxplot(position=position_dodge(),alpha=.5) +\n    #geom_jitter(width = 0.2, height = 0) +\n    labs(\n        title = \"Posterior Predictive Distribution\",\n        x = \"Effect Category\",\n        y = \"Expected Predicted Value\",\n        color = \"Condition\"\n    )\n\n\n(r_fit &lt;- e1_testConditVb_Dist_Gr %&gt;% \n  tidy() %&gt;% filter(effect==\"fixed\") |&gt; select(-effect,-component, -group) |&gt; \n  mutate(term = janitor::make_clean_names(term)) |&gt;\n    mutate(across(where(is.numeric), \\(x) round(x, 0))) |&gt; kbl() |&gt;\n    column_spec(1:5, width = \"5em\") ) \n \ncat(r_fit$term)\npaste(r_fit$term)"
  },
  {
    "objectID": "Misc/e1_brms_test.html#intercept-random-effects---interaction-condit-x-vb-1",
    "href": "Misc/e1_brms_test.html#intercept-random-effects---interaction-condit-x-vb-1",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "0 intercept; Random Effects - Interaction condit x vb",
    "text": "0 intercept; Random Effects - Interaction condit x vb\n\nCodemodelName &lt;- \"e1_testDistRF2_02\"\ne1_testDistRF2_02 &lt;- brm(dist ~ 0 + condit:vb + (0 + vb|id),data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDistRF2_02, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nGetModelStats(e1_testDistRF2_02)\n\nbayes_R2(e1_testDistRF2_02)\nsjPlot::tab_model(e1_testDistRF2_02)\n\n\nfixef(e1_testDistRF2_02)\nnewdat &lt;-data.frame(crossing(condit=c(\"Constant\",\"Varied\"), vb = unique(test$vb)))\npreds &lt;- fitted(e1_testDistRF2_02, re_formula = NA, newdata = newdat, probs = c(0.025, 0.975))\n\n\n#shinystan::launch_shinystan(e1_testDistRF2_02)\n\n\n\nCodemodelName &lt;- \"e1_test_Dist_Int\"\ne1_testDist_Int &lt;- brm(dist ~ 0 + Intercept + bandInt + condit + (1|id), \n                       data=test,\n                       iter=1000, chains=4, silent=0,\n                       file=paste0(here::here(\"data/model_cache\",modelName)))\n\ne1_testDist_Int\n\npp_check(e1_testDist_Int,group=\"bandInt\")\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(e1_testDist_Int,ndraws=100),group=test$bandInt)\npp_check(e1_testDist_Int,type=\"stat_grouped\",ndraws=500, group=\"bandInt\",stat=\"mean\")\ncoef(e1_testDist_Int)$id\n\n\n\nCodemodelName &lt;- \"e1Test_conditBand_RS1\"\ne1Test_conditBand_RS1 &lt;- brm(dist ~ 1 + bandInt + condit + (1+ bandInt|id), \n                       data=test,\n                       iter=1000, chains=4, silent=0,\n                       file=paste0(here::here(\"data/model_cache\",modelName)))\n\ne1Test_conditBand_RS1\nfixef(e1Test_conditBand_RS1)\n\npp_check(e1Test_conditBand_RS1,group=\"bandInt\")\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(e1Test_conditBand_RS1,ndraws=100),group=test$bandInt)\npp_check(e1Test_conditBand_RS1,type=\"stat_grouped\",ndraws=500, group=\"bandInt\",stat=\"mean\")\ncoef(e1Test_conditBand_RS1)$id\n\n\ne1Test_conditBand_RS1 |&gt; \n  tidy_draws() |&gt; \n  select(starts_with(\"b_\"),.chain,.iteration,.draw) \n  \n  \ne1Test_conditBand_RS1 |&gt; \n  spread_draws(b_Intercept,b_conditVaried) \n\n\n\nCodemodelName &lt;- \"e1Test_conditVb_RS1\"\ne1Test_conditVb_RS1 &lt;- brm(dist ~ 1 + vb + condit + (1+ vb|id), \n                       data=test,\n                       iter=1000, chains=4, silent=0,\n                       file=paste0(here::here(\"data/model_cache\",modelName)))\n\ne1Test_conditVb_RS1\n\npp_check(e1Test_conditVb_RS1,group=\"bandInt\")\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(e1Test_conditVb_RS1,ndraws=100),group=test$bandInt)\npp_check(e1Test_conditVb_RS1,type=\"stat_grouped\",ndraws=500, group=\"vb\",stat=\"mean\")\ncoef(e1Test_conditVb_RS1)$id"
  },
  {
    "objectID": "Misc/e1_brms_test.html#fixed-effects-only",
    "href": "Misc/e1_brms_test.html#fixed-effects-only",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Fixed Effects Only",
    "text": "Fixed Effects Only\n\nCodemodelName &lt;- \"e1_testDist\"\ne1_testDist &lt;- brm(dist ~ condit,data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDist, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\n\nmodelName &lt;- \"e1_testDist0\"\ne1_testDist0 &lt;- brm(dist ~ 0+condit,data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDist0, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\n\nmodelName &lt;- \"e1_testDist_uneq\"\n\ne1_testDist_uneq &lt;- brm(bf(dist ~ condit,sigma~condit),data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\n\nbrms_eq_tidy_uneq &lt;-tidyMCMC(e1_testDist_uneq, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nbrms_eq_tidy_uneq |&gt; mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\n\nmodelName &lt;- \"e1_testDist_uneq_robust\"\nbrms_uneq_robust &lt;- brm(\n  bf(dist ~ condit, sigma ~ condit), data=test,\n  family = student,file=paste0(here::here(\"data/model_cache\",modelName)))\n\nbrms_uneq_robust_tidy &lt;- tidyMCMC(brms_uneq_robust, conf.int = TRUE, conf.level = 0.95,estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;% mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\nbrms_uneq_robust_tidy\n\n\n\n#brm(dist ~ (0+vb)+(1+condit),data=test)\n\n\nbayes_R2(e1_testDist)\nsjPlot::tab_model(e1_testDist)"
  },
  {
    "objectID": "Misc/e1_brms_test.html#vx",
    "href": "Misc/e1_brms_test.html#vx",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Vx",
    "text": "Vx\n\nCodemodelName &lt;- \"e1_testVxRF\"\ne1_testDistRF &lt;- brm(vx ~ condit + bandInt+ (1 + vb|id),\n                     data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\n\nmodelName &lt;- \"e1_testVxRF2\"\ne1_testVxRF2 &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\n\nmodelName &lt;- \"e1_test_vx_Int\"\ne1_testDist_Int &lt;- brm(vx ~ 0 + Intercept + bandInt + condit + (1|id), \n                       data=test,\n                       iter=2000, chains=4, silent=0,\n                       file=paste0(here::here(\"data/model_cache\",modelName)))\n\n\nmodelName &lt;- \"e1_testVxRF2_02\"\ne1_testDistRF2_02 &lt;- brm(vx ~ 0 + condit:bandInt + (0 + bandInt|id),data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDistRF2_02, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\n\n\nmodelName &lt;- \"e1_testConditBand_vx_Gr\"\ne1_testConditBand_vx_Gr &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|gr(id,by=condit)),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\nmodelName &lt;- \"e1_testConditBand0_vx_Gr\"\ne1_testConditBand_vx_Gr &lt;- brm(vx ~ 0 + condit * bandInt + (0 + bandInt|gr(id,by=condit)),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\nmodelName &lt;- \"e1_testConditBand02_vx_Gr\"\ne1_testConditBand_vx_Gr &lt;- brm(vx ~ 1 + condit * bandInt + (0 + bandInt|gr(id,by=condit)),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4,silent=0)\n\n\n\nCodebform1 &lt;- bf(mvbind(vx, dist) ~ condit * vb + (1|p|id)) + set_rescor(TRUE)\nmv1 &lt;- brm(bform1, data = test, chains = 2, cores = 2)\n\nconditional_effects(mv1)\npp_check(mv1,resp=\"dist\")\npp_check(mv1,resp=\"vx\")\n\n\nppc_dens_overlay_grouped(test$vx,posterior_predict(mv1,ndraws=200),test$vb)\nbayes_R2(mv1)\n\n\nbf_dist &lt;- bf(dist|trunc(lb=-70) ~ condit * vb + (0 + vb|id) ) + gaussian()\nbf_vx &lt;- bf(vx|trunc(lb=0) ~ condit * vb + (0 + vb|id)) + gaussian()\n\nmv3 &lt;- brm(bf_dist + bf_vx + set_rescor(FALSE),\n            data = test, chains = 2, cores = 2,iter=1300, silent=0,\n           file=here::here(\"data/model_cache/mv_trunc2\"))\nbayes_R2(mv3)\npp_check(mv3,resp=\"dist\")\n\nppc_dens_overlay_grouped(test$vx,posterior_predict(mv3,ndraws=200),test$vb)\n\n\n\nbform2 &lt;- bf(mvbind(vx, vy) ~ condit * vb + (0+vb|id)) + set_rescor(FALSE)\nmv3 &lt;- brm(bform2, data = test, chains = 2, cores = 2,silent=0)\nbayes_R2(mv3)\npp_check(mv3,resp=\"vx\")\npp_check(mv3,resp=\"vy\")\n\n\n\nfitted(mv3) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(test %&gt;% select(id,condit,vx,vy,vb,bandInt)) %&gt;%\n  ggplot(aes(x = vx, y = Estimate.vx)) +\n  geom_abline(linetype = 2, color = \"grey50\", linewidth = .5) +  \n  geom_point(size = 1.5, aes(color=), alpha = 3/4) +\n  geom_linerange(aes(ymin = Q2.5.vx, ymax = Q97.5.vx),\n                 size = 1/4, color = \"firebrick4\") +\n  facet_wrap(~vb)\n\n\n\nCodeconditRF &lt;- brm(vx ~ vb + condit + (1+vb|condit),data=test,iter=1000,chains=2)\n\nconditRFX &lt;- brm(vx ~ vb * condit + (1+vb|condit),data=test,iter=1000,chains=2,silent=0,\n                 file=here(\"data/model_cache/e1_conditRFX\"))\n\n\nconditRFX %&gt;%\n  spread_draws(b_Intercept, r_condit[condit,]) %&gt;%\n  mutate(condition_mean = b_Intercept + r_condit) %&gt;%\n  ggplot(aes(y = condit, x = condition_mean)) +\n  stat_halfeye() + facet_wrap(~vb)\n\n\n\nCodevx_mm4 &lt;- brm(vx ~ (1+vb|id), data=test,chains=2,silent=0)\n\nmcmc_areas(vx_mm4,prob=.5,regex_pars=c(\"^r_id\\\\[1,.*\\\\]\"),regex=T)\n\n\nvx_mm5&lt;- brm(vx ~ 0 + (0+vb|id), data=test,chains=2,silent=0,iter=1200)\n\n\nplot_subject_fits &lt;- function(model, subject_code) {\n  pattern &lt;- glue(\"^r_id\\\\[{subject_code},.*\\\\]\")\n  plot &lt;- mcmc_areas(model, prob = .5, regex_pars = c(pattern)) +\n            ggtitle(glue(\"fit for subject #{subject_code}:\"))\n  return(plot)\n}\n\nplot_subject_fits(vx_mm5,86)\n\n\nindividual_coefs &lt;- coef(gt_vx)$id %&gt;% as_tibble(rownames = “id”) %&gt;% select(id, starts_with(“Estim”)) %&gt;% pivot_longer(cols = -id, names_to = “variable”, values_to = “value”) %&gt;% separate(variable, c(“type”, “effect”), sep = “\\.”)\ndist_gauss_trunc0 &lt;- brm(dist|trunc(lb=0) ~ 1+vb*condit + (0+vb|id),data=testExtrap,family=gaussian(), iter=2000, control=list(adapt_delta=0.92, max_treedepth=13), chains=3, file=paste0(here::here(“data/model_cache”,“gauss_t_vbCondit_dist_extrap_ml6”)), silent=0);\ngauss_trunc0 &lt;- brm(vx|trunc(lb=0) ~ 1+vb*condit + (0+vb|id),data=testExtrap,family=gaussian(), iter=2000, control=list(adapt_delta=0.92, max_treedepth=13), chains=3, file=paste0(here::here(“data/model_cache”,“gauss_t_vbCondit_vx_extrap_ml6”)), silent=0);\nintercept condit_varied vb350m550 vb600m800 vb800m1000 vb1000m1200 vb1200m1400 condit_varied_vb350m550 condit_varied_vb600m800 condit_varied_vb800m1000 condit_varied_vb1000m1200 condit_varied_vb1200m1400"
  },
  {
    "objectID": "Misc/data_dictionary.html",
    "href": "Misc/data_dictionary.html",
    "title": "HTW Data Dictionary",
    "section": "",
    "text": "id - unique subject identifier condit - primary training manipulation, Constant vs. Varied. tOrder - order of blocks in no-feedback testing stage (extrapolation first or re-do train first) bandType - whether band was trained on (trained) or novel in test phase (extrapolation) expMode2 - Phase of Experiment - Train, Train-Nf, Test, Test-Fb gt.train - training trial number - running total vb - velocity band on current trial - Levels: 100-300 350-550 600-800 800-1000 1000-1200 1200-1400 bandInt - numeric value of lower bound of vb: 100, 350, 600, 800, 1000, 1200 vx - X velocity generated by subject on current trial dist - absolute distance from edge of target velocity band (0 if vx falls within band) sdist - signed distance from edge of target velocity band vy - y velocity generated by subject (not relevant to task) result - categorical result of throw in relation to target band - Levels: Over, Under, Hit vxCat - which velocity band the current trial throw falls into prev - Target velocity band on previous throw bandSeq - Previous Target Band and current Target Band, e.g. 800-&gt;1000 1000-&gt;1200 etc.\n\nCodepacman::p_load(tidyverse, lme4, emmeans, here, knitr, kableExtra,gt)\noptions(dplyr.summarise.inform = FALSE)\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\")) %&gt;% ungroup()\nd$stage &lt;- factor(d$stage, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\"))\nd$tOrder &lt;- factor(d$tOrder, levels = c(\"testFirst\", \"trainFirst\"), labels = c(\"Test First\", \"Train First\"))\nd$bandOrder &lt;- factor(d$bandOrder, levels = c(\"orig\", \"rev\"), labels = c(\"Original\", \"Reverse\"))\nd$fb &lt;- factor(d$fb, levels = c(\"continuous\", \"ordinal\"), labels = c(\"Continuous\", \"Ordinal\"))\nd &lt;- d %&gt;%\n    mutate(id = as.factor(d$id), fullCond = as.factor(fullCond)) %&gt;%\n    relocate(bandOrder, .after = fb)\n\n\n\nCode# create table with number of unique ids per condition, use kable\nd %&gt;% select(fullCond,id) %&gt;% distinct() %&gt;% group_by(fullCond) %&gt;% summarise(n=n()) %&gt;% knitr::kable()\n\n\n\n\nfullCond\nn\n\n\n\nconstant.testFirst.orig.continuous\n52\n\n\nconstant.testFirst.orig.ordinal\n51\n\n\nconstant.testFirst.rev.continuous\n28\n\n\nconstant.testFirst.rev.ordinal\n31\n\n\nconstant.trainFirst.orig.continuous\n38\n\n\nconstant.trainFirst.rev.continuous\n27\n\n\nconstant.trainFirst.rev.ordinal\n28\n\n\nvaried.testFirst.orig.continuous\n39\n\n\nvaried.testFirst.orig.ordinal\n39\n\n\nvaried.testFirst.rev.continuous\n25\n\n\nvaried.testFirst.rev.ordinal\n28\n\n\nvaried.trainFirst.orig.continuous\n37\n\n\nvaried.trainFirst.rev.continuous\n30\n\n\nvaried.trainFirst.rev.ordinal\n18\n\n\n\n\n\nCodet1=d %&gt;% select(condit,fb,bandOrder,tOrder,id) %&gt;% distinct() %&gt;% \n  group_by(condit,fb,bandOrder,tOrder) %&gt;% summarise(n=n()) \nt1 %&gt;% kbl(col.names=c(\"Condition\",\"Feedback\",\"Band_Order\",\"Test Order\",\"N\"),escape=FALSE) \n\n\n\n\nCondition\nFeedback\nBand_Order\nTest Order\nN\n\n\n\nConstant\nNA\nNA\nTest First\n162\n\n\nConstant\nNA\nNA\nTrain First\n93\n\n\nVaried\nNA\nNA\nTest First\n131\n\n\nVaried\nNA\nNA\nTrain First\n85\n\n\n\n\n\nCodet1=d %&gt;% select(condit,fb,bandOrder,id) %&gt;% distinct() %&gt;% \n  group_by(condit,fb,bandOrder) %&gt;% summarise(n=n()) \nt1\n\n# A tibble: 2 × 4\n# Groups:   condit, fb [2]\n  condit   fb    bandOrder     n\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;     &lt;int&gt;\n1 Constant &lt;NA&gt;  &lt;NA&gt;        255\n2 Varied   &lt;NA&gt;  &lt;NA&gt;        216\n\n\n\nCoded |&gt; group_by(id,condit) |&gt; \n  summarize(n=n()) |&gt; \n  mutate(n=fct_infreq(factor(n))) |&gt;\n  ggplot(aes(x=id,y=n)) + geom_col()\n\n\n\n\n\n\nCodesorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\nconditional_bars &lt;- function(df, condition, var) {\n  df |&gt; \n    filter({{ condition }}) |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_bar()\n}\n\nhistogram &lt;- function(df, var, binwidth) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {binwidth}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth) + \n    labs(title = label)\n}\n# \n# sorted_bars(d |&gt; filter(id %in% 1:30),id) \n# \n# test |&gt; conditional_bars(id %in% 1:10, id)"
  },
  {
    "objectID": "Misc/data_dictionary.html#tibble1",
    "href": "Misc/data_dictionary.html#tibble1",
    "title": "HTW Data Dictionary",
    "section": "Tibble1",
    "text": "Tibble1\n\nCode# Create the tribble\ndata_tbl &lt;- tibble(\n  Variable_Name = c(\"condit\", \"fb\", \"bandOrder\", \"tOrder\", \"expMode\", \"trainStage\", \"expStage\", \"band\", \"vb\", \"lowBound\", \"feedback\", \"stage\"),\n  Variable_Levels = c(\"Constant, Varied\", \"Continuous, Ordinal\", \"Original, Reverse\", \"Test First, Train First\", \"train, train-Nf, test-Nf, test-train-nf, test-feedback\", \"Beginning, Middle, End, Test\", \"TrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\", \"1, 2, 3, 4, 5, 6\", \"100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\", \"100, 350, 600, 800, 1000, 1200\", \"0, 1\", \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\"),\n  Description = rep(\"Unknown\", 12)\n)\n\n# Print the tribble\ndata_tbl %&gt;% gt::gt()\n\n\n\n\n\n\nVariable_Name\nVariable_Levels\nDescription\n\n\n\ncondit\nConstant, Varied\nUnknown\n\n\nfb\nContinuous, Ordinal\nUnknown\n\n\nbandOrder\nOriginal, Reverse\nUnknown\n\n\ntOrder\nTest First, Train First\nUnknown\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nUnknown\n\n\ntrainStage\nBeginning, Middle, End, Test\nUnknown\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nUnknown\n\n\nband\n1, 2, 3, 4, 5, 6\nUnknown\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nUnknown\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nUnknown\n\n\nfeedback\n0, 1\nUnknown\n\n\nstage\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\nUnknown"
  },
  {
    "objectID": "Misc/data_dictionary.html#tibble",
    "href": "Misc/data_dictionary.html#tibble",
    "title": "HTW Data Dictionary",
    "section": "Tibble",
    "text": "Tibble\n\nCodedata &lt;- tibble(\n  `Variable Name` = c(\"condit\", \"fb\", \"bandOrder\", \"tOrder\", \"expMode\", \"trainStage\",\n                      \"expStage\", \"band\", \"vb\", \"lowBound\", \"feedback\", \"stage\"),\n  `Levels` = c(\"Constant, Varied\", \n               \"Continuous, Ordinal\",\n               \"Original, Reverse\",\n               \"Test First, Train First\",\n               \"train, train-Nf, test-Nf, test-train-nf, test-feedback\",\n               \"Beginning, Middle, End, Test\",\n               \"TrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\",\n               \"1, 2, 3, 4, 5, 6\",\n               \"100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\",\n               \"100, 350, 600, 800, 1000, 1200\",\n               \"0, 1\",\n               \"1 to 23\"),\n  `Description` = c(\"Conditions under which training was performed. In 'Constant' only one velocity band was used, in 'Varied' three different bands were used.\",\n                    \"Two types of feedback given to the subjects. The 'Continuous' type involved ongoing feedback whereas 'Ordinal' feedback type was rank-based\",\n                    \"The order in which the velocity bands were presented for the 'Hit The Wall' task - either 'Original' or 'Reverse'.\",\n                    \"The order of test and train stages.\",\n                    \"Specifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\",\n                    \"The stage of training for the individual - starting, middle, end, or if they're testing.\",\n                    \"The stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\",\n                    \"Identifies the 6 different velocity bands used in the experiment.\",\n                    \"Specifies the range of each velocity band used in the experiment.\",\n                    \"The lower boundary of each velocity band.\",\n                    \"Binary representation of whether feedback was provided (1) or not (0)\",\n                    \"The various stages during testing and feedback, represented by numbers from 1 to 23.\")\n)\ndata %&gt;% gt::gt()\n\n\n\n\n\n\nVariable Name\nLevels\nDescription\n\n\n\ncondit\nConstant, Varied\nConditions under which training was performed. In 'Constant' only one velocity band was used, in 'Varied' three different bands were used.\n\n\nfb\nContinuous, Ordinal\nTwo types of feedback given to the subjects. The 'Continuous' type involved ongoing feedback whereas 'Ordinal' feedback type was rank-based\n\n\nbandOrder\nOriginal, Reverse\nThe order in which the velocity bands were presented for the 'Hit The Wall' task - either 'Original' or 'Reverse'.\n\n\ntOrder\nTest First, Train First\nThe order of test and train stages.\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nSpecifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\n\n\ntrainStage\nBeginning, Middle, End, Test\nThe stage of training for the individual - starting, middle, end, or if they're testing.\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nThe stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\n\n\nband\n1, 2, 3, 4, 5, 6\nIdentifies the 6 different velocity bands used in the experiment.\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nSpecifies the range of each velocity band used in the experiment.\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nThe lower boundary of each velocity band.\n\n\nfeedback\n0, 1\nBinary representation of whether feedback was provided (1) or not (0)\n\n\nstage\n1 to 23\nThe various stages during testing and feedback, represented by numbers from 1 to 23."
  },
  {
    "objectID": "Misc/data_dictionary.html#tribble",
    "href": "Misc/data_dictionary.html#tribble",
    "title": "HTW Data Dictionary",
    "section": "Tribble",
    "text": "Tribble\n\nCodetribble(\n  ~Variable, ~Levels, ~Description,\n  \"condit\", \"Constant, Varied\", \"Conditions under which training was performed. In 'Constant' only one velocity band was used, in 'Varied' three different bands were used.\",\n  \"fb\", \"Continuous, Ordinal\", \"Two types of feedback given to the subjects. The 'Continuous' type involved ongoing feedback whereas 'Ordinal' feedback type was rank-based\",\n  \"bandOrder\", \"Original, Reverse\", \"The order in which the velocity bands were presented for the 'Hit The Wall' task - either 'Original' or 'Reverse'.\",\n  \"tOrder\", \"Test First, Train First\", \"The order of test and train stages.\",\n  \"expMode\", \"train, train-Nf, test-Nf, test-train-nf, test-feedback\", \"Specifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\",\n  \"trainStage\", \"Beginning, Middle, End, Test\", \"The stage of training for the individual - starting, middle, end, or if they're testing.\",\n  \"expStage\", \"TrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\", \"The stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\",\n  \"band\", \"1, 2, 3, 4, 5, 6\", \"Identifies the 6 different velocity bands used in the experiment.\",\n  \"vb\", \"100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\", \"Specifies the range of each velocity band used in the experiment.\",\n  \"lowBound\", \"100, 350, 600, 800, 1000, 1200\", \"The lower boundary of each velocity band.\",\n  \"feedback\", \"0, 1\", \"Binary representation of whether feedback was provided (1) or not (0)\",\n  \"stage\", \"1 to 23\", \"The various stages during testing and feedback, represented by numbers from 1 to 23.\"\n) %&gt;% gt::gt() %&gt;% \n        gt::tab_style(\n                style = cell_text(weight = \"bold\"),\n                locations = cells_column_labels()\n        )\n\n\n\n\n\n\nVariable\nLevels\nDescription\n\n\n\ncondit\nConstant, Varied\nConditions under which training was performed. In 'Constant' only one velocity band was used, in 'Varied' three different bands were used.\n\n\nfb\nContinuous, Ordinal\nTwo types of feedback given to the subjects. The 'Continuous' type involved ongoing feedback whereas 'Ordinal' feedback type was rank-based\n\n\nbandOrder\nOriginal, Reverse\nThe order in which the velocity bands were presented for the 'Hit The Wall' task - either 'Original' or 'Reverse'.\n\n\ntOrder\nTest First, Train First\nThe order of test and train stages.\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nSpecifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\n\n\ntrainStage\nBeginning, Middle, End, Test\nThe stage of training for the individual - starting, middle, end, or if they're testing.\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nThe stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\n\n\nband\n1, 2, 3, 4, 5, 6\nIdentifies the 6 different velocity bands used in the experiment.\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nSpecifies the range of each velocity band used in the experiment.\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nThe lower boundary of each velocity band.\n\n\nfeedback\n0, 1\nBinary representation of whether feedback was provided (1) or not (0)\n\n\nstage\n1 to 23\nThe various stages during testing and feedback, represented by numbers from 1 to 23.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nLevels\nDescription\n\n\n\ncondit\nConstant, Varied\nConditions under which training was performed. In “Constant” only one velocity band was used, in “Varied” three different bands were used.\n\n\nfb\nContinuous, Ordinal\nTwo types of feedback given to the subjects. The “Continuous” type involved ongoing feedback whereas “Ordinal” feedback type was rank-based\n\n\nbandOrder\nOriginal, Reverse\nThe order in which the velocity bands were presented for the “Hit The Wall” task - either “Original” or “Reverse”.\n\n\ntOrder\nTest First, Train First\nThe order of test and train stages.\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nSpecifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\n\n\ntrainStage\nBeginning, Middle, End, Test\nThe stage of training for the individual - starting, middle, end, or if they’re testing.\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nThe stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\n\n\nband\n1, 2, 3, 4, 5, 6\nIdentifies the 6 different velocity bands used in the experiment.\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nSpecifies the range of each velocity band used in the experiment.\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nThe lower boundary of each velocity band.\n\n\nfeedback\n0, 1\nBinary representation of whether feedback was provided (1) or not (0)\n\n\nstage\n1 to 23\nThe various stages during testing and feedback, represented by numbers from 1 to 23."
  },
  {
    "objectID": "Misc/data_dictionary.html#description-table",
    "href": "Misc/data_dictionary.html#description-table",
    "title": "HTW Data Dictionary",
    "section": "Description table",
    "text": "Description table\n\n\n\n\n\n\n\nVariable Name\nVariable Levels\nDescription\n\n\n\ncondit\nConstant, Varied\nCondition of the experiment: constant or varied\n\n\nfb\nContinuous, Ordinal\nType of feedback received: continuous or ordinal\n\n\nbandOrder\nOriginal, Reverse\nOrder of bands: original or reverse\n\n\ntOrder\nTest First, Train First\nOrder of testing and training stages: test first or train first\n\n\nexpMode\ntrain, train-Nf, test-Nf, etc.\nMode of the experiment: train, train-Nf, test-Nf, etc.\n\n\ntrainStage\nBeginning, Middle, End, Test\nStage of the training: beginning, middle, end, or test\n\n\nexpStage\nTrainStart, intTest1, etc.\nStage of the experiment: TrainStart, intTest1, TrainMid1, etc.\n\n\nband\n1, 2, 3, 4, 5, 6\nBand number\n\n\nvb\n100-300, 350-550, etc.\nVelocity band range\n\n\nlowBound\n100, 350, 600, etc.\nLower bound of the velocity band range\n\n\nfeedback\n0, 1\nFeedback type: 0 (no feedback), 1 (feedback)\n\n\nstage\n1, 2, 3, etc.\nStage number of the experiment"
  },
  {
    "objectID": "Misc/data_dictionary.html#ascii-table",
    "href": "Misc/data_dictionary.html#ascii-table",
    "title": "HTW Data Dictionary",
    "section": "Ascii table",
    "text": "Ascii table\n\n\n\n\n\n\n\nVariable Name\nVariable Levels\nDescription\n\n\n\ncondit\nConstant, Varied\nUnknown\n\n\nfb\nContinuous, Ordinal\nUnknown\n\n\nbandOrder\nOriginal, Reverse\nUnknown\n\n\ntOrder\nTest First, Train First\nUnknown\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nUnknown\n\n\ntrainStage\nBeginning, Middle, End, Test\nUnknown\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nUnknown\n\n\nband\n1, 2, 3, 4, 5, 6\nUnknown\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nUnknown\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nUnknown\n\n\nfeedback\n0, 1\nUnknown\n\n\nstage\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\nUnknown\n\n\n\n\nDemonstration of pipe table syntax\n\nDefault\nLeft\nRight\nCenter\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1"
  },
  {
    "objectID": "Misc/data_dictionary.html#smaller-table",
    "href": "Misc/data_dictionary.html#smaller-table",
    "title": "HTW Data Dictionary",
    "section": "smaller table",
    "text": "smaller table\n\n\n\n\n\n\nVariable Name\nVariable Levels\n\n\n\ncondit\n“Constant”, “Varied”\n\n\nfb\n“Continuous”, “Ordinal”\n\n\nbandOrder\n“Original”, “Reverse”\n\n\ntOrder\n“Test First”, “Train First”\n\n\nexpMode\n“train”, “train-Nf”, “test-Nf”, “test-train-nf”, “test-feedback”\n\n\ntrainStage\n“Beginning”, “Middle”, “End”, “Test”\n\n\nexpStage\n“TrainStart”, “intTest1”, “TrainMid1”, “intTest2”, “TrainMid2”, “intTest3”, “TrainEnd”, “Test1”, “Test2”, “Test3”\n\n\nband\n“1”, “2”, “3”, “4”, “5”, “6”\n\n\nvb\n“100-300”, “350-550”, “600-800”, “800-1000”, “1000-1200”, “1200-1400”\n\n\nlowBound\n“100”, “350”, “600”, “800”, “1000”, “1200”\n\n\nfeedback\n“0”, “1”\n\n\nstage\n“1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9”, “10”, “11”, “12”, “13”, “14”, “15”, “16”, “17”, “18”, “19”, “20”, “21”, “22”, “23”\n\n\n\nCode# Create a data frame\nvar_levels &lt;- list(\n  condit = c(\"Constant\", \"Varied\"),\n  fb = c(\"Continuous\", \"Ordinal\"),\n  bandOrder = c(\"Original\", \"Reverse\"),\n  tOrder = c(\"Test First\", \"Train First\"),\n  expMode = c(\"train\", \"train-Nf\", \"test-Nf\", \"test-train-nf\", \"test-feedback\"),\n  trainStage = c(\"Beginning\", \"Middle\", \"End\", \"Test\"),\n  expStage = c(\"TrainStart\", \"intTest1\", \"TrainMid1\", \"intTest2\", \"TrainMid2\", \"intTest3\", \"TrainEnd\", \"Test1\", \"Test2\", \"Test3\"),\n  band = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n  vb = c(\"100-300\", \"350-550\", \"600-800\", \"800-1000\", \"1000-1200\", \"1200-1400\"),\n  lowBound = c(\"100\", \"350\", \"600\", \"800\", \"1000\", \"1200\"),\n  feedback = c(\"0\", \"1\"),\n  stage = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\")\n)\n\n# Convert the list to a data frame\ndf &lt;- data.frame(\n  Variable_Name = names(var_levels),\n  Variable_Levels = sapply(var_levels, function(x) paste(x, collapse = \", \"))\n)\n\n# Create a markdown table\ndf %&gt;%\n  kable(format = \"markdown\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\n\nVariable_Name\nVariable_Levels\n\n\n\ncondit\ncondit\nConstant, Varied\n\n\nfb\nfb\nContinuous, Ordinal\n\n\nbandOrder\nbandOrder\nOriginal, Reverse\n\n\ntOrder\ntOrder\nTest First, Train First\n\n\nexpMode\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\n\n\ntrainStage\ntrainStage\nBeginning, Middle, End, Test\n\n\nexpStage\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\n\n\nband\nband\n1, 2, 3, 4, 5, 6\n\n\nvb\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\n\n\nlowBound\nlowBound\n100, 350, 600, 800, 1000, 1200\n\n\nfeedback\nfeedback\n0, 1\n\n\nstage\nstage\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\n\n\n\n\n\n\n\n\n\n\nVariable Name\nVariable Levels\n\n\n\ncondit\nConstant, Varied\n\n\nfb\nContinuous, Ordinal\n\n\nbandOrder\nOriginal, Reverse\n\n\ntOrder\nTest First, Train First\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\n\n\ntrainStage\nBeginning, Middle, End, Test\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\n\n\nband\n1, 2, 3, 4, 5, 6\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\n\n\nfeedback\n0, 1\n\n\nstage\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\n\n\n\nNumeric variables\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\ntrial\nTrial number\n\n\nnGoodTrial\nNumber of good trials\n\n\ngt.bandStage\nBand stage in game time\n\n\ngt.stage\nStage in game time\n\n\ngt.train\nTraining in game time\n\n\ninput\nInput from the participant\n\n\nbandInt\nBand interval\n\n\nhighBound\nHigh boundary of the velocity band\n\n\nrunTotal\nRunning total of trials\n\n\ndist\nDistance between produced x-velocity and the closest edge of the current velocity band\n\n\nsdist\nStandardized distance\n\n\nvx\nX-velocity of the projectile\n\n\nvxb\nX-velocity at the boundary\n\n\nvxi\nInitial X-velocity\n\n\nvy\nY-velocity of the projectile\n\n\nnTrain\nNumber of training trials\n\n\nnTestNf\nNumber of no-feedback testing trials\n\n\nnInt\nNumber of interleaved trials\n\n\nnTestF\nNumber of feedback testing trials\n\n\nnTotal\nTotal number of trials\n\n\nlastTrain\nLast training trial\n\n\nlastTrial\nLast trial"
  },
  {
    "objectID": "Misc/bmm_table.html",
    "href": "Misc/bmm_table.html",
    "title": "HTW Project",
    "section": "",
    "text": "# Create the data frame for the table\ntable_data &lt;- data.frame(\n  Type = c(\n    rep(\"Population-Level Effects\", 4),\n    rep(\"Group-Level Effects\", 2),\n    \"Family Specific Parameters\"\n  ),\n  Parameter = c(\n    \"\\\\(\\\\beta_0\\\\)\", \"\\\\(\\\\beta_1\\\\)\", \"\\\\(\\\\beta_2\\\\)\", \"\\\\(\\\\beta_3\\\\)\",\n    \"\\\\(\\\\sigma_{\\\\text{Intercept}}\\\\)\", \"\\\\(\\\\sigma_{\\\\text{bandInt}}\\\\)\", \"\\\\(\\\\sigma_{\\\\text{Observation}}\\\\)\"\n  ),\n  Term = c(\n    \"(Intercept)\", \"conditVaried\", \"bandInt\", \"conditVaried:bandInt\",\n    \"sd__(Intercept)\", \"sd__bandInt\", \"sd__Observation\"\n  ),\n  Description = c(\n    \"Intercept representing the baseline deviation\", \"Effect of condition (Varied vs. Constant) on deviation\", \"Effect of target velocity band (bandInt) on deviation\", \"Interaction effect between training condition and target velocity band on deviation\",\n    \"Standard deviation for (Intercept)\", \"Standard deviation for bandInt\", \"Standard deviation for Gaussian Family\"\n  )\n) |&gt;   mutate(\n    Term = glue::glue(\"&lt;code&gt;{Term}&lt;/code&gt;\")\n  ) \n\n# Create the table\nkable_out &lt;- table_data %&gt;%\n  kbl(format = 'html', escape = FALSE, booktabs = TRUE, \n      #caption = '&lt;span style = \"color:black;\"&gt;&lt;center&gt;&lt;strong&gt;Table 1: General Model Structure Information&lt;/strong&gt;&lt;/center&gt;&lt;/span&gt;',\n      col.names = c(\"Type\", \"Parameter\", \"Term\", \"Description\")) %&gt;%\n  kable_styling(position=\"left\", bootstrap_options = c(\"hover\"), full_width = FALSE) %&gt;%\n  column_spec(1, bold = FALSE, border_right = TRUE) %&gt;%\n  column_spec(2, width = '4cm') %&gt;%\n  column_spec(3, width = '4cm') %&gt;%\n  row_spec(c(4, 7), extra_css = \"border-bottom: 2px solid black;\") %&gt;%\n  pack_rows(\"\", 1, 4, bold = FALSE, italic = TRUE) %&gt;%\n  pack_rows(\"\", 5, 6, bold = FALSE, italic = TRUE) %&gt;%\n  pack_rows(\"\", 7, 7, bold = FALSE, italic = TRUE)\n\nkable_out\n\n\nTable 1: Mixed model structure and coefficient descriptions\n\n\n\n\n\nType\nParameter\nTerm\nDescription\n\n\n\n\n\n\nPopulation-Level Effects\n\\(\\beta_0\\)\n(Intercept)\nIntercept representing the baseline deviation\n\n\nPopulation-Level Effects\n\\(\\beta_1\\)\nconditVaried\nEffect of condition (Varied vs. Constant) on deviation\n\n\nPopulation-Level Effects\n\\(\\beta_2\\)\nbandInt\nEffect of target velocity band (bandInt) on deviation\n\n\nPopulation-Level Effects\n\\(\\beta_3\\)\nconditVaried:bandInt\nInteraction effect between training condition and target velocity band on deviation\n\n\n\n\n\nGroup-Level Effects\n\\(\\sigma_{\\text{Intercept}}\\)\nsd__(Intercept)\nStandard deviation for (Intercept)\n\n\nGroup-Level Effects\n\\(\\sigma_{\\text{bandInt}}\\)\nsd__bandInt\nStandard deviation for bandInt\n\n\n\n\n\nFamily Specific Parameters\n\\(\\sigma_{\\text{Observation}}\\)\nsd__Observation\nStandard deviation for Gaussian Family"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html",
    "href": "Misc/Visuals_Interactives/tikz_net.html",
    "title": "tikz",
    "section": "",
    "text": "https://www.andrewheiss.com/blog/2021/08/27/tikz-knitr-html-svg-fun/ https://gist.github.com/andrewheiss/4ece621813a27dfdcaef7f1c2d773237\nCode```{r}\n#lapply(c('tidyverse','data.table','igraph','ggraph','kableExtra'),library,character.only=TRUE))\npacman::p_load(tikzDevice, knitr) \n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#alcove",
    "href": "Misc/Visuals_Interactives/tikz_net.html#alcove",
    "title": "tikz",
    "section": "Alcove",
    "text": "Alcove\n\nCode```{r, engine = 'tikz'}\n\\usetikzlibrary{positioning}\n\n\n\\begin{tikzpicture}[node distance=1cm]\n    \\tikzset{\n        neuron/.style={\n            circle,\n            draw,\n            minimum size=1cm,\n        },\n        input neuron/.style={\n            neuron,\n            fill=green!20,\n        },\n        output neuron/.style={\n            neuron,\n            fill=red!20,\n        },\n        hidden neuron/.style={\n            neuron,\n            fill=blue!20,\n        },\n    }\n\n    \\node[input neuron] (input-1) at (0, 0) {Stimulus dimension node 1};\n    \\node[input neuron] (input-2) [below=of input-1] {Stimulus dimension node 2};\n    \\node[hidden neuron] (hidden-1) [right=of input-1] {Exemplar node 1};\n    \\node[hidden neuron] (hidden-2) [below=of hidden-1] {Exemplar node 2};\n    \\node[hidden neuron] (hidden-3) [below=of hidden-2] {Exemplar node 3};\n    \\node[output neuron] (output-1) [right=of hidden-2] {Category node 1};\n    \\node[output neuron] (output-2) [right=of hidden-3] {Category node 2};\n\n    \\draw[-&gt;] (input-1) -- (hidden-1);\n    \\draw[-&gt;] (input-1) -- (hidden-2);\n    \\draw[-&gt;] (input-1) -- (hidden-3);\n    \\draw[-&gt;] (input-2) -- (hidden-1);\n    \\draw[-&gt;] (input-2) -- (hidden-2);\n    \\draw[-&gt;] (input-2) -- (hidden-3);\n    \\draw[-&gt;] (hidden-1) -- node[midway, above, sloped]{Learned association weights} (output-1);\n    \\draw[-&gt;] (hidden-2) -- node[midway, above, sloped]{Learned association weights} (output-1);\n    \\draw[-&gt;] (hidden-3) -- node[midway, above, sloped]{Learned association weights} (output-2);\n\\end{tikzpicture}\n\n```\n\n\n\n\n\n\n\nAlcove 2\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    neuron/.style={\n        circle,\n        draw,\n        minimum size=1cm,\n    },\n    input neuron/.style={\n        neuron,\n        fill=green!20,\n    },\n    output neuron/.style={\n        neuron,\n        fill=red!20,\n    },\n    hidden neuron/.style={\n        neuron,\n        fill=blue!20,\n    },\n}\n\n\\begin{tikzpicture}[node distance=1cm]\n    \\node[input neuron] (input-1) at (0, 0) {Stimulus dimension node 1};\n    \\node[input neuron] (input-2) [below=of input-1] {Stimulus dimension node 2};\n    \\node[hidden neuron] (hidden-1) [right=of input-1] {Exemplar node 1};\n    \\node[hidden neuron] (hidden-2) [below=of hidden-1] {Exemplar node 2};\n    \\node[hidden neuron] (hidden-3) [below=of hidden-2] {Exemplar node 3};\n    \\node[output neuron] (output-1) [right=of hidden-2] {Category node 1};\n    \\node[output neuron] (output-2) [right=of hidden-3] {Category node 2};\n\n    \\draw[-&gt;] (input-1) -- (hidden-1);\n    \\draw[-&gt;] (input-1) -- (hidden-2);\n    \\draw[-&gt;] (input-1) -- (hidden-3);\n    \\draw[-&gt;] (input-2) -- (hidden-1);\n    \\draw[-&gt;] (input-2) -- (hidden-2);\n    \\draw[-&gt;] (input-2) -- (hidden-3);\n    \\draw[-&gt;] (hidden-1) -- node[midway, above, sloped]{Learned association weights} (output-1);\n    \\draw[-&gt;] (hidden-2) -- node[midway, above, sloped]{Learned association weights} (output-1);\n    \\draw[-&gt;] (hidden-3) -- node[midway, above, sloped]{Learned association weights} (output-2);\n\\end{tikzpicture}\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#intro",
    "href": "Misc/Visuals_Interactives/tikz_net.html#intro",
    "title": "tikz",
    "section": "Intro",
    "text": "Intro\nHere is a TikZ picture\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n\\tikzset{\n    declare function={\n        sig = 0.1;\n        mu = 0;\n        g(\\x) = 1/(sig*sqrt(2*pi)) * exp(-1/2 * ((\\x-mu)/sig)^2);\n    }\n}\n\n\n\n\\begin{tikzpicture}[\n    shorten &gt;=1pt,\n    -&gt;,\n    draw=black!50,\n    node distance=2.5cm,\n    scale=1.5,\n    every pin edge/.style={&lt;-,shorten &lt;=1pt},\n    neuron/.style={\n        circle,fill=black!25,minimum size=17pt,inner sep=0pt,\n        path picture={\n            \\draw[red,thick,-] plot[domain=-0.3:0.3,samples=11,smooth] ({\\x},{0.05*g(\\x)});\n        },\n    },\n    input neuron/.style={neuron, fill=green!50},\n    output neuron/.style={neuron, fill=red!50},\n    hidden neuron/.style={neuron, fill=blue!50},\n    annot/.style={text width=4em, text centered},\n]\n\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,...,4}\n    % This is the same as writing \\foreach \\name / \\y in {1/1,2/2,3/3,4/4}\n        \\node[input neuron, pin=left:Input \\y] (I-\\name) at (0,-\\y) {};\n    \n    % Draw the hidden layer nodes\n    \\foreach \\name / \\y in {1,...,5}\n        \\path[yshift=0.5cm]\n            node[hidden neuron] (H-\\name) at (2.5cm,-\\y cm) {};\n    \n    % Draw the output layer node\n    \\node[output neuron,pin={[pin edge={-&gt;}]right:Output}, right of=H-3] (O) {};\n    \n    % Connect every node in the input layer with every node in the\n    % hidden layer.\n    \\foreach \\source in {1,...,4}\n        \\foreach \\dest in {1,...,5}\n            \\path (I-\\source) edge (H-\\dest);\n    \n    % Connect every node in the hidden layer with the output layer\n    \\foreach \\source in {1,...,5}\n        \\path (H-\\source) edge (O);\n    \n    % Annotate the layers\n    \\node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};\n    \\node[annot,above of=I-1, node distance=1cm] {Input layer};\n    \\node[annot,above of=O] {Output layer};\n\n\n\\end{tikzpicture}\n\n```\n\n\n\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\n\\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}\n\n\\begin{tikzpicture}[\n    init/.style={\n      draw,\n      circle,\n      inner sep=2pt,\n      font=\\Huge,\n      join = by -latex\n    },\n    squa/.style={\n      draw,\n      inner sep=2pt,\n      font=\\Large,\n      join = by -latex\n    },\n    start chain=2,node distance=13mm\n    ]\n    \\node[on chain=2] \n      (x2) {$x_2$};\n    \\node[on chain=2,join=by o-latex] \n      {$w_2$};\n    \\node[on chain=2,init] (sigma) \n      {$\\displaystyle\\Sigma$};\n    \\node[on chain=2,squa,label=above:{\\parbox{2cm}{\\centering Activate \\\\ function}}]   \n      {$f$};\n    \\node[on chain=2,label=above:Output,join=by -latex] \n      {$y$};\n    \\begin{scope}[start chain=1]\n    \\node[on chain=1] at (0,1.5cm) \n      (x1) {$x_1$};\n    \\node[on chain=1,join=by o-latex] \n      (w1) {$w_1$};\n    \\end{scope}\n    \\begin{scope}[start chain=3]\n    \\node[on chain=3] at (0,-1.5cm) \n      (x3) {$x_3$};\n    \\node[on chain=3,label=below:Weights,join=by o-latex] \n      (w3) {$w_3$};\n    \\end{scope}\n    \\node[label=above:\\parbox{2cm}{\\centering Bias \\\\ $b$}] at (sigma|-w1) (b) {};\n    \n    \\draw[-latex] (w1) -- (sigma);\n    \\draw[-latex] (w3) -- (sigma);\n    \\draw[o-latex] (b) -- (sigma);\n    \n    \\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);\n\\end{tikzpicture}\n\n```\n\n\n\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{positioning}\n\n\\tikzset{basic/.style={draw,fill=blue!20,text width=1em,text badly centered}}\n\\tikzset{input/.style={basic,circle}}\n\\tikzset{weights/.style={basic,rectangle}}\n\\tikzset{functions/.style={basic,circle,fill=blue!10}}\n\n\n\\begin{tikzpicture}\n    \\node[functions] (center) {};\n    \\node[below of=center,font=\\scriptsize,text width=4em] {Activation function};\n    \\draw[thick] (0.5em,0.5em) -- (0,0.5em) -- (0,-0.5em) -- (-0.5em,-0.5em);\n    \\draw (0em,0.75em) -- (0em,-0.75em);\n    \\draw (0.75em,0em) -- (-0.75em,0em);\n    \\node[right of=center] (right) {};\n        \\path[draw,-&gt;] (center) -- (right);\n    \\node[functions,left=3em of center] (left) {$\\sum$};\n        \\path[draw,-&gt;] (left) -- (center);\n    \\node[weights,left=3em of left] (2) {$w_2$} -- (2) node[input,left of=2] (l2) {$x_2$};\n        \\path[draw,-&gt;] (l2) -- (2);\n        \\path[draw,-&gt;] (2) -- (left);\n    \\node[below of=2] (dots) {$\\vdots$} -- (dots) node[left of=dots] (ldots) {$\\vdots$};\n    \\node[weights,below of=dots] (n) {$w_n$} -- (n) node[input,left of=n] (ln) {$x_n$};\n        \\path[draw,-&gt;] (ln) -- (n);\n        \\path[draw,-&gt;] (n) -- (left);\n    \\node[weights,above of=2] (1) {$w_1$} -- (1) node[input,left of=1] (l1) {$x_1$};\n        \\path[draw,-&gt;] (l1) -- (1);\n        \\path[draw,-&gt;] (1) -- (left);\n    \\node[weights,above of=1] (0) {$w_0$} -- (0) node[input,left of=0] (l0) {$1$};\n        \\path[draw,-&gt;] (l0) -- (0);\n        \\path[draw,-&gt;] (0) -- (left);\n    \\node[below of=ln,font=\\scriptsize] {inputs};\n    \\node[below of=n,font=\\scriptsize] {weights};\n\\end{tikzpicture}\n\n```\n\n\n\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{%\n  calc,\n  fit,\n  shapes,\n  backgrounds\n}\n% the next macro is useful to create a table\n\\newcommand\\tabins[3]{%\n \\tikz[baseline=(Tab.base)] \n           \\node  [rectangle split, \n                   rectangle split parts=3, \n                   draw, \n                   align=right,\n                   inner sep=.5em,\n                   rectangle split horizontal] (Tab)\n                           {\\hbox to 4ex{#1}\n           \\nodepart{two}  {\\hbox to 8ex{\\hfill #2\\$}}  \n           \\nodepart{three}{\\hbox to 3ex{#3}}}; \n}\n\n\n\\parindent=0pt\n\n\\begin{tikzpicture}[%\n    %every node/.style={transform shape},% now is not necessary but good for a poster\n    x=1.25cm,y=2cm,  \n    font=\\footnotesize,\n    % every group of nodes have a style except for main, the style is named by a letter\n    main/.style={draw,fill=yellow,inner sep=.5em},\n    R/.style={draw,fill=purple!40!blue!30,inner sep=.5em},\n    M/.style={draw,fill=green!80!yellow,inner sep=.5em},\n    S/.style={anchor=east},\n    V/.style={anchor=west},\n    P/.style={anchor=center},\n    F/.style={anchor=west}\n    ]\n\n  % main node the reference Shuffle \n  \\node[main] (shuffle) {Group};\n  %group R reducer\n  \\node[R] at ($(shuffle)+(8,1)$)    (R1+) {Reduce};\n  \\node[R] at ($(shuffle)+(8, 0)$)   (R0)  {Reduce};\n  \\node[R] at ($(shuffle)+(8,-1)$)   (R1-) {Reduce};\n  % group M Mapper\n  \\node[M] at ($(shuffle)+(-6,+2.5)$)   (M3+)  {Map};\n  \\node[M] at ($(shuffle)+(-6,+ 1.5)$)  (M2+)  {Map};\n  \\node[M] at ($(shuffle)+(-6,+ .5)$)   (M1+)  {Map};\n  \\node[M] at ($(shuffle)+(-6,- .5)$)   (M1-)  {Map};\n  \\node[M] at ($(shuffle)+(-6,- 1.5)$)  (M2-)  {Map};\n  \\node[M] at ($(shuffle)+(-6,-2.5)$)   (M3-)  {Map};\n  % group S Start the first nodes\n  \\node[S] at ($(M3+)+(-1.5,0)$)  (S3+) {\\Big($k_1$,\\tabins{4711}{59.90}{NY}\\Big)};\n  \\node[S] at ($(M2+)+(-1.5,0)$)  (S2+) {\\Big($k_2$,\\tabins{4713}{142.99}{CA}\\Big)};\n  \\node[S] at ($(M1+)+(-1.5,0)$)  (S1+) {\\Big($k_3$,\\tabins{4714}{72.00}{NY}\\Big)}; \n  \\node[S] at ($(M1-)+(-1.5,0)$)  (S1-) {\\Big($k_4$,\\tabins{4715}{108.75}{NY}\\Big)}; \n  \\node[S] at ($(M2-)+(-1.5,0)$)  (S2-) {\\Big($k_5$,\\tabins{4718}{19.89}{WA}\\Big)};  \n  \\node[S] at ($(M3-)+(-1.5,0)$)  (S3-) {\\Big($k_6$,\\tabins{4719}{36.60}{CA}\\Big)};  \n  % group V  why not\n  \\node[V] at ($(M3+)+(1.5,0)$)  (V3+) {\\Big(NY,59.90\\$\\Big)};\n  \\node[V] at ($(M2+)+(1.5,0)$)  (V2+) {\\Big(CA,142.99\\$\\Big)};\n  \\node[V] at ($(M1+)+(1.5,0)$)  (V1+) {\\Big(NY,72.00\\$\\Big)}; \n  \\node[V] at ($(M1-)+(1.5,0)$)  (V1-) {\\Big(NY,108.75\\$\\Big)}; \n  \\node[V] at ($(M2-)+(1.5,0)$)  (V2-) {\\Big(WA,19.89\\$\\Big)};  \n  \\node[V] at ($(M3-)+(1.5,0)$)  (V3-) {\\Big(CA,36.60\\$\\Big)};   \n\n  \\node[P] at ($(R1+)+(-4,0)$) (P1+) {\\Big(CA,\\big[142.99\\$,36.60\\$\\big]\\Big)};\n  \\node[P] at ($(R0) +(-4,0)$) (P0)  {\\Big(NY,\\big[59.90\\$,72.00\\$,108.75\\big]\\Big)};\n  \\node[P] at ($(R1-)+(-4,0)$) (P1-) {\\Big(WA,\\big[19.89\\$\\big]\\Big)}; \n\n  \\node[F] (F1+) at ($(R1+)+(1.5,0)$) {(CA,89.80\\$)};\n  \\node[F] (F0)  at ($(R0) +(1.5,0)$) {(NY,80.22\\$)}; \n  \\node[F] (F1-) at ($(R1-)+(1.5,0)$) {(WA,72.00\\$)}; \n\n  % wrappers\n  \\begin{scope}[on background layer]\n      \\node[fill=lightgray!50,inner sep = 4mm,fit=(shuffle),label=above:Shuffle] {}; \n  \\end{scope} \n  \\begin{scope}[on background layer]\n      \\node[fill=lightgray!50,inner sep = 4mm,fit=(R1+)(R1-),label=above:Reducer] {}; \n  \\end{scope}  \n  \\begin{scope}[on background layer]\n      \\node[fill=lightgray!50,inner sep = 4mm,fit=(M3+)(M3-),label=above:Mapper] {}; \n  \\end{scope}\n\n  %edges\n\n  \\foreach \\indice in {3+,2+,1+,1-,2-,3-} \\draw[-&gt;] (S\\indice.east) -- (M\\indice.west); \n  \\foreach \\indice in {3+,2+,1+,1-,2-,3-} \\draw[-&gt;] (M\\indice.east) -- (V\\indice.west);\n  \\foreach \\indice in {3+,2+,1+,1-,2-,3-} \\draw[-&gt;] (V\\indice.east) to [out=0,in=180] (shuffle.west); \n  \\foreach \\indice in {1+,0,1-} \\draw[-&gt;] (shuffle.east) to [out=0,in=180] (P\\indice.west);  \n  \\foreach \\indice in {1+,0,1-} \\draw[-&gt;] (P\\indice.east) -- (R\\indice.west);\n  \\foreach \\indice in {1+,0,1-} \\draw[-&gt;] (R\\indice.east) -- (F\\indice.west);   \n\\end{tikzpicture} \n\n```\n\n\n\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n#|\n\\def\\layersep{3cm}\n\\def\\nodeinlayersep{1.5cm}\n\n\\begin{tikzpicture}\n  [\n    shorten &gt;=1pt,-&gt;,\n    draw=black!50,\n    node distance=\\layersep,\n    every pin edge/.style={&lt;-,shorten &lt;=1pt},\n    neuron/.style={circle,fill=black!25,minimum size=17pt,inner sep=0pt},\n    input neuron/.style={neuron, fill=green!50,},\n    output neuron/.style={neuron, fill=red!50},\n    hidden neuron/.style={neuron, fill=blue!50},\n    annot/.style={text width=4em, text centered},\n    bias/.style={neuron, fill=yellow!50,minimum size=4em},%&lt;-- added %%%\n  ]\n\n  % Draw the input layer nodes\n  \\foreach \\name / \\y in {1,...,3}\n    \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y-2.5) {};  \n\n  % set number of hidden layers\n  \\newcommand\\Nhidden{2}\n\n  % Draw the hidden layer nodes\n  \\foreach \\N in {0,...,\\Nhidden} {\n      \\foreach \\y in {0,...,5} { % &lt;-- added 0 instead of 1 %%%%%\n      \\ifnum \\y=4\n      \\ifnum \\N&gt;0 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n        \\node at (\\N*\\layersep,-\\y*\\nodeinlayersep) {$\\vdots$};  % add dots\n        \\else\\fi %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n      \\else\n          \\ifnum \\y=0 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n          \\ifnum \\N&lt;3 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n            \\node[bias] (H\\N-\\y) at (\\N*\\layersep,-\\y*\\nodeinlayersep ) {Bias}; %&lt;-- added\n            \\else\\fi %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n          \\else %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n            \\ifnum \\N&gt;0 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%\n            % print function\n            \\node[hidden neuron] (H\\N-\\y) at (\\N*\\layersep,-\\y*\\nodeinlayersep ) {$\\frac{1}{1+e^{-x}}$}; %&lt;-- added %%%%%%%%%%%\n                \\else\\fi %&lt;-- added %%%%%%%%%%%%\n          \\fi %&lt;-- added %%%%%%%\n      \\fi\n    }\n      \\ifnum \\N&gt;0 %&lt;-- added %%%%%%\n      % print hidden layer labels at the top\n    \\node[annot,above of=H\\N-1, node distance=1cm,yshift=2cm] (hl\\N) {Hidden layer \\N}; % &lt;- added yshift=2cm %%%%%%%%%%%%\n    \\else\\fi %&lt;-- added %%%%%\n  }\n\n  % Draw the output layer node and label\n  \\node[output neuron,pin={[pin edge={-&gt;}]right:Output}, right of=H\\Nhidden-3] (O) {}; \n  \n  % Connect bias every node in the input layer with every node in the\n  % hidden layer.\n  \\foreach \\source in {1,...,3}\n      \\foreach \\dest in {1,...,3,5} {\n        % \\path[yellow] (H-0) edge (H1-\\dest);\n        \\path[dashed,orange] (H0-0) edge (H1-\\dest); %&lt;-- added %%%%%\n          \\path[green!50] (I-\\source) edge (H1-\\dest);  % change to green, yellow gets blended\n    };\n\n  % connect all hidden stuff\n  \\foreach [remember=\\N as \\lastN (initially 1)] \\N in {2,...,\\Nhidden}\n      \\foreach \\source in {0,...,3,5} \n          \\foreach \\dest in {1,...,3,5}{\n              \\ifnum \\source=0 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%\n          \\path[dashed,red](H\\lastN-\\source) edge (H\\N-\\dest);%&lt;-- added \n            \\else %&lt;-- added %%%\n            \\path[blue!50] (H\\lastN-\\source) edge (H\\N-\\dest);%&lt;-- added \n            \\fi %&lt;-- added %%%\n            }; %&lt;-- added %%%%\n\n\n  % Connect every node in the hidden layer with the output layer\n  \\foreach \\source in {1,...,3,5}\n    \\path[green!50] (H\\Nhidden-\\source) edge (O);\n    \\path[dashed,red] (H2-0) edge (O); %&lt;-- added %%%%\n\n% Annotate the input and output layers\n  \\node[annot,left of=hl1] {Input layer};\n  \\node[annot,right of=hl\\Nhidden] {Output layer};  \n\\end{tikzpicture}\n\n```\n\n\n\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{positioning}\n\n\\tikzstyle{inputNode}=[draw,circle,minimum size=10pt,inner sep=0pt]\n\\tikzstyle{stateTransition}=[-stealth, thick]\n\n\\begin{tikzpicture}\n    \\node[draw,circle,minimum size=25pt,inner sep=0pt] (x) at (0,0) {$\\Sigma$ $\\sigma$};\n\n    \\node[inputNode] (x0) at (-2, 1.5) {$\\tiny +1$};\n    \\node[inputNode] (x1) at (-2, 0.75) {$\\tiny x_1$};\n    \\node[inputNode] (x2) at (-2, 0) {$\\tiny x_2$};\n    \\node[inputNode] (x3) at (-2, -0.75) {$\\tiny x_3$};\n    \\node[inputNode] (xn) at (-2, -1.75) {$\\tiny x_n$};\n\n    \\draw[stateTransition] (x0) to[out=0,in=120] node [midway, sloped, above] {$w_0$} (x);\n    \\draw[stateTransition] (x1) to[out=0,in=150] node [midway, sloped, above] {$w_1$} (x);\n    \\draw[stateTransition] (x2) to[out=0,in=180] node [midway, sloped, above] {$w_2$} (x);\n    \\draw[stateTransition] (x3) to[out=0,in=210] node [midway, sloped, above] {$w_3$} (x);\n    \\draw[stateTransition] (xn) to[out=0,in=240] node [midway, sloped, above] {$w_n$} (x);\n    \\draw[stateTransition] (x) -- (4,0) node [midway,above] {$\\sigma\\left(w_0 + \\sum\\limits_{i=1}^{n}{w_ix_i}\\right)$};\n    \\draw[dashed] (0,-0.43) -- (0,0.43);\n    \\node (dots) at (-2, -1.15) {$\\vdots$};\n    \\node[inputNode, thick] (i1) at (6, 0.75) {};\n    \\node[inputNode, thick] (i2) at (6, 0) {};\n    \\node[inputNode, thick] (i3) at (6, -0.75) {};\n    \n    \\node[inputNode, thick] (h1) at (8, 1.5) {};\n    \\node[inputNode, thick] (h2) at (8, 0.75) {};\n    \\node[inputNode, thick] (h3) at (8, 0) {};\n    \\node[inputNode, thick] (h4) at (8, -0.75) {};\n    \\node[inputNode, thick] (h5) at (8, -1.5) {};\n    \n    \\node[inputNode, thick] (o1) at (10, 0.75) {};\n    \\node[inputNode, thick] (o2) at (10, -0.75) {};\n    \n    \\draw[stateTransition] (5, 0.75) -- node[above] {$I_1$} (i1);\n    \\draw[stateTransition] (5, 0) -- node[above] {$I_2$} (i2);\n    \\draw[stateTransition] (5, -0.75) -- node[above] {$I_3$} (i3);\n    \n    \\draw[stateTransition] (i1) -- (h1);\n    \\draw[stateTransition] (i1) -- (h2);\n    \\draw[stateTransition] (i1) -- (h3);\n    \\draw[stateTransition] (i1) -- (h4);\n    \\draw[stateTransition] (i1) -- (h5);\n    \\draw[stateTransition] (i2) -- (h1);\n    \\draw[stateTransition] (i2) -- (h2);\n    \\draw[stateTransition] (i2) -- (h3);\n    \\draw[stateTransition] (i2) -- (h4);\n    \\draw[stateTransition] (i2) -- (h5);\n    \\draw[stateTransition] (i3) -- (h1);\n    \\draw[stateTransition] (i3) -- (h2);\n    \\draw[stateTransition] (i3) -- (h3);\n    \\draw[stateTransition] (i3) -- (h4);\n    \\draw[stateTransition] (i3) -- (h5);\n    \n    \\draw[stateTransition] (h1) -- (o1);\n    \\draw[stateTransition] (h1) -- (o2);\n    \\draw[stateTransition] (h2) -- (o1);\n    \\draw[stateTransition] (h2) -- (o2);\n    \\draw[stateTransition] (h3) -- (o1);\n    \\draw[stateTransition] (h3) -- (o2);\n    \\draw[stateTransition] (h4) -- (o1);\n    \\draw[stateTransition] (h4) -- (o2);\n    \\draw[stateTransition] (h5) -- (o1);\n    \\draw[stateTransition] (h5) -- (o2);\n    \n    \\node[above=of i1, align=center] (l1) {Input \\\\ layer};\n    \\node[right=2.3em of l1, align=center] (l2) {Hidden \\\\ layer};\n    \\node[right=2.3em of l2, align=center] (l3) {Output \\\\ layer};\n    \n    \\draw[stateTransition] (o1) -- node[above] {$O_1$} (11, 0.75);\n    \\draw[stateTransition] (o2) -- node[above] {$O_2$} (11, -0.75);\n    \n    \\path[dashed, double, ultra thick, gray] (x.north) edge[bend left=0] (h5.north);\n    \\path[dashed, double, ultra thick, gray] (x.south) edge[bend right=0] (h5.south);\n\\end{tikzpicture}\n\n```\n\n\n\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n\n\\usetikzlibrary{positioning,decorations.pathreplacing,shapes}\n\n\n\\newcommand*{\\cancer}{\\text{cancer}}\n\\newcommand*{\\testp}{\\text{test}+}\n\n\n\\begin{tikzpicture}[%\n   % common options for blocks:\n   block/.style = {draw, fill=blue!30, align=center, anchor=west,\n               minimum height=0.65cm, inner sep=0},\n   % common options for the circles:\n   ball/.style = {circle, draw, align=center, anchor=north, inner sep=0}]\n\n   % circle illustrating all women\n   \\node[ball,text width=3cm,fill=purple!20] (all) at (6,0) {All women};\n\n   % two circles showing split of p{cancer} and p{~cancer}\n   \\node[ball,fill=red!70,text width=0.1cm,anchor=base] (pcan) at (3.5,-5.5) {};\n   \\node[ball,fill=blue!40,text width=2.9cm,anchor=base] (pncan) at (8.5,-6)\n      {Women without cancer\\\\\n      $\\p({\\sim}\\cancer) = 99\\%$};\n\n   % arrows showing split from all women to cancer and ~cancer\n   \\draw[-&gt;,thick,draw=red!50] (all.south) to [out=270,in=90] (pcan.north);\n   \\draw[-&gt;,thick,draw=blue!80] (all.south) to [out=270,in=110] (pncan.100);\n\n   % transition from all women to actual cancer rates\n   \\node[anchor=north,text width=10cm,inner sep=.05cm,align=center,fill=white]\n   (why1) at (6,-3.7) {In measuring, we find:};\n\n   % note illustration the p{cancer} circle (text wont fit inside)\n   \\node[inner sep=0,anchor=east,text width=3.3cm] (note1) at (3.2,-5.5) {\n      Women with cancer $\\p(\\cancer) = 1\\%$};\n\n   % draw the sieves\n   \\node[block,anchor=north,text width=4.4cm,fill=green!50] (tray1) at\n      (3.5,-8.8) {\\small{$\\p(\\testp\\mid\\cancer)=0.8$}};\n\n   \\node[block,anchor=north,text width=4.4cm,fill=green!50] (tray2) at\n      (8.5,-8.8) {$\\p(\\testp\\mid{\\sim}\\cancer)=0.096$};\n\n   % text explaining how p{cancer} and p{~cancer} behave as they\n   % pass through the sieves\n   \\node[anchor=west,text width=6cm] (note1) at (-6,-9.1) {\n      Now we pass both groups through the sieve; note that both\n      sieves are \\emph{the same}; they just behave differently\n      depending on which group is passing through. \\\\ \n      Let $\\testp=$ a positve mammography.};\n\n   % arrows showing the circles passing through the seives\n   \\draw[-&gt;,thick,draw=red!80] (3.5,-5.9) -- (3.5,-8.6);\n   \\draw[-&gt;,thick,draw=blue!50] (8.5,-8.1) -- (8.5,-8.6);\n\n   % numerator\n   \\node[ball,text width=0.05cm,fill=red!70] (can) at (6,-10.5) {};\n\n   % dividing line\n   \\draw[thick] (5,-11) -- (7,-11);\n\n   % demoniator\n   \\node[ball,text width=0.39cm,fill=blue!40,anchor=base] (ncan) at (6.5,-11.5) {};\n   \\node[ball,text width=0.05cm,fill=red!70,anchor=base] (can2) at (5.5,-11.5) {};\n\n   % plus sign in denominator\n   \\draw[thick] (5.9,-11.4) -- (5.9,-11.6);\n   \\draw[thick] (5.8,-11.5) -- (6,-11.5);\n\n   % arrows showing the output of the sieves formed the fraction\n   \\draw[-&gt;,thick,draw=red!80] (tray1.south) to [out=280,in=180] (can);\n   \\draw[-&gt;,thick,draw=red!80] (tray1.south) to [out=280,in=180] (can2);\n   \\node[anchor=north,inner sep=.1cm,align=center,fill=white] (why2) at\n      (3.8,-9.8) {$1\\% * 80\\%$};\n\n   \\draw[-&gt;,thick,draw=blue!50] (tray2.south) to [out=265,in=0] (ncan);\n   \\node[anchor=north,inner sep=.1cm,align=center,fill=white] (why2) at\n      (8.4,-9.8) {$99\\% * 9.6\\%$};\n\n   % explanation of final formula\n   \\node[anchor=north west,text width=6.5cm] (note2) at (-6,-12.5)\n      {Finally, to find the probability that a positive test\n         \\emph{actually means cancer}, we look at those who passed\n         through the sieve \\emph{with cancer}, and divide by all who\n         received a positive test, cancer or not.}; \n\n   % illustrated fraction turned into math\n   \\node[anchor=north,text width=10cm] (solution) at (6,-12.5) {\n   \\begin{align*}\n         \\frac{\\p(\\testp\\mid\\cancer)}{\\p(\\testp\\mid\\cancer)\n         + \\p(\\testp\\mid{\\sim}\\cancer)} &= \\\\\n         \\frac{1\\% * 80\\%}{(1\\% * 80\\%) + (99\\% * 9.6\\%)} &= 7.8\\%\n         = \\p(\\cancer\\mid\\testp)\n      \\end{align*}};\n\\end{tikzpicture}\n\n```\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n\n\n# \\usetikzlibrary{arrows}\n# \\usetikzlibrary{positioning}\n# \\usetikzlibrary{calc}\n# \\usetikzlibrary{arrows.meta}\n# \\usetikzlibrary{decorations.pathreplacing}\n\n# \\begin{tikzpicture}\n# \\draw (0,0)node(a){} -- (10,0) node (b) {} ;\n# \\foreach \\x in  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10} % edit here for the vertical lines\n# \\draw[shift={(\\x,0)},color=black] (0pt,3pt) -- (0pt,-3pt);\n# \\foreach \\x in {0, 0.2, 0.4, 0.6, 0.8, 1} % edit here for the numbers\n# \\draw[shift={(\\x*10,0)},color=black] (0pt,0pt) -- (0pt,-3pt) node[below]\n# {$\\x$};\n# \\node at (8, 0.5) (eq1) {$\\textcolor{red}{\\boldsymbol{SQ}}$};\n# \\node at (4, 0.5) (eq2) {$\\textcolor{purple}{\\boldsymbol{G_i(0)}}$}; \n# \\node at (7, 0.5) (eq2) {$\\textcolor{purple}{\\boldsymbol{G_i(1)}}$}; \n# \\node at (3, 0.5) (eq3) {$\\textcolor{blue}{\\boldsymbol{P}}$};\n# \\node at (0, 0.5) (eq4) {$\\textcolor{black}{\\boldsymbol{x_i}}$};\n# \\draw[decorate, decoration={brace, amplitude=6pt, mirror},] ([yshift=0.5cm]4,0.5)-- node[above=0.25cm]\n# {\\shortstack{Text}}([yshift=0.5cm]3,0.5);\n# \\draw[decorate, decoration={brace, amplitude=6pt},] ([yshift=-1cm]7,0)-- node[below=0.25cm]\n# {\\shortstack{Text}}([yshift=-1cm]3,0);\n# \\end{tikzpicture}\n```\n\n\nTikz Neural Networks\nhttps://tikz.net/neural_networks/\n\nCode```{r, engine = 'tikz', engine.opts=font_opts2, cache=TRUE}\n#| cache: true\n\n\\usetikzlibrary{arrows.meta} % for arrow size\n\n\\tikzset{&gt;=latex} % for LaTeX arrow head\n\\colorlet{myred}{red!80!black}\n\\colorlet{myblue}{blue!80!black}\n\\colorlet{mygreen}{green!60!black}\n\\colorlet{myorange}{orange!70!red!60!black}\n\\colorlet{mydarkred}{red!30!black}\n\\colorlet{mydarkblue}{blue!40!black}\n\\colorlet{mydarkgreen}{green!30!black}\n\\tikzstyle{node}=[thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]\n\\tikzstyle{node in}=[node,green!20!black,draw=mygreen!30!black,fill=mygreen!25]\n\\tikzstyle{node hidden}=[node,blue!20!black,draw=myblue!30!black,fill=myblue!20]\n\\tikzstyle{node convol}=[node,orange!20!black,draw=myorange!30!black,fill=myorange!20]\n\\tikzstyle{node out}=[node,red!20!black,draw=myred!30!black,fill=myred!20]\n\\tikzstyle{connect}=[thick,mydarkblue] %,line cap=round\n\\tikzstyle{connect arrow}=[-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten &lt;=0.5,shorten &gt;=1]\n\\tikzset{ % node styles, numbered for easy mapping with \\nstyle\n  node 1/.style={node in},\n  node 2/.style={node hidden},\n  node 3/.style={node out},\n}\n\\def\\nstyle{int(\\lay&lt;\\Nnodlen?min(2,\\lay):3)} % map layer number onto 1, 2, or 3\n\n\n\\begin{tikzpicture}[x=2.7cm,y=1.6cm]\n  \\message{^^JNeural network activation}\n  \\def\\NI{5} % number of nodes in input layers\n  \\def\\NO{4} % number of nodes in output layers\n  \\def\\yshift{0.4} % shift last node for dots\n  \n  % INPUT LAYER\n  \\foreach \\i [evaluate={\\c=int(\\i==\\NI); \\y=\\NI/2-\\i-\\c*\\yshift; \\index=(\\i&lt;\\NI?int(\\i):\"n\");}]\n              in {1,...,\\NI}{ % loop over nodes\n    \\node[node in,outer sep=0.6] (NI-\\i) at (0,\\y) {$a_{\\index}^{(0)}$};\n  }\n  \n  % OUTPUT LAYER\n  \\foreach \\i [evaluate={\\c=int(\\i==\\NO); \\y=\\NO/2-\\i-\\c*\\yshift; \\index=(\\i&lt;\\NO?int(\\i):\"m\");}]\n    in {\\NO,...,1}{ % loop over nodes\n    \\ifnum\\i=1 % high-lighted node\n      \\node[node hidden]\n        (NO-\\i) at (1,\\y) {$a_{\\index}^{(1)}$};\n      \\foreach \\j [evaluate={\\index=(\\j&lt;\\NI?int(\\j):\"n\");}] in {1,...,\\NI}{ % loop over nodes in previous layer\n        \\draw[connect,white,line width=1.2] (NI-\\j) -- (NO-\\i);\n        \\draw[connect] (NI-\\j) -- (NO-\\i)\n          node[pos=0.50] {\\contour{white}{$w_{1,\\index}$}};\n      }\n    \\else % other light-colored nodes\n      \\node[node,blue!20!black!80,draw=myblue!20,fill=myblue!5]\n        (NO-\\i) at (1,\\y) {$a_{\\index}^{(1)}$};\n      \\foreach \\j in {1,...,\\NI}{ % loop over nodes in previous layer\n        %\\draw[connect,white,line width=1.2] (NI-\\j) -- (NO-\\i);\n        \\draw[connect,myblue!20] (NI-\\j) -- (NO-\\i);\n      }\n    \\fi\n  }\n  \n  % DOTS\n  \\path (NI-\\NI) --++ (0,1+\\yshift) node[midway,scale=1.2] {$\\vdots$};\n  \\path (NO-\\NO) --++ (0,1+\\yshift) node[midway,scale=1.2] {$\\vdots$};\n  \n  % EQUATIONS\n  \\def\\agr#1{{\\color{mydarkgreen}a_{#1}^{(0)}}}\n  \\node[below=17,right=11,mydarkblue,scale=0.95] at (NO-1)\n    {$\\begin{aligned} %\\underset{\\text{bias}}{b_1}\n       &= \\color{mydarkred}\\sigma\\left( \\color{black}\n            w_{1,0}\\agr{0} + w_{1,1}\\agr{1} + \\ldots + w_{1,n}\\agr{n} + b_1^{(0)}\n          \\color{mydarkred}\\right)\\\\\n       &= \\color{mydarkred}\\sigma\\left( \\color{black}\n            \\sum_{i=1}^{n} w_{1,i}\\agr{i} + b_1^{(0)}\n           \\color{mydarkred}\\right)\n     \\end{aligned}$};\n  \\node[right,scale=0.9] at (1.3,-1.3)\n    {$\\begin{aligned}\n      {\\color{mydarkblue}\n      \\begin{pmatrix}\n        a_{1}^{(1)} \\\\[0.3em]\n        a_{2}^{(1)} \\\\\n        \\vdots \\\\\n        a_{m}^{(1)}\n      \\end{pmatrix}}\n      &=\n      \\color{mydarkred}\\sigma\\left[ \\color{black}\n      \\begin{pmatrix}\n        w_{1,0} & w_{1,1} & \\ldots & w_{1,n} \\\\\n        w_{2,0} & w_{2,1} & \\ldots & w_{2,n} \\\\\n        \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n        w_{m,0} & w_{m,1} & \\ldots & w_{m,n}\n      \\end{pmatrix}\n      {\\color{mydarkgreen}\n      \\begin{pmatrix}\n        a_{1}^{(0)} \\\\[0.3em]\n        a_{2}^{(0)} \\\\\n        \\vdots \\\\\n        a_{n}^{(0)}\n      \\end{pmatrix}}\n      +\n      \\begin{pmatrix}\n        b_{1}^{(0)} \\\\[0.3em]\n        b_{2}^{(0)} \\\\\n        \\vdots \\\\\n        b_{m}^{(0)}\n      \\end{pmatrix}\n      \\color{mydarkred}\\right]\\\\[0.5em]\n      {\\color{mydarkblue}a^{(1)}}\n      &= \\color{mydarkred}\\sigma\\left( \\color{black}\n           \\mathbf{W}^{(0)} {\\color{mydarkgreen}a^{(0)}}+\\mathbf{b}^{(0)}\n         \\color{mydarkred}\\right)\n         %\\color{black},\\quad \\mathbf{W}^{(0)} \\in \\mathbb{R}^{m\\times n}\n    \\end{aligned}$};\n  \n\\end{tikzpicture}\n\n```\n\n\n\n\n\n\n\nAutoencoder\nhttps://tikz.net/autoencoder/\n\nCode```{r, engine = 'tikz', engine.opts=font_opts}\n#| eval: true\n#| cache: true\n\n\\newcommand{\\xin}[2]{$x_#2$}\n\\newcommand{\\xout}[2]{$\\hat x_#2$}\n\n\\begin{neuralnetwork}[height=8]\n  \\tikzstyle{input neuron}=[neuron, fill=orange!70];\n  \\tikzstyle{output neuron}=[neuron, fill=blue!60!black, text=white];\n\n  \\inputlayer[count=8, bias=false, title=Input Layer, text=\\xin]\n\n  \\hiddenlayer[count=5, bias=false]\n  \\linklayers\n\n  \\hiddenlayer[count=3, bias=false, title=Latent\\\\Representation]\n  \\linklayers\n\n  \\hiddenlayer[count=5, bias=false]\n  \\linklayers\n\n  \\outputlayer[count=8, title=Output Layer, text=\\xout]\n  \\linklayers\n\n\\end{neuralnetwork}\n\n```\n\n\n\n\n\n\n\nVAE\nhttps://tikz.net/vae/\n\nCode```{r, engine = 'tikz', engine.opts=font_opts, cache=TRUE}\n#| eval: true\n\n\\usetikzlibrary{fit,positioning}\n\n\\newcommand\\drawNodes[2]{\n  % #1 (str): namespace\n  % #2 (list[list[str]]): list of labels to print in the node of each neuron\n  \\foreach \\neurons [count=\\lyrIdx] in #2 {\n    \\StrCount{\\neurons}{,}[\\lyrLength] % use xstring package to save each layer size into \\lyrLength macro\n    \\foreach \\n [count=\\nIdx] in \\neurons\n      \\node[neuron] (#1-\\lyrIdx-\\nIdx) at (2*\\lyrIdx, \\lyrLength/2-1.4*\\nIdx) {\\n};\n  }\n}\n\n\\newcommand\\denselyConnectNodes[2]{\n  % #1 (str): namespace\n  % #2 (list[int]): number of nodes in each layer\n  \\foreach \\n [count=\\lyrIdx, remember=\\lyrIdx as \\previdx, remember=\\n as \\prevn] in #2 {\n    \\foreach \\y in {1,...,\\n} {\n      \\ifnum \\lyrIdx &gt; 1\n        \\foreach \\x in {1,...,\\prevn}\n          \\draw[-&gt;] (#1-\\previdx-\\x) -- (#1-\\lyrIdx-\\y);\n      \\fi\n    }\n  }\n}\n\n\\begin{tikzpicture}[\n    shorten &gt;=1pt, shorten &lt;=1pt,\n    neuron/.style={circle, draw, minimum size=4ex, thick},\n    legend/.style={font=\\large\\bfseries},\n  ]\n\n  % encoder\n  \\drawNodes{encoder}{{{,,,,}, {,,,}, {,,}}}\n  \\denselyConnectNodes{encoder}{{5, 4, 3}}\n\n  % decoder\n  \\begin{scope}[xshift=11cm]\n    \\drawNodes{decoder}{{{,,}, {,,,}, {,,,,}}}\n    \\denselyConnectNodes{decoder}{{3, 4, 5}}\n  \\end{scope}\n\n  % mu, sigma, sample nodes\n  \\foreach \\idx in {1,...,3} {\n      \\coordinate[neuron, right=2 of encoder-3-2, yshift=\\idx cm,, fill=yellow, fill opacity=0.2] (mu-\\idx);\n      \\coordinate[neuron, right=2 of encoder-3-2, yshift=-\\idx cm, fill=blue, fill opacity=0.1] (sigma-\\idx);\n      \\coordinate[neuron, right=4 of encoder-3-2, yshift=\\idx cm-2cm, fill=green, fill opacity=0.1] (sample-\\idx);\n    }\n\n  % mu, sigma, sample boxes\n  \\node [label=$\\mu$, fit=(mu-1) (mu-3), draw, fill=yellow, opacity=0.45] (mu) {};\n  \\node [label=$\\sigma$, fit=(sigma-1) (sigma-3), draw, fill=blue, opacity=0.3] (sigma) {};\n  \\node [label=sample, fit=(sample-1) (sample-3), draw, fill=green, opacity=0.3] (sample) {};\n\n  % mu, sigma, sample connections\n  \\draw[-&gt;] (mu.east) edge (sample.west) (sigma.east) -- (sample.west);\n  \\foreach \\a in {1,2,3}\n  \\foreach \\b in {1,2,3} {\n      \\draw[-&gt;] (encoder-3-\\a) -- (mu-\\b);\n      \\draw[-&gt;] (encoder-3-\\a) -- (sigma-\\b);\n      \\draw[-&gt;] (sample-\\a) -- (decoder-1-\\b);\n    }\n\n  % input + output labels\n  \\foreach \\idx in {1,...,5} {\n      \\node[left=0 of encoder-1-\\idx] {$x_\\idx$};\n      \\node[right=0 of decoder-3-\\idx] {$\\hat x_\\idx$};\n    }\n  \\node[above=0.1 of encoder-1-1] {input};\n  \\node[above=0.1 of decoder-3-1] {output};\n\n\\end{tikzpicture}\n\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#bayes-vs-regular-nn",
    "href": "Misc/Visuals_Interactives/tikz_net.html#bayes-vs-regular-nn",
    "title": "tikz",
    "section": "bayes vs regular nn",
    "text": "bayes vs regular nn\nhttps://tikz.net/regular-vs-bayes-nn/\n\nCode```{r, engine = 'tikz', engine.opts=font_opts}\n#| cache: true\n#| fig-align: \"center\"\n\n\n\\usetikzlibrary{calc}\n\\def\\layersep{3cm}\n\n\\newcommand\\nn[1]{\n    % Input layer\n    \\foreach \\y in {1,...,2}\n        \\node[neuron, fill=green!40] (i\\y-#1) at (0,\\y+1) {$i\\y$};\n\n    % Hidden layer\n    \\foreach \\y in {1,...,4}\n        \\path node[neuron, fill=blue!40] (h\\y-#1) at (\\layersep,\\y) {$h\\y$};\n\n    % Output node\n    \\node[neuron, fill=red!40] (o-#1) at (2*\\layersep,2.5) {$o$};\n\n    % Connect every node in the input layer with every node in the hidden layer.\n    \\foreach \\source in {1,...,2}\n        \\foreach \\dest in {1,...,4}\n            \\path (i\\source-#1) edge (h\\dest-#1);\n\n    % Connect every node in the hidden layer with the output layer\n    \\foreach \\source in {1,...,4}\n        \\path (h\\source-#1) edge (o-#1);\n}\n\n\n\\begin{tikzpicture}[\n    scale=1.2,\n    shorten &gt;=1pt,-&gt;,draw=black!70, node distance=\\layersep,\n    neuron/.style={circle,fill=black!25,minimum size=20,inner sep=0},\n    edge/.style 2 args={pos={(mod(#1+#2,2)+1)*0.33}, font=\\tiny},\n    distro/.style 2 args={\n        edge={#1}{#2}, node contents={}, minimum size=0.6cm, path picture={\\draw[double=orange,white,thick,double distance=1pt,shorten &gt;=0pt] plot[variable=\\t,domain=-1:1,samples=51] ({\\t},{0.2*exp(-100*(\\t-0.05*(#1-1))^2 - 3*\\t*#2))});}\n      },\n    weight/.style 2 args={\n        edge={#1}{#2}, node contents={\\pgfmathparse{0.35*#1-#2*0.15}\\pgfmathprintnumber[fixed]{\\pgfmathresult}}, fill=white, inner sep=2pt\n      }\n  ]\n  \\nn{regular}\n\n  \\begin{scope}[xshift=8cm]\n    \\nn{bayes}\n  \\end{scope}\n\n  % Draw weights for all regular edges.\n  \\foreach \\i in {1,...,2}\n  \\foreach \\j in {1,...,4}\n  \\path (i\\i-regular) -- (h\\j-regular) node[weight={\\i}{\\j}];\n  \\foreach \\i in {1,...,4}\n  \\path (h\\i-regular) -- (o-regular) node[weight={\\i}{1}];\n\n  % Draw distros for all Bayesian edges.\n  \\foreach \\i in {1,...,2}\n  \\foreach \\j in {1,...,4}\n  \\path (i\\i-bayes) -- (h\\j-bayes) node[distro={\\i}{\\j}];\n  \\foreach \\i in {1,...,4}\n  \\path (h\\i-bayes) -- (o-bayes) node[distro={\\i}{1}];\n\\end{tikzpicture}\n\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#nndiagram",
    "href": "Misc/Visuals_Interactives/tikz_net.html#nndiagram",
    "title": "tikz",
    "section": "nndiagram",
    "text": "nndiagram\nhttps://github.com/ccfang2/nndiagram\nCode```{r}\n#| results: 'asis'\n#| eval: false\n\nlibrary(nndiagram)\nnnd &lt;- nndiagram(input=3, hidden=c(4,4,4))\ncat(paste(nnd,\"\\n\"))\n```\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\def\\layersep{2.5cm} \n\\newcommand*\\circled[1]{\\tikz[baseline=(char.base)]{ \n  \\node[shape=rectangle,inner sep=3pt, draw=black!100, fill= black!25] (char) {#1};}} \n \n\n\\centering \n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!100, node distance=\\layersep, scale=1] \n  \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]; \n  \\tikzstyle{neuron}=[circle, draw=black!100, minimum size=17pt,inner sep=0pt]; \n  \\tikzstyle{input neuron}=[neuron]; \n  \\tikzstyle{output neuron}=[neuron]; \n  \\tikzstyle{hidden neuron}=[neuron]; \n  \\tikzstyle{annot} = [text width=4em, text centered, text=black!100] \n \n  % drawing neurons \n  \\foreach \\name / \\y in {1,...,3} \n      \\node [input neuron, pin=left:\\textcolor{black!100}{Input \\y}] (I-\\name) at (0,-0.5-\\y) {};\n  \\foreach \\name / \\y in {1/1,2/2,3/3,4/4} \n      \\path[yshift=0cm] \n          node[hidden neuron] (H-\\name) at (1* \\layersep,-\\y cm) {};\n  \\foreach \\name / \\y in {5/1,6/2,7/3,8/4} \n      \\path[yshift=0cm] \n          node[hidden neuron] (H-\\name) at (2* \\layersep,-\\y cm) {};\n  \\foreach \\name / \\y in {9/1,10/2,11/3,12/4} \n      \\path[yshift=0cm] \n          node[hidden neuron] (H-\\name) at (3* \\layersep,-\\y cm) {};\n  \\node[output neuron,pin={[pin edge={-&gt;}]right:\\textcolor{black!100}{Output}}, right of=H-10, yshift=-0.5cm] (O) {}; \n\n   % drawing arrows \n  \\foreach \\source in {1,...,3} \n      \\foreach \\dest in {1,...,4}\n           \\path (I-\\source) edge (H-\\dest); \n  \\foreach \\source in {1,...,4} \n      \\foreach \\dest in {5,...,8}\n           \\path (H-\\source) edge (H-\\dest); \n  \\foreach \\source in {5,...,8} \n      \\foreach \\dest in {9,...,12}\n           \\path (H-\\source) edge (H-\\dest); \n  \\foreach \\source in {9,...,12}\n           \\path (H-\\source) edge (O);\n \n   % annotations \n  \\node[annot,above of=I-1, node distance=2.5cm] {Input layer}; \n  \\node[annot,above of=I-1, node distance=1.5cm] {$\\circled{3}$}; \n  \\node[annot,above of=H-1, node distance=2cm] (hl1) {Hidden layer 1}; \n  \\node[annot,above of=H-1, node distance=1cm] (hl1) {$\\circled{4}$}; \n  \\node[annot,above of=H-5, node distance=2cm] (hl2) {Hidden layer 2}; \n  \\node[annot,above of=H-5, node distance=1cm] (hl2) {$\\circled{4}$}; \n  \\node[annot,above of=H-9, node distance=2cm] (hl3) {Hidden layer 3}; \n  \\node[annot,above of=H-9, node distance=1cm] (hl3) {$\\circled{4}$}; \n  \\node[annot,above of =O, node distance=3.5cm] {Output layer}; \n  \\node[annot,above of =O, node distance=2.5cm] {$\\circled{1}$}; \n\n\\end{tikzpicture} \n\n```\n\n\n\n\n\n\n\nLatent Space Projection\nhttps://tikz.net/manifold/\n\nCode```{{r, engine = 'tikz',engine.opts=list(extra.preamble = c(\"\\\\usepackage{cmbright}\"))}}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{arrows.meta}\n\\definecolor{green}{rgb}{0.0,0.50,0.0}\n\\tikzset{&gt;={Straight Barb[angle'=80, scale=1.1]}}\n\\begin{tikzpicture}\n\n\\draw[-&gt;] (0, 0) -- ++(0, 2);\n\\draw[-&gt;] (0, 0) -- ++(2.5, 0.6);\n\\draw[-&gt;] (0, 0) -- ++(3, 0) node[midway, below, yshift=-0.5em]\n    {Original space ${\\cal X}$};\n\n\\draw[fill=green!50, draw=none, shift={(0.2, 0.7)},scale=0.5]\n  (0, 0) to[out=20, in=140] (1.5, -0.2) to [out=60, in=160]\n  (5, 0.5) to[out=130, in=60]\n  cycle;\n\n\\shade[thin, left color=green!10, right color=green!50, draw=none,\n  shift={(0.2, 0.7)},scale=0.5]\n  (0, 0) to[out=10, in=140] (3.3, -0.8) to [out=60, in=190] (5, 0.5)\n    to[out=130, in=60] cycle;\n\n  \\draw[-&gt;] (4.8, 0.8) -- ++(0, 2);\n  \\draw[-&gt;] (4.8, 0.8) -- ++(2, 0) node[midway, below, yshift=-0.5em]\n      {Latent space ${\\cal F}$};\n\n  \\draw[thin, fill=green!30, draw=none, shift={(5.4, 1.1)}, rotate=20]\n    (0, 0) -- (1, 0) -- (1, 1) -- (0, 1) -- cycle;\n\n  \\draw[thick,-&gt;,red]\n    (1.5, 1.3) to [out=55, in=150] node[midway, above, xshift=6pt, yshift=2pt]\n    {$f$} (5.7, 2);\n\n  \\draw[thick,-&gt;,blue] (1.5, 1.3) ++(4.03, 0.3) to [out=150, in=55]\n    node[midway, below, xshift=2pt, yshift=-2pt] {$g$} ++(-3.6, -0.5);\n\n\\end{tikzpicture}\n\n```\n\n\n\n\n\n\n\nFlow\nhttps://tikz.net/maf/\n\nCode```{r, engine = 'tikz'}\n#| cache: true\n\n\\usetikzlibrary{calc,positioning}\n\n\n\\begin{tikzpicture}[\n    thick, text centered,\n    box/.style={draw, thin, minimum width=1cm},\n    func/.style={circle, text=white},\n    input/.style={draw=red, very thick},\n  ]\n\n  % x nodes\n  \\node[box, input, fill=blue!20] (x1) {$x_1$};\n  \\node[box, input, fill=blue!20, right of=x1] (x2) {$x_2$};\n  \\node[right of=x2] (xdots1) {\\dots};\n  \\node[box, input, fill=blue!20, right of=xdots1] (xd) {$x_d$};\n  \\node[box, fill=green!60!black, text opacity=1, opacity=0.4, right=2 of xd] (xdp1) {$x_{d+1}$};\n  \\node[right of=xdp1] (xdots2) {\\dots};\n  \\node[box, fill=green!60!black, text opacity=1, opacity=0.4, right of=xdots2] (xD) {$x_D$};\n\n  % z nodes\n  \\node[box, fill=blue!20, below=3 of x1] (z1) {$z_1$};\n  \\node[box, fill=blue!20, right of=z1] (z2) {$z_2$};\n  \\node[right of=z2] (zdots1) {\\dots};\n  \\node[box, fill=blue!20, right of=zdots1] (zd) {$z_d$};\n  \\node[box, input, fill=orange!40, right=2 of zd] (zdp1) {$z_{d+1}$};\n  \\node[right of=zdp1] (zdots2) {\\dots};\n  \\node[box, fill=orange!40, right of=zdots2] (zD) {$z_D$};\n\n  % z to x lines\n  \\draw[-&gt;] (zdp1) -- (xdp1);\n\n  % scale and translate functions\n  \\node[func, font=\\large, fill=teal, above right=0.1] (t) at ($(zd)!0.5!(xdp1)$) {$t$};\n  \\fill[teal, opacity=0.5] (x1.south west) -- (t.center) -- (xd.south east) -- (x1.south west);\n\n  \\node[func, font=\\large, fill=orange, below left=0.1] (s) at ($(zd)!0.5!(xdp1)$) {$s$};\n  \\fill[orange, opacity=0.5] (x1.south west) -- (s.center) -- (xd.south east) -- (x1.south west);\n\n  % feeding in s and t\n  \\node[func, inner sep=0, fill=orange] (odot1) at ($(zdp1)!0.4!(xdp1)$) {$\\odot$};\n  \\node[func, inner sep=0, fill=teal] (oplus1) at ($(zdp1)!0.7!(xdp1)$) {$\\oplus$};\n  \\draw[orange, -&gt;] (s) to[bend right=5] (odot1);\n  \\draw[teal, -&gt;] (t) to[bend right=5] (oplus1);\n\n\\end{tikzpicture}\n\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#alm",
    "href": "Misc/Visuals_Interactives/tikz_net.html#alm",
    "title": "tikz",
    "section": "ALM",
    "text": "ALM\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n\\usetikzlibrary{positioning}\n\n\\def\\layersep{3.5cm}\n\n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!50, node distance=\\layersep]\n    \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{annot} = [text width=4em, text centered]\n\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,2}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {};\n\n    % Draw the output layer nodes\n    \\foreach \\name / \\y in {1,2,3}\n        \\path[yshift=1.0cm]\n            node[output neuron] (O-\\name) at (\\layersep,-\\y cm) {};\n\n    % Connect every node in the input layer with every node in the\n    % output layer.\n    \\foreach \\source in {1,2}\n        \\foreach \\dest in {1,2,3}\n            \\path (I-\\source) edge (O-\\dest);\n\n    % Annotate the layers with equations\n    \\node[above of=I-1, node distance=1.5cm] (il) {$a_i(X)=e^{-\\gamma \\cdot (X-X_i)^2}$ \\\\ $\\frac{a_i(X)}{\\sum a_i(X)}$};\n    \\node[above of=O-1, node distance=1.5cm] (ol) {$O_j(X)=\\sum_{i=1}^M w_{j i} \\cdot a_i(X)$ \\\\ $P[Y_j|X]=\\frac{O_j(X)}{\\sum_{k=1}^L O_k(X)}$};\n\\end{tikzpicture}\n\n```\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n#|\n\\usetikzlibrary{positioning}\n\n\\def\\layersep{3.5cm}\n\n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!50, node distance=\\layersep]\n    \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{annot} = [text width=4em, text centered]\n\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,2}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {$X_{\\y}$};\n\n    % Draw the output layer nodes\n    \\foreach \\name / \\y in {1,2,3}\n        \\path[yshift=1.0cm]\n            node[output neuron] (O-\\name) at (\\layersep,-\\y cm) {$Y_{\\y}$};\n\n    % Connect every node in the input layer with every node in the\n    % output layer.\n    \\foreach \\source in {1,2}\n        \\foreach \\dest in {1,2,3}\n            \\path (I-\\source) edge (O-\\dest);\n\n    % Annotate the layers with equations\n    \\node[above of=I-1, node distance=1.5cm] (il) {$a_i(X)=e^{-\\gamma \\cdot (X-X_i)^2}$ \\\\ $\\frac{a_i(X)}{\\sum a_i(X)}$};\n    \\node[above of=O-1, node distance=1.5cm] (ol) {$O_j(X)=\\sum_{i=1}^M w_{j i} \\cdot a_i(X)$ \\\\ $P[Y_j|X]=\\frac{O_j(X)}{\\sum_{k=1}^L O_k(X)}$};\n\\end{tikzpicture}\n\n```\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n#|\n\\usetikzlibrary{positioning, fit}\n\n\\def\\layersep{3.5cm}\n\n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!50, node distance=\\layersep]\n    \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{annot} = [text width=4em, text centered]\n\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,2}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {$X_{\\y}$};\n\n    % Draw the output layer nodes\n    \\foreach \\name / \\y in {1,2,3}\n        \\path[yshift=1.0cm]\n            node[output neuron] (O-\\name) at (\\layersep,-\\y cm) {$Y_{\\y}$};\n\n    % Connect every node in the input layer with every node in the\n    % output layer.\n    \\foreach \\source in {1,2}\n        \\foreach \\dest in {1,2,3}\n            \\path (I-\\source) edge (O-\\dest);\n\n    % Annotate the layers with equations\n    \\node[above of=I-1, node distance=1.5cm] (il) {$a_i(X)=e^{-\\gamma \\cdot (X-X_i)^2}$ \\\\ $\\frac{a_i(X)}{\\sum a_i(X)}$};\n    \\node[above of=O-1, node distance=1.5cm] (ol) {$O_j(X)=\\sum_{i=1}^M w_{j i} \\cdot a_i(X)$ \\\\ $P[Y_j|X]=\\frac{O_j(X)}{\\sum_{k=1}^L O_k(X)}$};\n\n    % Draw rectangles over the input and output layer nodes\n    \\node[rectangle, draw=black, inner sep=0.5cm, fit=(I-1) (I-2)] (inputbox) {};\n    \\node[rectangle, draw=black, inner sep=0.5cm, fit=(O-1) (O-2) (O-3)] (outputbox) {};\n\n\\end{tikzpicture}\n\n```\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n\n\\usetikzlibrary{positioning, fit, calc}\n\n\\def\\layersep{5cm}\n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!50, node distance=\\layersep]\n    \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{annot} = [text centered]\n\n    % Draw the input layer nodes horizontally\n    \\foreach \\name / \\y in {1,2}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at ($(1.5*\\y cm,0)$) {$X_{\\y}$};\n\n    % Draw the output layer nodes\n    \\foreach \\name / \\y in {1,2,3}\n        \\node[output neuron] (O-\\name) at ($(1.5*\\name cm, -\\layersep)$) {$Y_{\\y}$};\n\n    % Connect every node in the input layer with every node in the\n    % output layer.\n    \\foreach \\source in {1,2}\n        \\foreach \\dest in {1,2,3}\n            \\path (I-\\source) edge (O-\\dest);\n\n    % Add input stimulus symbol above the input layer\n    \\node[above=2cm of I-1, anchor=south] (input-stimulus) {Stimulus $S$};\n\n    % Add output response symbol below the output layer\n    \\node[below=2cm of O-2, anchor=north] (output-response) {Response $R$};\n\n    % Annotate the layers with equations\n    \\node[left=4cm of I-1, anchor=east, font=\\small, text width=5cm] (ile) {$a_i(X)=e^{-\\gamma \\cdot (X-X_i)^2}$ \\\\ $\\frac{a_i(X)}{\\sum a_i(X)}$};\n    \\node[left=4cm of O-1, anchor=east, font=\\small, text width=5cm] (ole) {$O_j(X)=\\sum_{i=1}^M w_{j i} \\cdot a_i(X)$ \\\\ $P[Y_j|X]=\\frac{O_j(X)}{\\sum_{k=1}^L O_k(X)}$};\n\n    % Add rectangles around input and output layers\n    \\node[draw,rectangle,fit=(I-1) (I-2),minimum width=4cm, label=above:Input Layer] (input-rect) {};\n    \\node[draw,rectangle,fit=(O-1) (O-2) (O-3),minimum width=4cm, label=above:Output Layer] (output-rect) {};\n\n    % Add rectangle for the decoding process\n    \\node[draw,rectangle,fit=(output-response), label=above:Decoding Process, minimum width=3cm] (decoding-rect) {};\n\n    % Add arrow from output layer to decoding process\n    \\draw[-&gt;,thick] (output-rect) -- (decoding-rect);\n\n\\end{tikzpicture}\n\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/ojs_alm.html",
    "href": "Misc/Visuals_Interactives/ojs_alm.html",
    "title": "OJS ALM",
    "section": "",
    "text": "Codepacman::p_load(tidyverse)\nd &lt;- tibble(x=1:20,y=x^2)\nojs_define(d = d)\n\ninputNodes = seq(1,7,1)  # \noutputNodes = seq(50,1600,50)\n#wm=matrix(rnorm(length(inputNodes)*length(outputNodes),5,2),nrow=length(outputNodes),ncol=length(inputNodes))\n#ojs_define(iN = inputNodes, outputNodes = outputNodes, wm = wm)\n\n\n\nCoded3 = require(\"d3@7\")\nmath = require('mathjs')\n// let inputNodes = Array.from({ length: 7 }, (_, i) =&gt; i + 1);\n// let outputNodes = Array.from({ length: 32 }, (_, i) =&gt; (i + 1) * 50);\n// let wm = Array.from({ length: outputNodes.length }, () =&gt;\n//   Array.from({ length: inputNodes.length }, () =&gt; 0.0)\n// );\n\nfunction inputActivation(xTarget, c) {\n  console.log(inputNodes)\n  return inputNodes.map((inputNode) =&gt;\n    Math.exp(-1 * c * Math.pow(xTarget - inputNode, 2))\n  );\n}\n\n\nfunction outputActivation(xTarget, weights, c) {\n  const inputAct = inputActivation(xTarget, c);\n  return math.multiply(weights, inputAct);\n}\n\nfunction meanPrediction(xTarget, weights, c) {\n  const outputAct = outputActivation(xTarget, weights, c);\n  const probability = math.divide(outputAct, math.sum(outputAct));\n  return math.multiply(outputNodes, probability);\n}\n\n\nfunction updateWeights(xNew, yNew, weights, c, lr) {\n  const yFeedbackActivation = outputNodes.map(\n    (outputNode) =&gt; Math.exp(-1 * c * Math.pow(yNew - outputNode, 2))\n  );\n //console.log(yFeedbackActivation)\n  const xFeedbackActivation = outputActivation(xNew, weights, c);\n  const inputAct = inputActivation(xNew, c);\n  const inputActReshaped = math.reshape(inputAct, [inputAct.length, 1]);\n  const yFeedbackActivationReshaped = math.reshape(yFeedbackActivation, [yFeedbackActivation.length, 1]);\n  const xFeedbackActivationReshaped = math.reshape(xFeedbackActivation, [xFeedbackActivation.length, 1]);\n  const error = math.reshape(math.subtract(yFeedbackActivationReshaped, xFeedbackActivationReshaped), [yFeedbackActivation.length, 1]);\n  // console.log(math.size(math.transpose(inputActReshaped)))\n  const weightUpdate = math.multiply(error, math.transpose(inputActReshaped));\n  //console.log(weightUpdate)\n  const raw_Weights = math.add(weights, math.multiply(lr, weightUpdate));\n\n  const new_Weights = raw_Weights\n  //return JSON.parse(result);\n  return(new_Weights)\n}\n\nfunction randomNormal(mean, sd) {\n  let u = 0,\n    v = 0;\n  while (u === 0) u = Math.random();\n  while (v === 0) v = Math.random();\n  const z = Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2.0 * Math.PI * v);\n  return mean + z * sd;\n}\n\nfunction examPrediction(xTarget, weights, c, trainVec) {\n  const nearestTrain = trainVec[math.argmin(math.abs(trainVec - xTarget))];\n  const aResp = meanPrediction(nearestTrain, weights, c);\n  const xUnder = math.min(trainVec) === nearestTrain ? nearestTrain : trainVec[math.findIndex(trainVec, (d) =&gt; d === nearestTrain) - 1];\n  const xOver = math.max(trainVec) === nearestTrain ? nearestTrain : trainVec[math.findIndex(trainVec, (d) =&gt; d === nearestTrain) + 1];\n  const mUnder = meanPrediction(xUnder, weights, c);\n  const mOver = meanPrediction(xOver, weights, c);\n  const examOutput = math.round(aResp + ((mOver - mUnder) / (xOver - xUnder)) * (xTarget - nearestTrain), 3);\n  return examOutput;\n}\n\n\nfunction trainALM(dat, c, lr, weights) {\n    console.log('training')\n  const almTrain = new Array(dat.input.length).fill(NaN);\n  for (let i = 0; i &lt; dat.input.length; i++) {\n    console.log('i: ', i, ' dat.input[i]: ', dat.input[i], ' dat.vx[i]: ', dat.vx[i], ' c: ', c, ' lr: ', lr)\n    weights = updateWeights(dat.input[i], dat.vx[i], weights, c, lr);\n    const resp = math.round(meanPrediction(dat.input[i], weights, c),0);\n    // round resp to 1 decimal place\n    almTrain[i] = resp;\n      weights = math.map(weights, (value) =&gt; {\n        return value &lt; 0 ? 0 : value;\n      });\n  }\n  console.log('almTrain: ', almTrain)\n  console.log(weights)\n  return {almTrain, weights};\n}\n\nfunction trainTestALM(dat, c = 0.05, lr = 0.5, weights, testVec) {\n  const almTrain = new Array(dat.length).fill(NaN);\n  \n  for (let i = 0; i &lt; dat.length; i++) {\n    weights = updateWeights(dat[i].input, dat[i].vx, weights, c, lr);\n    const resp = meanPrediction(dat[i].input, weights, c);\n    almTrain[i] = resp;\n    weights = math.map(weights, (value) =&gt; {\n      return value &lt; 0 ? 0 : value;\n    });\n  }\n\n  const almPred = testVec.map((value) =&gt; {\n    return meanPrediction(value, weights, c);\n  });\n\n  const examPred = testVec.map((value) =&gt; {\n    return examPrediction(value, weights, c, [1, ...math.sort(math.unique(dat.map((d) =&gt; d.input)))]);\n  });\n    \n  return { almTrain, almPred, examPred };\n}\n\n// Modify the sim_data function to accept the dataset as an argument\n// function sim_data(dat, c=0.5, lr=0.2, inNodes=7, outNodes=32, trainVec=[5,6,7]) {\n//   inputNodes = math.range(1,7,inNodes).toArray();  \n//   outputNodes = math.range(50,1600,outNodes).toArray(); \n//   wm = math.zeros(outputNodes.length, inputNodes.length)._data;\n//   tt = trainTest_alm(dat, c, lr, wm, trainVec);\n// }\n\nfunction gen_train(trainVec, trainRep, noise) {\n   let bandVec=[0,100,350,600,800,1000,1200];\n   let ts = [];\n   for (let i=0; i&lt;trainRep; i++) {\n       ts.push(...trainVec);\n   }\n    let mean = 0;\n    let stdDev = 1;\n   //let noiseVec = math.random([ts.length])._data;\n   //noiseVec = math.multiply(noiseVec, noise)._data;\n   //if(noise==0) {noiseVec=noiseVec*0}\n   let inputArr = [];\n   let vxArr = [];\n   for (let i=0; i&lt;ts.length; i++) {\n       inputArr.push(ts[i]);\n       vxArr.push(bandVec[ts[i]]);\n       //vxArr.push(bandVec[ts[i]]+noiseVec[i]);\n   }\n   return {input: inputArr, vx: vxArr};\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation\n\n\nCodeviewof c = Inputs.range([.0001, 2], {value: .00005, step: .05, label: \"c value:\"})\nviewof lr = Inputs.range([.001, 2], {value: .05, step: .01, label: \"lr value:\"})\n\nviewof n_inputNodes = Inputs.range([1, 50], {value: 7, step: 1, label: \"N Input Nodes:\"})\nviewof n_outputNodes = Inputs.range([1, 200], {value: 32, step: 1, label: \"N Output Nodes:\"})\n\nviewof weight_mean = Inputs.range([0, 1], {value: 0, step: .0005, label: \"initial weight mean:\"})\nviewof weight_sd = Inputs.range([.00000001, 1], {value: .000001, step: .0001, label: \"initial weight sd:\"})\n\nviewof trainRep = Inputs.range([4, 50], {value: 1, step: 1, label: \"Train Reps:\"})\n\n //inputNodes = Array.from({ length: n_inputNodes }, (_, i) =&gt; i + 1);\n inputNodes = Array.from({ length: n_inputNodes }, (_, i) =&gt; 1 + i * (7 - 1) / (n_inputNodes - 1));\n\n\n\nstart = 0;\nend = 1800;\nN_Steps = n_outputNodes; // replace with desired length\nstepSize = (end - start) / (N_Steps - 1);\noutputNodes = Array.from({ length: N_Steps }, (_, i) =&gt; start + i * stepSize);\n\nconsole.log(inputNodes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeconsole.log(outputNodes)\n\n\n\n\n\n\n\nCodewm = outputNodes.map(() =&gt; {\n  return inputNodes.map(() =&gt; randomNormal(weight_mean, weight_sd));\n});\n\nnoise=0\nviewof trainVec = Inputs.checkbox([1, 2, 3, 4, 5, 6], {value: [4,5,6], label: \"Select training examples:\"});\ngd = gen_train(trainVec, trainRep, noise);\n\n//trainVec = [1,2,4,5,6];\n//gd = gen_train(trainVec, trainRep, noise)\n// w2= updateWeights(4, 800, wm,c,lr)\n\ntalm = trainALM(gd, c, lr, wm);\n\n//inputNodes = transpose(iN)\nia = inputActivation(inputX, c, inputNodes)\noa = outputActivation(inputX, wm, c)\nmp = meanPrediction(inputX, wm, c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodetdat = gd.vx.map((value, index) =&gt; {\n  return { Trial: index, Vx: value, Response: talm.almTrain[index], Error: Math.abs(value -  talm.almTrain[index]) };\n});\n\n\n\n\n\n\nCodePlot.plot({\n  marks: [\n    Plot.line(tdat, {\n      x: \"Trial\",      // feature for the x channel\n      y: \"Response\",     // feature for the y channel\n      stroke: \"Vx\",     \n    }),\n  ],\n  x: {label: \"Trial Number\"},\n  y: {label: \"Vx\", domain: [0, 1800],grid: true},\n  color: {legend: true, scheme: \"Turbo\",type: \"categorical\"},\n  width: 400,\n  height: 400\n});\n  //caption: html`Figure 1. This chart has a &lt;i&gt;fancy&lt;/i&gt; caption.`\nPlot.plot({\n  marks: [\n    Plot.line(tdat, {\n      x: \"Trial\",      // feature for the x channel\n      y: \"Error\",     // feature for the y channel\n      stroke: \"Vx\",     // feature for the fill channel\n    }),\n  ],\n  y: {label: \"Error\",grid: true},\n  color: {legend: true, scheme: \"Turbo\",type: \"categorical\"},\n  width: 400,\n  height: 400\n});\n\n\nVx Across Training\n\n\n\n\n\nTraining Error\n\n\n\n\n\n\n\n\n\n\nWeight Matrices\n\nCodePlotly = require(\"https://cdn.plot.ly/plotly-latest.min.js\")\n//div = DOM.element('div');\nP1=Plotly.newPlot(\"plot-canvas\", [{\n  z: wm,\n  x: outputNodes,\n  y: inputNodes,\n  type: 'heatmap',\n  colorscale: 'Viridis'\n}],{width:500});\n\nconsole.log(inputNodes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeconsole.log(outputNodes)\n\n\n\n\n\n\n\nCodeP2=Plotly.newPlot(\"plot-tw\", [{\n  z: talm.weights,\n  x: inputNodes,\n  y: outputNodes,\n  type: 'heatmap',\n  colorscale: 'Viridis'\n}],{width:600});\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting Weights\n\nFinal Weights\n\n\n\n\nInput and Output layer activations\n\nCodein_data = ia.map((value, index) =&gt; {\n  return { Node: inputNodes[index], Activation: value };\n});\n\n out_data = oa.map((value, index) =&gt; {\n  return { Node: outputNodes[index], Activation: value };\n});\n\nviewof inputX = Inputs.range([1, 7], {value: 4, step: 1, label: \"input value:\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodePlot.plot({\n  marks: [\n    Plot.dot(in_data, {\n      x: \"Node\",      // feature for the x channel\n      y: \"Activation\",     // feature for the y channel\n    }),\n  ],\n  width: 400,\n  height: 200,\n  title: \"Input Activation Plot\",\n});\nPlot.plot({\n  marks: [\n    Plot.dot(out_data, {\n      x: \"Node\",      // feature for the x channel\n      y: \"Activation\",     // feature for the y channel\n    }),\n  ],\n  width: 400,\n  height: 200,\n  title: \"Output Activation Plot\",\n});\n\n\n\n\n\n\nInput Activation\n\n\n\n\n\nSecond\n\n\n\n\nCharts\n\n\n\n\nCodeconsole.log(wm)\n\n\n\n\n\n\n\nCodeconsole.log(talm.weights)\n\n\n\n\n\n\n\n\nTesting\n\nCodeviewof xV = Inputs.range(\n  [1, 20], \n  {value: 1, step: 1, label: \"x range:\"}\n)\nviewof yV = Inputs.range(\n  [1, 400], \n  {value: 1, step: 10, label: \"y range:\"}\n)\n\ndO = transpose(d)\nfiltered = dO.filter(function(dO) {\n  return dO.x&gt;=xV && dO.y &gt;= yV;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTab 1\nTab 2\n\n\n\n\nCodePlot.plot({\n  marks: [\n    Plot.dot(filtered, \n      { x: \"x\", y: \"y\"}, \n      { stroke: \"black\" }\n    )\n  ]\n})\n\n\n\n\n\n\nx:  y: \n\n\n\nCode//Plot = require(\"plot\")\nPlot.plot({\n  marks: [\n    Plot.line(transpose(d), \n      { x: \"x\", y: \"y\"}, \n      { stroke: \"black\" }\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n```{ojs}\n//| include: false\n```",
    "crumbs": [
      "Interactive",
      "OJS ALM"
    ]
  },
  {
    "objectID": "Misc/Visuals_Interactives/mermaid_consort.html",
    "href": "Misc/Visuals_Interactives/mermaid_consort.html",
    "title": "Mermaid with R + consort",
    "section": "",
    "text": "Code#lapply(c('tidyverse','data.table','igraph','ggraph','kableExtra'),library,character.only=TRUE))\npacman::p_load(tidyverse,data.table,igraph,ggraph,kableExtra,DiagrammeR,png,consort,data.table)\nflowchart LR\n    data1(Varied Training&lt;br/&gt;800-1000&lt;br/&gt;1000-1200&lt;br/&gt;1200-1400)\n    data2(Constant Training&lt;br/&gt;800-1000)\n    Test1(Testing - No Feedback&lt;br/&gt;100-300&lt;br/&gt;350-550&lt;br/&gt;600-800)\n    Test2(Test From Train&lt;br/&gt;800-1000&lt;br/&gt;1000-1200&lt;br/&gt;1200-1400)\n    Test3(Testing - Feedback&lt;br/&gt;100-300&lt;br/&gt;350-550&lt;br/&gt;600-800)\n\n    data1 --&gt; Test1\n    data2 --&gt; Test1\n    Test1 --&gt; Test2\n    Test2 --&gt; Test3\n    \n%% https://quarto.org/docs/authoring/diagrams.html\n\n\n Mean Vx over training blocks\ndata1\nVaried Training800-10001000-12001200-1400Test1\nTesting - No Feedback100-300350-550600-800data1-&gt;Test1\ndata2\nConstant Training800-1000data2-&gt;Test1\nTest2\nTest From Train800-10001000-12001200-1400Test1-&gt;Test2\nTest3\nTesting - Feedback100-300350-550600-800Test2-&gt;Test3\n\n\n\n\nFigure 1: This is a simple graphviz graph.\ndata1\nVaried Training800-10001000-12001200-1400Test1\nTesting - No Feedback100-300350-550600-800data1-&gt;Test1\ndata2\nConstant Training800-1000data2-&gt;Test1\nTest2\nTest From Train800-10001000-12001200-1400Test1-&gt;Test2\nTest3\nTesting - Feedback100-300350-550600-800Test2-&gt;Test3\n\n\n\n\nFigure 2: This is a simple graphviz graph."
  },
  {
    "objectID": "Misc/Visuals_Interactives/mermaid_consort.html#subgraph",
    "href": "Misc/Visuals_Interactives/mermaid_consort.html#subgraph",
    "title": "Mermaid with R + consort",
    "section": "subgraph",
    "text": "subgraph\n\n\n\n\n\ncluster_counterbalanced\nCounterbalanced Orderdata1\nVaried Training800-10001000-12001200-1400subgraph1\nsubgraph1data1-&gt;subgraph1\ndata2\nConstant Training800-1000data2-&gt;subgraph1\nTest1\nTesting - No Feedback100-300350-550600-800Test2\nTest From Train800-10001000-12001200-1400Test3\nTesting - Feedback100-300350-550600-800subgraph1-&gt;Test3\n\n\n\n\n\n\n\n\n\n\n\ncluster\nCounterbalanced Orderdata1\nVaried Training800-10001000-12001200-1400Test1\nTesting - No Feedback100-300350-550600-800data1-&gt;Test1\ndata2\nConstant Training800-1000data2-&gt;Test1\nTest3\nTesting - Feedback100-300350-550600-800Test2\nTest From Train800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\n\n\n\ncluster\nCounterbalanced Orderdata1\nVaried Training800-10001000-12001200-1400Test1\nTesting - No Feedback100-300350-550600-800data1-&gt;Test1\ndata2\nConstant Training800-1000data2-&gt;Test1\nTest3\nTesting - Feedback100-300350-550600-800Test2\nTest From Train800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\n\n\n\nALM\nInput1\nInput1Input2\nInput2Output1\nOutput1Input1-&gt;Output1\nw_{11}Output2\nOutput2Input1-&gt;Output2\nw_{12}Output3\nOutput3Input1-&gt;Output3\nw_{13}Input2-&gt;Output1\nw_{21}Input2-&gt;Output2\nw_{22}Input2-&gt;Output3\nw_{23}equation1\na_i(X) = e^{-γ(X-X_i)^2}, normalized: a_i(X) / Σa_i(X)equation2\nO_j(X) = Σ w_{ji} ⋅ a_i(X), P[Y_j | X] = O_j(X) / Σ O_k(X)\n\n\n\n\n\ntest\n\n\n\n\n\nALM\ncluster_input\nInput Layercluster_output\nOutput LayerInput1\nInput1Output1\nOutput1Input1-&gt;Output1\nw_11Output2\nOutput2Input1-&gt;Output2\nw_12Output3\nOutput3Input1-&gt;Output3\nw_13Input2\nInput2Input2-&gt;Output1\nw_21Input2-&gt;Output2\nw_22Input2-&gt;Output3\nw_23equation1\nInput activation ai(X) = exp(-gamma*(X-Xi)^2), normalized by dividing by sumequation2\nOutput Oj(X) = sum(w_ji * ai(X)), Probability P[Yj | X] = Oj(X) / sum(Ok(X))\n\n\n\n\n\ntest2\n\n\n\n\n\nG\nE1\n⋮z\nzE2\n⋮x100\nx100a\nab\nbx1\nx1a-&gt;x1\nx2\nx2a-&gt;x2\na-&gt;x100\nb-&gt;x1\nb-&gt;x2\nb-&gt;x100\nz-&gt;x1\nz-&gt;x2\nz-&gt;x100\nY\nYx1-&gt;Y\nx2-&gt;Y\nx100-&gt;Y\n\n\n\n\n\n\ntest3\n\n\n\n\n\nG\nclusterX\nn1n2n1-&gt;n2\nn3n1-&gt;n3\nn4\nn2-&gt;n4\nn5\nn4-&gt;n5\nn6\nn5-&gt;n6\nJ1J2\nJ1-&gt;J2\nJ6\nJ1-&gt;J6\nJ4J5\nJ5-&gt;J4\nJ6-&gt;J5\n\n\n\n\n\n\ntest4\nNest Cluster - GraphViz\n\n\n\n\n\nD\ncluster_p\nParentcluster_c2\nChild twocluster_c1\nChild onecluster_gc_1\nGrand-Child onecluster_gc_2\nGrand-Child twoa\nab\nbc\ncd\nde\ne\n\n\n\n\n\n\n\n\n\n\nG\nE1\nE1z\nzE2\n⋮x100\nx100a\nab\nbx1\nx1a-&gt;x1\nx2\nx2a-&gt;x2\na-&gt;x100\nb-&gt;x1\nb-&gt;x2\nb-&gt;x100\nz-&gt;x1\nz-&gt;x2\nz-&gt;x100\nY\nYx1-&gt;Y\nx2-&gt;Y\nx100-&gt;Y\n\n\n\n\n\n\ntest5\n\n\n\n\n\nG\nn1n2n1-&gt;n2\nn4n2-&gt;n4\n\n\n\n\n\n\ntest6\n\n\n\n\n\nALM\nInput1\nX1Input2\nX2Output1\nY1Input1:e-&gt;Output1:w\nw11Output2\nY2Input1:e-&gt;Output2:w\nw12Output3\nY3Input1:e-&gt;Output3:w\nw13Input2:e-&gt;Output1:w\nw21Input2:e-&gt;Output2:w\nw22Input2:e-&gt;Output3:w\nw23label_input\nInput Layerlabel_output\nOutput Layer\n\n\n\n\n\ntest7\n\n\n\n\n\nG\ncluster_0\nlayer 1 (input layer)cluster_1\nlayer 2 (hidden layer)cluster_2\nlayer 3 (hidden layer)cluster_3\nlayer 4 (output layer)x0\nx0a02\na0(2)x0:e-&gt;a02:wa12\na5(2)x0:e-&gt;a12:w\na22\na1(2)x0:e-&gt;a22:w\na32\na2(2)x0:e-&gt;a32:w\na42\na3(2)x0:e-&gt;a42:w\na52\na4(2)x0:e-&gt;a52:w\nx1\nx1x1:e-&gt;a12:w\nx1:e-&gt;a22:w\nx1:e-&gt;a32:w\nx1:e-&gt;a42:w\nx1:e-&gt;a52:w\nx2\nx2x2:e-&gt;a12:w\nx2:e-&gt;a22:w\nx2:e-&gt;a32:w\nx2:e-&gt;a42:w\nx2:e-&gt;a52:w\nx3\nx3x3:e-&gt;a12:w\nx3:e-&gt;a22:w\nx3:e-&gt;a32:w\nx3:e-&gt;a42:w\nx3:e-&gt;a52:w\na03\na0(3)a02:e-&gt;a03:wa13\na3(3)a02:e-&gt;a13:w\na23\na4(3)a02:e-&gt;a23:w\na33\na5(3)a02:e-&gt;a33:w\na43\na1(3)a02:e-&gt;a43:w\na53\na2(3)a02:e-&gt;a53:w\na12:e-&gt;a13:w\na12:e-&gt;a23:w\na12:e-&gt;a33:w\na12:e-&gt;a43:w\na12:e-&gt;a53:w\na22:e-&gt;a13:w\na22:e-&gt;a23:w\na22:e-&gt;a33:w\na22:e-&gt;a43:w\na22:e-&gt;a53:w\na32:e-&gt;a13:w\na32:e-&gt;a23:w\na32:e-&gt;a33:w\na32:e-&gt;a43:w\na32:e-&gt;a53:w\na42:e-&gt;a13:w\na42:e-&gt;a23:w\na42:e-&gt;a33:w\na42:e-&gt;a43:w\na42:e-&gt;a53:w\na52:e-&gt;a13:w\na52:e-&gt;a23:w\na52:e-&gt;a33:w\na52:e-&gt;a43:w\na52:e-&gt;a53:w\nO1\na1(4)a03:e-&gt;O1:w\nO2\na2(4)a03:e-&gt;O2:w\nO3\na3(4)a03:e-&gt;O3:w\nO4\na4(4)a03:e-&gt;O4:w\na13:e-&gt;O1:w\na13:e-&gt;O2:w\na13:e-&gt;O3:w\na13:e-&gt;O4:w\na23:e-&gt;O1:w\na23:e-&gt;O2:w\na23:e-&gt;O3:w\na23:e-&gt;O4:w\na33:e-&gt;O1:w\na33:e-&gt;O2:w\na33:e-&gt;O3:w\na33:e-&gt;O4:w\na43:e-&gt;O1:w\na43:e-&gt;O2:w\na43:e-&gt;O3:w\na43:e-&gt;O4:w\na53:e-&gt;O1:w\na53:e-&gt;O2:w\na53:e-&gt;O3:w\na53:e-&gt;O4:w\n\n\n\n\n\n\ntest8\n\n\n\n\n\nG\ncluster_0\nlayer 1 (Input layer)cluster_1\nlayer 2 (hidden layer)cluster_2\nlayer 3 (output layer)x1\na12\nx1-&gt;a12\na22\nx1-&gt;a22\na32\nx1-&gt;a32\nx2\nx2-&gt;a12\nx2-&gt;a22\nx2-&gt;a32\nx3\nx3-&gt;a12\nx3-&gt;a22\nx3-&gt;a32\nO\na12-&gt;O\na22-&gt;O\na32-&gt;O\n\n\n\n\n\n\n\n\n\n\n\nG\nx0\nx0x1\nx1a12\na1(2)x0:e-&gt;a12:w\na22\na2(2)x0:e-&gt;a22:w\na32\na3(2)x0:e-&gt;a32:w\na42\na4(2)x0:e-&gt;a42:w\na52\na5(2)x0:e-&gt;a52:w\na02\na0(2)a03\na0(3)a13\na1(3)a02:e-&gt;a13:w\na23\na2(3)a02:e-&gt;a23:w\na33\na3(3)a02:e-&gt;a33:w\na43\na4(3)a02:e-&gt;a43:w\na53\na5(3)a02:e-&gt;a53:w\nO1\na1(4)a03:e-&gt;O1:w\nO2\na2(4)a03:e-&gt;O2:w\nO3\na3(4)a03:e-&gt;O3:w\nO4\na4(4)a03:e-&gt;O4:w\nx2\nx2x1:e-&gt;a12:w\nx1:e-&gt;a22:w\nx1:e-&gt;a32:w\nx1:e-&gt;a42:w\nx1:e-&gt;a52:w\nx3\nx3x2:e-&gt;a12:w\nx2:e-&gt;a22:w\nx2:e-&gt;a32:w\nx2:e-&gt;a42:w\nx2:e-&gt;a52:w\nx3:e-&gt;a12:w\nx3:e-&gt;a22:w\nx3:e-&gt;a32:w\nx3:e-&gt;a42:w\nx3:e-&gt;a52:w\na12:e-&gt;a13:w\na12:e-&gt;a23:w\na12:e-&gt;a33:w\na12:e-&gt;a43:w\na12:e-&gt;a53:w\na22:e-&gt;a13:w\na22:e-&gt;a23:w\na22:e-&gt;a33:w\na22:e-&gt;a43:w\na22:e-&gt;a53:w\na32:e-&gt;a13:w\na32:e-&gt;a23:w\na32:e-&gt;a33:w\na32:e-&gt;a43:w\na32:e-&gt;a53:w\na42:e-&gt;a13:w\na42:e-&gt;a23:w\na42:e-&gt;a33:w\na42:e-&gt;a43:w\na42:e-&gt;a53:w\na52:e-&gt;a13:w\na52:e-&gt;a23:w\na52:e-&gt;a33:w\na52:e-&gt;a43:w\na52:e-&gt;a53:w\na13:e-&gt;O1:w\na13:e-&gt;O2:w\na13:e-&gt;O3:w\na13:e-&gt;O4:w\na23:e-&gt;O1:w\na23:e-&gt;O2:w\na23:e-&gt;O3:w\na23:e-&gt;O4:w\na33:e-&gt;O1:w\na33:e-&gt;O2:w\na33:e-&gt;O3:w\na33:e-&gt;O4:w\na43:e-&gt;O1:w\na43:e-&gt;O2:w\na43:e-&gt;O3:w\na43:e-&gt;O4:w\na53:e-&gt;O1:w\na53:e-&gt;O2:w\na53:e-&gt;O3:w\na53:e-&gt;O4:w\nl0\nlayer 1 (input layer)l1\nlayer 2 (hidden layer)l2\nlayer 3 (hidden layer)l3\nlayer 4 (output layer)\n\n\n\n\n\n\n\n\n\n\nthreevar\nk\n?X1\nX1k-&gt;X1\nn@_{S}X2\nX2k-&gt;X2\nθ@^{(h)}X3\nX3k-&gt;X3\n?3z1\nd1z1-&gt;X1\nz2\nd2z2-&gt;X2\nz3\nd3z3-&gt;X3\n\n\n\n\n\n\n\nCodegrViz('digraph threevar {\n      rankdir=LR;\n      size=\"8,4\";\n      node [fontsize=14 shape=box];\n      edge [fontsize=10];\n      center=1;\n      {rank=min k }\n      {rank=same X1 X2 X3 }\n      {rank=max z1 z2 z3 }\n      z1 [shape=circle label=\"d1\"];\n      z2 [shape=circle label=\"d2\"];\n      z3 [shape=circle label=\"d3\"];\n      k [label=\"?\" shape=\"ellipse\"];\n      k -&gt; X1 [label=\"n@_{S}\"];\n      k -&gt; X2 [label=\"&theta;@^{(h)}\"];\n      k -&gt; X3 [label=\"?3\"];\n      z1 -&gt; X1;\n      z2 -&gt; X2;\n      z3 -&gt; X3;\n    }\n      ')\n\n\n\n\n\n\nCodegrViz(\"digraph dot {\n      graph [style = filled, fillcolor = white]\n      \n      node [shape = circle,\n            style = filled, fillcolor = white,\n            fixedsize = true, width = 0.5, height = 0.5,\n            fontname = 'Times-italic']\n        c\n        d\n      node [shape = doublecircle,\n            style = filled, fillcolor = white,\n            fixedsize = true, width = 0.5, height = 0.5,\n            fontname = 'Times-italic']\n        thetah [label = '&theta;@^{(h)}']\n        thetaf [label = '&theta;@^{(f)}']\n      node [shape = square,\n            style = filled, fillcolor = grey,\n            fixedsize = true, width=0.5, height=0.5,\n            fontname = 'Times-italic']\n        h\n        f\n        ns [label = 'n@_{S}']\n        nn [label = 'n@_{N}']\n      \n      edge [color = black]\n        c -&gt; thetah -&gt; h\n        d -&gt; thetaf -&gt; f\n        c -&gt; thetaf\n        d -&gt; thetah\n        ns -&gt; h\n        nn -&gt; f\n      {rank = max; ns; nn}\n      }\")\n\n\n\n\nCodegrViz(\"digraph dot {\n      graph [style = filled, fillcolor = white,\n             rankdir = LR,\n             newrank = true]\n      \n      node [shape = circle,\n            style = filled, fillcolor = white,\n            fixedsize = true, width = 0.5, height = 0.5,\n            fontname = 'Times-italic']\n        p [fillcolor = gray]\n        gamma [label = '&gamma;']\n        omega [label = '&omega;', shape = doublecircle]\n        beta [label = '&beta;']\n      subgraph cluster_out{\n        fontsize = 8\n        label = &lt;&lt;I&gt;j&lt;/I&gt;:試行&gt;\n        labelloc = b\n        labeljust = r\n        subgraph cluster_in{\n          fontsize = 8\n          label = &lt;&lt;I&gt;k&lt;/I&gt;:回数&gt;\n          labelloc = b\n          labeljust = r\n          thetajk [label = '&theta;@_{jk}', shape = doublecircle]\n          djk [label = 'd@_{jk}', shape = square, fillcolor = gray]\n        }}\n      edge [color=black]\n        p -&gt; omega -&gt; thetajk -&gt; djk\n        gamma -&gt; omega\n        beta -&gt; thetajk \n      {rank = same; gamma; omega}\n      {rank = same; beta; thetajk}\n      }\")\n\n\n\n\nCodegrViz('\ndigraph G {\n    fontname=\"Helvetica,Arial,sans-serif\"\n    node [fontname=\"Helvetica,Arial,sans-serif\"]\n    edge [fontname=\"Helvetica,Arial,sans-serif\"]\n\n    subgraph cluster_0 {\n        style=filled;\n        color=lightgrey;\n        node [style=filled,color=white];\n        a0 -&gt; a1 -&gt; a2 -&gt; a3;\n        label = \"process #1\";\n    }\n\n    subgraph cluster_1 {\n        node [style=filled];\n        b0 -&gt; b1 -&gt; b2 -&gt; b3;\n        label = \"process #2\";\n        color=blue\n    }\n    start -&gt; a0;\n    start -&gt; b0;\n    a1 -&gt; b3;\n    b2 -&gt; a3;\n    a3 -&gt; a0;\n    a3 -&gt; end;\n    b3 -&gt; end;\n\n    start [shape=Mdiamond];\n    end [shape=Msquare];\n}')\n\n\n\n\nCode#https://graphviz.org/Gallery/directed/neural-network.html\n\ngrViz('digraph G {\n  fontname=\"Helvetica,Arial,sans-serif\"\n  node [fontname=\"Helvetica,Arial,sans-serif\"]\n  edge [fontname=\"Helvetica,Arial,sans-serif\"]\n  concentrate=True;\n  rankdir=TB;\n  node [shape=record];\n  140087530674552 [label=\"title: InputLayer\\n|{input:|output:}|{{[(?, ?)]}|{[(?, ?)]}}\"];\n  140087537895856 [label=\"body: InputLayer\\n|{input:|output:}|{{[(?, ?)]}|{[(?, ?)]}}\"];\n  140087531105640 [label=\"embedding_2: Embedding\\n|{input:|output:}|{{(?, ?)}|{(?, ?, 64)}}\"];\n  140087530711024 [label=\"embedding_3: Embedding\\n|{input:|output:}|{{(?, ?)}|{(?, ?, 64)}}\"];\n  140087537980360 [label=\"lstm_2: LSTM\\n|{input:|output:}|{{(?, ?, 64)}|{(?, 128)}}\"];\n  140087531256464 [label=\"lstm_3: LSTM\\n|{input:|output:}|{{(?, ?, 64)}|{(?, 32)}}\"];\n  140087531106200 [label=\"tags: InputLayer\\n|{input:|output:}|{{[(?, 12)]}|{[(?, 12)]}}\"];\n  140087530348048 [label=\"concatenate_1: Concatenate\\n|{input:|output:}|{{[(?, 128), (?, 32), (?, 12)]}|{(?, 172)}}\"];\n  140087530347992 [label=\"priority: Dense\\n|{input:|output:}|{{(?, 172)}|{(?, 1)}}\"];\n  140087530711304 [label=\"department: Dense\\n|{input:|output:}|{{(?, 172)}|{(?, 4)}}\"];\n  140087530674552 -&gt; 140087531105640;\n  140087537895856 -&gt; 140087530711024;\n  140087531105640 -&gt; 140087537980360;\n  140087530711024 -&gt; 140087531256464;\n  140087537980360 -&gt; 140087530348048;\n  140087531256464 -&gt; 140087530348048;\n  140087531106200 -&gt; 140087530348048;\n  140087530348048 -&gt; 140087530347992;\n  140087530348048 -&gt; 140087530711304;\n}')\n\n\n\n\n\n\nCoderequire(data.table)\nrequire(qreport)\nrequire(consort)\n\n# Load the necessary libraries\nrequire(data.table)\nrequire(qreport)\n\n# Define the mermaid diagram\nx &lt;- '\ngraph LR\n  InputLayer[Input Layer] --&gt; I1[{{I1}}]\n  InputLayer --&gt; I2[{{I2}}]\n  InputLayer --&gt; I3[{{I3}}]\n  I1 --&gt; O1[w1]\n  I1 --&gt; O2[w2]\n  I1 --&gt; O3[w3]\n  I1 --&gt; O4[w4]\n  I2 --&gt; O1[w5]\n  I2 --&gt; O2[w6]\n  I2 --&gt; O3[w7]\n  I2 --&gt; O4[w8]\n  I3 --&gt; O1[w9]\n  I3 --&gt; O2[w10]\n  I3 --&gt; O3[w11]\n  I3 --&gt; O4[w12]\n  OutputLayer[Output Layer] --&gt; O1\n  OutputLayer --&gt; O2\n  OutputLayer --&gt; O3\n  OutputLayer --&gt; O4\n  OutputLayer --&gt; ALM[ALM Response]\n  OutputLayer --&gt; EXAM[EXAM Response]\n  ALM --&gt; EXAM[Pure ALM Model]\n  EXAM --&gt; ALM[ALM with EXAM Response Component]\n'\n\n# Generate the mermaid diagram\nmakemermaid(x,\n            I1 = '![Gaussian Curve](gaussian_curve_input_node_1.png)',\n            I2 = '![Gaussian Curve](gaussian_curve_input_node_2.png)',\n            I3 = '![Gaussian Curve](gaussian_curve_input_node_3.png)',\n            file = 'assets/alm_exam.mer'\n            )\n\n# Output the mermaid diagram\n# cat('```{mermaid}\\n')\n# cat(readLines('assets/alm_exam.mer'), sep = '\\n')\n# cat('```\\n')\n\n\n\n\n\n\n\ngraph LR\n  InputLayer[Input Layer] --&gt; I1[![Gaussian Curve](gaussian_curve_input_node_1.png)]\n  InputLayer --&gt; I2[![Gaussian Curve](gaussian_curve_input_node_2.png)]\n  InputLayer --&gt; I3[![Gaussian Curve](gaussian_curve_input_node_3.png)]\n  I1 --&gt; O1[w1]\n  I1 --&gt; O2[w2]\n  I1 --&gt; O3[w3]\n  I1 --&gt; O4[w4]\n  I2 --&gt; O1[w5]\n  I2 --&gt; O2[w6]\n  I2 --&gt; O3[w7]\n  I2 --&gt; O4[w8]\n  I3 --&gt; O1[w9]\n  I3 --&gt; O2[w10]\n  I3 --&gt; O3[w11]\n  I3 --&gt; O4[w12]\n  OutputLayer[Output Layer] --&gt; O1\n  OutputLayer --&gt; O2\n  OutputLayer --&gt; O3\n  OutputLayer --&gt; O4\n  OutputLayer --&gt; ALM[ALM Response]\n  OutputLayer --&gt; EXAM[EXAM Response]\n  ALM --&gt; EXAM[Pure ALM Model]\n  EXAM --&gt; ALM[ALM with EXAM Response Component]\n\n\n\n\nFigure 3: ALM diagram produced with mermaid with individual exclusions linked to the overall exclusions node, and with a tooltip to show more detail\n\n\n\n\n\nCodelibrary(consort)\nset.seed(1001)\nN &lt;- 300\n\ntrialno &lt;- sample(c(1000:2000), N)\nexc &lt;- rep(NA, N)\nexc[sample(1:N, 15)] &lt;- sample(c(\"Sample not collected\", \"MRI not collected\", \"Other\"),\n                                15, replace = T, prob = c(0.4, 0.4, 0.2))\n\narm &lt;- rep(NA, N)\narm[is.na(exc)] &lt;- sample(c(\"Conc\", \"Seq\"), sum(is.na(exc)), replace = T)\n\nfow1 &lt;- rep(NA, N)\nfow1[!is.na(arm)] &lt;- sample(c(\"Withdraw\", \"Discontinued\", \"Death\", \"Other\", NA),\n                            sum(!is.na(arm)), replace = T, \n                            prob = c(0.05, 0.05, 0.05, 0.05, 0.8))\nfow2 &lt;- rep(NA, N)\nfow2[!is.na(arm) & is.na(fow1)] &lt;- sample(c(\"Protocol deviation\", \"Outcome missing\", NA),\n                                          sum(!is.na(arm) & is.na(fow1)), replace = T, \n                                          prob = c(0.05, 0.05, 0.9))\ndf &lt;- data.frame(trialno, exc, arm, fow1, fow2)\nout &lt;- consort_plot(data = df,\n             order = c(trialno = \"Population\",\n                          exc    = \"Excluded\",\n                          arm     = \"Randomized patient\",\n                          fow1    = \"Lost of Follow-up\",\n                          trialno = \"Finished Followup\",\n                          fow2    = \"Not evaluable\",\n                          trialno = \"Final Analysis\"),\n             side_box = c(\"exc\", \"fow1\", \"fow2\"),\n             allocation = \"arm\",\n             labels = c(\"1\" = \"Screening\", \"2\" = \"Randomization\",\n                        \"5\" = \"Final\"),\n             cex = 0.6)\n\nplot(out)\n\n\n\n\n\n\n\n\nCodeplot(out, grViz = TRUE)\n\n\n\n\n\n\nCoderequire(Hmisc)\nrequire(data.table)\nrequire(qreport)\nhookaddcap()\nN &lt;- 1000\nset.seed(1)\nr &lt;- data.table(\n  id    = 1 : N,\n  age   = round(rnorm(N, 60, 15)),\n  pain  = sample(0 : 5, N, replace=TRUE),\n  hxmed = sample(0 : 1, N, replace=TRUE, prob=c(0.95, 0.05))   )\n# Set consent status to those not excluded at screening\nr[age &gt;= 40 & pain &gt; 0 & hxmed == 0,\n  consent := sample(0 : 1, .N, replace=TRUE, prob=c(0.1, 0.9))]\n# Set randomization status for those consenting\nr[consent == 1,\n  randomized := sample(0 : 1, .N, replace=TRUE, prob=c(0.15, 0.85))]\n# Add treatment and follow-up time to randomized subjects\nr[randomized == 1, tx     := sample(c('A', 'B'), .N, replace=TRUE)]\nr[randomized == 1, futime := pmin(runif(.N, 0, 10), 3)]\n# Add outcome status for those followed 3 years\n# Make a few of those followed 3 years missing\nr[futime == 3,\n  y := sample(c(0, 1, NA), .N, replace=TRUE, prob=c(0.75, 0.2, 0.05))]\n# Print first 15 subjects\nkabl(r[1 : 15, ])\n\n\n\nid\nage\npain\nhxmed\nconsent\nrandomized\ntx\nfutime\ny\n\n\n\n1\n51\n2\n1\nNA\nNA\nNA\nNA\nNA\n\n\n2\n63\n2\n0\n1\n1\nA\n3.0000\n0\n\n\n3\n47\n1\n0\n1\n1\nA\n3.0000\nNA\n\n\n4\n84\n5\n0\n1\n0\nNA\nNA\nNA\n\n\n5\n65\n3\n0\n1\n1\nB\n3.0000\n1\n\n\n6\n48\n4\n0\n1\n1\nA\n3.0000\n0\n\n\n7\n67\n3\n0\n1\n1\nB\n2.0566\nNA\n\n\n8\n71\n0\n0\nNA\nNA\nNA\nNA\nNA\n\n\n9\n69\n5\n0\n1\n1\nB\n1.2815\nNA\n\n\n10\n55\n2\n0\n1\n1\nB\n1.2388\nNA\n\n\n11\n83\n4\n0\n1\n1\nA\n3.0000\n1\n\n\n12\n66\n5\n0\n1\n1\nA\n3.0000\n0\n\n\n13\n51\n1\n0\n1\n1\nB\n3.0000\n0\n\n\n14\n27\n2\n0\nNA\nNA\nNA\nNA\nNA\n\n\n15\n77\n2\n0\n1\n1\nA\n3.0000\n0\n\n\n\n\n\n\nCoder[, exc := seqFreq('pain-free'  = pain  == 0,\n                    'Hx med'    = hxmed == 1,\n                                  age &lt; 40,\n                    noneNA=TRUE)]\neo  &lt;- attr(r[, exc], 'obs.per.numcond')\nmult &lt;- paste0('1, 2, ≥3 exclusions: n=',\n                eo[2], ', ',\n                eo[3], ', ',\n                eo[-(1:3)]  )\n\nr[, .q(qual, consent, fin) :=\n    .(is.na(exc),\n      ifelse(consent == 1, 1, NA),\n      ifelse(futime  &gt;= 3, 1, NA))]\n            \nrequire(consort)\n# consort_plot used to take a coords=c(0.4, 0.6) argument that prevented\n# the collision you see here\nconsort_plot(r,\n             orders = c(id      = 'Screened',\n                        exc     = 'Excluded',\n                        qual    = 'Qualified for Randomization',\n                        consent = 'Consented',\n                        tx      = 'Randomized',\n                        fin     = 'Finished',\n                        y       = 'Outcome\\nassessed'),\n             side_box = 'exc',\n             allocation = 'tx',\n             labels=c('1'='Screening', '3'='Consent', '4'='Randomization', '6'='Follow-up'))\n\n\n\n\n\n\n\nhtab\n\nCodeh &lt;- function(n, label) paste0(label, ' (n=', n, ')')\nhtab &lt;- function(x, label=NULL, split=! length(label), br='\\n') {\n  tab &lt;- table(x)\n  w &lt;- if(length(label)) paste0(h(sum(tab), label), ':', br)\n  f &lt;- if(split) h(tab, names(tab)) \n  else\n    paste(paste0('   ', h(tab, names(tab))), collapse=br)\n  if(split) return(f)\n  paste(w, f, sep=if(length(label))'' else br)\n}  \ncount &lt;- function(x, by=rep(1, length(x)))\n  tapply(x, by, sum, na.rm=TRUE)\n\nw &lt;- r[, {\n g &lt;-\n   add_box(txt=h(nrow(r),       'Screened'))                    |&gt;\n   add_side_box(htab(exc,       'Excluded'))                    |&gt;\n   add_box(h(count(is.na(exc)), 'Qualified for Randomization')) |&gt;\n   add_box(h(count(consent),    'Consented'))                   |&gt;\n   add_box(h(count(randomized), 'Randomized'))                  |&gt;\n   add_split(htab(tx))                                          |&gt;\n   add_box(h(count(fin, tx),    'Finished'))                    |&gt;\n   add_box(h(count(! is.na(y), tx), 'Outcome\\nassessed'))       |&gt;\n   add_label_box(c('1'='Screening',     '3'='Consent',\n                   '4'='Randomization', '6'='Follow-up'))\n plot(g)\n}\n]\n\n\n\n\n\n\n\nmermaid maker\n\nCodeaddCap('fig-doverview-mermaid1', 'Consort diagram produced by `mermaid`')\nx &lt;- 'flowchart TD\n  S[\"Screened (n={{N0}})\"] --&gt; E[\"{{excl}}\"]\n  S   --&gt; Q[\"Qualified for Randomization (n={{Nq}})\"]\n  Q   --&gt; C[\"Consented (n={{Nc}})\"]\n  C   --&gt; R[\"Randomized (n={{Nr}})\"]\n  R   --&gt; TxA[\"A (n={{Ntxa}})\"]\n  R   --&gt; TxB[\"B (n={{Ntxb}})\"]\n  TxA --&gt; FA[\"Finished (n={{Ntxaf}})\"]\n  TxB --&gt; FB[\"Finished (n={{Ntxbf}})\"]\n  FA  --&gt; OA[\"Outcome assessed (n={{Ntxao}})\"]\n  FB  --&gt; OB[\"Outcome assessed (n={{Ntxbo}})\"]\nclassDef largert fill:lightgray,width:1.5in,height:10em,text-align:right,font-size:0.8em;\nclass E largert;\n'\n\nw &lt;- r[, \nmakemermaid(x,\n            N0   = nrow(r),\n            excl = htab(exc, 'Excluded', br='&lt;br&gt;'),\n            Nq   = count(is.na(exc)),\n            Nc   = count(consent),\n            Nr   = count(randomized),\n            Ntxa = count(tx == 'A'),\n            Ntxb = count(tx == 'B'),\n            Ntxaf= count(tx == 'A' & fin),\n            Ntxbf= count(tx == 'B' & fin),\n            Ntxao= count(tx == 'A' & ! is.na(y)),\n            Ntxbo= count(tx == 'B' & ! is.na(y)),\n            file = 'assets/mermaid1.mer'\n            )\n]\n\n\n\n\n\n\n\nflowchart TD\n  S[\"Screened (n=1000)\"] --&gt; E[\"Excluded (n=286):&lt;br&gt;   pain-free (n=156)&lt;br&gt;   age &lt; 40 (n=85)&lt;br&gt;   Hx med (n=45)\"]\n  S   --&gt; Q[\"Qualified for Randomization (n=714)\"]\n  Q   --&gt; C[\"Consented (n=634)\"]\n  C   --&gt; R[\"Randomized (n=534)\"]\n  R   --&gt; TxA[\"A (n=285)\"]\n  R   --&gt; TxB[\"B (n=249)\"]\n  TxA --&gt; FA[\"Finished (n=204)\"]\n  TxB --&gt; FB[\"Finished (n=175)\"]\n  FA  --&gt; OA[\"Outcome assessed (n=196)\"]\n  FB  --&gt; OB[\"Outcome assessed (n=165)\"]\nclassDef largert fill:lightgray,width:1.5in,height:10em,text-align:right,font-size:0.8em;\nclass E largert;\n\n\n\n\nFigure 4: Consort diagram produced by mermaid\n\n\n\n\nnode plot\n\nCode# Create some service functions so later it will be easy to change from\n# mermaid to graphviz\nmakenode       &lt;- function(name, label) paste0(name, '[\"', label, '\"]')\nmakeconnection &lt;- function(from, to)    paste0(from, ' --&gt; ', to)\n\nexclnodes &lt;- function(x, from='E', root='E', seq=FALSE, remain=FALSE) {\n  # Create complete node specifications for individual exclusions, each\n  # linking to overall exclusion count assumed to be in node root.\n  # Set seq=TRUE to make use of the fact that the exclusions were\n  # done in frequency priority order so that each exclusion is in\n  # addition to the previous one.  Leave seq=FALSE to make all exclusions\n  # subservient to root.  Use remain=TRUE to include # obs remaining\n    # remain=TRUE assumes noneNA specified to seqFreq\n  tab &lt;- table(x)\n  i &lt;- 1 : length(tab)\n    rem &lt;- if(remain) paste0(', ', length(x) - cumsum(tab), ' remain')\n  labels &lt;- paste0(names(tab), ' (n=', tab, rem, ')')\n  nodes  &lt;- if(seq) makenode(ifelse(i == 1, paste0(root, '1'), paste0(root, i)),\n                             labels)\n            else    makenode(paste0(root, i), labels)\n  connects &lt;- if(seq) makeconnection(ifelse(i == 1, from, paste0(root, i - 1)),\n                                     paste0(root, i))\n              else makeconnection(from, paste0(root, i))\n   paste(c(nodes, connects), collapse='\\n')\n}\n\n# Create parallel treatment nodes\n# Treatments are assumed to be in order by the tx variable\n# and will appear left to right in the diagram\n# Treatment node names correspond to that and are Tx1, Tx2, ...\n# root: root of new nodes, from: single node name to connect from\n# fromparallel: root of connected-from node name which is to be\n# expanded by adding the integers 1, 2, ... number of treatments.\n\nTxs &lt;- r[, if(is.factor(tx)) levels(tx) else sort(unique(tx))]\n\nparNodes &lt;- function(counts, root, from=NULL, fromparallel=NULL,\n                      label=Txs) {\n  if(! identical(names(counts), Txs)) stop('Txs not consistent')\n  k &lt;- length(Txs)\n  ns &lt;- paste0(' (n=', counts, ')')\n   nodenames &lt;- paste0(root, 1 : k)\n  nodes &lt;- makenode(nodenames, paste0(label, ns))\n  connects &lt;- if(length(fromparallel)) makeconnection(paste0(fromparallel, 1 : k), nodenames)\n              else                     makeconnection(from,                        nodenames)\n  paste(c(nodes, connects), collapse='\\n')\n    }\n\n# Create tooltip text from tabulation created by seqFreq earlier\nefreq &lt;- data.frame('# Exclusions'= (1 : length(eo)) - 1,\n                    '# Subjects'  = eo, check.names=FALSE)\nefreq &lt;- subset(efreq, `# Subjects` &gt; 0)\n# Convert to text which will be wrapped by the html\nexcltab &lt;- paste(capture.output(print(efreq, row.names=FALSE)),\n                 collapse='\\n')\n\n\naddCap('fig-doverview-mermaid2', 'Consort diagram produced with `mermaid` with individual exclusions linked to the overall exclusions node, and with a tooltip to show more detail')\n\nx &lt;- '\nflowchart TD\n  S[\"Screened (n={{N0}})\"] --&gt; E[\"Excluded (n={{Ne}})\"]\n  {{exclsep}}\n  E1 & E2 & E3 --&gt; M[\"{{mult}}\"]\n  S   --&gt; Q[\"Qualified for Randomization (n={{Nq}})\"]\n  Q   --&gt; C[\"Consented (n={{Nc}})\"]\n  C   --&gt; R[\"Randomized (n={{Nr}})\"]\n  {{txcounts}}\n  {{finished}}\n  {{outcome}}\nclick E callback \"{{excltab}}\"\n'\n\nw &lt;- r[, \nmakemermaid(x,\n  N0       = nrow(r),\n  Ne       = count(! is.na(exc)),\n  exclsep  = exclnodes(exc),  # add seq=TRUE to put exclusions vertical\n  excltab  = excltab,         # tooltip text\n  mult     = mult,  # separate node: count multiple exclusions\n  Nq       = count(is.na(exc)),\n  Nc       = count(consent),\n  Nr       = count(randomized),\n  txcounts = parNodes(table(tx),         'Tx', from='R'),\n  finished = parNodes(count(fin, by=tx), 'F',  fromparallel='Tx',\n                      label='Finished'),\n  outcome  = parNodes(count(! is.na(y), by=tx), 'O',\n                      fromparallel='F', label='Outcome assessed'),\n  file='mermaid2.mer'  # save generated code for another use\n)\n]\n\n\nmakenode       &lt;- function(name, label) paste0(name, ' [label=\"', label, '\"];')\nmakeconnection &lt;- function(from, to)    paste0(from, ' -&gt; ', to, ';')\n\n# Create data frame from tabulation created by seqFreq earlier\nefreq &lt;- data.frame('# Exclusions'= (1 : length(eo)) - 1,\n                    '# Subjects'  = eo, check.names=FALSE)\nefreq &lt;- subset(efreq, `# Subjects` &gt; 0)\n\nx &lt;- 'digraph {\n  graph [pad=\"0.5\", nodesep=\"0.5\", ranksep=\"2\", splines=ortho]\n  //  splines=ortho for square connections\n  node  [shape=box, fontsize=\"30\"]\n  rankdir=TD;\n  S [label=\"Screened (n={{N0}})\"];\n  E [label=\"Excluded (n={{Ne}})\"];\n  S -&gt; E;\n  {{exclsep}}\n  M [label=\"{{mult}}\"];\n  E1 -&gt; M;\n  E2 -&gt; M;\n  E3 -&gt; M;\n  Q [label=\"Qualified for Randomization (n={{Nq}})\"];\n  C [label=\"Consented (n={{Nc}})\"];\n  R [label=\"Randomized (n={{Nr}})\"];\n  S -&gt; Q;\n  Q -&gt; C;\n  C -&gt; R;\n  {{txcounts}}\n  {{finished}}\n  {{outcome}}\n  efreq [label=&lt;{{efreq}}&gt;];\n  M -&gt; efreq [dir=none, style=dotted];\n}\n'\n\nw &lt;- r[, \nmakegraphviz(x,\n  N0       = nrow(r),\n  Ne       = count(! is.na(exc)),\n  exclsep  = exclnodes(exc),  # add seq=TRUE to put exclusions vertical\n  efreq    = efreq,\n  mult     = mult,  # separate node: count multiple exclusions\n  Nq       = count(is.na(exc)),\n  Nc       = count(consent),\n  Nr       = count(randomized),\n  txcounts = parNodes(table(tx),         'Tx', from='R'),\n  finished = parNodes(count(fin, by=tx), 'F',  fromparallel='Tx',\n                      label='Finished'),\n  outcome  = parNodes(count(! is.na(y), by=tx), 'O',\n                      fromparallel='F', label='Outcome assessed'),\n  file='graphviz.dot'\n)\n]\n#  addCap('fig-doverview-graphviza', 'Consort diagram produced with `graphviz` with detailed exclusion frequencies in a separate node', scap='Consort diagram produced with `graphviz`')\n\n\n\n\n\n\n\nflowchart TD\n  S[\"Screened (n=1000)\"] --&gt; E[\"Excluded (n=286)\"]\n  E1[\"pain-free (n=156)\"]\nE2[\"age &lt; 40 (n=85)\"]\nE3[\"Hx med (n=45)\"]\nE --&gt; E1\nE --&gt; E2\nE --&gt; E3\n  E1 & E2 & E3 --&gt; M[\"1, 2, ≥3 exclusions: n=260, 25, 1\"]\n  S   --&gt; Q[\"Qualified for Randomization (n=714)\"]\n  Q   --&gt; C[\"Consented (n=634)\"]\n  C   --&gt; R[\"Randomized (n=534)\"]\n  Tx1[\"A (n=285)\"]\nTx2[\"B (n=249)\"]\nR --&gt; Tx1\nR --&gt; Tx2\n  F1[\"Finished (n=204)\"]\nF2[\"Finished (n=175)\"]\nTx1 --&gt; F1\nTx2 --&gt; F2\n  O1[\"Outcome assessed (n=196)\"]\nO2[\"Outcome assessed (n=165)\"]\nF1 --&gt; O1\nF2 --&gt; O2\nclick E callback \" # Exclusions # Subjects\n            0        714\n            1        260\n            2         25\n            3          1\"\n\n\n\n\nFigure 5: Consort diagram produced with mermaid with individual exclusions linked to the overall exclusions node, and with a tooltip to show more detail\n\n\n\n\n\nCodegetHdata(support)\nsetDT(support)\n# addCap('fig-doverview-missflow', 'Flowchart of sequential exclusion of observations due to missing values')\nvars &lt;-  .q(age, sex, dzgroup, edu, income, meanbp, wblc,\n            alb, bili, crea, glucose, bun, urine)\nex &lt;- missChk(support, use=vars, type='seq') # seq: don't make report\n\n# Create tooltip text from tabulation created by seqFreq\noc   &lt;- attr(ex, 'obs.per.numcond')\nfreq &lt;- data.frame('# Exclusions'= (1 : length(oc)) - 1,\n                   '# Subjects'  = oc, check.names=FALSE)\nfreq &lt;- subset(freq, `# Subjects` &gt; 0)\n\nx &lt;- '\ndigraph {\n  graph [pad=\"0.5\", nodesep=\"0.5\", ranksep=\"2\", splines=ortho]\n  //  splines=ortho for square connections\n  node  [shape=box, fontsize=\"30\"]\n  rankdir=TD;\n  Enr [label=\"Enrolled (n={{N0}})\"];\n  Enr;\n  {{exclsep}}\n    Extab [label=&lt;{{excltab}}&gt;];\n  Enr:e -&gt; Extab [dir=none];\n}\n'\nmakegraphviz(x,\n  N0        = nrow(support),\n  exclsep   = exclnodes(ex, from='Enr', seq=TRUE, remain=TRUE),\n  excltab   = freq,\n  file      = 'support.dot'\n)\n\n\n\nCodegrViz('\ngraph G {\nfontname=\"Helvetica,Arial,sans-serif\"\nnode [fontname=\"Helvetica,Arial,sans-serif\"]\nedge [fontname=\"Helvetica,Arial,sans-serif\"]\nI5 [shape=ellipse,color=red,style=bold,label=\"Caroline Bouvier Kennedy\\nb. 27.11.1957 New York\",image=\"images/165px-Caroline_Kennedy.jpg\",labelloc=b];\nI1 [shape=box,color=blue,style=bold,label=\"John Fitzgerald Kennedy\\nb. 29.5.1917 Brookline\\nd. 22.11.1963 Dallas\",image=\"images/kennedyface.jpg\",labelloc=b];\nI6 [shape=box,color=blue,style=bold,label=\"John Fitzgerald Kennedy\\nb. 25.11.1960 Washington\\nd. 16.7.1999 over the Atlantic Ocean, near Aquinnah, MA, USA\",image=\"images/180px-JFKJr2.jpg\",labelloc=b];\nI7 [shape=box,color=blue,style=bold,label=\"Patrick Bouvier Kennedy\\nb. 7.8.1963\\nd. 9.8.1963\"];\nI2 [shape=ellipse,color=red,style=bold,label=\"Jaqueline Lee Bouvier\\nb. 28.7.1929 Southampton\\nd. 19.5.1994 New York City\",image=\"images/jacqueline-kennedy-onassis.jpg\",labelloc=b];\nI8 [shape=box,color=blue,style=bold,label=\"Joseph Patrick Kennedy\\nb. 6.9.1888 East Boston\\nd. 16.11.1969 Hyannis Port\",image=\"images/1025901671.jpg\",labelloc=b];\nI10 [shape=box,color=blue,style=bold,label=\"Joseph Patrick Kennedy Jr\\nb. 1915\\nd. 1944\"];\nI11 [shape=ellipse,color=red,style=bold,label=\"Rosemary Kennedy\\nb. 13.9.1918\\nd. 7.1.2005\",image=\"images/rosemary.jpg\",labelloc=b];\nI12 [shape=ellipse,color=red,style=bold,label=\"Kathleen Kennedy\\nb. 1920\\nd. 1948\"];\nI13 [shape=ellipse,color=red,style=bold,label=\"Eunice Mary Kennedy\\nb. 10.7.1921 Brookline\"];\nI9 [shape=ellipse,color=red,style=bold,label=\"Rose Elizabeth Fitzgerald\\nb. 22.7.1890 Boston\\nd. 22.1.1995 Hyannis Port\",image=\"images/Rose_kennedy.JPG\",labelloc=b];\nI15 [shape=box,color=blue,style=bold,label=\"Aristotle Onassis\"];\nI3 [shape=box,color=blue,style=bold,label=\"John Vernou Bouvier III\\nb. 1891\\nd. 1957\",image=\"images/BE037819.jpg\",labelloc=b];\nI4 [shape=ellipse,color=red,style=bold,label=\"Janet Norton Lee\\nb. 2.10.1877\\nd. 3.1.1968\",image=\"images/n48862003257_1275276_1366.jpg\",labelloc=b];\n I1 -- I5  [style=bold,color=blue]; \n I1 -- I6  [style=bold,color=orange]; \n I2 -- I6  [style=bold,color=orange]; \n I1 -- I7  [style=bold,color=orange]; \n I2 -- I7  [style=bold,color=orange]; \n I1 -- I2  [style=bold,color=violet]; \n I8 -- I1  [style=bold,color=blue]; \n I8 -- I10  [style=bold,color=orange]; \n I9 -- I10  [style=bold,color=orange]; \n I8 -- I11  [style=bold,color=orange]; \n I9 -- I11  [style=bold,color=orange]; \n I8 -- I12  [style=bold,color=orange]; \n I9 -- I12  [style=bold,color=orange]; \n I8 -- I13  [style=bold,color=orange]; \n I9 -- I13  [style=bold,color=orange]; \n I8 -- I9  [style=bold,color=violet]; \n I9 -- I1  [style=bold,color=red]; \n I2 -- I5  [style=bold,color=red]; \n I2 -- I15  [style=bold,color=violet]; \n I3 -- I2  [style=bold,color=blue]; \n I3 -- I4  [style=bold,color=violet]; \n I4 -- I2  [style=bold,color=red]; \n}')"
  },
  {
    "objectID": "Misc/Visuals_Interactives/ALM_Shiny.html",
    "href": "Misc/Visuals_Interactives/ALM_Shiny.html",
    "title": "ALM Shiny App Code",
    "section": "",
    "text": "Shiny App Simulating ALM and EXAM  \n\n\nYou can play with the embedded version of the app below, or go to direct link\nYou can adjust the values of the Association parameter (i.e. the c parameter), and the Update parameter, (i.e. the learning rate parameter). The App also allows you to control the number and location of training instances. And the shape of the true function (linear, quadratic or exponential)\n\n\nAlternatively, you can run the app locally by copying the code below into a .R file.\n\nShow App Codepacman::p_load(tidyverse,shiny,reactable,shinydashboard,shinydashboardPlus)\n\ninput.activation &lt;- function(x.target, association.parameter) {\n    return(exp(-1 * association.parameter * (x.target - x.plotting)^2))\n}\n\noutput.activation &lt;- function(x.target, weights, association.parameter) {\n    return(weights %*% input.activation(x.target, association.parameter))\n}\n\nmean.prediction &lt;- function(x.target, weights, association.parameter) {\n    probability &lt;- output.activation(x.target, weights, association.parameter) / sum(output.activation(x.target, weights, association.parameter))\n    return(y.plotting %*% probability)\n}\n# function to generate exam predictions\nexam.prediction &lt;- function(x.target, weights, association.parameter) {\n    trainVec &lt;- sort(unique(x.learning))\n    nearestTrain &lt;- trainVec[which.min(abs(trainVec - x.target))]\n    aresp &lt;- mean.prediction(nearestTrain, weights, association.parameter)\n    xUnder &lt;- ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n    xOver &lt;- ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n    mUnder &lt;- mean.prediction(xUnder, weights, association.parameter)\n    mOver &lt;- mean.prediction(xOver, weights, association.parameter)\n    exam.output &lt;- round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n    exam.output\n}\n\nupdate.weights &lt;- function(x.new, y.new, weights, association.parameter, update.parameter) {\n    y.feedback.activation &lt;- exp(-1 * association.parameter * (y.new - y.plotting)^2)\n    x.feedback.activation &lt;- output.activation(x.new, weights, association.parameter)\n    return(weights + update.parameter * (y.feedback.activation - x.feedback.activation) %*% t(input.activation(x.new, association.parameter)))\n}\n\nlearn.alm &lt;- function(y.learning, association.parameter = 0.05, update.parameter = 0.5) {\n    weights &lt;- matrix(rep(0.00, length(y.plotting) * length(x.plotting)), nrow = length(y.plotting), ncol = length(x.plotting))\n    for (i in 1:length(y.learning)) {\n        weights &lt;- update.weights(x.learning[i], y.learning[i], weights, association.parameter, update.parameter)\n        weights[weights &lt; 0] &lt;- 0\n    }\n    alm.predictions &lt;- sapply(x.plotting, mean.prediction, weights = weights, association.parameter = association.parameter)\n    exam.predictions &lt;- sapply(x.plotting, exam.prediction, weights = weights, association.parameter = association.parameter)\n    return(list(alm.predictions = alm.predictions, exam.predictions = exam.predictions))\n    # return(list(alm.predictions=alm.predictions, exam.predictions=exam.predictions,wmFinal=weights))\n}\n\n\n\nx.plotting &lt;&lt;- seq(0, 90, .5)\ny.plotting &lt;&lt;- seq(0, 210, by = 2)\n# trainOptions=round(seq(1,length(x.plotting),length.out=21),0)\ntrainOptions &lt;- x.plotting[seq(1, 181, by = 4)]\ntrainItems &lt;- trainOptions[c(10, 11, 12)]\n\n\n\n# Define UI for application\n# \nui &lt;- dashboardPage(\n\n  skin = \"black\",\n  dashboardHeader(title = \"ALM Simulation App\"),\n  dashboardSidebar(\n    sidebarMenu(\n      menuItem(\"Home\", tabName = \"home\", icon = icon(\"home\")),\n      menuItem(\"Code\", tabName = \"code\", icon = icon(\"code\"))\n    )\n  ),\n  dashboardBody(\n    tabItems(\n      tabItem(tabName = \"home\",\n              fluidRow(\n                column(4,\n                       box(\n                         title = \"Simulation Parameters\",\n                         status = \"primary\",\n                         solidHeader = TRUE,\n                         collapsible = TRUE,\n                         collapsed = FALSE,\n                         width = 12,\n                         sliderInput(\"assoc\", \"Association Parameter (c):\",\n                                     min = .001, max = 1, value = 0.5, step = 0.01),\n                         sliderInput(\"update\", \"Update Parameter:\",\n                                     min = 0, max = 1, value = 0.5, step = 0.1),\n                         sliderInput(\"trainRep\", \"Training Repetitions Per Item:\",\n                                     min = 1, max = 200, value = 1, step = 1),\n                         sliderInput(\"Noise\",\"Noise Level:\",\n                                     min = 0, max = 50, value = 0.00, step = 1),\n                         checkboxGroupInput(\"trainItems\", \"Training Items:\", choices = trainOptions, selected = trainOptions[c(10,15,35)],inline=TRUE),\n                         # radio buttons for selecting function form\n                         radioButtons(\"functionForm\", \"Function Form:\",\n                                      choices = c(\"Linear\", \"Quadratic\", \"Exponential\"),\n                                      selected = \"Quadratic\"),\n                        # numericInput(\"nRep\", \"Number of Replications:\", value = 1, min = 1, max = 100),\n                         actionButton(\"run\", \"Run Simulation\")\n                       )\n                ),\n                column(8,\n                       box(\n                         title = \"Model Performance\",\n                         status = \"primary\",\n                         solidHeader = TRUE,\n                         collapsible = TRUE,\n                         collapsed = FALSE,\n                          width = 12,\n                         plotOutput(\"plot\"),\n                         h5(\"*Dashed line shows true function. Red shows ALM, and blue depicts EXAM predictions*\"),\n                         h4(\"Average Model Performance\"),\n                         reactableOutput(\"table\"),\n                         h4(\"Model Performance by Item Type\"),\n                         reactableOutput(\"table2\")\n                       )\n                )\n              )\n      ),\n      tabItem(tabName = \"code\",\n              fluidRow(\n                column(12,\n                       box(\n                         title = \"Code\",\n                         status = \"primary\",\n                         solidHeader = TRUE,\n                         collapsible = TRUE,\n                         collapsed = FALSE,\n                         width = 12,\n                         verbatimTextOutput(\"code\")\n                       )\n                )\n                )\n        )\n    )\n    )\n)\n\n# Define server \n\n\n\nserver &lt;- function(input, output, session) {\n  \n  nRep=1\n  user_choice &lt;- eventReactive(input$run, {\n    return(list(assoc = input$assoc, update = input$update, Noise=input$Noise,\n                functionForm=input$functionForm,trainRep = as.numeric(input$trainRep),\n                trainItems = input$trainItems))\n    \n  }, ignoreNULL = FALSE)\n  \n\n    output_df &lt;- eventReactive(input$run, {\n      uc &lt;- reactive({user_choice()})\n    if (uc()$functionForm == \"Linear\") {\n      f.plotting &lt;&lt;- as.numeric(x.plotting * 2.2 + 30)\n    } else if (uc()$functionForm == \"Quadratic\") {\n      f.plotting &lt;&lt;- as.numeric(210 - ((x.plotting - 50)^2) / 12)\n    } else if (uc()$functionForm == \"Exponential\") {\n      # f.plotting&lt;&lt;-as.numeric(scale(200*(1-exp(-x.plotting/25))))\n      f.plotting &lt;&lt;- as.numeric(200 * (1 - exp(-x.plotting / 25)))\n    }\n    trainItems &lt;- as.numeric(uc()$trainItems)\n    y.plotting &lt;&lt;- seq(0, max(f.plotting), by = 1)\n    x.learning &lt;&lt;- rep(trainItems, times = uc()$trainRep)\n    f.learning &lt;&lt;- rep(f.plotting[which(x.plotting %in% trainItems)], times = uc()$trainRep)\n    # print(x.learning)\n    # print(f.learning)\n    # print(uc()$trainRep)\n    # print(trainItems)\n    # print(uc()$functionForm)\n    \n    \n    output_list &lt;- replicate(nRep, list(learn.alm(f.learning + rnorm(length(f.learning), sd = uc()$Noise),\n                                                  association.parameter = uc()$assoc, update.parameter = uc()$update)))\n    \n    output_df &lt;- lapply(output_list, function(x) as.data.frame(x))\n    #output_df &lt;- lapply(output_list, function(x) lapply(x, as.data.frame)) # 10 dfs x 9 lists\n    output_df &lt;- Reduce(rbind, output_df) %&gt;% mutate(x = x.plotting, y = f.plotting)\n    #output_df &lt;- lapply(output_df, function(x) Reduce(rbind,x))# 1 df x 9 lists\n    output_df &lt;- output_df %&gt;%\n      pivot_longer(names_to = \"Model\", values_to = \"Prediction\", cols = c(alm.predictions, exam.predictions)) %&gt;%\n      rbind(data.frame(data.frame(x = x.plotting, y = f.plotting, Model = \"True Function\", Prediction = f.plotting)), .)\n    #str(output_df)\n    return(output_df)\n    \n    }, ignoreNULL = FALSE)\n    \n    output$plot &lt;- renderPlot({\n      \n      output_df2 &lt;- reactive({output_df()})\n      ggplot(data = output_df2(), aes(x = x, y = Prediction,color=Model),alpha=.2) + \n        geom_line(aes(linetype=Model,alpha=Model)) + \n        geom_point(data = data.frame(x.learning, f.learning), \n                   aes(x = x.learning,y = f.learning),color=\"black\",size=4,shape=4) +\n        # geom_line(data = data.frame(x.plotting, f.plotting), \n        #           aes(x = x.plotting, y = f.plotting),linetype=2, color = \"black\",alpha=.3) + \n        scale_color_manual(values = c(\"red\", \"blue\", \"black\"))+\n        scale_alpha_manual(values=c(.8,.8,.4))+\n        scale_linetype_manual(values=c(1,1,2))+\n        ylim(c(0,250))#+\n        # ggtitle(paste(\"Association Parameter:\", user_choice()$assoc, \" Update Parameter:\", \n        #               uc$update, \" Train Reps:\", \n        #               uc$trainRep, \" Noise:\", uc$Noise))\n    }) \n    # table 1 reports the summary stats for all items. Table uses GT library to make gt table\n    output$table &lt;- renderReactable({\n      output_df &lt;- output_df()\n      output_df() %&gt;% group_by(Model) %&gt;% filter(Model !=\"True Function\") %&gt;%\n        summarise(MeanDeviation = mean(abs(Prediction - y)), \n                  RMSD = sqrt(mean((Prediction -y)^2)),Correlation = cor(Prediction, y)) %&gt;%\n        mutate(across(where(is.numeric), round, 1)) %&gt;%\n        reactable::reactable(compact=TRUE,bordered = TRUE, highlight = TRUE, resizable=TRUE)\n    })\n    # table 2 reports the summary stats separately for training items, interpolation items, and extrapolation items\n    output$table2 &lt;- renderReactable({\n      uc &lt;- reactive({user_choice()})\n      output_df() %&gt;% filter(Model !=\"True Function\") %&gt;% \n        mutate(ItemType = ifelse(x %in% x.learning, \"Training\", ifelse(x &gt; min(x.learning) & x &lt; max(x.learning), \"Interpolation\", \"Extrapolation\"))) %&gt;%\n        group_by(ItemType,Model) %&gt;%\n        summarise(MeanDeviation = mean(abs(Prediction - y)), \n                  RMSD = sqrt(mean((Prediction -y)^2)),Correlation = cor(Prediction, y),\n                  .groups=\"keep\") %&gt;% \n        mutate(across(where(is.numeric), round, 1)) %&gt;%\n        reactable::reactable(compact=TRUE,bordered = TRUE, highlight = TRUE, resizable=TRUE) \n    })\n    \n    \n    output$code &lt;- renderPrint({\n      # code to implement the ALM and EXAM models\n      # code to generate data\n      # code to run models\n      # code to format output\n      cat(\" input.activation&lt;-function(x.target, association.parameter){\n  return(exp(-1*association.parameter*(x.target-x.plotting)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, association.parameter){\n  return(weights%*%input.activation(x.target, association.parameter))\n}\n\nmean.prediction&lt;-function(x.target, weights, association.parameter){\n  probability&lt;-output.activation(x.target, weights, association.parameter)/sum(output.activation(x.target, weights, association.parameter))\n  return(y.plotting%*%probability)\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, association.parameter){\n  trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, association.parameter)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, association.parameter)\n  mOver = mean.prediction(xOver, weights, association.parameter)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n\nupdate.weights&lt;-function(x.new, y.new, weights, association.parameter, update.parameter){\n  y.feedback.activation&lt;-exp(-1*association.parameter*(y.new-y.plotting)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, association.parameter)\n  return(weights+update.parameter*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, association.parameter)))\n}\n\nlearn.alm&lt;-function(y.learning, association.parameter=0.05, update.parameter=0.5){\n  weights&lt;-matrix(rep(0.00, length(y.plotting)*length(x.plotting)), nrow=length(y.plotting), ncol=length(x.plotting))\n  for (i in 1:length(y.learning)){\n    weights&lt;-update.weights(x.learning[i], y.learning[i], weights, association.parameter, update.parameter)\n    weights[weights&lt;0]=0\n  }\n  alm.predictions&lt;-sapply(x.plotting, mean.prediction, weights=weights, association.parameter=association.parameter)\n  exam.predictions &lt;- sapply(x.plotting, exam.prediction, weights=weights, association.parameter=association.parameter)\n  return(list(alm.predictions=alm.predictions, exam.predictions=exam.predictions))\n  #return(list(alm.predictions=alm.predictions, exam.predictions=exam.predictions,wmFinal=weights))\n}\n\n    \")\n    })\n    \n}\n\n\n\n# Run the application\n\n\nshinyApp(ui, server)",
    "crumbs": [
      "Interactive",
      "ALM Shiny App Code"
    ]
  },
  {
    "objectID": "Misc/Distributional_Explore.html",
    "href": "Misc/Distributional_Explore.html",
    "title": "Distributional_Explorations",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes,brms,bayesplot,broom,broom.mixed,lme4,emmeans,here,knitr,kableExtra,gt,gghalves,patchwork,ggforce,ggdist,moments)\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\nsource(here(\"Functions/org_functions.R\"))\ntest &lt;- e1 |&gt; \n  filter(expMode2 == \"Test\") |&gt; \n  group_by(vb) |&gt; \n  mutate(distS = as.numeric(scale(dist, scale = FALSE)), \n         distS2 = custom_scale(dist))\n\noptions(brms.backend=\"cmdstanr\",mc.cores=4)\n\ntest %&gt;% group_by(condit) |&gt; summarise(mean=mean(dist),sd=sd(dist),sk=moments::skewness(dist)) \ntest %&gt;% group_by(vb) |&gt; summarise(mean=mean(dist),sd=sd(dist),sk=moments::skewness(dist)) \n\n\n\nCode# test |&gt; ggplot(aes(x=dist))+geom_histogram() + facet_wrap(~vb) + ggtitle(\"empirical_dist\")  +\n# test |&gt; ggplot(aes(x=distS))+geom_histogram() + facet_wrap(~vb) + ggtitle(\"centered dist\")  +\n# test |&gt; ggplot(aes(x=distS2))+geom_histogram() + facet_wrap(~vb) + ggtitle(\"scaled dist\")\n\ntest |&gt; filter(id %in% 1:5) |&gt; ggplot(aes(x=dist))+geom_histogram() + facet_wrap(id~vb) + ggtitle(\"empirical_dist\")  \nplot(density(test$vx))\nplot(density(test$dist))\n\n\ntest |&gt; ggplot(aes(x=dist))+geom_density() + facet_wrap(~vb) + ggtitle(\"empirical_dist\") \ntest |&gt; ggplot(aes(x=vx))+geom_density() + facet_wrap(~vb) + ggtitle(\"empirical_dist\") \n\n\n\nCodesk1 &lt;- brm(dist ~ vb,family=skew_normal(),data=test,iter=800,chains=2)\ng1 &lt;- brm(dist ~ vb,family=gaussian(),data=test,iter=800,chains=2)\nga2 &lt;- brm(dist+.01 ~ vb,family=Gamma(),data=test,iter=800,chains=2)\nln1 &lt;- brm(dist+.01 ~ vb,family=lognormal(),data=test,iter=800,chains=2)\nln2 &lt;- brm(dist+.0001 ~ vb,family=lognormal(link=\"inverse\"),data=test,iter=800,chains=2)\n\ng2 &lt;- brm(bf(dist+.001|trunc(lb=0) ~ vb),data=testS,iter=800,chains=2,family=gaussian())\nbayesplot::ppc_dens_overlay_grouped(testS$dist,yrep=posterior_predict(g2,ndraws=200),group=testS$vb)\npp_check(g2,type=\"stat_grouped\",ndraws=200, group=\"vb\",stat=\"mean\")\n\n\n\n\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(sk1,ndraws=200),group=test$vb)\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(g1,ndraws=200),group=test$vb)\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(ga2,ndraws=200),group=test$vb)\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(ln1,ndraws=200),group=test$vb)\n\n\n\nbayes_R2(g1)\nbayes_R2(ga1)\nbayes_R2(sk1)\nbayes_R2(sk1)\nbayes_R2(g1S)\n\n\n#testS &lt;- test %&gt;% group_by(id,vb) |&gt; filter(id %in% c(\"1\",\"2\",\"139\")) |&gt; select(id,vb,gt.stage,trial,condit,vx,dist,distS,distS2)\ntestS &lt;- test %&gt;% group_by(id,vb) |&gt; filter(id %in% 1:15) |&gt; select(id,vb,gt.stage,trial,condit,vx,dist,distS,distS2)\n\ntestS |&gt; ggplot(aes(x=trial,y=dist,col=vb))+geom_line()+facet_wrap(~id)\ntestS |&gt; ggplot(aes(x=trial,y=distS2,col=vb))+geom_line()+facet_wrap(~id)\n\n\n\ng1S &lt;- brm(distS ~ 0+ vb + condit,family=gaussian(),data=test,iter=1000,chains=4)\nbayesplot::ppc_dens_overlay_grouped(test$distS,yrep=posterior_predict(g1S,ndraws=200),group=test$vb)\n\ng1S2 &lt;- brm(distS2 ~ vb,family=gaussian(),data=test,iter=1000,chains=4)\nbayesplot::ppc_dens_overlay_grouped(test$distS2,yrep=posterior_predict(g1S2,ndraws=200),group=test$vb)\n\n\ng1S_F &lt;- brm(distS ~ 0+ vb + condit + (0+vb|id),family=gaussian(),data=test,iter=1000,chains=4,\n             file=paste0(here::here(\"data/model_cache\",\"e1_test_centeredDistS\")))\n\nbayesplot::ppc_dens_overlay_grouped(test$distS,yrep=posterior_predict(g1S_F,ndraws=200),group=test$vb)\nbayesplot::ppc_dens_overlay_grouped(test$distS,yrep=posterior_predict(g1S_F,ndraws=200),group=test$vb)\n\n\n\ng1S_gamma &lt;- brm(dist+.001 ~ 1+ vb + condit + (0+vb|id),family=Gamma(),data=test,iter=1000,chains=4,\n             file=paste0(here::here(\"data/model_cache\",\"e1_test_Gamma\")))\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(g1S_gamma,ndraws=200),group=test$vb)\n\npp_check(g1S_weibull)+ xlim(c(-300,300))\n\n\ng1S_exG &lt;- brm(dist ~ 0 + vb + condit + (0+vb|id),family=exgaussian(),data=test,iter=1000,chains=4,\n             file=paste0(here::here(\"data/model_cache\",\"e1_test_exgauss\")))\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(g1S_exG,ndraws=200),group=test$vb)"
  },
  {
    "objectID": "Analysis/full_analysis.html",
    "href": "Analysis/full_analysis.html",
    "title": "Combined Analysis of All 3 Experiments",
    "section": "",
    "text": "Questions - difficulty difference between bands - effect of ordinal feedback. - effect of training with easier bands - do varied subjects who discriminate by end of training do better on test?\n\nsource(here::here(\"Functions\", \"packages.R\"))\ndAll &lt;- readRDS(here(\"data/dAll_08-21-23.rds\"))\n\n\ndAll &lt;- readRDS(here(\"data/dAll_08-21-23.rds\"))\n\nf &lt;- (`Band` = vb)* expMode2 ~ dist * Exp* condit * ((`Avg.` = Mean)*Arguments(fmt='%.0f') )\n\ndatasummary(f,\n            data = dAll,\n            output = 'gt',\n            sparse_header = TRUE) \n\nAre there differences in difficulty between bands?\n\nmTv &lt;- lmer(dist ~ 0+vb + Exp + (1|id), data = dAll |&gt; filter(expMode2==\"Train\",condit==\"Varied\"))\nsummary(mTv)\n\nmTvb &lt;- lmer(dist ~ 0+bandInt + (1|id), data = dAll |&gt; filter(expMode2==\"Train\",condit==\"Varied\"))\nsummary(mTvb)\n\nsummary(lmer(dist ~ 0+bandInt + (1|id), data = dAll |&gt; filter(expMode2==\"Train\")))\nsummary(lmer(dist ~ 0+bandInt + (1|id), data = dAll |&gt; filter(expMode==\"test-feedback\")))\n\nsummary(lmer(dist ~ 0+bandInt + (1|id), data = dAll |&gt; filter(expMode==\"test-train-nf\")))\n\nIs there an effect of ordinal feedback?\n\nmfb &lt;- lmer(dist ~ fb*vb + (1|id), data = dAll |&gt; filter(expMode2==\"Test\"))\nsummary(mfb)\n\nmfb2 &lt;- lmer(dist ~ fb*vb*condit+ (1|id), data = dAll |&gt; filter(expMode2==\"Test\"))\nsummary(mfb2)\n\n\nmfb3 &lt;- lmer(dist ~ fb*bandInt*condit+ (1|id), data = dAll |&gt; filter(expMode2==\"Test\"))\nsummary(mfb3)\n\nIs there an effect of training with easier bands?\n\nmr &lt;- lmer(dist ~ bandOrder*vb + (1|id), data = dAll |&gt; filter(expMode2==\"Test\"))\nsummary(mr)\n\nmr2 &lt;- lmer(dist ~ bandOrder*vb*condit + (1|id), data = dAll |&gt; filter(expMode2==\"Test\"))\nsummary(mr2)\n\nmr3 &lt;- lmer(dist ~ bandOrder*bandInt*condit + (1|id), data = dAll |&gt; filter(expMode2==\"Test\"))\nsummary(mr3)\n\n\ndAll |&gt; filter(condit==\"Varied\",expMode2==\"Train\") |&gt; group_by(id,vb,bandInt,Exp) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=vb,y=m,fill=vb)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(~Exp) \n\n\ndAll |&gt; filter(condit==\"Varied\",expMode2==\"Test\") |&gt; group_by(id,vb,bandInt,Exp) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=vb,y=m,fill=vb)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(~Exp) \n\ndAll |&gt; filter(condit==\"Varied\",expMode2==\"Test\") |&gt; group_by(id,vb,bandInt,bandOrder) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=vb,y=m,fill=vb)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(~bandOrder) \n\ndAll |&gt; filter(condit==\"Varied\") |&gt; group_by(id,vb,bandInt,bandOrder) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=vb,y=m,fill=vb)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(~bandOrder) \n\n\n\ndAll  |&gt; group_by(id,vb,bandInt,bandOrder,condit) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=vb,y=m,fill=vb)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(condit~bandOrder) \n\n\ndAll  |&gt; filter(expMode2==\"Test\") |&gt; group_by(id,vb,bandInt,fb,condit) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=vb,y=m,fill=vb)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(condit~fb) \n\n\ndAll  |&gt; filter(expMode2==\"Test\") |&gt; group_by(id,vb,bandInt,fb,condit,Exp) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=vb,y=m,fill=vb)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(condit~Exp) \n\n\ndAll  |&gt; filter(expMode2==\"Test\") |&gt; group_by(id,vb,bandInt,fb,condit,Exp) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=Exp,y=m,fill=Exp)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(condit~vb) \n\ndAll  |&gt; filter(expMode2==\"Test\") |&gt; group_by(id,vb,bandInt,fb,condit,Exp) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=Exp,y=m,fill=condit)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(~vb)\n\n\ndAll  |&gt; filter(expMode2==\"Test\") |&gt; group_by(id,vb,bandType,bandInt,fb,condit,Exp) |&gt; summarize(m=mean(dist)) |&gt; ggplot(aes(x=Exp,y=m,fill=condit)) + \n     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4) +\n  facet_wrap(~bandType)\n\n\ndAll|&gt; group_by(id,vb,bandInt,fb,condit,expMode2) |&gt; summarize(m=mean(dist)) |&gt; filter(expMode2==\"Test\" | expMode2==\"Train\") |&gt; group_by(id,fb,condit,expMode2) |&gt; summarise(m=mean(m)) |&gt; # pivot expMode into separate columns\npivot_wider( names_from = expMode2, values_from = m) |&gt; ggplot(aes(x=Train,y=Test,fill=condit)) + geom_point() + geom_abline(intercept = 0, slope = 1) + facet_wrap(~fb)\n  \n# quantify correlation between train and test for varied and fixed\ndAll|&gt; group_by(id,vb,bandInt,fb,condit,expMode2) |&gt; summarize(m=mean(dist)) |&gt; filter(expMode2==\"Test\" | expMode2==\"Train\") |&gt; group_by(id,fb,condit,expMode2) |&gt; summarise(m=mean(m)) |&gt; # pivot expMode into separate columns\npivot_wider( names_from = expMode2, values_from = m) |&gt; filter(condit==\"Varied\") %&gt;%cor.test(~Train+Test)\n\nresult &lt;- dAll %&gt;% \n  # Group and calculate mean\n  group_by(id, vb, bandInt, fb, condit, expMode2) %&gt;% \n  summarize(m = mean(dist), .groups = \"drop\") %&gt;% \n  # Filter for Test and Train\n  filter(expMode2 == \"Test\" | expMode2 == \"Train\") %&gt;% \n  group_by(id, fb, condit, expMode2) %&gt;% \n  summarise(m = mean(m), .groups = \"drop\") %&gt;% \n  # Pivot expMode into separate columns\n  pivot_wider(names_from = expMode2, values_from = m) %&gt;% \n  # Filter for condition 'Varied'\n  filter(condit == \"Varied\")\n\n# Check if the resulting dataframe has 'Train' and 'Test' columns and they are numeric\nif(\"Train\" %in% colnames(result) & \"Test\" %in% colnames(result)){\n  if(is.numeric(result$Train) & is.numeric(result$Test)){\n    cor_result &lt;- cor.test(result$Train, result$Test)\n  } else {\n    stop(\"Either 'Train' or 'Test' column is not numeric\")\n  }\n} else {\n  stop(\"The data frame does not have the expected 'Train' and 'Test' columns\")\n}\n\nprint(cor_result)\n\n\nresult &lt;- dAll %&gt;% \n  group_by(id, vb, bandInt, fb, condit, expMode2) %&gt;% \n  summarize(m = mean(dist), .groups = \"drop\") %&gt;% \n  filter(expMode2 == \"Test\" | expMode2 == \"Train\") %&gt;% \n  group_by(id, fb, condit, expMode2) %&gt;% \n  summarise(m = mean(m), .groups = \"drop\") %&gt;% \n  pivot_wider(names_from = expMode2, values_from = m)\n\n# Function to compute correlation for each condition\ncompute_correlation &lt;- function(data) {\n  if(\"Train\" %in% colnames(data) & \"Test\" %in% colnames(data)){\n    if(is.numeric(data$Train) & is.numeric(data$Test)){\n      return(cor.test(data$Train, data$Test))\n    } else {\n      stop(\"Either 'Train' or 'Test' column is not numeric\")\n    }\n  } else {\n    stop(\"The data frame does not have the expected 'Train' and 'Test' columns\")\n  }\n}\n\n\n\ncorrelations &lt;- result %&gt;%\n  group_by(condit) %&gt;%\n  group_map(~ compute_correlation(.x))\nnames(correlations) &lt;- levels(result$condit)\nprint(correlations)\n\n\nggplot(result, aes(x=Train, y=Test)) + \n  geom_point(aes(color=condit), alpha=0.5) +  # Plot points with different colors based on condition\n  geom_smooth(aes(color=condit), method='lm', se=FALSE) +  # Overlay a regression line\n  facet_wrap(~condit) +  # Separate plot for each condition\n  theme_minimal() + \n  labs(title=\"Correlation between Train and Test\",\n       subtitle=\"Faceted by Condition (Varied vs. Constant)\",\n       x=\"Train\", y=\"Test\", color=\"Condition\") + \n  theme(legend.position=\"bottom\")\n\n\n\n\ncorrelations &lt;- result %&gt;%\n  group_by(condit,fb) %&gt;%\n  group_map(~ compute_correlation(.x))\nnames(correlations) &lt;- levels(result$condit)\nprint(correlations)\n\n\nggplot(result, aes(x=Train, y=Test)) + \n  geom_point(aes(color=condit), alpha=0.5) +  # Plot points with different colors based on condition\n  geom_smooth(aes(color=condit), method='lm', se=FALSE) +  # Overlay a regression line\n  facet_wrap(fb~condit) +  # Separate plot for each condition\n  theme_minimal() + \n  labs(title=\"Correlation between Train and Test\",\n       subtitle=\"Faceted by Condition (Varied vs. Constant)\",\n       x=\"Train\", y=\"Test\", color=\"Condition\") + \n  theme(legend.position=\"bottom\")"
  },
  {
    "objectID": "Analysis/e3.html",
    "href": "Analysis/e3.html",
    "title": "Experiment 3",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n  brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n  stringr, here,conflicted, patchwork, knitr,kableExtra)\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\nwalk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne3 &lt;- readRDS(here(\"data/e3_08-04-23.rds\")) |&gt; \n    mutate(trainCon=case_when(\n    bandOrder==\"Original\" ~ \"800\",\n    bandOrder==\"Reverse\" ~ \"600\",\n    TRUE ~ NA_character_\n    ), trainCon=as.numeric(trainCon)) \ne3Sbjs &lt;- e3 |&gt; group_by(id,condit,bandOrder) |&gt; summarise(n=n())\ntestE3 &lt;- e3 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE3 &lt;-  e3 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit,bandOrder, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE3_max &lt;- trainE3 |&gt; filter(Trial_Bin == nbins, bandInt==trainCon)\n\n\nMethods & Procedure\nThe major adjustment of Experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the prior experiments. After each training throw, participants are informed whether a throw was too soft, too hard, or correct (i.e. within the target velocity range). All other aspects of the task and design are identical to Experiments 1 and 2. We utilized the order of training and testing bands from both of the prior experiments, thus assigning participants to both an order condition (Original or Reverse) and a training condition (Constant or Varied). Participants were once again recruited from the online Indiana University Introductory Psychology Course pool. Following exclusions, 195 participants were included in the final analysis, n=51 in the Constant-Original condition, n=59 in the Constant-Reverse condition, n=39 in the Varied-Original condition, and n=46 in the Varied-Reverse condition.\nResults\n\nCodebmm_e3_train &lt;- trainE3_max %&gt;% \n  brm(dist ~ condit*bandOrder, \n      file=here(\"data/model_cache/e3_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\n# mtr3 &lt;- as.data.frame(describe_posterior(bmm_e3_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mtr3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mtr3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\ncd3tr1 &lt;- get_coef_details(bmm_e3_train, \"conditVaried\")\ncd3tr2 &lt;-get_coef_details(bmm_e3_train, \"bandOrderReverse\")\ncd3tr3 &lt;-get_coef_details(bmm_e3_train, \"conditVaried:bandOrderReverse\")\n\n\n\n\nTable 1: Experiment 3 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n121.86\n109.24\n134.60\n1.00\n\n\nconditVaried\n64.93\n36.99\n90.80\n1.00\n\n\nbandOrderReverse\n1.11\n-16.02\n18.16\n0.55\n\n\nconditVaried:bandOrderReverse\n-77.02\n-114.16\n-39.61\n1.00\n\n\n\n\n\n\nTraining. Figure 1 displays the average deviations from the target band across training blocks, and Table 1 shows the results of the Bayesian regression model predicting the deviation from the common band at the end of training (600-800 for reversed order, and 800-1000 for original order conditions). The main effect of training condition is significant, with the varied condition showing larger deviations ( \\(\\beta\\) = 64.93, 95% CrI [36.99, 90.8]; pd = 100%). The main effect of band order is not significant \\(\\beta\\) = 1.11, 95% CrI [-16.02, 18.16]; pd = 55.4%, however the interaction between training condition and band order is significant, with the varied condition showing greater accuracy in the reverse order condition ( \\(\\beta\\) = -77.02, 95% CrI [-114.16, -39.61]; pd = 100%).\n\nCodep1 &lt;- trainE3 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    ggh4x::facet_nested_wrap(~bandOrder*vb,ncol=3)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e3_train_deviation.png\"), p1, width = 9, height = 8,bg=\"white\")\np1\n\n\n\n\n\n\nFigure 1: E3. Deviations from target band during testing without feedback stage.\n\n\n\n\n\nCode#options(brms.backend=\"cmdstanr\",mc.cores=4)\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e3_dist_Cond_Type_RF_2\")\nbmtd3 &lt;- brm(dist ~ condit * bandType*bandOrder + (1|bandInt) + (1|id), \n    data=testE3, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted3 &lt;- as.data.frame(describe_posterior(bmtd3, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\n#ce_bmtd3 &lt;- plot(conditional_effects(bmtd3),points=FALSE,plot=FALSE)\n#wrap_plots(ce_bmtd3)\n\n#ggsave(here::here(\"Assets/figs\", \"e3_cond_effects_dist.png\"), wrap_plots(ce_bmtd3), width=11, height=11, bg=\"white\")\n\ncd3ted1 &lt;- get_coef_details(bmtd3, \"conditVaried\")\ncd3ted2 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation\")\ncd3ted3 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation\")\ncd3ted4 &lt;-get_coef_details(bmtd3, \"bandOrderReverse\")\ncd3ted5 &lt;-get_coef_details(bmtd3, \"conditVaried:bandOrderReverse\")\ncd3ted6 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation:bandOrderReverse\")\ncd3ted7 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation:bandOrderReverse\")\n\n\n\n\nTable 2: Experiment 3 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition, (constant training; trained bands & original order), and the remaining coefficients reflect the deviation from that baseline. Positive coefficients thus represent worse performance relative to the baseline, - and a positive interaction coefficient indicates disproportionate deviation for the varied condition or reverse order condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n288.65\n199.45\n374.07\n1.00\n\n\nconditVaried\n-40.19\n-104.68\n23.13\n0.89\n\n\nbandTypeExtrapolation\n-23.35\n-57.28\n10.35\n0.92\n\n\nbandOrderReverse\n-73.72\n-136.69\n-11.07\n0.99\n\n\nconditVaried:bandTypeExtrapolation\n52.66\n14.16\n90.23\n1.00\n\n\nconditVaried:bandOrderReverse\n-37.48\n-123.28\n49.37\n0.80\n\n\nbandTypeExtrapolation:bandOrderReverse\n80.69\n30.01\n130.93\n1.00\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n30.42\n-21.00\n81.65\n0.87\n\n\n\n\n\n\nTesting Accuracy. Table 2 presents the results of the Bayesian mixed efects model predicting absolute deviation from the target band during the testing stage. There was no significant main effect of training condition,\\(\\beta\\) = -40.19, 95% CrI [-104.68, 23.13]; pd = 89.31%, or band type,\\(\\beta\\) = -23.35, 95% CrI [-57.28, 10.35]; pd = 91.52%. However the effect of band order was significant, with the reverse order condition showing lower deviations, \\(\\beta\\) = -73.72, 95% CrI [-136.69, -11.07]; pd = 98.89%. The interaction between training condition and band type was also significant \\(\\beta\\) = 52.66, 95% CrI [14.16, 90.23]; pd = 99.59%, with the varied condition showing disproprionately large deviations on the extrapolation bands compared to the constant group. There was also a significant interaction between band type and band order, \\(\\beta\\) = 80.69, 95% CrI [30.01, 130.93]; pd = 99.89%, such that the reverse order condition showed larger deviations on the extrapolation bands. No other interactions were significant.\n\n\nCodecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3td &lt;- testE3 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n    facet_wrap(~bandOrder,ncol=1) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\npe3ce &lt;- bmtd3 |&gt; emmeans( ~condit *bandOrder*bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\") + \n facet_wrap(~bandOrder,ncol=1)\n\np2 &lt;- pe3td + pe3ce + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e3_test-dev.png\"), p2, width=9, height=8, bg=\"white\")\np2\n\n\n\n\n\n\nFigure 2: Experiment 3 Testing Accuracy. A) Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (training vs. extrapolation) on testing accuracy. Error bars represent 95% confidence intervals.\n\n\n\n\n\nCode##| label: tbl-e3-bmm-vx\n##| tbl-cap: \"Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne3_vxBMM &lt;- brm(vx ~ condit * bandOrder * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e3_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n# m1 &lt;- as.data.frame(describe_posterior(e3_vxBMM, centrality = \"Mean\"))\n# m2 &lt;- fixef(e3_vxBMM)\n# mp3 &lt;- m1[, c(1,2,4,5,6)]\n# colnames(mp3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")                       \n# mp3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_replace_all(Term, \"b_bandInt\", \"Band\")) |&gt;\n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T)\n\n\n\n#wrap_plots(plot(conditional_effects(e3_vxBMM),points=FALSE,plot=FALSE))\n\ncd1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried:bandInt\")\nintCoef2 &lt;- get_coef_details(e3_vxBMM, \"bandOrderReverse:bandInt\")\ncoef3 &lt;- get_coef_details(e3_vxBMM,\"conditVaried:bandOrderReverse:bandInt\")\n\n\n\n\nTable 3: Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band. The Intercept represents the baseline condition (constant training & original order), and the Band coefficient represents the linear slope for the baseline condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nconditVaried:Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:bandInt\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:bandInt\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\nTesting Discrimination. The full results of the discrimination model are presented in Table 2. For the purposes of assessing group differences in discrimination, only the coefficients including the band variable are of interest. The baseline effect of band represents the slope cofficient for the constant training - original order condition, this effect was significant \\(\\beta\\) = 0.49, 95% CrI [0.36, 0.62]; pd = 100%. Neither of the two way interactions reached significance, \\(\\beta\\) = -0.04, 95% CrI [-0.23, 0.15]; pd = 66.63%, \\(\\beta\\) = -0.1, 95% CrI [-0.27, 0.08]; pd = 86.35%. However, the three way interaction between training condition, band order, and target band was significant, \\(\\beta\\) = 0.42, 95% CrI [0.17, 0.7]; pd = 99.96% - indicating that the varied condition showed a greater slope coefficient on the reverse order bands, compared to the constant condition - this is clearly shown in Figure 3, where the steepness of the best fitting line for the varied-reversed condition is noticably steeper than the other conditions.\n\nCode##| column: screen-inset-right\n# testE3 |&gt; filter(bandOrder==\"Original\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit()\n# testE3 |&gt; filter(bandOrder==\"Reverse\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit() +ggtitle(\"test\")\n\ntestE3 |&gt; group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + \n   ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\n\n\n\n\nFigure 3: e3 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\nCode##| eval: FALSE\n# pe3tv &lt;- testE3 %&gt;% group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3vce &lt;- e3_vxBMM |&gt; emmeans( ~condit* bandOrder* bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  facet_wrap(~bandOrder,ncol=1) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e3_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e3_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction1 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandInt\"]\nfixed_effect_interaction2 &lt;- fixef(e3_vxBMM)[,1][\"bandOrderReverse:bandInt\"]\nfixed_effect_interaction3 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandOrderReverse:bandInt\"]\n\nre &lt;- data.frame(ranef(e3_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e3Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction1*(condit==\"Varied\") + \n           fixed_effect_interaction2*(bandOrder==\"Reverse\") + \n           fixed_effect_interaction3*(condit==\"Varied\" & bandOrder==\"Reverse\"),\n  slope = Estimate + adjust )\n\npid_den3 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\") +\n  facet_wrap(~bandOrder,ncol=1)\n\npid_slopes3 &lt;- re |&gt;  \n    mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n    geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n    theme(legend.title=element_blank(), \n      axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_nested_wrap(bandOrder~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe3vce + pid_den3 + pid_slopes3) + plot_annotation(tag_levels= 'A')\n\n#ggsave(here::here(\"Assets/figs\", \"e3_test-vx.png\"), p3,width=11,height=13, bg=\"white\",dpi=800)\np3\n\n\n\n\n\n\nFigure 4: Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\nExperiment 3 Summary\nIn Experiment 3, we investigated the effects of training condition (constant vs. varied) and band type (training vs. extrapolation) on participants’ accuracy and discrimination during the testing phase. Unlike the previous experiments, participants received ordinal feedback during the training phase. Additionally, Experiment 3 included both the original order condition from Experiment 1 and the reverse order condition from Experiment 2. The results revealed no significant main effects of training condition on testing accuracy, nor was there a significant difference between groups in band discrimination. However, we observed a significant three-way interaction for the discrimination analysis, indicating that the varied condition showed a steeper slope coefficient on the reverse order bands compared to the constant condition. This result suggests that varied training enhanced participants’ ability to discriminate between velocity bands, but only when the band order was reversed during testing.",
    "crumbs": [
      "Analyses",
      "Experiment 3"
    ]
  },
  {
    "objectID": "Analysis/e2.html",
    "href": "Analysis/e2.html",
    "title": "Experiment 2",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n  brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n  stringr, here,conflicted, patchwork, knitr,kableExtra)\n#options(brms.backend=\"cmdstanr\",mc.cores=4)\noptions(dplyr.summarise.inform = FALSE)\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\nwalk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne2 &lt;- readRDS(here(\"data/e2_08-04-23.rds\")) \ne2Sbjs &lt;- e2 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ntestE2 &lt;- e2 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE2 &lt;-  e2 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE2_max &lt;- trainE2 |&gt; filter(Trial_Bin == nbins, bandInt==600)\n\n# e2 |&gt; group_by(condit, bandOrder) |&gt; summarise(n_distinct(id))\n\n\nMethods & Procedure\nThe task and procedure of Experiment 2 was identical to Experiment 1, with the exception that the training and testing bands were reversed (see Figure 1). The Varied group trained on bands 100-300, 350-550, 600-800, and the constant group trained on band 600-800. Both groups were tested from all six bands. A total of 110 participants completed the experiment (Varied: 55, Constant: 55).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 100-300350-550600-800Test1\nTest  Novel Bands  800-10001000-12001200-1400data1-&gt;Test1\ndata2\n Constant Training 600-800data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  800-10001000-12001200-1400Test2\n  Test   Varied Training Bands  100-300350-550600-800Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 1: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.\n\n\n\n\nResults\n\nCodep1 &lt;- trainE2 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e2_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\nFigure 2: Experiment 2 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\nCodebmm_e2_train &lt;- trainE2_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e2_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\nmtr2 &lt;- as.data.frame(describe_posterior(bmm_e2_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\ncdtr2 &lt;- get_coef_details(bmm_e2_train, \"conditVaried\")\n# mtr2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\n\n\n\nTable 1: Experiment 2 - End of training performance. The Intercept represents the average of the baseline (constant condition), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n91.01\n80.67\n101.26\n1\n\n\nconditVaried\n36.15\n16.35\n55.67\n1\n\n\n\n\n\n\n\nTraining. Figure 2 presents the deviations across training blocks for both constant and varied training groups. We again compared training performance on the band common to both groups (600-800). The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, ( \\(\\beta\\) = 36.15, 95% CrI [16.35, 55.67]; pd = 99.95%).\n\nCodemodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e2_dist_Cond_Type_RF_2\")\nbmtd2 &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE2, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted2 &lt;- as.data.frame(describe_posterior(bmtd2, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\ncd2ted1 &lt;- get_coef_details(bmtd2, \"conditVaried\")\ncd2ted2 &lt;-get_coef_details(bmtd2, \"bandTypeExtrapolation\")\ncd2ted3 &lt;-get_coef_details(bmtd2, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\nTable 2: Experiment 2 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. Larger coefficient estimates indicate larger deviations from the baselines (constant & trained bands) - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\n \nTesting Accuracy. The analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 2, reveal no significant main effect of training condition (\\(\\beta\\) = -20.58, 95% CrI [-72.94, 33.08]; pd = 77.81%). However, the interaction between training condition and band type was significant (\\(\\beta\\) = 82, 95% CrI [41.89, 121.31]; pd = 100%), with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands (see Figure 3).\n\nCodecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\npe2td &lt;- testE2 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\npe2ce &lt;- bmtd2 |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\np2 &lt;- (pe2td + pe2ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\nFigure 3: A) Deviations from target band during testing without feedback stage. B) Estimated marginal means for the interaction between training condition and band type. Error bars represent 95% confidence intervals.\n\n\n\n\n\nCode##| label: tbl-e2-bmm-vx\n##| tbl-cap: \"Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne2_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e2_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e2_vxBMM ) |&gt; kable(escape=F,booktabs=T, caption=\"Fit to all 6 bands\")\n\ncd2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried\")\nsc2 &lt;- get_coef_details(e2_vxBMM, \"bandInt\")\nintCoef2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\nTable 3: Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\nTesting Discrimination. Finally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. The full model results are shown in Table 4. The overall slope on target velocity band predictor was significantly positive, (\\(\\beta\\) = 0.71, 95% CrI [0.58, 0.84]; pd= 100%), indicating that participants exhibited discrimination between bands. The interaction between slope and condition was not significant, (\\(\\beta\\) = -0.06, 95% CrI [-0.24, 0.13]; pd= 72.67%), suggesting that the two conditions did not differ in their ability to discriminate between bands (see Figure 4).\n\nCodetestE2 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\nFigure 4: E2 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\nCodecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n}\n\npe2vce &lt;- e2_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e2_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e2_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e2_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e2_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e2Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\npid_den2 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes2 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n      theme(legend.title=element_blank(), \n        axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe2vce + pid_den2 + pid_slopes2) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\nTable 4\n\n\n\n\nConditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\n\nExperiment 2 Summary\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied group did not show a significant difference in discrimination between bands.",
    "crumbs": [
      "Analyses",
      "Experiment 2"
    ]
  },
  {
    "objectID": "Analysis/e2.html#e2-discussion",
    "href": "Analysis/e2.html#e2-discussion",
    "title": "Experiment 2",
    "section": "E2 Discussion",
    "text": "E2 Discussion\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied group did not show a significant difference in discrimination between bands.",
    "crumbs": [
      "Analyses",
      "Experiment 2"
    ]
  },
  {
    "objectID": "Analysis/e1_Analysis.html",
    "href": "Analysis/e1_Analysis.html",
    "title": "E1 Extras",
    "section": "",
    "text": "plot_violin_condit &lt;- function(fcondit, fvb, pvar) {\n\n  full_range &lt;- testAvg |&gt; pull({{pvar}}) |&gt; range(na.rm=TRUE)\n  testAvg |&gt;\n    filter(condit == fcondit, vb==fvb) |&gt; \n    ggplot(aes(.data[[pvar]])) +\n    geom_density(fill = 'dodgerblue4') +\n    theme_void() +\n    coord_cartesian(xlim = full_range)\n}\n\nunique_combinations = testAvg |&gt; ungroup() |&gt;\n  distinct(condit, vb)\n\ntestAvg |&gt; group_by(condit,vb) |&gt; \n  summarize(mean=mean(dist),sd=sd(dist)) |&gt; \n  mutate(Distribution=vb) |&gt; \n  gt() |&gt;\n  tab_spanner(\n    label = 'Training Condit',\n    columns = -condit\n  ) |&gt; \n  cols_label_with(fn = str_to_title) |&gt; \n  fmt_number(decimals = 2) |&gt; \n  cols_align('left', columns = condit) |&gt; \n  text_transform(\n    locations = cells_body(columns = 'Distribution'),\n    fn = function(column) {\n      map2(unique_combinations$condit, unique_combinations$vb,~plot_violin_condit(.x,.y,'vx')) |&gt;\n        ggplot_image(height = px(50), aspect_ratio = 3)\n    }\n  )\n\n\n\n\n\n\nTraining Condit\n\n\nVb\nMean\nSd\nDistribution\n\n\n\n\nConstant\n\n\n100-300\n252.99\n219.63\n\n\n\n350-550\n191.58\n159.52\n\n\n\n600-800\n150.40\n110.83\n\n\n\n800-1000\n188.91\n160.60\n\n\n\n1000-1200\n240.57\n173.05\n\n\n\n1200-1400\n295.42\n186.14\n\n\n\nVaried\n\n\n100-300\n387.25\n343.45\n\n\n\n350-550\n289.01\n272.48\n\n\n\n600-800\n236.17\n188.89\n\n\n\n800-1000\n224.16\n145.95\n\n\n\n1000-1200\n209.20\n130.32\n\n\n\n1200-1400\n242.13\n136.30\n\n\n\n\n\n\nvars_to_plot &lt;- c('dist', 'vx')\n\ntables &lt;- map(vars_to_plot, function(pvar) {\n  testAvg |&gt;\n    group_by(condit, vb) |&gt;\n    summarize(mean = mean(.data[[pvar]]), sd = sd(.data[[pvar]])) |&gt;\n    mutate(Distribution = vb) |&gt;\n    gt() |&gt;\n    tab_spanner(\n      label = 'Training Condit',\n      columns = -condit\n    ) |&gt;\n    cols_label_with(fn = str_to_title) |&gt;\n    fmt_number(decimals = 2) |&gt;\n    cols_align('left', columns = condit) |&gt;\n    text_transform(\n      locations = cells_body(columns = 'Distribution'),\n      fn = function(column) {\n        map2(unique_combinations$condit, unique_combinations$vb, ~plot_violin_condit(.x, .y, pvar)) |&gt;\n          ggplot_image(height = px(50), aspect_ratio = 3)\n      }\n    )\n})\nprint(tables[[1]])\n\n&lt;div id=\"powslhjhnr\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\"&gt;\n  &lt;style&gt;#powslhjhnr table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#powslhjhnr thead, #powslhjhnr tbody, #powslhjhnr tfoot, #powslhjhnr tr, #powslhjhnr td, #powslhjhnr th {\n  border-style: none;\n}\n\n#powslhjhnr p {\n  margin: 0;\n  padding: 0;\n}\n\n#powslhjhnr .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#powslhjhnr .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#powslhjhnr .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#powslhjhnr .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#powslhjhnr .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#powslhjhnr .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#powslhjhnr .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#powslhjhnr .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#powslhjhnr .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#powslhjhnr .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#powslhjhnr .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#powslhjhnr .gt_from_md &gt; :first-child {\n  margin-top: 0;\n}\n\n#powslhjhnr .gt_from_md &gt; :last-child {\n  margin-bottom: 0;\n}\n\n#powslhjhnr .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#powslhjhnr .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#powslhjhnr .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#powslhjhnr .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#powslhjhnr .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#powslhjhnr .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#powslhjhnr .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#powslhjhnr .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#powslhjhnr .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#powslhjhnr .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#powslhjhnr .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#powslhjhnr .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#powslhjhnr .gt_left {\n  text-align: left;\n}\n\n#powslhjhnr .gt_center {\n  text-align: center;\n}\n\n#powslhjhnr .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#powslhjhnr .gt_font_normal {\n  font-weight: normal;\n}\n\n#powslhjhnr .gt_font_bold {\n  font-weight: bold;\n}\n\n#powslhjhnr .gt_font_italic {\n  font-style: italic;\n}\n\n#powslhjhnr .gt_super {\n  font-size: 65%;\n}\n\n#powslhjhnr .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#powslhjhnr .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#powslhjhnr .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#powslhjhnr .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#powslhjhnr .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#powslhjhnr .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#powslhjhnr .gt_indent_5 {\n  text-indent: 25px;\n}\n&lt;/style&gt;\n  &lt;table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\"&gt;\n  &lt;thead&gt;\n    &lt;tr class=\"gt_col_headings gt_spanner_row\"&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"Training Condit\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;Training Condit&lt;/span&gt;\n      &lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings\"&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Vb\"&gt;Vb&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Mean\"&gt;Mean&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Sd\"&gt;Sd&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Distribution\"&gt;Distribution&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody class=\"gt_table_body\"&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"4\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Constant\"&gt;Constant&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;100-300&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;252.99&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;219.63&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;350-550&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;191.58&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;159.52&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;600-800&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;150.40&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;110.83&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;800-1000&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;188.91&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;160.60&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;1000-1200&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;240.57&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;173.05&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;1200-1400&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;295.42&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;186.14&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"4\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Varied\"&gt;Varied&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;100-300&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;387.25&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;343.45&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;350-550&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;289.01&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;272.48&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;600-800&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;236.17&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;188.89&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;800-1000&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;224.16&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;145.95&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABdwAAAH0CAYAAAAnhe8sAAAABmJLR0QAAAAAAAD5Q7t/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgAElEQVR4nOzdebTn913f99fn+/2t997Z91Wzz2hmpBlpJI2k0b5Zlnd5kfGCXbxhbMBgx6xuCBACAUMhLeFg9q2kBFJK2wBpA6VNUg6QlCWHgrvgxGC2emMztu7v2z8kxau20b338/vd3+Nxjv6Rj2eeZ5b7x+t+9P4lAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADA51Qe/QcAAAAAYGoZMZkmJcnVSe5IcmNKc1WSA+kmS4/8r81Hku596brfSfJrSX4lyW8m6erkAgAAAAB8ksGdabAzyZtTmtemmxxKkmZhy3K7cW/bLGxN6Y+SJN3f/lUmf/PBLH/0jx+e/M2He0mS0rwv3eQHknxvkj+ukw8AAAAAYHCnri1JvjqlvDVdhr0dxzPYf03p7TiRZrTxCf+Pk499NA//+f+VT7z/33af+NPfT5LlpPvBJH8/yftWPx0AAAAA4NMZ3KmhSfK6lOYfJt2mwYHryuj4XWkWt1/WDzb56w/lb/+fX8nf/sH/PslkeZJ035Xk65L8xQo2AwAAAAA8IYM7a+2KlPKD6bo7e9uPduOrXljaDbtX5AeefOyj+djv/4t8/A9+tUspf5xu8qYkP7ciPzgAAAAAwJMwuLOW7kppfqa0vQ3jsy9sBgevy2r8EVz+8Pvz1//HfzNZ/ugHmiT/VZJ3JPnYiv9EAAAAAACfwuDOWnlTku9uN+zO4sUvaJqFLav7s02W87Hf+4V87L2/lJTmt9JNXpDkD1b3JwUAAAAA5pnBndVWkrwryd/r7z6ThWs/L6U3XLOf/OE//b381a//2HK3/PG/SDd5MMkvrdlPDgAAAADMlbZ2AOtaSfKtSb56cMXFLF778pS2v6YBzeL2DPadax7+s98fdB//61cleV+S31zTCAAAAABgLhjcWS0lybckecfw6G1ZuOqFSWnqhPQXMjhwbVn+8PvL5K//vxflkXvu/6pKDAAAAACwbhncWS3vSvK1wyO3Znz2eal9vag0vQz2nS+Tv/lQlj/6gXuSdEn+l6pRAAAAAMC6YnBnNbw2yXcODt6QhatflNpj+39SmvR3n8nkbz+a5Y/84Z2P/lujOwAAAACwIgzurLQ7k/JP+ztPlcULryi1zsg8rlLS33VlJn/7F1n+yPvviPMyAAAAAMAKMbizkg6mNL/Ubtg5WrzpDc1af0DqU/bY6P43H8ryR//oniR/luTXamcBAAAAALPN4M5KGaWUf1HawcGlS29um9GG2j1PrJT0d5/O8l/8SSZ/+acPJPndJP++dhYAAAAAMLum7N4HM+zb0nXXLFx4Vdssbqvd8tSUJgsXXpHetqNJyo8nuaNyEQAAAAAwwwzurIQHk7xldPzO9Hedqt3ytJSml8WLry3thp1NSvOzSU7WbgIAAAAAZpPBnWfqQErzQ+2WKyajk8+q3XJZSm+UxRtf35T+eDGl+fkkW2s3AQAAAACzx+DOM9GklB8uTW9x8cIrmzSz+5EAzXhzlm58XZtSDqaUn0rSq90EAAAAAMyW2V1ImQZfnOTNC+dfUnrbj9Zuecaa0aY0C1vLJz7w24eTbEzyC7WbAAAAAIDZYXDnch1OKT/b332mNz797CSlds+KaDfuSffwx7P8offdmOS9SX67dhMAAAAAMBuclOFylJTyj0vb74+vfjDrZWx/zPj0A+ltO9qllO9PclXtHgAAAABgNhjcuRwPpeueNTr93KYZbazdsvJKk8XrXlWa4YZ+SvOzeeS8DAAAAADAE3JShqdrY0rzz9stB8YLVz9YUtbX6/bHlN4g7dZDzcf/w69tTsqxpPuntZsAAAAAgOlmcOfp+saku3fp4n9W1uXr9k/RjDen9Ibl4T/7vTNJ/jTJr9VuAgAAAACml8Gdp+PKpPzI8NBNzeCKi7Vb1kRv68Esf+SPuslf/fl9Sf7bPDK8AwAAAAB8FjfceepK+c7SH2Z06v7aJWuoZOH8y0oZLDUpzU8lWahdBAAAAABMJ4M7T9Wz03X3jk49uy2D+dqcy2Ahixde2aabnEzybbV7AAAAAIDp5KQMT0Uvpfm5ZnHb5sXzL2tS5u/7NM3C1mTycB7+4B9cn+Q3kvx+7SYAAAAAYLrM33LK5Xh9usmJ8dkXtGnm93s0o5PPSrtp7ySl+eEku2r3AAAAAADTxeDOk1lIab6+t+1o1991snZLXU2bxQuvalKazUn5/iSldhIAAAAAMD3m97kyT9WXJ90LFq57ZWnGm2u3VFcGiymDhfLwn/zuiSTvT/JvazcBAAAAANPBC3eeyOaU5qv7u8+kt+WK2i1TY3jopvR2nOhSynclOVK7BwAAAACYDgZ3nsiXpZtsHF15f+2OKVOycM1DpbSDQVJ+KP4eAQAAAABxUobHtyWl/FR/37nB8NDNtVumTukN04w3N5/4wG9fkeSDSX61dhMAAAAAUJeXuTyeL03XLY5O3Fu7Y2oN9l+T/u4zSSn/MMnx2j0AAAAAQF0Gdz6XzSnl7f2959Ju2FW7ZYqVLJx7cUpv2EtxWgYAAAAA5p2BkM/lLem6pdGJu2t3TL0y3JDxVQ+26bqbk7y1dg8AAAAAUI/Bnc+0mNK8vb/7TNqNe2q3zITB/vPp7z7TPXpa5mjtHgAAAACgDoM7n+n16SZbRse9bn/qSsZXP1hKO+illO+Pv1cAAAAAMJfa2gFMlUFK89O9HccWR8fvKrVjZknpDdOMNjaf+MDvHEryx0l+vXISAAAAALDGvMTlU70i3WT36PidxvbLMDhwIf2dJ7uU8m1JDtTuAQAAAADWlsGdx5SU5ivaTXsnve3HarfMqJLxuReX0vTGSfmeJL5xAQAAAABzxODOY+5PNzk1PHZnYye+fM14S0ann9sk3QNJPq92DwAAAACwdgzuPKKUr2jGm5YHe6+uXTLzhoduSm/rFZOU5r9Msr12DwAAAACwNgzuJMm5dN3tw6O3tyn+SDxjpWR8/mVNks1J/ovaOQAAAADA2rCukiRfXNr+ZHDg+tod60a7tDOjk/eWJK9M8uzaPQAAAADA6jO4sz2lvHpw8Iam9Ee1W9aV0bE7027YNUlp3pNkqXYPAAAAALC6DO68IV03GB6+VLtj/WnaLJx/qEk32ZfkG2vnAAAAAACrq60dQFW9lOYn+ztPLA2P3Fpqx6xHzXhTuk98LMsf+g8Xk/x8kj+s3QQAAAAArA4v3Ofb89JN9gwOXzK2r6LRqWelGW+apDQ/kGRQuwcAAAAAWB0G93lWylub8ebl/s6TtUvWtdIbZnzupW26yekkf6d2DwAAAACwOgzu8+tkuu6u4eFLbYo/Bqutv/NkBvuvTVK+LsmpyjkAAAAAwCqwtM6vN6dpJ4OD19fumBvjs89P6Y9KSvm++LsHAAAAAOuOD02dT+OU5icG+64ZPfLqmrVQ2kGa8abmEx/47YNJPpDkN2o3AQAAAAArxyvb+fSydJMNg0M31u6YO4P916S/82SXUt6dZF/tHgAAAABg5Rjc51Epb26Xdi73tl5Ru2QOlYzPvbiUpjdOyncnKbWLAAAAAICVYXCfP1el6y4ODt/c2nrraMZbMrry2U3SPT/JS2r3AAAAAAArw+A+f96Ypjdxu72u4eFLaTcfmKQ0/zjJ1to9AAAAAMAzZ3CfL6OU5vMH+841pT+u3TLfSpOFax5q8sjY/u21cwAAAACAZ87gPl9ekG6ycXDwhtodJGk37MroxD0lyWuS3Fe7BwAAAAB4Zgzu86SUNzQLW5d72w7XLuFRo+N3pd2wazml+f4kG2r3AAAAAACXz+A+P65I1901OHiDD0udJk2bhfMPtem6fUm+qXYOAAAAAHD5DO7z4/OTlMGB62p38BnaLQcyPHpbSfLWJLfW7gEAAAAALo/BfT6UlOYLejtOdM14U+0WPofRqWelWdi6nNL8UJKF2j0AAAAAwNNncJ8Pl9JNDg0OXOeWzJQqbT8L1zzUppscSfL1tXsAAAAAgKfP4D4fXlPawaS/52ztDp5Ab9uRDA9fSpIvT3JT5RwAAAAA4GkyuK9/45Tyef1955vS9mu38CRGpx9IM94ySWl+JMm4dg8AAAAA8NQZ3Ne/F6TrFn1Y6mwo7eCx0zLH4rQMAAAAAMwUg/u6V17djDcv97Ydqh3CU9TbfvSx0zJvT3Kpcg4AAAAA8BQZ3Ne3HUl3/+DAdW3i81Jnyej0A2kWtk5Smh9Nsli7BwAAAAB4cgb39e3lSZrB/mtrd/A0lXaQhWtf3qabHE7yzbV7AAAAAIAnZ3Bfz0p5Tbt5/6RZ2lG7hMvQ23o4w6O3J8lbk9xdOQcAAAAAeBIG9/XreLruwmD/Bb/HM2x05f1plnYsP3paZkvtHgAAAADg8Rlj169XJKUb7DtXu4NnoDS9LF54VZtkd5Lvrt0DAAAAADy+tnYAq6KkNO/p7Ti2dXjoZp+WOuOa0YaUtlce/rP3nk3y3iS/XbsJAAAAAPhsXrivT9ekmxwb7LvG2L5ODI/ent7Ww11K+d4kh2r3AAAAAACfzeC+Pn1emnbS33NV7Q5WSmmycOEVpbSDUUr58fivUwAAAABg6hjt1p8mpfmh/u7TGwcHrqvdwgoq/VGaha3NJ/7otw4keTjJr9RuAgAAAAA+yQv39efmdJM9g33na3ewCgb7zufRb6R8XZKb69YAAAAAAJ/K4L7+vKw0vUlv15W1O1gl46temGZha5fS/JMkm2v3AAAAAACPMLivL21K8/LenjNNaQe1W1glpTfM4nWvbpPsS8oPJvHhuAAAAAAwBQzu68st6SY7BnvP1e5glbWb92d85nkl6V6Y5K21ewAAAAAAg/t687LS9ie9nadqd7AGhkcupb/nbJLy7Umur90DAAAAAPPO4L5+tCnNQ/09Z5vS9mu3sCZKFs6/LM14U0lpfibJ1tpFAAAAADDPDO7rx63pJtv6e6+u3cEaKv1xFm94bZtS9qaUH4u/0wAAAABQjXFu/XhJafuT3o6TtTtYY+2mfVm4+sEmXffsJO+q3QMAAAAA86qtHcCKaFKaHxzsuWppsO987RYqaDfty+RjH83yR/7w9iS/nuS9tZsAAAAAYN544b4+XEo32eGczHwbX/XCtJv3dynlJ5OcqN0DAAAAAPPG4L4+vKQ0vUlv56naHVRUml4Wr39NU/rjcUrzc0k21m4CAAAAgHlicJ99TUrzst7u001p+7VbqKwZb37kQ1ST40n5iTgbBQAAAABrxuA++65PN9nd33NV7Q6mRG/r4Sxc/WBJuuck+ZbaPQAAAAAwLwzus+/FKU3X33Vl7Q6myOCKixkevS1J3p7kDZVzAAAAAGAuGNxnW0lpXtrfebKU3rB2C1NmfPo56e863SX5niT31u4BAAAAgPXO4D7brk43OdTfe3XtDqZRabJw4RWl3bQ3KeWfJfEHBQAAAABWkcF9tj2YUrr+rtO1O5hSpTfM4sXXNc1wwyil+YUkB2o3AQAAAMB6ZXCfZaV5SW/b0ZTBQu0Splgz2pjFm97Ylra/I6X5xSRbajcBAAAAwHpkcJ9dR9NNTvf3nC21Q5h+7YZdWbz4ujalnEgp/2OSxdpNAAAAALDeGNxn1wuSpL/7TO0OZkRv2+EsXvf5TZKLKeWnk/ikXQAAAABYQQb3mVVe1G7aN2nGm2uHMEP6u09n4ZqHSrruWUn5r5P0ajcBAAAAwHphcJ9NO5LuUn/PWb9/PG2D/RcyvvrBJN2LkvxwkrZyEgAAAACsCwbb2fTcJMU5GS7X8NBNGZ95bpK8Isn3xdcCAAAAAHjGjGyz6YXNePNyu3F37Q5m2PDo7Rld+UCSvDbJe+LrAQAAAAA8I05JzJ5xSnnP4MD1/f7OU7VbmHG9bYeT0uThP/+/r0lyOMnPJekqZwEAAADATPKidfbck64bOifDShmduOexl+6fn+THk/TrFgEAAADAbDK4z57nl95oubf1UO0O1pHR8TszPvv8JHkoKf8sybhyEgAAAADMHIP7bGlSmhf2d59u07gGxMoaHrk1C+dfmqR7TlJ+Icmm2k0AAAAAMEsM7rPl+nST7f3dp2t3sE4NDt6QxetenZRyKaX5X5P4ZF4AAAAAeIoM7rPl+SlN19t5snYH61h/79VZuukNTWnaMynNryY5UbsJAAAAAGaBwX2WlOaFve1HU3qj2iWsc73tx7J0y1ua0h/ve3R0v1S7CQAAAACmncF9dlyRbnK6v+t0qR3CfGg37cuG276kbRa2bkgpv5Tk82o3AQAAAMA0M7jPjuclifvtrKVmYWs23PYlbW/bkV6Sn0jydUl80wcAAAAAPgeD+6wo5Xnt0s7lZmFr7RLmTOmPs3TjG8rgihuT5O8m+SdJFupWAQAAAMD0MbjPhg3pcld/z5m2dghzqmmzcO7BjM8+P0l5SUrzb5IcrJ0FAAAAANPE4D4b7k26Xm+XczLUVDI8cmuWbnp9KW3/TErz75LcUbsKAAAAAKaFwX02PKf0x8u9LR4UU19vx4lsuP1tbbu4fVOS/znJ2+KuOwAAAADEiZLp16Q07xnsuWqpv/fq2i2QJCmDhQwOXNdM/urPy+Qv/uT+JKeS/HySj1dOAwAAAIBqvHCfftemm+zo7TpVuwM+TekNs3jdqzI+87yklJelNL+eR4Z3AAAAAJhLBvfp95ykdP0dJ2t3wOdQMjx6W5Zu/sJS+uNjKeU3kjxUuwoAAAAAanBSZtqV8u7e1iv2DA9fciObqdUsbMlg/7XN8of/Y2/yNx96aZIdeeS++3LlNAAAAABYM164T7dd6boLvV2nje1MvWa0MUs3v6mMjt+ZJG9JKf86yeHKWQAAAACwZgzu0+3+JOm7386sKE1GVz6QxYtfkNIbnkspv5XkRbWzAAAAAGAtGNyn2wPNaONyu3F37Q54Wvq7rsyGO97etpv2LyT5mSTfkWRQOQsAAAAAVpUb7tOrl9J8X3/f+VF/9+naLfC0lf4owwPXlW75E1n+0PtuTCn3J/nFJB+p3QYAAAAAq8EL9+l1Y7rJhv6uk7U74PI1bcZnnpvFG16b0g6uTWl+K8lza2cBAAAAwGowuE+vB1Karrf9eO0OeMb6u888emJm71KSn0vyzUl6lbMAAAAAYEU5KTOtSvMdvW1Hdg6vuFhqp8BKKP3xIydmPvGxLH/4P96SlDuT/PMkf1m7DQAAAABWghfu02lvuslV/V2njO2sL00v46temIULr0xpe5dSmt9MckvtLAAAAABYCQb36fSsJOntPFW7A1bFYN/5LN32tqZZ2LotKb+c5EuS+AYTAAAAADPN4D6dnt2MNi23G3bW7oBV027YmQ23v63t7z3bJvnOJD+aZKFyFgAAAABcNoP79OmlNPf3dl3ZevDLeld6wyxe9+qMTz+QpLwipfzrJAdqdwEAAADA5TC4T5+L6SYb+rtO1u6ANVIyPHZnlm58XSnt4GxK8++S3FS7CgAAAACeLoP79Hl2StP1th+v3QFrqrfzZDbc9qVts7Blc1J+JcmrajcBAAAAwNPR1g7gM5Tm3b2th3YPD93ongxzpwwWMzhwoVn+yPvL5K8/+GAe+Rr1y5WzAAAAAOApMbhPl51J9+7h4ZtKb+vh2i1QRWn7Gew7XyYf/8ssf/j9tyc5meS/T7JcOQ0AAAAAnpCTMtPlviTp7TxVuwPqatosXP1gxmeemyQvTym/kGRT5SoAAAAAeEIG9+nyrDJcWm437q7dAVOgZHj09ixe96ok5baU5l8l2VO7CgAAAAAej8F9ejQpzQP9nafaxPl2eEx/77ks3fTGUprelSnNv0lytHYTAAAAAHwuBvfpcU26yda+czLwWXrbj2bpli9qSn+0/9HR/UztJgAAAAD4TAb36fGsJOntOF67A6ZSu2lfNtzy1rYMFremNP9bkmtrNwEAAADApzK4T43yQLt5/6QMFmqHwNRqlnZkw61vbZvRpg0pzS8nub52EwAAAAA8xuA+HTYmubG/60q/H/AkmoWtWbrlLW0z3ryQUv5lkhtqNwEAAABAYnCfFnckXdvbcaJ2B8yEZrwpS7d8UduMt45Tmv8pyfnaTQAAAABgcJ8O95V2sNzbfKB2B8yMZrQpS5e+sG1GGxdSmn+Z5HTtJgAAAADmm8F9GpTmgd6OE22atnYJzJRmvDlLl97clsHixkdH98O1mwAAAACYXwb3+o6kmxzu7XROBi5Hs7A1Szd/YVt6w+0pzS8l2V27CQAAAID5ZHCv794k6Rvc4bK1G3Zm6aY3tqXpHUhpfjGPfBAxAAAAAKwpg3t99zXjLcvNwrbaHTDT2s37s3jxtU2Ssynlv0syrN0EAAAAwHwxuNfVSyn39XedcrwdVkBv+/EsXvuKkq67Pcl7kpTaTQAAAADMD0NvXReTvHl0/O60G3bWboF1od24O6Xt5+E/e++5JMtJfqV2EwAAAADzwQv3uu5NStfbfrR2B6wrw2N3ZHDwhiT5+iQvrZwDAAAAwJwwuNdUyv3tlgNd6Y9rl8A6U7Jw9YPpbT3cpZQfTXJN7SIAAAAA1j+Dez2b0nUX+ztP+j2A1dC0WbzhNaUZbeqlNP9DEnebAAAAAFhVxt567kjS9HacqN0B61YZLGbx4he0pWl3pZSfTtKv3QQAAADA+mVwr+e+0g4mvc0HanfAutZu3JOFa17epOtuSfJttXsAAAAAWL/a2gFzqzT/qL/z5NbB/mtrl8C6127YlW75E1n+4B9cTPJ/Jvn3tZsAAAAAWH+8cK/jinSTI70dx2t3wNwYX/nsxz5E9QeSnKrdAwAAAMD6Y3Cv494k6bvfDmunNFm47lWl9BeGKc1PJxnXTgIAAABgfTG413FvM9q43Cxtr90Bc6UZbczihVe26Sank7y7dg8AAAAA64sb7muvSWm+t7/3qoX+7rO1W2DuNIvbkskkD3/w/70+ye8k+d3aTQAAAACsD164r73z6Sabe87JQDWjU/el3XJwktL8QJL9tXsAAAAAWB8M7mvv0fvtPjAVqilNFi+8silNbyml/Fh8LQQAAABgBRiZ1lop97Yb90zKYLF2Ccy1ZmFrxude3KTrbk/y5bV7AAAAAJh9Bve1NU6X23o7T/p1hykw2H9N+nvPJSn/IMlVtXsAAAAAmG2G37V1c9L1nZOBaVGycPWDKcPFklJ+IsmgdhEAAAAAs8vgvrbuSWm6duvh2h3Ao8pgIQvXPNSm684m+ZraPQAAAADMLoP7Wirlvt62wyltv3YJ8Cn6O09lcPD6JOVrkpyv3QMAAADAbDK4r52t6bprejuOl9ohwGcbn3leynApKc2PJPFdMQAAAACeNoP72rkzSelvd78dplHpj7Nw/qVtuslVSd5euwcAAACA2WNwXzv3lN5wud28v3YH8Dj6u67MYN/5JOXrk/juGAAAAABPi8F9rZTmvt72422KX3KYZuOzL0jpD5uU8n1JnIACAAAA4Cmz/q6Ng+kmR3o7jtXuAJ5EGS5lfPYFbbrutiSvqd0DAAAAwOwwuK+Nu5Okt8OFCpgFgwMX0tt2pEtpviPJ9to9AAAAAMwGg/vauLsMl5bbpR21O4CnpGR87sUlyaYk31q7BgAAAIDZYHBffSWlua+/82TrHDTMjnZpZ0Yn7i5JXpvklso5AAAAAMwAg/vqO51usqO33TkZmDXDY3emGW9eTmm+J0mvdg8AAAAA083gvvruSRIfmAqzp7T9jK96UZtucibJW2r3AAAAADDdDO6r755mcftyM9pUuwO4DP3dp9PfdWWXUr4xiQ9iAAAAAOBxGdxXVy+l3NnfeaKtHQJcvvHZ55ekLCb5+7VbAAAAAJheBvfVdV26btH9dphtzeL2DI/eVpK8PsmF2j0AAAAATCeD++q6Oyldb/vR2h3AMzQ6cU/KYGmSUr4rSandAwAAAMD0MbivplLubTft7Up/XLsEeIZKb5jxmee06bqbk7ykdg8AAAAA08fgvnrGSW7u7Tjh1xjWicH+C2k37Z2kNN+eZFS7BwAAAIDpYgxePZfSdf3+jmO1O4CVUkrGZ1/YpJvsT/JltXMAAAAAmC4G99VzV0rTtVsP1+4AVlBv2+H091yVlPI1SXbU7gEAAABgehjcV0sp9/a2Hkpp+7VLgBU2Pv1AkrKQ5O/WbgEAAABgehjcV8fmdN2F3o7jpXYIsPKaxe0ZHr5Ukrw5ycnaPQAAAABMB4P76rgtSeltd78d1qvRiXtSesMuKf+gdgsAAAAA08HgvjruLm1/0tt8oHYHsErKYCHD43e1SfeiJDfW7gEAAACgPoP7aijNfb3tR5s0be0SYBUNj9yaMlxaTsq3JnFCCgAAAGDOGdxX3q50k1O97cdrdwCrrLT9jE/d3ybdLUmeXbsHAAAAgLoM7ivvziRxvx3mw+Dg9WkWty2nlG+Jr6kAAAAAc804tPLuLv3xcrtxT+0OYC2UJuMrH2jTdWeTPFQ7BwAAAIB6DO4rrTT39nYcb1Occ4Z50d9zVdpNeycpzTcl6dfuAQAAAKAOg/vKOpRucoVzMjBnSsn4ygeadJNDSV5buQYAAACASgzuK+uuJOkb3GHu9HaeSG/r4S6l+XtJRrV7AAAAAFh7BveVdVcZblhulrbX7gDWXMnoyvtLusmeJG+sXQMAAADA2jO4r5yS0tzb33G8Tdxvh3nU23YkvR0nupTmXUkWavcAAAAAsLYM7ivnZLrJzt6O47U7gIrGp+4v6Sbbk7y5dgsAAAAAa8vgvnLuShIfmArzrd1yIP2dp7qU5quTLNbuAQAAAGDtGNxXzt3NwtblZry5dgdQ2ejUs0q6ydYkb6ndAgAAAMDaMbivjCaluae340RbOwSorxdmuOMAACAASURBVN28P/1dp7uU5quSLNXuAQAAAGBtGNxXxrl0k41952SAR41O3lvSTTYn+aLaLQAAAACsDYP7ynj0fvvR2h3AlHjklfuVXUrzlXHLHQAAAGAuGNxXxo8vXvvKPy1DlyOATxqdvK+km2yJV+4AAAAAc8HgvjL+uL///MdqRwDTpd28P/2dp7qU5iuSLNTuAQAAAGB1GdwBVtHw5D0l3WRbkjfWbgEAAABgdRncAVZRb8sV6e043qU0X5VkVLsHAAAAgNVjcAdYZaOT95Z0k51JXle7BQAAAIDVY3AHWGW9rYfT23bksVfug9o9AAAAAKwOgzvAGhiduKekm+xL8qraLQAAAACsDoM7wBro7TiWdvP+SUrztUna2j0AAAAArDyDO8CaKBmduKdJNzmc5GW1awAAAABYeQZ3gDXS33067YZdj71yL7V7AAAAAFhZBneANVMyPHF3k25yOslzatcAAAAAsLIM7gBraLD3XJqFLcsp5WvilTsAAADAumJwB1hLpcno+F1tuu7GJLfVzgEAAABg5RjcAdbY4MB1KcOl5ZTyVbVbAAAAAFg5BneAtdb0Mjp2R5uue1aS87VzAAAAAFgZBneACgZXXEzpDZeTfGXtFgAAAABWhsEdoILSG2V45JY2ycuSHK3dAwAAAMAzZ3AHqGR45Nakabsk76jdAgAAAMAzZ3AHqKQMFjM8eLFJKa9Lsqt2DwAAAADPjMEdoKLhsduSLr0kX1q7BQAAAIBnxuAOUFGzsC2DfedKSvniJBtr9wAAAABw+QzuAJUNj92ZdN1SkjfUbgEAAADg8hncASprN+1Nb8fxLqV5R5JB7R4AAAAALo/BHWAKjI7dWdJNdid5ee0WAAAAAC6PwR1gCvR2HEu7cc8kpfmqJKV2DwAAAABPn8EdYCqUDI/f2aSbnEry7No1AAAAADx9BneAKTHYey7NeNNySvmK2i0AAAAAPH0Gd4BpUZoMj97RputuS3Jd7RwAAAAAnh6DO8AUGRy8PqU3Wk7yztotAAAAADw9BneAKVJ6wwyPXGqTvCTJkdo9AAAAADx1BneAKTM8fClp2i7J22q3AAAAAPDUGdwBpkwZbsjgwHVNSnlDkm21ewAAAAB4agzuAFNoePS2pOtGSd5cuwUAAACAp8bgDjCF2qWd6e8+nZTmbUlGtXsAAAAAeHIGd4ApNTx2R9JNtiV5Ve0WAAAAAJ6cwR1gSvW2Hkq7+cAkpXlnfL0GAAAAmHoGHICpVTI6dkeTbnI8yXNq1wAAAADwxAzuAFOsv+dsmvGW5aT8ndotAAAAADwxgzvANCtNhsdub5Pu1iTX184BAAAA4PEZ3AGm3ODg9Sm90XKSd9RuAQAAAODxGdwBplxpBxkeudQmeWmSw7V7AAAAAPjcDO4AM2Bw+FJSmiT50totAAAAAHxuBneAGdAMN2Rw4LqSUt6YZEvtHgAAAAA+m8EdYEYMj96WdN04yZtqtwAAAADw2QzuADOi3bAr/V2nupTmy5IMa/cAAAAA8OkM7gAzZHj0jpJusjPJK2q3AAAAAPDpDO4AM6S3/UjaTfsmKc07k5TaPQAAAAB8ksEdYKaUDI/d0aSbnEryrNo1AAAAAHySwR1gxgz2Xp1mtGk5pbyzdgsAAAAAn2RwB5g1pcnw2O1tuu7OJNfUzgEAAADgEQZ3gBk0OHhDSm+4nOQdtVsAAAAAeITBHWAGld4ww8M3t0lenuRg7R4AAAAADO4AM2tw+JakNCXJ22q3AAAAAGBwB5hZzWhjBvuvLSnlTUk21+4BAAAAmHcGd4AZNjx2R9J1C0m+sHYLAAAAwLwzuAPMsHbDrvR3nupSmi9PMqzdAwAAADDPDO4AM2547I6SbrIjyStrtwAAAADMM4M7wIzrbT+SdtO+SUrzFfF1HQAAAKAawwzAzCsZHb+zSTc5keSB2jUAAAAA88rgDrAO9PdclWa8eTkpX1m7BQAAAGBeGdwB1oPSZHjszjbpLiW5sXYOAAAAwDwyuAOsE4OD16f0x8tJeWftFgAAAIB5ZHAHWCdK28/wyC1t0r0wyYnaPQAAAADzxuAOsI4MD19Kml6X5B21WwAAAADmjcEdYB0pg8UMr7jYJOW1SXbX7gEAAACYJwZ3gHVmePT2pKSX5EtrtwAAAADME4M7wDrTLGzJYN/5klK+OMmm2j0AAAAA88LgDrAODY/dmXTdYpI31W4BAAAAmBcGd4B1qN24J/2dp7qU5h1JRrV7AAAAAOaBwR1gnRoev6ukm+xI8praLQAAAADzwOAOsE71th1Ku+WKSUrzlUl6tXsAAAAA1juDO8C6VTI6cXeTbnIoyUtq1wAAAACsdwZ3gHWsv+tU2g27Jinla5KU2j0AAAAA65nBHWBdKxkev7tJ151N8kDtGgAAAID1zOAOsM4N9p1LM96ynFK+Nl65AwAAAKwagzvAeleajI7f1abrbkxyW+0cAAAAgPXK4A4wBwYHr0sZLj32yh0AAACAVWBwB5gHTS+jY3e26bp7klxfOwcAAABgPTK4A8yJwaEbU/rj5cQrdwAAAIDVYHAHmBOlHWR47PY26Z6f5OraPQAAAADrjcEdYI4MD19K6Q2Xk7yrdgsAAADAemNwB5gjpTfK8OhtbZIXJzlduwcAAABgPTG4A8yZ4eFbUtpBl8QtdwAAAIAVZHAHmDNlsJDhkVubJC9PcrJ2DwAAAMB6YXAHmEPDo7eltP0uyVfXbgEAAABYLwzuAHOoDBYyOHxLk+TVSY7X7gEAAABYDwzuAHNqdOz2x165f03tFgAAAID1wOAOMKfKYPFTX7kfq90DAAAAMOsM7gBz7NFX7knyrtotAAAAALPO4A4wxz7jlfvJ2j0AAAAAs8zgDjDnRsfuSGkHXZKvq90CAAAAMMsM7gBzrgwWMjx2e5PkoSRX1e4BAAAAmFUGdwAyPHJrSm80Sco31G4BAAAAmFUGdwBS+uMMj9/ZJt0LklxfuwcAAABgFhncAUiSDI/ckjJYWE4p31S7BQAAAGAWGdwBSJKUdpDRyfvadN09Se6o3QMAAAAwawzuAPwnwysuphltWk4p35yk1O4BAAAAmCUGdwA+qelldOX9bbruYpLn184BAAAAmCUGdwA+zWD/tWmWdiynlG9J0tbuAQAAAJgVBncAPl1pMj79nDZddzLJq2vnAAAAAMwKgzsAn6W/+3TaLVdMUppvSjKu3QMAAAAwCwzuAHwOJeMzz23STfYk+ZLaNQAAAACzwOAOwOfU23oo/T1nk1LelWR77R4AAACAaWdwB+Bxja98IElZSPKf124BAAAAmHYGdwAeV7O0I8NDN5Ukb0lyqnYPAAAAwDQzuAPwhEYn70vpDbukfGvtFgAAAIBpZnAH4AmVwUJGJ+9rk+65Se6p3QMAAAAwrQzuADyp4eFLaRa2Lac0/yhJr3YPAAAAwDQyuAPw5Jo247PPb9NNTiV5Y+0cAAAAgGlkcAfgKenvvjK9HSe6lOabkmyt3QMAAAAwbQzuADxFJeOzzy9JNib5hto1AAAAANPG4A7AU9Zu2JXhkVtKkjcnOVe7BwAAAGCaGNwBeFpGJ+5NGSxOUsp3Jym1ewAAAACmhcEdgKel9EcZn3lem667Ocnn1+4BAAAAmBYGdwCetsGBa9PbdqRLab4jPkAVAAAAIInBHYDLUjK++sGSZHOSb6ldAwAAADANDO4AXJZ2w66Mjt1Rkrw+ya21ewAAAABqM7gDcNmGJ+5Js7B1OaX5viTD2j0AAAAANRncAbhspe1n4dxL2nSTE0m+snYPAAAAQE0GdwCekd6O4xkcuJCkfG2S07V7AAAAAGoxuAPwjI3PPD9lMC4p5QeTtLV7AAAAAGowuAPwjJXBQhaufnGbrrshyZfU7gEAAACoweAOwIro770q/T1nk1K+OcnJ2j0AAAAAa83gDsAKKVm4+sUpvVGbUn4kTssAAAAAc8bgDsCKKcOlLJx7yWOnZd5ZuwcAAABgLRncAVhR/b1XZ7D/2iTlG5JcW7sHAAAAYK0Y3AFYceOrXpRmtDEpzU8mWajdAwAAALAWDO4ArLjSH2XhwivadJPjSd5duwcAAABgLRjcAVgVvW1HMjpxd5J8YZIHK+cAAAAArDqDOwCrZnTi3rRbDk5Syg8luaJ2DwAAAMBqMrgDsHqaNosXXtWU/7+9Ow+S/LzrO/5+nl93z/TM9Bx7z+5Kq/uWrdWBJWGLwwYTA+YfoIxTYAqDwBhEgCSk8kf+SyVVDlUJCVSoAooiEKCckDgugzA2tmThQ4d1H3tJe2mPuXZ27pnu3+/JHz17SLZ1uXd+c7xfVU/Nqvs3s5+faqtm+jNPf5+s1kMInwZqZUeSJEmSJEm6VCzcJUmXVOwZouf2j2SkdBfwH8rOI0mSJEmSdKlYuEuSLrnqjlvouvo+gN8CfrLkOJIkSZIkSZeEhbskaUXUb/wQlU1XFITwZ8DNZeeRJEmSJEnqNAt3SdLKiBk9d/1cDLW+GiF+FhgqO5IkSZIkSVInWbhLklZM7GrQ9z0/nxHCnuVDVCtlZ5IkSZIkSeoUC3dJ0orKhi6n57afiqT0fuB3y84jSZIkSZLUKRbukqQVV9t9B13XfD/AA8CvlptGkiRJkiSpMyzcJUmlqN/4Iao7bgb4r8CPlBxHkiRJkiTpu2bhLkkqRwj03PFRsoFdEMLfAHeWHUmSJEmSJOm7YeEuSSpNyGr03f3xGOtDNUL8e+CasjNJkiRJkiS9UxbukqRSha4Gfffcn4VqfYAQvwTsLjuTJEmSJEnSO2HhLkkqXezdTN+9v5yFrDq8XLpvKzuTJEmSJEnS22XhLklaFbL+YXrv+aUsxOyq5dJ9S9mZJEmSJEmS3g4Ld0nSqlEZ2kPv3b8YCfEGQvgSsLnsTJIkSZIkSW+VhbskaVWpbL6Kvrs/HgnZTYT4MI6XkSRJkiRJa4SFuyRp1alsuYa+u38xhpjdQIiP4EGqkiRJkiRpDbBwlyStSpUtV9N7z/0xZNWrCPHrwA1lZ5IkSZIkSXojFu6SpFWrsukK+t77ySzUenYQ4teAe8vOJEmSJEmS9J1YuEuSVrWsf5jG+x7IYs+mBiF8GfjJsjNJkiRJkiR9OxbukqRVL/YM0bjvgawydEUF+DTwb4FQcixJkiRJkqTXsHCXJK0JoVqn7977Q+2yOwH+PfAXQG+5qSRJkiRJki6wcJckrR2xQs/en6Z+848B4SOE+BhwXdmxJEmSJEmSwMJdkrTmBLqu/j767v3lEKrd1xHCk8BHy04lSZIkSZJk4S5JWpMqW66m8f2/nVU2XVWnPV7mT4FGuakkSZIkSdJGZuEuSVqzYnc/fffeH7pv+CCE8HOE+CzwvrJzSZIkSZKkjcnCXZK0toVI93UfoPHeXw+xPrQbeAj4z0BfyckkSZIkSdIGY+EuSVoXsqHLaPzAb2ddV98XgN8gxH3Ah8vOJUmSJEmSNg4Ld0nSuhGyKvWbf5zG+x4ga2zbAXwGwueAa8rOJkmSJEmS1j8Ld0nSupMNXUbj+34z1m/5CUKl9kEILwKfAgbLziZJkiRJktavrOwA60X39T/8m8BA2TkkSctCoDJ0OV2Xf09M+ULMJ1+9hxA/AakAngSaZUeUJEmSJEnri4V7h1i4S9LqFCo1qttvorrz1pDmJ7uKmdEfIsRfWS7en8biXZIkSZIkdYiFe4dYuEvS6ha7+qjt2huq224gzU/Wi9nxHybEX4XUBTwHzJedUZIkSZIkrW0W7h1i4S5Ja0OsD1DbfXuobr+RtDTTXcyM/AAh/AtgD3AUOF1yREmSJEmStEZZuHeIhbskrS2xe4Dartuo7doLpKyYOnkbqfgEIfwI0AIO4LgZSZIkSZL0NoSyA6wXgx/+1BHg8rJzSJLemdRcYOn4Eyy98tU8nxnJCGGWlP4K+DPgEaAoOaIkSZIkSVrlLNw7xMJdktaLROvMUZaOPkbz1SeL1FqKhHiKVPwl8L+Ar2P5LkmSJEmSvg0L9w6xcJek9SflTVqnX2Tp1adonn6hoMgjIY6Qiv8DfBb4EjBXckxJkiRJkrRKWLh3iIW7JK1vqbVIa2QfzVPP0Tz1Qp5aixkhLJF4CNKDwBeA53D3uyRJkiRJG5aFe4dYuEvSBlLktCYO0xx5idbpl/J8+lT7EPIQJ0jFF4GHgK8AzwN5iUklSZIkSdIKsnDvEAt3Sdq4ioUpWmMH22v0YF7Mn1ku4MMsia9C+irt2e+PAhNlZpUkSZIkSZeOhXuHWLhLks4p5s/SmniFfOIwrYlXinzqZCCl9vfcEI+Rim8ATwDfBJ4CRkqMK0mSJEmSOsTCvUMs3CVJ30nKm+STx8knj5FPHqc1eTQvZsez8xeEOEoqngSeAZ6lPQv+RWC+nMSSJEmSJOmdsHDvEAt3SdLbkVoL5GdPkk+dIJ86SX721aKYOkUqWvHcJYR4lFQ8S7t8fxF4CdiHY2kkSZIkSVqVKmUHkCRpIwqVbiqbr6Sy+cpzD0VSopgbJ58+TT51KhQzp/fkU6cuy2dGP8SFIh5CPENKL0F6CThw0ToIzK70vUiSJEmSpDYLd0mSVosQiL1biL1bqO64+dyj7SJ+YZJ8eoRiZoRiZnQonxm9u5g+fVexOP3a7+UhjpDSPkgHgUOvW+6MlyRJkiTpErJwlyRptQuBWB8i1odg2/XnHwUqKW9SzI6dX/ns+LZidmxrMTN6T7Ew9foyfhp4mVQc4EIJ//LyOga0VuyeJEmSJElahyzcJUlaw0JWJesfJusffs3DQCUVLYq5CYrZ8faam2gUs2PvzmfHbinmJgJFHi/6nIIQj5PSAUivL+MPAWdX7KYkSZIkSVqjLNwlSVqnQqyQ9W0j69v2+qcyUqJYnKaYGz9XysdiduLyYm7ssnxm7PvS0uzrd8dPQjrQLuQ5eNE6AIwDaQVuSZIkSZKkVc3CXZKkjSgEYnc/sbsfNl35mmeASsqXXrc7fnwwnx2/q5gZ3VvMT2akIlz4jDgFaX/7IFf2Ay8trwPAwgrelSRJkiRJpbJwlyRJ3yJkNbLGDrLGjtc/VSEVFHNnyM/Njp8Z7S9mx+7Mp0duK+YnM0jnyvhEiMdIxTPAC8Bzy+tFLOIlSZIkSeuQhbskSXp7QiT2bib2bgauv/iZCkWLfHacYvo0+cxIKGZGLs+nTu3OZ0Y+dNHM+ESIL5OKJ4CngaeAJ4GTK3sjkiRJkiR1loW7JEnqnFgha2wna2ynetGj7V3xE+TTp8inToV86uTV+eTxK4q5iZ8+f1WIo6Ti68DjwKPAY7Tnw0uSJEmStCaEN79Eb8Xghz91BLi87BySJK0lqbVIPnWS/OwJ8slj5JPH8nx6JJ4fSxPiIVLxCPAI8E+0Z8N7QKskSZIkaVWycO8QC3dJkjoj5Uvkk6+STx6lNXGE1vjLeVqazQAIcYJUfBn4EvCPtOfBW8BLkiRJklYFC/cOsXCXJOlSSRRzE7TGX2mvsQN5MXfmXAE/QioeBD6/vEbLTCpJkiRJ2tgs3DvEwl2SpJVTzE/SGjtEa3Q/zZF953bAJ0L4Jil9Fvgs7YNY3f0uSZIkSVoxFu4dYuEuSVJJUiKfOklzZB+t0y+m1sRhIAVCPEEq/jfwN7RnwLdKzSlJkiRJWvcs3DvEwl2SpNUhLc3RHHmR5snnaZ1+sUhFKy7Pfv808NfAw0BeckxJkiRJ0jpk4d4hFu6SJK0+KW+2x86ceIbmyeeKlC9FQjxNKv4c+HPgaRw7I0mSJEnqEAv3DrFwlyRpdUt5k9bISywdf5Lm6RcKijwS4guk4o9pl+8jZWeUJEmSJK1tFu4dYuEuSdLakZrzNE88w9Kxx4rWxJEIoQXp/wF/CHwBKEqOKEmSJElagyzcO8TCXZKktamYGWXx6GMsHX00T0uzGSEeIRV/APwJMFZ2PkmSJEnS2mHh3iEW7pIkrXFFTvPU8ywe/lpqjR0MhNAkpb8Afg94sux4kiRJkqTVz8K9QyzcJUlaP4qZURYPf5WlI4+2D1olPALpPwGfxXEzkiRJkqTvwMK9QyzcJUlaf1JrkaWjj7F46OG8mD+TEeLLpOJTwJ8CCyXHkyRJkiStMlnZAdaL7ut/+DeBgbJzSJKkzgmxQmXocrqu+t6YDeykmJsYSAtnf5wQPwEpAs8Ci2XnlCRJkiStDhbuHWLhLknSOhYCWWM7XXveEypbryMtTteL2bEPEMKvAb3A08B8ySklSZIkSSWzcO8QC3dJkjaGWB+ktntvqA7fCs35Wj596j5C+HWgn3bxPldyREmSJElSSSzcO8TCXZKkjSV2NajufBe1XXtJrcVqPn3yXkJ4gHbx/hQW75IkSZK04Vi4d4iFuyRJG1Oo9VIdvoXa7r0hNRcq+dTJe5dHzdSBJ/FwVUmSJEnaMCzcO8TCXZKkjS3UetrF+67bQmrOVfOpU/cR4ichQbt4b5YcUZIkSZJ0iVm4d4iFuyRJgnM73m+luvNdpIWprmJm5P2EeD+kOdoz3vOyM0qSJEmSLo1YdgBJkqT1KGvsoPeuj9G47wEqW67eDPw3QtwPfAR/BpMkSZKkdckXe5IkSZdQNngZfffcH/ruuZ+sf/gy4C8J4Qng/WVnkyRJkiR1loW7JEnSCqhsvZbGfb8Re+7458TuwVuBLxDCg8CtZWeTJEmSJHWGhbskSdJKCYHartvof//vZPVbPkyodH+A9lz3PwZ2lpxOkiRJkvRd8tDUDvHQVEmS9JaFSGVoD11X3BOBkE8eezfwa0ANeBxYKjWfJEmSJOkdcYe7JElSSUK1m/pNH6L//b8Ta7tu6wb+HSG+DHwcN0ZIkiRJ0ppj4S5JklSyWB+i5/aP0rjvASpDl28G/ogQngY+UHY2SZIkSdJbZ+EuSZK0SmSDl9H33l+NvXd9jFgfugH4BwifA24sO5skSZIk6c1ZuEuSJK0qgerwLfT/4L9ePli164PAc8DvA1tLDidJkiRJegPOBu0QD02VJEkdde5g1T13R4o85JPH7yTwSSAHngBaJSeUJEmSJL2OO9wlSZJWsVDroX7Lh+n/wX8Vqjtu7gX+IyEeAH4GCCXHkyRJkiRdxMJdkiRpDYi9W+i962P0fe8nyPqHdwL/kxAeBd5XdjZJkiRJUpuFuyRJ0hpS2XwVjft+I/bc/lFid/9e4GEInwFuKDubJEmSJG10Fu6SJElrTQjUdu+l8f5/k9Vv+lFCpfajwPPAfweGS04nSZIkSRuWh6Z2iIemSpKklRZCpLLpivbBqqkI+eSx2wn8GtANfBNYLDmiJEmSJG0oFu4dYuEuSZLKErIq1W3XU7vszpCW5ir51Mn7CPFXIDWBp4BW2RklSZIkaSNwpIwkSdI6EXuG6Ln9IzS+/7eobrt+APhdQnwZuB+olhxPkiRJktY9C3dJkqR1Jusfpvc9vxD63vtJKkN7tgF/SIgHgI8BlZLjSZIkSdK6ZeEuSZK0TlU2XUHfez8R++75JbKBnZcBf0qI+7F4lyRJkqRLwsJdkiRpXQtUtl5H474HYu97Pk7WP7yHdvF+APg4UCs5oCRJkiStGx6a2iEemipJkla3QNa3ha4r3hOyocspZkb708LUTxDiL0BqAc8BzbJTSpIkSdJaFsoOsF4MfvhTR4DLy84hSZL01iRaYwdZ2P/F1Bo7FAjxLKn4PeD3gdNlp1NH1YA60E378NwKF97pWgAtYAlYBObwFy+SJEnSO2bh3iEW7pIkaa3KJ4+xcPDLNE88mwi0SOl/AP8FeKbsbPq2uoHLltcuYCewA9gGbCGErRA2AQOk1Afpbc7rDy1CmAGmII2S0igwQvsXMaeAV4FjwOHl/y46cVOSJEnSemDh3iEW7pIkaa0r5sZZfPkRlo58o0h5M0J4BNLvAZ+hvQNaK6cBXL+8rgWuIYRrIFxDKja//uJQ6cpDV1+KXY0s1HpDqNYJ1W5CpZtQ6YKsSogZxAqEwIUN7glSAUWLlLegaJJaS6TWIqk5R1qaIy3NFsXCdJGWZmL738Vr/uYmIRwlFS8B+4F9wAvA88DEJfu/I0mSJK1SFu4dYuEuSZLWi9RcYOnYYyy+/EhezE1khDhGKv4I+CPgUNn51ple4GbgVuAWCLcQwq2kYvv5K0JIsT6Yx96tlax3M6E+RKwPLq9+QvcAIb7NTezvUGotUiycJc1PUsydoZibIJ8bp5gZK4qZUVK+dKGQD3GUlJ6C9CTwJPBN4CDuiJckSdI6ZuHeIRbukiRp3UmJ1tgBFg9/neap5xOpCBC+AulPgL8BpsqOuIYE2qNfbgP2ArcR4h2kYs/yc4SsVsT+HWSN7TFrbCf2bSPr20qsD0HMSoz+ViWKhWmK6VPk06fJp06RT50o8qlTULTaRXyIM5AeJaVvAN8AvkZ7XI0kSZK0Lli4d4iFuyRJWs/S4jRLx55g8eijeTEzmhHCEil9Bvhr4G+B+ZIjriYBuAK4vb3CnYRwx4VRMCHF3s1FNrgry/p3kvUPk/UPE+sDrMsfz1NBPjNKPnmcfPIYrTNHi/zsiUDK2zcb4mFS8RDwFeBh2rvgU4mJJUmSpHdsHf5EXw4Ld0mStDEk8slXWXr1SZaOfzNPizMZIcyT0ueA/wv8HRtrdncEruFCuX4HIdxJKvoBCFnK+nekbHB3zAZ2kQ3sJOvfSciqZWYuX9EiP3uC1pkjtCYO0xo7lKel2fY2/hBHScU/Al8G/hE4gAW8JEmS1ggL9w6xcJckSRtOSrQmDtM88TRLJ57J0+J0BiQIX4f0d8DngSeAVrlBO6YbuInzY2HCHQTeTUo9AMRKkQ3spDK4O2YDu8kGd5H1bV8j42DKlihmx2mNv0Jr/GVaYwfyYv7suQL+FKn4PPDF5fVqmUklSZKkN2Lh3iEW7pIkaUNLiXzqBM1TL9AceanIzxwLkAIhzJF4GNLDtOd1Pw7MlJz2zWTAlbQP3+S2fQAACZZJREFUM70FuIUQ95KKa2nvaCdUuvJsYFfMBnaFbGAX2eBusr6tEOIbfFm9HcXcBK2xQ7TGDtAc2X/xDvh9pOJB2r/QeQiYLTOnJEmSdDEL9w6xcJckSbogNefPl6WtsZfzfPrUuW3eiRAPkIrHgaeB54B9wGEgX8GIGe1DTK+kPRLmWuBaQryJlK6BdH7mS+zZ1MoGdlWy/mGygWGy/p3EniH8UXolJfLpEVqj+2mNHqA1eqBIRStCaBL4J1J6EHgQeAbHz0iSJKlEvkroEAt3SZKk7yw155cPzDxGfvZV8sljrWJ+snLhitAihGOk4hBwBDgBnAbGgHHgLDBNezfzAu1yvlj+5AhUgS6gB2gA/cDQ8toKbAOGIewmhD2ktAPShVkvMStiz6aUNbZnWd82YmM75z5u+Hnrq1HRojVxhNboPpoj+4r87In2Wwva89//lnb5/g+0/+1IkiRJK8bCvUMs3CVJkt6e1Fognz5NMTNGMTtGMTdOMTeRirkzebE4k5GKjv2sGirdeezuJ9QHslgfpL2GiD2biL2bid39joNZw9LiDM3R/bRG9tMceTFPS3Pt8wRCeJyUzhXwj7Gy76KQJEnSBmTh3iEW7pIkSR2UEqk5T1qapWjOQXOB1Fok5U1SvgQpcX5ySAiEkEGstHejV7oIlS5CtYdQqxOrPR5cupGkRH72VZoj+2iN7EutM4chpUCIU8uz3x8E/p72uyikjSAAA7Tf7bMF2Axsov0OoAHa7wrqA3ppv0uom/Y7hirtFQK16idYWnq2hOySJK05Fu4dYuEuSZIkrT7t8wQOtgv40y/mxcLUucNXX1geP/N54Cu0RxVJa02gPTLrCmAP7deklwG7CWE3hN2ktPXicym+5QtktZxKLYWsRsiqgawaQqwEYhba7/wJVLdc+zPzL37ur1bihiRJWusqb36JJEmSJK1NoVqnOnwr1eFbgZTl0yO0RvbRHNl3Y2v80A0U+b8khCUSD0H6PO3Z789y4YwAqWwB2AVcz7kDnuFqQrwO0pWk1P2aiyvdeawPEOuDWejuJ3b3E7oahFovsauXUO0l1HoI1W5C1kX7LUJvLKZ4eP7Fz12Sm5Mkab2xcJckSZK0QQSyxnayxna6rr4vpLwZ8olXaI7sr7VG9v1gPn3qh9qXxXFS8Q/AF5fXK2Wm1oZRBa4BblpeNxDizZCuI6X6uYtCVi1i7+YUe7dm58+h6Nm0fC7FICGrOUNLkqQSWbhLkiRJ2pBCVqWy9ToqW6+Dm38sS4vTNEcP0hrdv7k1uv+nioWpj7QvjMdJxReALwEPAUfKzK01LwOuAm5ZXjcT4rtJ6RpIy6/RQ4o9g3nW2FGJfdvI+rYS+7YR+7YQu/qi02ElSVq9LNwlSZIkCQhdDWq791LbvRdIWTE73p7/Pnpwd2vswM+mpbmfb18Yj5OKL9Ge/f4VYB/nT/GVzgvAMBeK9VsJ4TbgRlLqWr4kxd5NRdY/nGWNHcTGNrLGDmLf1hBixdfrkiStQX4DlyRJkqRvEYi9W6j1bqG2527Oz38ff5nW+KHdrbFDH02LMz/bvjROkopHgH8CvgY8DsyWl10lGABu5lyxTngXIbyLVAyeuyB297eygZ2V2NhB1r+jXaw3tocQK46AkSRpHbFwlyRJkqQ3ddH89yvuAVJWzE3QGn+F1sThwdb4y/+smBn9seWLC0J8gVR8DXiUdgH/PNAsK706pg+4kXa53i7YQ3w3qRg+d0GodOVZ/3DM+odD7B8+X66Hat3X35IkbQB+w5ckSZKkty0QezZT69lM7bI7AbLUnCc/c5TWmaMxnzx6S2viyI2pOf9Ly9c3CTxHSo8DTwFPA88A02Xdgb6jAGwBblheNwI3EeKtpGLn+YtipYiN7WT9wzFrbCf2D7d3rdf7M2esS5K0cVm4S5IkSVIHhGqdyrbrqWy7fvmRlBVzk+Rnj5NPHq/mZ1/d25o89q60NHdhhEh7HvxTtHfAvwC8SHsm/NSK38DGMwRcDVwLXANcSwg3QLiBVDTOXRSyWrE8Wz1mjW3Exg6yxjZifVMkWKxLkqTXsnCXJEmSpEsiEHuGiD1DVIdvPfdgVixMUUydJJ86ST51anc+dWJnPjPyIYo8XvjUOEpKL0HaDxwEDgGvLK8JPKT1zQSgH7gc2LO8rgCuJMRrgCtJRf9Fl6dYH8izxvZK7N1C7NtG1thK7NtG7O6P7liXJElvlYW7JEmSJK2g2N1P7O6/aCc8kVRQzJ0hnxmhmBkhnxndWsyMbslnRu9Ji9Ovfd0WwhyEY6TiFeAYcBx4FTgBnAROA6NAa8VuauWcK9K3X7SGgR3ALmAXIe6BtIuUel7zibFSxJ5NKfZtyWLPJmLPJmLvFmLvZrKeTYFY8fWxJEn6rvkDhSRJkiSVLURi72Zi72bYfuP5R4FKypsUcxMUs+MU82co5s70FPNnri/mzlxbzJ8p0uJsBun1W7ATIZ4FRknpNKRR4MzymgDOLq8p2nPkZ5bXHDALzAMLQN6pOwSqQB3oBnqA3uXVBzSWVz8wAAzSHvmyCdhCiFuBLaQ0CKn6LV89ZkXsahSxPpjF+mAI3QPE+gCxPkSsDxJ7hgi1HneqS5KkS87CXZIkSZJWsZBVyRrbyRrbX/9U5Nzu+IVp0uIUxcIUaXGGYnE6pMWZwWJpZjAtzV2bFmfytDSbUnMhpHwp+zZ/zXdSEMIShBbtHfNN2iV8QXuszbl1IU/7dWbWXqlKovptS/I3vOdaHqrdKVTrIXT1ZaHWS6z1Emo9hFofoauP2NVof+zuJ1S7I4T45l9ZkiTp0rJwlyRJkqS1LERifQDqA7xBk37hqVSQmguk1jyptUhqLUJriZQvkZY/kjdJRQuKVqRodacih1RAkZNSav/5W8bIBwiBECKEACGDmBFiBrFCiBXIqhc+ZjVCpQZZjVDtJlS6CJX2x/YnSZIkrT0W7h0TfjalorvsFJIkSZL0xgJU64RqfVUOWEkAyTNhV5Miiy+VnUGSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSpEvr/wPouOhq/XDvSQAAAABJRU5ErkJggg==\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;1000-1200&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;209.20&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;130.32&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;1200-1400&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;242.13&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;136.30&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABdwAAAH0CAYAAAAnhe8sAAAABmJLR0QAAAAAAAD5Q7t/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgAElEQVR4nOzd+bfdZ2Hf+8/z/e7hnH2OZsmaZ1m2JA+SbEmWbcAYc42NIcUMtpnBTGEobZq26V3N7eoiTTqtNDdpkzRpA6TQpqG5SW9p0jTJapP2tgtSwhASCEMYbAjG2JZtPEp7f+8PghQIgwfpPGd4vf4A663lfaS1Pnr28yQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADzSKkdAAAAAAALiUENlq5+koNJLktyQVL2pJStSdal62aSrp+UkynlgSR3putuT7pPJPlYkvcl+WCSR6rVAwAAAMA8Y3CHpWVdkhuT8n0peVq6bpQkZTAzbmfXNs30qlIGMyn96aTpJZOT6U4+nO7RBzJ56N5u/NUvj7tHH+glSUp5NMn/TNf9epL3JPlMtd8VAAAAAMwDBndY/Jokz0rK65PuhiRNM7N23D/nvLa3dnfaVdvTTC1/zP+x7tEHcuruz+XU3Z/JqTs/ORnf+4UmSVLK+9N1/zLJv0ly/9n4jQAAAADAfGZwh8VrmOTWlOYH0012luHseLjtWNvfcijtsvVn7BeZPHQiJ7/4kTx62+9Pxvd9qUkpD6brfiHJj8epdwAAAACWEIM7LD5NkltSmh9LN9naW719Mtz9tKa/4UBSmrP6C49P3J5HPvs/8ujtfzDJZNIl3duTvC3J58/qLwwAAAAA84DBHRaXIynln6brjrYrt0ym99/Q9NbunvOIySP355FP/dc88pn/b5JuMk7X/VROD+8n5jwGAAAAAOaIwR0Wh9mcHrTfWobLJtMHntMOthxM7R/xycP35uE/+e08+rn3dSnl7nSTH0zyziRd1TAAAAAAOAsM7rDwHU1p/m26yY7hrqdk6vxrU3rD2k3fZHzfn+Whj/xqd+ruz5SU8l/Sda9J8qe1uwAAAADgTGprBwBPWJPkryflXzfTK1bMXHZrM9x+LKXp1e76C5rhsgy2XVqa6ZU59ZVPbU83eUOSu5N8oHYbAAAAAJwpTrjDwrQqKf8q6Z492Hwo0xffmNKbqt30mEwevi8Pffg93ck7Pl5Sym+m616W5M7aXQAAAADwZBncYeE5N6X5jSS7Rhc+rwx2HMvC+1Hu8ujn3p8H//DXJukmd6abvCDJf69dBQAAAABPhitlYGG5MqX53dKfOmf28tc1/Y0XZOGN7UlS0q7ckv7GC8qpL39iujv50CuTnEjy/sphAAAAAPCEGdxh4XheSnlvM7N2uOzKN7Xtsg21e560ZjibwbYjzeSrdzaTr375uiS7k/xGklOV0wAAAADgcTO4w8Lw6iTv7q3eUWYvf0PbDJfV7jljStPLYPNFSdPPqa988qKUcm2S/zfJA7XbAAAAAODxWIh3UcBS81eS/JP++v3d6NKXltL2a/ecNSe/9Md58APvmnST8e3pJs9M8onaTQAAAADwWBncYX57a5KfGGw+mNGhm5Nm8X8pZXzvF/LV//nz4+7kQ/enmzwryftqNwEAAADAY7H41ztYuF6X5J8NNh/M6PAtS2JsT5JmankGmy5uTt7xR/3u1MMvT/KBJJ+q3QUAAAAA38vSWPBg4bklydv7Gw5k5pKXlKUytn9d6U9nsOVQc+rOTzbdI199cZI/SvKx2l0AAAAA8N0srRUPFobrk/Ke3to9ZfboK0uaXu2eKko7yGDLoTK+6zNl8tCJF+b0Kfc/rN0FAAAAAN+JwR3mlyMp5T+1Kzf3Zi97TVPaQe2eqkrTS3/zwTI+8flMHrz7xiSfTvKR2l0AAAAA8O0Y3GH+2JXS/G4zvWpm2RXf35b+dO2eeaE0bfqbLy7je27L5MG7nhcn3QEAAACYpwzuMD+sSGl+t/SGm5Zd+ca2mV5Zu2deKaVJf9NF33jS/WM5fa87AAAAAMwbBneor00pv5KUY7PHX9u0KzbX7pmXStOeHt3v/mwmD93z/CQfTvIntbsAAAAA4Oua2gFA/l667rrRxc8vvdU7a7fMa6XtZ+bYq0pv9faklPckuap2EwAAAAB8ncEd6ropyd8c7rwig21Ha7csCKU3zMyxW5t2dn2TUt6b5HDtJgAAAABIDO5Q04Up5R29Nbu66QPPqd2yoJT+dGaOv7ZtplZMpTS/mWRX7SYAAAAAMLhDHctSml8tg9n+zKUvK2k8p/B4NVPLM3v569vSG65KaX47ybraTQAAAAAsbQZ3mHslyT9Pul0zR17eluFs7Z4Fq5lZm9njr21LabanlP+YZFS7CQAAAICly+AOc+9VSW6Z3nd96a3eUbtlwWtXbs3oyMubdLk0Ke9O4usCAAAAAFRhmIK5dV5K+Q+9dXvb0UU3lpRSu2dRaGfXpQxny6k7PnZ+Tp9y/63aTQAAAAAsPQZ3mDvDlOa3Sn96w+zlr2tKb1i7Z1Hprdya7tQjGd/zucuT3J7kg7WbAAAAAFhaXCkDc+dt6SYXjg7f0jbDZbVbFqXp/c9Of/35XVL+eZKn1e4BAAAAYGkxuMPcuCrJDw53XpH+OefXblm8SpPRJS8t7ey6pDS/lmRn7SQAAAAAlg6DO5x9K1Kadzez6yZT+59du2XRK71hZi57dVt6w2Upza8n8XUCAAAAAOaEwR3Ovn+UdBtnDr+4LW2/dsuS0IzWZObIK9qkOy8p70jidVoAAAAAzjqPpsLZdU2Sn5ja+4wy2HK4dsuS0oxWpwxG5dSXP74vySNJ/nvtJgAAAAAWN4M7nD2zKc1vtbPrZmYueUmT4gslc623amsmD96T8X1fvDrJ/0zy6dpNAAAAACxeFkA4e340Xbd5dOjmNk2vdssSVTJ90Y1pl2/sUppfTrKjdhEAAAAAi5fBHc6OK5K8ebj7KaVdtbV2y5JW2n5mjr6yKe1gNqX8WpKp2k0AAAAALE6ulIEzb5DS/GYzvWLVzJGXN6XxY1Zb6U+nXbGpOXn7H2xIck6S99ZuAgAAAGDxsQTCmfdDSffCmSMva9rZc2q38DXtzNokyam7/vSSJJ9N8uGaPQAAAAAsPgZ3OLN2p5RfHmw+1A73XFW7hW/RW7Mzp+75fDd56O7rk/xqkjtrNwEAAACweLjDHc6cklL+WWkH7dQFz6ndwrdTmswcvqWUwUwvpfmVJDO1kwAAAABYPAzucOb8pXTdtVP7r2+b4bLaLXwHZTibmUtf1qbr9ib5qdo9AAAAACwerpSBM2OU0vxGu3zD7OjiF5SUUruH76IZrUpKKae+8ulDST6d5CO1mwAAAABY+JxwhzPjb6WbbJ6++PlNih+rhWDq3KvTW7u7Syk/l2Rv7R4AAAAAFj7LIDx5u1LK3xxsvTS9Vdtrt/BYlSajwy8upT89SGnek2RYOwkAAACAhc2VMvCklXeWXv+82WOvKqVns11ISm+YdvnG5uTtf7A+px9Q/c3aTQAAAAAsXAZ3eHKuTfK26X3Xl946t5IsRO3M2nSnHsn4ns8dT/L+JJ+q3QQAAADAwuRKGXjieinNTzQza8bDnVfUbuFJmN53XdrlGycpzS8mOad2DwAAAAALk8Ednrhb003Onz7w3DaNL4ssaE0vo0tf2qQ0q1PKO5KU2kkAAAAALDxWQnhilqc0/6G3dtfU9L5nFfvswtcMZtIMZsvJO/743CR3Jvn92k0AAAAALCxOuMMT80PpJqunDzzX2L6IDHYcS3/D/i6l/HiS/bV7AAAAAFhYDO7w+G1NKX9tsPXStCs21W7hjCoZHXxRKf1Rm9L8UpJh7SIAAAAAFg5XysDj95Np2kMzR19VSn+qdgtnWGkHaZdvbE7e/oH1SXpJfqd2EwAAAAALg8EdHp+LkvzM1J6rm/7GC2q3cJa0M2vTPfJAxiduuyKnB/fP124CAAAAYP5zpQw8HqX8/dKfngz3XFW7hLNs6sANaWbWTlKadydZVrsHAAAAgPnP4A6P3VXpuuumzntm6yqZxa+0/cwcfnGbdFuT/HjtHgAAAADmP1fKwGNTUsovN9MrNswcvqVJ8W9VS0EzvSJJV07d9aeHk3wgyScqJwEAAAAwj1kN4bF5brruyNT517VperVbmENT5z4j7YpNk5Tm7UnW1u4BAAAAYP4yuMP31qQ0P9rMrhsPthyq3cJca9qMDr+4SSmrk/IzSUrtJAAAAADmJ1fKwPd2c9K9YXTR85t2+YbaLVTQDGdT2n45decn9if5kyQfrd0EAAAAwPzjhDt8d72U5m3t8o2T/sYLa7dQ0XDXU9JbvWOSUn42ycbaPQAAAADMPwZ3+O5emm6ye2rfdU2Km0SWtNJkdOjmppR2Nik/F1fLAAAAAPAtXCkD39kgpfnVduWWZdMHnl3sq5TBKKU/Kqe+/LG9ST6T5MO1mwAAAACYP5xwh+/sVekmW6f3XdcY2/m64Y7j6a3Z3aWUf5pkc+0eAAAAAOYPgzt8e8OU5v/qrd4x6a3bU7uF+aSUjA7dVErTG6WUfxH/GgMAAADA1xjc4dt7ZbrJpqnzr3W6nb+gGa3K1IHnNOm6ZyV5Ze0eAAAAAOYHgzv8RYOU5od7q3d2vbW7a7cwTw13XJbe2t1dSvnJJFtq9wAAAABQn8Ed/qJXpptsnjr/mR5K5bsoGR3886tlfi4+LAAAAABLXls7AOaZfkrzK73V25ZNnf8sgzvfVelPpwxG5dQdHzs3yWeSfLh2EwAAAAD1OOEO3+wl6SZbp/Y+093tPCbD7Zelt2ZXl9L8VJJNtXsAAAAAqMfgDv9bm9L8cLti86R3zt7aLSwUpWR06EWllGYmKT8b/1IDAAAAsGQZ3OF/e1G6ya6p85xu5/FpRmsytf/ZTdI9J8kttXsAAAAAqMPgDqc1Kc0Pt8s2TPob9tduYQEa7rwivdU7JinNTydZX7sHAAAAgLlncIfTnpNusm9q7zVOt/PElJLRwRc1KWVZUv5Z7RwAAAAA5l5bOwDmgZJS3tXMrFk/uujGJsXgzhNTBjMpbb+cuvMT+5N8NMnHajcBAAAAMHeccIfkGem6S6bOfUab4keCJ2e46ylpV26ZpDQ/m2R17R4AAAAA5o51EUr5P5up5ePBlsO1S1gMSpPRoZubJGuS/ETtHAAAAADmjsGdpe7SdN3Th3uuatO4YYkzo122PlN7rylJXpbk+to9AAAAAMwNgztL3Q+V3tR4sO1o7Q4Wmalzr067bMMkpfkXSZbX7gEAAADg7DO4s5TtSXLjcNeVbekNa7ew2DRtRoduapJuQ5J/UDsHAAAAgLPP4M5S9tfStN1w15W1O1ik2pVbMrXnqpLkDUmeVrsHAAAAgLPL4M5SdU5KuXW47VhTBjO1W1jEhnufmWZmzTileXuSUe0eAAAAAM4egztL1ZvTpTfc/dTaHSxype1ndOimNt1kZ5K/W7sHAAAAgLOnrR0AFcykNL/c33Th1HDHZbVbWAKa6VXpHn0g4xO3HU/y60m+WLsJAAAAgDPPCXeWolelm6yY2n1V7Q6WkKl916eZWjFJad6RZFC7BwAAAIAzzwl3lpo2pfnl3uody6fOu6bUjmHpKE0vzbL1zcnbP7Auyakkv1u7CQAAAIAzywl3lprnpZtsG+65ytjOnOufc14GWy9JUn44yYHaPQAAAACcWQZ3lpKSUv5GM7N23F+/r3YLS9T0geemDKZLSnl7fMsIAAAAYFExuLOUHE/XHZna87Q2xQF36iiDUUYXPb9N1x1J8pbaPQAAAACcOQZ3lpIfKP3pcX/LJbU7WOL6my5Mf+MFSSk/lmRX7R4AAAAAzgyDO0vFziQ3Dnde0Za2X7uFJa9k+sLnpbSDfkr5l0l85QIAAABgETC4s1S8JaXpBjsvr90BSZJmanmmL/i+Nl13VZJba/cAAAAA8OQZ3FkKlqeU1w22HGqa4bLaLfDnBtsuTW/duV1K+Ykkm2v3AAAAAPDkGNxZCl6VrpsZ7npq7Q74FiWji19YStObTsrPxtUyAAAAAAtaWzsAzrI2pfk3vTU7l0+de7Uxk3mn9KdTesNy6ssf35vk40k+WrsJAAAAgCfGCXcWu+ekm2wf7n6qzzrz1nDnFWlXbZ+kND+dZF3tHgAAAACeGCMki1spf7WZXjXur99XuwS+s1IyOvSiJqWsSPKTtXMAAAAAeGIM7ixmF6Xrnjrc/ZQ2xUed+a2dPSdT513bJLk5yXNq9wAAAADw+FkhWcz+cmn7k8HWI7U74DGZ2vO0tCs2TVKan0+ysnYPAAAAAI+PwZ3Fam1Kedlg25Gm9Kdqt8BjU5qMDt3UJDknyT+unQMAAADA42NwZ7F6TbpuMNx5Ze0OeFza5Zsyde7VJcmtSZ5ZuwcAAACAx87gzmLUS2ne0lu3t2tm19Vugcdtau8z0s6eM05pfiHJsto9AAAAADw2BncWo+ekm2wa7rqy1A6BJ6TpZXTo5jZdtznJ36+dAwAAAMBjY3Bn8Snlrc1o1bh/znm1S+AJa1dtzXDP00qSNyZ5Wu0eAAAAAL43gzuLzYF03dOGO69sU3y8Wdimzvs/0sysGac070gyqt0DAAAAwHdnkWSxeXNpepPBtiO1O+BJK23/a1fLTHYk+dHaPQAAAAB8d23tADiDVqaUdw22He0PNl1UuwXOiGZ6ZbqTD2d8z+ePJfntJLfVbgIAAADg23PCncXkFem6qeHOy2t3wBk1te+6NKPVk5TmF+NqGQAAAIB5ywl3FosmpXlXb/WOVVPnXl1qx8CZVJo27YotzaOff/+qJNNJ/nPtJgAAAAD+IifcWSyuSTfZPdx1hbGdRam3ZmeGu65Mkr+axNc4AAAAAOYhgzuLRHlTGcyM+xsuqB0CZ83pq2VWff1qmenaPQAAAAB8M4M7i8G2pHvOcOflbRq3JLF4lXaQ0aGb23ST3Ul+pHYPAAAAAN/M4M5i8NqUksG2Y7U74Kzrrdn1jVfLXFk5BwAAAIBvYHBnoRukNG/obzhQmukVtVtgTpy+Wmb116+WmandAwAAAMBpBncWuuelm6wd7vCGJEtHaQcZHb6lTTfZmeTHavcAAAAAcJrBnQWuvKkZrRn31u6pHQJzqrd6R4a7n5Ykb0ny9Mo5AAAAAMTgzsK2P+meMtx5eZtSarfAnJs6/9o0M2vHX7taZlntHgAAAIClzuDOQvaGNO1ksPXS2h1QRWn7mbnkxW3SbU7y47V7AAAAAJa6tnYAPEEzKeXdgy2Hh4Mth2q3QDXN1Iqkm5RTd/3p4SS/n+STtZsAAAAAlion3FmobkrXzQ53HK/dAdVN7b0m7fKNk5Tm7UnW1O4BAAAAWKoM7ixMpbyxXb5x0q7aWrsE6mvajA7f0iRZl+Sna+cAAAAALFUGdxaiS9J1lwx3XN4kHkuFJGmXb8z0vutKkhcluaV2DwAAAMBSZHBnIXp9afuTvrvb4ZsMdz81vdU7JinlnyfZXLsHAAAAYKkxuLPQLE8pL+1vvaQpvWHtFphfSpPR4Vua0vRmUso74894AAAAgDlljGGheUm6bnq4/bLaHTAvNaPVmb7weU267hlJ3ly7BwAAAGApMbizkJSU8sZ25ZZJu8JtGfCdDLZdmv6GA0kp/yjJ/to9AAAAAEuFwZ2F5Ei67oLhjuM+t/BdlYwOvjClP2pTml9KMqhdBAAAALAUGC5ZSF5X2sGkv+ni2h0w75XBTEaHb27TTS5M8iO1ewAAAACWgrZ2ADxGK1LKLw63He33N15YuwUWhHZmbbpHH8z4xG2XJ/m9JJ+tnAQAAACwqDnhzkLx4nTd1MBjqfC4TO1/dprZdZOU5l8nWV27BwAAAGAxM7izEHzDY6mbarfAglLafmYufWmbZH1Sfj5Jqd0EAAAAsFgZ3FkIPJYKT0K7fFOmD9xQku7GJK+u3QMAAACwWBkwWQg8lgpP0nDnlemt29ullH+a5LzaPQAAAACLkcGd+W5ZSnlxf+vhpvSGtVtg4Solo8M3l9Kf7qc070niBwoAAADgDDO4M9/dkq6bHm47VrsDFrxmuCyjwy9u000uTPIPavcAAAAALDZt7QD4rkr5uXb5xvVT51/roUc4A9qZtelOPZrxPZ+7LMkHknyidhMAAADAYuGEO/PZwXTd4YHHUuGMmt53XdoVmycpzbuSbKndAwAAALBYGDKZz16bpjcZbD5UuwMWl6bNzKUva0rTW5ZSfilJr3YSAAAAwGLgShnmq1FKeddgy+HhYPPB2i2w6JTBKO3s2nLyix/ZlqQk+S+1mwAAAAAWOifcma9ekK6bHWz3WCqcLf1NF2ew47Ik+dtJrqmcAwAAALDgGdyZn0p5XTO7btxbvb12CSxq0weem3bZ+i6l+aUkm2r3AAAAACxkBnfmo/PSdVcMt1/Wnr7pAjhbStvPzJFXNKVpVyXl38Z97gAAAABPmMGd+ejWlKYbbL2kdgcsCc3suowO3tQk3ZVJfqR2DwAAAMBC5dFU5ptBSvOu/sYLR4Otl9ZugSWjXb4h3aMPZnzitiuTfDDJn9RuAgAAAFhonHBnvrkh3WTNcPvR2h2w5EwfuCHtyi2TlPLuJLtq9wAAAAAsNAZ35pny2mZ6xbi39tzaIbD0NL3MHHl5U3rD6ZTm15JM104CAAAAWEgM7swnW5Pu2sH2y9oUj6VCDc30qsxc+rI23eTCJD8dLxcDAAAAPGbucGc+eWuSq0eHbk7pT9VugSWrmVmTlCanvvLpg0m+lOR/1W4CAAAAWAiccGe+aFKa1/bPOa9rplfWboElb+rcZ6S/YX+XlJ9Kcrx2DwAAAMBCYHBnvrg63WTrYNtR11fAfFBKRoduKc3M6vK1+9w31k4CAAAAmO8M7swXt5b+9Li/4UDtDuBrSn8qM0df3Zamtzal/GqSYe0mAAAAgPnMHe7MB6tTyi8Mdxzv9defX7sF+AbNcCbt8g3l5Bc+tCXJhiTvrd0EAAAAMF8Z3JkPXpPkhtHBF6YZztZuAb5FO3tOkuTUXX96OMmXk/x+1SAAAACAecqVMtRWUsrr2pVbJ+2y9bVbgO9gau8z0994QZL8VJKrK+cAAAAAzEsGd2o7lK67YLj9mM8izGelZHTo5rTLNnQpzf+TZE/tJAAAAID5xshJbbeWpjfpb764dgfwPZTeMDOXvbot/anZlObXk6ys3QQAAAAwnxjcqWk6pXlZf/PBpvSmarcAj0EzvSozR1/VJtmTUt6TpFe7CQAAAGC+MLhT0/PSTZYNth2t3QE8Dr3VOzI69KKSrrsmyU/U7gEAAACYL9raASxhpfzfzWj1tukLbmiSUrsGeBza5ZuSbpJTd33maJJ7kryvdhMAAABAbU64U8vOdN3TB9uPtcZ2WJimzrs2/U0XJadPuX9f5RwAAACA6gzu1PLKpHSDrZfW7gCeqFIyOnxL2lXbu5Tyb5O4HwoAAABY0gzu1NCmNLf2zzkvzdTy2i3Ak1CaXmaPvappplf3UprfSLK7dhMAAABALQZ3arg63WTzYPtRd8nAIlAGM5k9/tq29KdWpDS/lWRd7SYAAACAGgzu1PDq0p8e99fvr90BnCHNzJrMHntNm9JsTym/nmSmdhMAAADAXDO4M9dWpZTnD7YdadO0tVuAM6hdtTUzR17eJLkkpfy7JP3aTQAAAABzyeDOXLslXdcfbDtSuwM4C/rr92V08QtLuu5ZSX4h/p4BAAAAlhBHjJlbpfxcu2LzOVN7r3F/OyxS7YrNKe0gp+785EVJViT5z7WbAAAAAOaCwZ25dFGSvzt13jNLb+XW2i3AWdRbvSPd+GTGd3/2siSnkvxe7SYAAACAs83gzlz6oTTt0ZlDN5fSutoZFrv+unMzefi+jO/9wtVJ7kry/tpNAAAAAGeTwZ25Mkhp3jXYfPFosOVw7RZgTpT015+f8Ve/nMn9d1yf5HNJPlS7CgAAAOBs8Zgdc+WGdJNVg60eS4UlpTSZOfzi9M85v8vpR1Rvrp0EAAAAcLYY3Jkj5dZmasW4t3ZP7RBgrjVtRkdfUb728//uJDdWLgIAAAA4KwzuzIVNSXfdYPvRNqXUbgEqKE0vM8deVXqrd5ak/HKS59ZuAgAAADjTDO7MhZclKYOtl9buACoq7SAzl91a2tXbSlJ+JckNtZsAAAAAziSDO2dbSWle01uzq2tGq2u3AJWV3jCzl72maVdtbZLyqzG6AwAAAIuIwZ2z7Xi6yZ7B9qPukgGSJKU3ldnjr/3G0f15tZsAAAAAzgSDO2fbq0o7mPQ3Xli7A5hHTo/ur2t6q7c3Sf5dkptqNwEAAAA8WW3tABa1mZTyzsHWS4cDgzvwLUrTS3/zwTK+53OZPHjPC5J8LsmHancBAAAAPFFOuHM23Zium/FYKvCdlHaQmWO3lv455yfJ25O8uXISAAAAwBPmhDtnTyk/2cys3Tp94NlN4gp34NsrTZvBpovL+Kt3ZHL/l69PMk7y32p3AQAAADxeBnfOlp1J/slwz1VNb83O2i3AfFeaDDZemMnD92Z87xevTrIiyW8l6SqXAQAAADxmBnfOlr+SlKeNDt9cSm9YuwVYCEpJf8P+dOOTGd/92cuSnJvkvTl94h0AAABg3jO4czY0Kc0v9s85b/lwx3F3yQCPQ0l/3d6U3jCn7vzEhSnleJJfS/Jo7TIAAACA78WjqZwNT0832TLYdsTYDjwhw91Py+jwLUnKM1LK7yXZULsJAAAA4HsxuHM2vKr0p8b9DftrdwAL2GDL4cxedmspTf+ilOb3k/hDBQAAAJjXDO6caStSygsGWy5p0/RqtwALXG/d3sw+5U1NGcxsTCnvS3JN7SYAAACA78Tgzpl2U7puONh2pHYHsEi0yzdl2dPe2rbLNoyS8p+SvKZ2EwAAAMC349FUzqxSfrpdvmHj1PnXur8dOGNKbyqDLYfL5L4vlckDdwP3cWIAACAASURBVD43ybIkv5Okq5wGAAAA8OcM7pxJ+5L86NS5V5fequ21W4BFpjS9DDZfXLrxyYzv+dzlKeVIkvcmeaR2GwAAAEDiShnOrFemNN1gy+HaHcBiVZpMH7gho4MvSlKuTWk+kOT82lkAAAAAicGdM6eX0ryqv+FAKYOZ2i3AIjfYdiSzV76xKf3pHSnlfyV5bu0mAAAAAIM7Z8q16SbrPJYKzJXequ1ZdtUPtO2KLdNJ/n2St8VVaQAAAEBFhgnOlL9fBjN7Rxc/v0nxXiowN0pvmOHWS8rk0QcyPnH7U1PK8SS/keTB2m0AAADA0uOEO2fC2qQ8d7DtSJviIwXMsaaX0UXPz+jQTUlpnpHSfCTJ8dpZAAAAwNJjHeVMeHHS9QZbXScD1DPYemmWPfWtTTO96pyk/LckP5DEV24AAACAOeNKGZ680vx8u3LLuqm9Vxu2gKqa4bIMtl3aTB68q5ncf8e1Sbk0yX+OK2YAAACAOeCEO0/WwXSTC4fbj/osAfNC6U1l5pKXZHTR85OmuS6l+WiSq2t3AQAAAIufE+48WX8rTe/I6NDNpbS92i0AX1PSrtyS/sYLyvgrn57uHn3gFUmmk/xeknHlOAAAAGCRMrjzZAxSmncNNl88PdhyqHYLwF/QDGcz2HakyamHy/jEbVemNM9Nut9N8pXabQAAAMDi4xoQnoznpJusHGzzWCowf5W2n+kL/1Jmjr06pT91QUr5UJK/HH8HAgAAAGeYE+48CeUfN9Mrdk1f8H1NivdSgfmtnV2X4bajzeSrdzaTr955XUp5epL/muRE5TQAAABgkTC480RtSvIzw91PbXpr99RuAXhMSjvIYPPFpRmtyak7P7kl3eT1Se5K8ge12wAAAICFz9fpeaJelqQMtl5auwPgcSoZbL0ky57+19v+ur3TSX42pfxOkp21ywAAAICFzQl3noiS0ryjt2bXquHup7hLBliQSn8qgy2HSjO9Kqe+8qlt6Sbfn+SBJL+fpKucBwAAACxATrjzRBxPN9k92H7U2A4scCWDbUey7Oq/0fY3HJhK8k9SyvuSXFS7DAAAAFh4nHDnifg7pR0cHB26qZTGRwhY+EpvmMHmg2mXb8ypOz+1PuNTr08ySvI/kpysnAcAAAAsENZSHq+ZlPKLg62XDgYbL6zdAnBGtcvWZ7j9WNOdfKgZ33v7lSnNy5Puk0k+UbsNAAAAmP9cKcPj9fx03Wiw7UjtDoCzovSnM7r4+Zm98k1pZ9dtTvIfkvLvk2yv3QYAAADMb0648ziVn2pm1m6Z3n99k7jCHVi8mumVGW4/Vkp/OuO7//TcpHtjTj+m+v4k48p5AAAAwDxkMeXx2JXk09P7n53hnqtqtwDMmcnD9+bhP3pvHv3Ch5LSfDbd5C1J3lu7CwAAAJhfnHDn8firSXnq6NBNpfSGtVsA5kzpTaW/6aL01uzO+MTnl3ePPvCSlHJZkg8k+UrtPgAAAGB+MLjzWLUpzb/qrz9/+XDHZbVbAKpoRqsz3HFZKcPZjO/67M5Mxm9Msiqnr5l5uHIeAAAAUJnBncfqmUn3xun9z0677JzaLQD1lJLeqm0Zbj/W5NSjzfje24+lNN+fdA8k+WCSSe1EAAAAoA6DO4/Vj5TBaN/o4hc0KU3tFoDqSjtIf/2+9DddVLoHvjKcPHDX9SnNLUn3uSSfqN0HAAAAzD2DO4/F6pTyC8Mdx3v99efXbgGYV5rhbAZbDpd21fZMTty2onv0gRenlKcn+WiSL9buAwAAAOaOwZ3H4tVJnjM6+II0w9naLQDzUjuzNsMdx5tmekVO3f25LRmffH2SfUn+IMk9lfMAAACAOWBw53srzc+3KzefM7X3mlI7BWBeKyXtyi0Z7jjelKaX8YnP70/XvTnJ2iQfSPJg5UIAAADgLDK4870cTLq/M733maVduaV2C8CCUJpeemt3Z7j9aMn4ZDO+9wtHU/KmJL2cPvH+aOVEAAAA4CwwuPO9/K007ZHRoZtLaXu1WwAWlNIbpr9+XwZbDpXJw/cPJvff8fSU5nVJ93CSDyUZ124EAAAAzhyDO9/NMKV512DzwenBlkO1WwAWrDIYZbDpovQ3HMjkgbumJw/edV1K88qkuyfJHybpKicCAAAAZ4DBne/m+Un30ukLnptmtLp2C8CC10wtz2DrJaW3ZnfGX71jtnv43uelNLck3ZeSfDyGdwAAAFjQDO58Z6X8eDO9csf0Bd/XpHgvFeBMaUarM9x+tLQrtmRy35+t7B796k0pzY1J9/kkn6rdBwAAADwxVlS+k61JPjd1/rVlau81tVsAFq+uy6Nf/HAe/vh/Gk8euKtNKe9L1/3tJL8TJ94BAABgQXHCne/krUmuHh26OaU/VbsFYPEqJe3yDRnuvLxpRqszvvcLG7tTD78iKVfn9Gn3z9dOBAAAAB4bgzvfTpPS/GJv3bnLhzuv8C0IgLlQStoVmzPceUVTppZnfOL2LRk/emtKeUqSTyS5vXYiAAAA8N0Z3Pl2np50f3l6/3WlXbahdgvA0lKa9FZuzXDXFU0zmM34xG3bMz752pRyNMmfJPmz2okAAADAt2dw59v5kdKf3j86+MImpandArAkldKkt2pbhjuvaJrBdMYnbt+V8ck3JOWSJB9P8qXajQAAAMA3M7jzrVYl5e3DHZf1+uv31W4BWPJK06a3ekeGOy5vmv5Uxidu25PJqe9PysEkH0tyR+1GAAAA4DSDO9/q1UmeM7r4BWmGy2q3APA1pemlt3pnhjsvb0o7zPjE7edmcuqNSS5M8sdJvlw5EQAAAJY8gzvfrDQ/367cfM7U3ms8lgowD5Wml96aXRnuuLwpvUHGJ24/L5PxG5McSPJHSe6snAgAAABLlsGdb3Qo6f7O1N5rSm/l1totAHwXpf368H68KW2/jO+9bV8m4zclOT+nh/evVE4EAACAJcfgzjf622l6l84curmUtle7BYDHoLT99NbuznDH5aU0bRmfuH1/uvGbk5yb08P7XZUTAQAAYMkwuPN10ynNuwZbDk4NNh+s3QLA43R6eN+T4Y7jXx/eL0g3eXOS3Uk+muTuyokAAACw6Bnc+boXJd0t0xc+L81oVe0WAJ6gbxreS1PG995+4deG911J/jDJPZUTAQAAYNEyuHNaKT/ZzKzZOn3ghibxXirAQlfafnrrzs1wx2UlSZmcuP2idJO3JNme08P7ibqFAAAAsPgY3ElOXzfw41PnPr3prd5ZuwWAM6i0g/TX7c1w++nhfXzvFy5O170lyZYkH0lyb91CAAAAWDwM7iTJD6Y0V44O31xKb1i7BYCzoPQG6Z+zN4Ptx0rSNeN7bz+ULm9JsinJh5PcVzkRAAAAFjyDO72U5l399ftmh9uP1W4B4CwrvWH655yXwbajJZNTzfi+Lx5Ol7cmWZfTw/v9lRMBAABgwTK4c33SvXb6wHPSzq6r3QLAHCm9Yfrrz89g65GSyclmfO8Xj6bkLUnWJPlQkgcqJwIAAMCCY3Bf8so/LsNle0YX3dikeCwVYKkp/an01+/LYNuR0p16pB3f92fHvja8L8/p4f3ByokAAACwYBjcl7ZNSX5mavdT2966c2u3AFBR6U+lv2F/BlsPl+7kw73xfX92eUp5c5JRkg8mebhyIgAAAMx7Bvel7S1JrhkdujmlP127BYB5oPRH6W+8IIPNB0t38qH++L4/e2pKeVOSfk4P749UTgQAAIB5y+C+dDUpzb/qrduzfLjrSnfJAPBNymAm/Y0Xpr/p4nSPfHUwuf+Op6c0b0y6LqevmjlZuxEAAADmG4P70nV10r1lev+zS7tsfe0WAOapZjibwaaL0994QbqH7xtOvnrnM1Oa1yfdozk9vJ+q3QgAAADzhcF96fqx0p/eNzr4wialqd0CwDzXDJdlsPlg6a/fl+6he6YnD9z1rJTmNUn3QJKPJBnXbgQAAIDaDO5L09qk/Mvhzst7/fXn124BYAFpplZksOVw6a07N5MHvjIzeeieG1KaVyTdPUk+mqSr3QgAAAC1GNyXpjckuW506EVpBjO1WwBYgJrpVRlsu7T0Vu/M+P47lnUP3/e8lOaWpLsjycdieAcAAGAJMrgvPSWleWdv9Y7VU3uu8lgqAE9CSTOzJsPtR0u7Yksm9//Zqu6Rr74opbkx6W5L8snahQAAADCXDO5Lz5VJ94NT+55V2uWbarcAsCiUtLPrMtx+vLTL12d87xfWdicffElKuS7Jp5N8tnIgAAAAzAmD+9LzttIbXjA6eFNTGv/7ATiDSkm7bEOGOy9vmtHqjO+9fUN36pFXpZSnJfl4ki/UTgQAAICzyeK6tKxKKe8Y7ris39+wv3YLAItVKWlXbM5w5+VNGS7P+J7Pb8v45OuScjinH1b9cu1EAAAAOBsM7kvLa5PcMDr4wjTD2dotACx2pUlv1dYMd17RNP2pjO/5/J5MTr0xyblJPpTknsqFAAAAcEYZ3JeOktK8vV21bc3UuVd7LBWAOVOaNr3VOzLccbwpTVvGJ267IF33liQbk3wwyf2VEwEAAOCMMLgvHZcn3d+c3ves0q7wWCoAc6+0/fTW7slw+7GSbtyMT9x+SUrenGRZkj9I8lDlRAAAAHhSDO5Lx9tKb3jh6JDHUgGoq/QG6Z9zXgbbjpTu5MO98b1fvDylvDlJk9PD+6OVEwEAAOAJsbwuDatSyjuH24/1+xsO1G4BgCRJ6U+lv+FABpsPlskj9w8m999xdUrz+qR7KMmHk4xrNwIAAMDjYXBfGk4/lnroRR5LBWDeKYOZDDZdnP76/Zk8ePf05MG7rk9pXpF0dyX5aJKudiMAAAA8Fgb3xa+kNO9oV21b7bFUAOazZmp5BlsvKb01uzO+/0vLuofvuzGleVHS3Z7kE7X7AAAA4HsxuC9+VyTd3/BYKgALRTNaneH2o6VdsSnje7+wqnv0wRenlGfl9Oj+udp9AAAA8J0Y3Be/v1d6wws8lgrAwlLSzp6T4Y7jTTOzJuN7Pr+xO/XIq1PKZTl9zcwdtQsBAADgW1lgF7fVKeUdwx3He/0N+2u3AMDjV0raFZsy2HlF0wxmMr7n8zszOfX9SfYm+WCSE5ULAQAA4M8Z3Be3NyS5fnTwpjTDmdotAPCEldKkt2p7hjuPN6W0ZXzitgPpujcnWZfkA0keqJwIAAAABvdFrKQ07+yt3rF66tyrPJYKwKJQml56a/dkuP1oyeRUM773C0dT8qYkg5we3h+tnAgAAMASZnBfvJ6edD8wte/60i7fWLsFAM6o0humv/78DLYcLt0jD/TH93/pqpTm9Un3YJIPJRnXbgQAAGDpMbgvXv+wDEbnzxx8YZPS1G4BgLOiDEbpb7ow/Q0HMnngK1OTB+++PqV5edLdkeSPk3S1GwEAAFg6XDWyOG1Iyu3DPVe10/uvr90CAHPm1Fc+mYf+6D9Oxvd+oUkpH07X/WCS367dBQAAwNLghPvi9NYk18wcvjmlP6rdAgBzphmtyXD7sdIuX5/xidvXdScfekVKuTLJR5N8qXYfAAAAi5vBffFp///27jvIz/yw7/vn+/zaFuyilzscyuF674W8ziJGolhEUqRJiSJlMuxFIumaxD1lxs5EtjLJOHImsRMldmxFytgeOxlZsmRLpCmKTSfSbMfrdzyUXSyALdj9PU/+WBwPhzvyGoBnd/F6zfxmj/hhMZ/f8B/se774PinVr3e3XjIx2Henf8EAwLmnlHQmdmSw99VVGazLcOrhPRkufjTJpUm+nGS65YUAAACsUYL72vPTSfPh0avfVDrrtrW9BQDaU6p0N+7OYO+rqlI6GU49fFWa5pNJNib5UpK5lhcCAACwxgjua00pf7cambxw7Nq3VSkOuANAqbrpbrk4/T23lQyPV8PDj92eUj6eZJjlE+9LLU8EAABgjRDc15YLk/y9wcX3Vd0tF7W9BQBWlNIdpLf9ivR2Xleauel+fXT/61OqX0yaA0n+JEnT9kYAAABWN8F9bfmLKdUdYze9u5TuoO0tALAiVf3x9Hden+7WSzI88uR4M3/4bSnV25LmO0keaHsfAAAAq5fgvnaMpFT/R++8a0YHu29pewsArHjV6MYM9txaOhM7Mpx+ZEuzOPe+lPKqJF9L8lTb+wAAAFh9BPe1491J856xa9+Wamxj21sAYJUo6Uxsz2Dvq6syWJfhoYcuTL30sSS7svxg1SMtDwQAAGAVEdzXilJ+rbNu247Rq95YJR6WCgAvSanS3bg7g723V0nKcPqR65N8Ikkvy+H9eKv7AAAAWBUE97XhxiR/Y+TyN1TdDbva3gIAq1bp9NLbekn6u24qzcKx7nDmiXtTqg8mzVSSr8eDVQEAAPgxBPe14W+VTu+6sRvfXUrVbXsLAKx6pTea3nnXpLf9ytRHfjBWz02/JaV6uwerAgAA8OMI7qvfppTyjwZ7buv2zru67S0AsKZUI5Pp7765dNafn+HUI5ubxbn3JeXWJF9JcqDtfQAAAKwsgvvq9/EkPzl2w59JNRhvewsArEElnXXbMtj7qqr0xzI89NC+1EsfT7ItyReTzLY8EAAAgBVCcF/dOinVr3e3XLR+5OJ7PCkVAM6kUqW7cU8Ge26vUg/LcPrRW1LysSQLSf44ybDlhQAAALRMcF/dfippPjZ69ZtKZ922trcAwDmhdHrpbbss/Z3Xl3puqlcf3f+GlOrnk+b7Sb7d9j4AAADaI7ivZqX899XI5N6xa99epTjgDgBnU+mPp7/zhtLddGGG04+sb44fe09KuSvJl5M81fY+AAAAzj7BffW6NMmvDC55bdXdvK/tLQBwzqrGN2ew9/ZSRiYzPPTg7tRLH0uyPcl/iPvdAQAAzimC++r1V1J1bhm/6T2ldPptbwGAc1sp6W7YlcHe26s0wzKcfuTmlHw0ybEsn3ivW14IAADAWeAektVpIqU80b/gpvGxG97V9hYA4BT10f2Zu///aRaf+lZJqb6Vpv5Ekt9uexcAAABnlhPuq9MHk7x17PqfTTUy2fYWAOAUpT+e/gU3ls6G3RlOPbSxWZx7X1KuT/LFJNNt7wMAAODMENxXnyql+vXOpt0bRi59nX+hAAArWGfdlgz2vqoqvZEMDz14aVJ/PEkvy/e7L7Y8DwAAgNNMcF99fiJpPjV21ZtKZ2JH21sAgBdSqnQ37U1/9y2lOX6sM5x5/J6U6v1J83CSb7Y9DwAAgNNHcF9tSvnVMpjYN3bdO6qUqu01AMCLVLqD9M67Ot1tl2U4/ei6ZuHIu1LKXUn+KMmBtvcBAADwygnuq8ulSf7uyKWvqbpbLmp7CwDwMlSjGzLYc2upRtdn6eADu1MPP5ZkPMkXkhxveR4AAACvgOC+uvy1VJ2bx2/6uVI6/ba3AAAvVynpbLgggz23V83SXDWcfvSOlOrPJs0jSb7R9jwAAABeHg/dXD02pJTH+7tuGR27/mfb3gIAnEbD6Ucz+/XfqIfTj1Yp5XfTNB9N8q22dwEAAPDSOOG+enwsyRvHbnhXqsFE21sAgNOoGpnMYPdtpRqdzNKBH14zM5Lla2YWW54HAADAiyS4rw6dlOr/7G7ZNzly8X3+VQIArEU/vGbmtqo5fqwaHn78rpTqF5Lm20m+0/Y8AAAAXpjgvjq8NWk+NHr1W0pn3ba2twAAZ1Dp9NPbcVW6Wy/NcOqhieb4sZ9LynVJ/iDJTNv7AAAA+NEE99WglH9QjW3aOXbNz1QpDrgDwLmgGt2QwZ7bSumNZHjwgcuS5qNJZpN8KUnd8jwAAACeh3q78t2U5Euj17w1gwvvaHsLANCCem46c3/ym83ik98oKeVraZoPZjm8AwAAsII44b7y/e3SHVw1duO7q1J1294CALSg9EbS33lD6aw/P8ODD2xtlhY+lGRjlq+ZOd7yPAAAAE4Q3Fe2nUn5nwf77uz0tl/R9hYAoGWdddvS33N7leFiGU49fHtK9b6k+U6Sb7e9DQAAAMF9pfvPUqo7x296Tym9kba3AAArQKm66W27LL3tV2Q49eB4s3D0PUm5JsnvJzna9j4AAIBzmeC+cq1Lqf5xf+d1g/7uW9reAgCsMNXI+gx231ZKd5Clgw9cnuQjSbM/yVfa3gYAAHCuEtxXrg8nzVvHrn9XqpHJtrcAACtRqdLdtDf9nTeU4cwT/Xr20JtTyr1J/n2SQy2vAwAAOOcI7itTJ6X6x91Ne9ePXPra0vYYAGBlK/2x9HfdVKqxTVna/91daYYfTbKQ5ItJ6pbnAQAAnDME95XpbUnzodFrfqZ01m1tewsAsCqUdNafn/7uW6p6dqpTH/nB61PKm5L8YZKn2l4HAABwLhDcV56SUv7XanzLjrFr3lqlOOAOALx4pTtI//zr0ll/fpb2f3dbhosfSVJlObwPW54HAACwpqm5K88dSf792HVvT3/P7W1vAQBWsWZxLnN/+s9z/OE/Skr1jTT1+5J8qe1dAAAAa5UT7itO+dXSH7tk7IY/U5XK/z0AwMtXOr30dlyVzsY9GR74zqZmaeE/TTKS5YeqOu0OAABwmjnhvrJcluSbI5f9RBm57PVtbwEA1pBmaT5z3/iXOf7gF5JSfStN/d4kf9T2LgAAgLXEEeqV5b8uVfeG8Zt/vpROr+0tAMAaUqpuetuvTHfzhVk68J2NzdLCh5IM4rQ7AADAaSO4rxw7kvIPB3tf1e2df03bWwCANaoa25z+ntuqLM6V4fSjd6VU70iaLyR5ou1tAAAAq53gvnL85ZRy9/hNP19Kb7TtLQDAGrZ82v2KdDddmKX9Pzzt3knyh3HaHQAA4GUT3FeGyZTyT/rnXz/o77m17S0AwDmiGt+c/p5bq2bhWBkefuyelOotSfMHSZ5qexsAAMBqJLivDJ9K8saxG9+damSi7S0AwDmkVN30dlyVzoZdWdr/7S0ZLn44yfEkX0jStDwPAABgVRHc2zdIqf5pb9ulY4OL7yltjwEAzk2ddVsz2H1r1cxNV8MjT74upfxEkn+bZKrlaQAAAKtG1fYAsrMamczg4vvEdgCgVaU/lrGb3pPxm9+b0h3cklL+JMmHk/h7CgAAwIvghHv7pta/8W/eXo1tvNLPsgDAStCZ2J7+rpur+siT3frYwTellFuT/E6So21vAwAAWMmccF8xxHYAYOWoRiYzfvsHyui1b0spnTekVN9I8ra2dwEAAKxkgjsAAD9CyWDvqzJx32erzoYLJpP8RpL/Jclky8MAAABWJMEdAIAfqxrfkok7P16NXP6GpJT3pVT3J7mr7V0AAAArjeAOAMALK1VGLn1dJu76ZKnGNp6f5PeS/FdJ+i0vAwAAWDEEdwAAXrTOhl2ZuPcznf7e20uSv5RSvpDksrZ3AQAArASCOwAAL0np9DN27dszfusvpvRGr00pX03yoXgKPAAAcI4T3AEAeFl6O67MxH2f6/S2XjpI8veT8ltJtrS9CwAAoC2COwAAL1s1mMj47R8oo9e8Namqn06pvpHk9W3vAgAAaIPgDgDAK1QyuPCOTNzzy1Vn3dbNSf6/JH8nyaDlYQAAAGeV4A4AwGnRmdiedff8UjXYd1eSfDal+mKSy1ueBQAAcNYI7gAAnDal6mb06jdn/PYPpPRGrkopX0nywXigKgAAcA4Q3AEAOO162y7P5DMPVP21pPzTJBvb3gUAAHAmCe4AAJwRZTCR8ds+UEavfnNSys+kVPcnuavtXQAAAGeK4A4AwJlTSgb77srE3Z+uqrFN25P8XpK/lqTb7jAAAIDTT3AHAOCM66w/PxP3/nKnv+e2kuSvppTfT7Kn7V0AAACnk+AOAMBZUTr9jF33jozf/N6UTv/WlOpPkry97V0AAACni+AOAMBZ1Tv/2kzc99lOZ8Ou8ST/LMnfTzLW8iwAAIBXTHAHAOCsq0Y3ZuLOj1Ujl74uST6UUv1xkqtbngUAAPCKCO4AALSjVBm5/A1Z9+qPpPTHL0kpf5zko0lK29MAAABeDsEdAIBWdbdclMn7Ptvpbbuil+R/SMpvJNnU9i4AAICXSnAHAKB1pT+e8dveX0avfktSyltPPFD1zrZ3AQAAvBSCOwAAK0TJYN+dmbj7U6Ua27g9ye8n+c+TdFoeBgAA8KII7gAArCid9Tszcc8vd/q7bi5J/mZK+Z0kO9veBQAA8EIEdwAAVpzSHWTshndl7MZ3p1TdO1Oq+5P8dNu7AAAAfhzBHQCAFat/wY2ZuPczVWfyvMkk/zzJryQZtDwLAADgeQnuAACsaNX4lkzc9clqcNE9SfLplOqLSS5teRYAAMBzCO4AAKx8VSejV/10xm//YEpv5KqU8tUkv9D2LAAAgJMJ7gAArBq9bZdl4t7PdrqbLxpJ8g+T/O9JJlqeBQAAkERwBwBglalGJrPuVR8qI1f8VFLKe1KqryW5qe1dAAAAgjsAAKtPKRm55L6su/PjpRqZ3J2ULyT5bPz9FgAAaJEfSAAAWLW6G/dk4t7Pdno7r+0m+Tsp5V8l2dH2LgAA4NwkuAMAsKqV3kjGb/q5jF3/syml87qU6v4k/0nbuwAAgHOP4A4AwBpQ0t99a9bd+8tVZ2L7xiT/Ksl/m2TQ8jAAAOAcIrgDALBmdNZty8Tdn6oG++5Kks+kVF9MclnLswAAgHOE4A4AwNpSdTN69ZszftsHUnojV6WUryb5QJLS9jQAAGBtE9wBAFiTetsvz8R9n+t0t1wySPIPkvJ/JdnY9i4AAGDtEtwBAFizqsFE1t3+wTJ61ZuSUt5+4oGqd7e9CwAAWJsEdwAA1rZSMrjo7kzc/alSjW3anuTfJvkvk/TaHQYAAKw1gjsAAOeEzvqdmbj3M53+3ttLkr+cUj6f5JK2dwEAAGuH4A4AwDmjdHoZu/btGb/1/SndketTytfigaoAAMBpIrgDAHDO6e246ukHqo5k+YGq/3eSLW3vAgAAVjfBHQCAc1I1Mrn8QNWrO1v/twAAEf9JREFU35JU1ZtTqm8keUPbuwAAgNVLcAcA4NxVSgb77szEPb9cdSa2bU7yr5P8apKxlpcBAACrkOAOAMA5rzOxPRN3f7oaXHxvknwipfpqkpvbXQUAAKw2gjsAACRJ1c3olW/Mujs+mmpkYl9SvpDkryTptT0NAABYHQR3AAA4SXfzvkzc97lOf9dNnSR/PaV8Psnlbe8CAABWPsEdAABOUbojGbvhXRm/9f0pvdHrU8rXknw6/v4MAAD8GH5gAACAH6G346pM3vfnOr0dV/WT/EpK+d0kF7a9CwAAWJkEdwAA+DHKYF3Gb/mFjN347pRO/46U8qdJPpKktL0NAABYWQR3AAB4QSX9C27MxGv+XKe39dKRJP9jSvntJHvaXgYAAKwcgjsAALxI1cj6jN/+gTJ2/TtTOr17U8o3snza3d+rAQAAPxgAAMBLU9LffUsm7vvzVW/rpaNZPu3+O0n2tb0MAABol+AOAAAvQzV64rT7De9K6fTvPHG3+y8l6bS9DQAAaIfgDgAAL1tJf9fNmXjNn+/0tl85kuS/SymfT3JV28sAAICzT3AHAIBXqBqZzPit78v4ze9N6Y3emJSvJvkbSQZtbwMAAM4ewR0AAE6Lkt7512byNX+h0999czfJf5FS3Z/krraXAQAAZ4fgDgAAp1Hpj2Xs+ndm3as/lGp0w4VJfj/JryXZ1PI0AADgDBPcAQDgDOhuuSQT932uM3LJa5JSfSCl+k6S9yYpbW8DAADODMEdAADOkNLpZeSKn8zEvZ8p3Y27NyT5Rynld5Nc0fY2AADg9BPcAQDgDOtMbM+6Oz5WjV3/zpTuyJ1J+XqS/ybJeNvbAACA00dwBwCAs6GU9HffksnX/sVOf8+t3SR/4cQ1M++Ma2YAAGBNENwBAOAsKv2xjF33jkzc9al01p+/Pck/OXHNzFVtbwMAAF4ZwR0AAFrQ2bgrE3d9qhq77h0nrpnJ15P8vSSbWp4GAAC8TII7AAC0pZT099yWydf9pc5g311VSvlESvVAkk8k6bU9DwAAeGkEdwAAaFnpjWb06jdn4t7Pld7WSyaT/GpKdX+Sn4r73QEAYNUQ3AEAYIXoTGzL+O0fLOO3fyDV+OaLkvzLlPJvktzQ9jYAAOCFCe4AALDC9LZdnsn7PtcZvfZtKb3Ru5P8cZL/LcnedpcBAAA/TqftASQjl//EO5Nc2fYOAABWkFLS3bArg72vrkqpynDq4avTNJ9MsjnLAX625YUAAMApBPcVQHAHAOBHKVU33S0Xp7/71pLh8Wp4+PHbUvKJJKNJvpJkvuWJAADACYL7CiC4AwDwQkp3kN72K9K/4IbSHD/WG848eXdK9bGkKUm+muR42xsBAOBcJ7ivAII7AAAvVumPpXfeNemdd02ahZlBffSp16ZUH02aJsnXIrwDAEBrBPcVQHAHAOClqgYT6e+8Pr3tV6aZnxmpj+1//YkT750sh/eFtjcCAMC5RnBfAQR3AABermpkMv0Lbii97VemWTgyUh/d/9qU6hNJM5Lk/ni4KgAAnDWC+woguAMA8EpVI5PLJ97PuybN4rFBfeQH96SUTyXZnuSbSaZbnggAAGue4L4CCO4AAJwu1WAi/fOvTX/nDWnqpe5w5olbk+ZTSS5P8mCSJ9pdCAAAa5fgvgII7gAAnG6lP57ejivT33NbKVWnDGeeuDL18CNJeW2SqSTfSdK0PBMAANYUwX0FENwBADhTSneQ7tZLMrjwjqoaTKQ++tQFzeLcu1OqX0yaKsl/TDLf9k4AAFgLBPcVQHAHAOBMK1U33Y27M7jwjqqz4YI0x49O1rOH3pBSfinJhUkeP/ECAABeJsF9BRDcAQA4a0pJZ9229HfdVHrnX5eS0q2P/OC6NMMPp5SfSVIn+XaS4y0vBQCAVUdwXwEEdwAA2lAN1qW3/fIM9t1ZOmObUs9Nb2sWjrz5xKn3i7N81/vDLc8EAIBVQ3BfAQR3AADaVKpuOhsuyGDvq0pvx1UpKb366P5rUg//bEr1/qRZn+TRLAd4AADgRxDcVwDBHQCAlaIamUxv+xUZ7Lur6kzuSLM4v76ePXRvkk+nlJ9MMpLkoSTHWh0KAAArkOC+AgjuAACsNKXqpDO5I/1dN5X+nttKNTKZZuHo+c3CkTcm+WxKuTvJIMtXzsy2uxYAAFaG0vYAkg1v+dv/LE3e3vYOAAB4IfXR/Tn+2Fdz/NEvD+tjBzpJ6pTy79I0v5Hkt5I80vJEAABojeC+AgjuAACsRsMjP8ji41/P4hNfHw5nnlz+17OlfDVN81tJ/kWSrySp29wIAABnk+C+AgjuAACsdvWxg1l88k+z+MT9zdLUg0nTlJRqf5r6XyT510l+O8mhdlcCAMCZJbivAII7AABrSXN8Nov7v5WlH/zHLP7gm8Nmca6TpEkpX07T/L9J/k2SP0wy3+5SAAA4vQT3FUBwBwBgzWrqDA8/lsWnvp2lp77VLE09lDR1ScpiSj6fpvndJL+X5D/Ew1cBAFjlum0PAAAA1rBSpbNhVzobdiWXvrY0w+NZOvj9LB34Xm/pwHfvHB5+9K40TUnKMCVfTtP8uySfP/F6rOX1AADwkgjuAADAWVM6/fS2XZbetsuSpGqWFjKceihLB7/fWTr4wM3DqYdvauqlzyz/5urJNPUfJPmjJF9K8uUkU62NBwCAFyC4AwAArSndQbpbL01366VJUtLUZTjzRJYOPZTh9MM7lg499Nb62IFnrl8s1cNp6i8m+UqSr514PZakaWM/AACcTHAHAABWjlKls35nOut3Jnl1knSaxfkMDz+W4eFHszT9yO7h9GM762MH3nHS98ykab6aNPcn+dOTXgfa+AgAAJy7BHcAAGBFK72RdLdclO6WizJY/qVOMzyeeubJDA8/nuGRJyaHM0/cNZx54o5mcb7zzDdW02mabyTNN5N868TrO0keSLJw9j8JAABrneAOAACsOqXTT2fj7nQ27v7hLyVNp144mvrIkxkeeSr1kac2DI8+9arhkSdvbRaOnvyzT5NSPZam+VbSfDfJ97Ic4R9I8v0k02f30wAAsFYI7gAAwBpRUg0mUg0m0t1yyTO/mHSbpYXUxw6kPnogw2P7S33s4AX1sf07h0cP3NMcP/bsn4tKdSTJg2nq7yV5MMlDJ74+fOJ1MO6MBwDgeQjuAADAmle6gx/eDd876ZeTdJvh8dSzh1IfO7j8dXZqop49dE197OCV9dxUmqWFzrP/sLKQlEfT1A9mOcY/kuTRU77OnJ1PBgDASiK4AwAA57TS6aczsSOdiR2nvtVJkmZxPvXcVOq56dSzU6nnpgfN3NRF9dzUvnp2algvHOmkacqz/9Aym5TH0tQPZznAP5blGP/oSf99IE7KAwCsKYI7AADAj1F6I+n0zktn8rznvJWkm6ZOvXAk9dx0mvmZ5TA/d3isnp++pJk7fPEzUb4up3z7Ykr5QZrmoaR5OsqfGuafSLJ4pj8jAACnh+AOAADwSpQq1cj6VCPrn/fdJN00TZrjR1M/HeTnZ9LMHe7V89MX1HMzF9RzU8NmfibN8HjnlO9vUqoDSfPIcpj/YYw/+fqax5McP4OfEACAF0lwBwAAONNKSRlMpDOYSGf9zuf7HcvX1yzNp56bSTM//XScL/X84a3N3PTWenb6unp+Os3i/KlRPinVwaR5KE3zYJYj/CN55iGvDyf5QZL6DH06AABOENwBAABWiNIdSWdiJJnY9nxvL0f54WKa+cNPX11z4iqbw5vruenN9eyh6+q56ec+6DVlKaU8nqb5ftI8lOWHvT6U5Rj/4ImvC2fwowEAnBMEdwAAgFWkdHop41tSjW95vrdPnJRfWA7xc9Mnwvx0t56b2l3PTu2qZw8N6/mZ594pv3x1zffTNA9kOcKf/HooydwZ+1AAAGuE4A4AALDGlO4gnYntycT257yVkx/0OjuVem7q6a9b6tmpLfWxgzfUc1NV6mH17O+sDibN99I038tyhP/+Sa+H4+GuAACCOwAAwDnnWQ963Xvqu92kSbNwNMPZQ2lmpzOcPZR67tDmenZqc33swI317HQnzfDkE/JNSvV4mua7SfO9PBPiHzjxeipJcxY+GQBAqwR3AAAATrH8kNfuYCLZuCe9Z7/ZTdOkXphJPXto+XXsUKlnD+2sZw+eXx89cEe9cOTZP2uWMp+UB9PU38kzEf7p14NJZs/GpwIAONMEdwAAAF6aUp45Ib/pwme9k6Sbemn5mprZQ8un42cPjdSzhy6vj+6/tJ491Dznoa6lOnDidPx389wg/0SS+ux8MACAV0ZwBwAA4PSquqnWbU21buupP3RWSdIcn33mdPzsoQyPHdxSzx7aUh/df3M9f/iUB7qWxZTy8Emn40++rub7SabPzocCAHhhgjsAAABnVemPpdMfS2fDBae+tfxA17np1LMHUx87lHr2UK+ePXTR8NiBC+tjh5pmcfbU0/FHknw/Tf3dPBPjHzzpq+tqAICzRnAHAABg5ShVqrFNqcY2JVue9c7y6filhWedjq9nD03Us1PX1sf2X1XPTpVmuFid8ucdSpoH0jRP3xf/9OuhE69jZ/wzAQDnDMEdAACAVaN0B+lMnpfO5HmnvtVJmpOuq5l6OshvqmcPbRoeO3h9PTddpV46NchPJ82DJ4L8Q0kePuXrwSTNmf9kAMBaILgDAACwRpSU/ng6/fF0Nuw69c1u0qRZOLYc4+dOBPm5qQ317NT19eyha+q56Tz3ga5lISmPpqkfzHKAf+R5XkfP+EcDAFYFwR0AAIBzREkZrEtnsC6djc8J8p0kaRbnlu+Qn5s+EeWnBs3c4Yvqual99ezUsF6Y6aRpyrP/2OpIkqej/KPP83osyUyclAeANU9wBwAAgBNKbzSd3ujzXVlT8vRDXReOpJ6bTvPDMH94op6bvqKem76snpuum4WjneTUKF9mk/JEmubhpHk6wj/9evzE1yeTLJ3xDwkAnDGCOwAAALxYpUo1sj7VyPpk455T362SVGnq1PNHUs9Pp5mfWY7y8zNjzdzhi+r56X313PSwnp+pUg+rU76/SakOJnksTf1Inonxjyd54qT/fSBJfUY/JwDwsgjuAAAAcDqVKtXo+lSj65/33Tx9n/zxudTzh5ej/PxM6vnDpZmf2VLPz2yp56avqecP183Cseeelk8ZppT9SfNomubRPDvKP37SywNfAeAsE9wBAADgrCsp/bF0+mPJc6+vSU4+Lb9w9OQon2Z+plMvHNnRzB3eUc8fvqGeP9w0x2ef5+f7sphSnlqO8s8K8ye/3C8PAKeR4A4AAAArValSjUwmI5PLT3V9ruVfrocnhfnDqRdm0szP9Or5mZ31/MzOZm765np+Js3i3HP/mFLmkvLkiWtsnr5f/vGT/vuxLJ+eXzwTHxEA1hLBHQAAAFa7qrN8hc3o+nSy6/l+x4kwv3TifvnDz8T5+ZnRZn7mwnr+8N56dnrYLMxUzXDx+e6XP5A0Dy8/+DWPZTnIP5rkkZP++/gZ+4wAsAoI7gAAAHCuqLqpxjamGtv4fO8+c7/84sKJKH849dzh1POHSz1/eGszd3hrPTt1fT1/+Eeclq8OJs1DaZoHk3wyyyflAeCcIbgDAAAAJykpvZF0eiPJxPbn+w2dJGmGi8un5OemU89Pp56bTjN3eHM9N725nj103egFr/6rR7/5m4I7AOcUwR0AAAB4yUqnlzK+OdX45ud7u1Oa4Uy++ZtnexYAtOrUO9kAAAAAAICXQXAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDTQHAHAAAAAIDToNv2AJKU5m81df6ntmcAAADA6bJupL9/qu0RAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8KL8//7aPYtCypsqAAAAAElFTkSuQmCC\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n  &lt;/tbody&gt;\n  \n  \n&lt;/table&gt;\n&lt;/div&gt;\n\nprint(tables[[2]])\n\n&lt;div id=\"vbmxqasdhh\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\"&gt;\n  &lt;style&gt;#vbmxqasdhh table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#vbmxqasdhh thead, #vbmxqasdhh tbody, #vbmxqasdhh tfoot, #vbmxqasdhh tr, #vbmxqasdhh td, #vbmxqasdhh th {\n  border-style: none;\n}\n\n#vbmxqasdhh p {\n  margin: 0;\n  padding: 0;\n}\n\n#vbmxqasdhh .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#vbmxqasdhh .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#vbmxqasdhh .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#vbmxqasdhh .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#vbmxqasdhh .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#vbmxqasdhh .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#vbmxqasdhh .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#vbmxqasdhh .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#vbmxqasdhh .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#vbmxqasdhh .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#vbmxqasdhh .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#vbmxqasdhh .gt_from_md &gt; :first-child {\n  margin-top: 0;\n}\n\n#vbmxqasdhh .gt_from_md &gt; :last-child {\n  margin-bottom: 0;\n}\n\n#vbmxqasdhh .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#vbmxqasdhh .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vbmxqasdhh .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#vbmxqasdhh .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#vbmxqasdhh .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#vbmxqasdhh .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vbmxqasdhh .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#vbmxqasdhh .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vbmxqasdhh .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#vbmxqasdhh .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vbmxqasdhh .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#vbmxqasdhh .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vbmxqasdhh .gt_left {\n  text-align: left;\n}\n\n#vbmxqasdhh .gt_center {\n  text-align: center;\n}\n\n#vbmxqasdhh .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#vbmxqasdhh .gt_font_normal {\n  font-weight: normal;\n}\n\n#vbmxqasdhh .gt_font_bold {\n  font-weight: bold;\n}\n\n#vbmxqasdhh .gt_font_italic {\n  font-style: italic;\n}\n\n#vbmxqasdhh .gt_super {\n  font-size: 65%;\n}\n\n#vbmxqasdhh .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#vbmxqasdhh .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#vbmxqasdhh .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#vbmxqasdhh .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#vbmxqasdhh .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#vbmxqasdhh .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#vbmxqasdhh .gt_indent_5 {\n  text-indent: 25px;\n}\n&lt;/style&gt;\n  &lt;table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\"&gt;\n  &lt;thead&gt;\n    &lt;tr class=\"gt_col_headings gt_spanner_row\"&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"Training Condit\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;Training Condit&lt;/span&gt;\n      &lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings\"&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Vb\"&gt;Vb&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Mean\"&gt;Mean&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Sd\"&gt;Sd&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Distribution\"&gt;Distribution&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody class=\"gt_table_body\"&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"4\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Constant\"&gt;Constant&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;100-300&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;522.83&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;245.10&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;350-550&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;660.61&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;224.30&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;600-800&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;771.75&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;210.50&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;800-1000&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;1,008.76&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;254.58&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;1000-1200&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;1,173.42&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;307.89&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABdwAAAH0CAYAAAAnhe8sAAAABmJLR0QAAAAAAAD5Q7t/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgAElEQVR4nOzdabSmZ13n+99138+4h5qH1JhUJZVKVaWSSlUmkhAgIWaCQELAKIgBFMSxW22H7nNojqIiKnbbtq3derQHG+lGHFCwFQe0G1CQ0yraEAlDAghkloSEqr2f+7woUESGVFJV1x4+n7V4Q1jJN2uxsvbzy39fTwIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUEFbOwAAABaQXpKSpKsdAgAALD6ldgAAAJxE65IcSnJOkt1J2ZlSTkuyJl03nXS9z/zvJinNw0nuTdd9KOnen+S2JO9K8idJ7jn56QAAwEJncAcAYClbneSqJFenNFekm5z22T9QhrNz7fS6tplaXcpgOqU3TNr+0T84mUs3dzjd4Ycyefj+TB66e27y8P29v/uzluY96Sa/leTXk/xRkrmT+PcEAAAsUAZ3AACWmjVJbk4pt6TrnpSkKf3xfG/dGW1vzWlpV29Lu2JTSm90TH/Sbv5w5h/4aObv/WDm7rk9R+563ySTuSaluTfd5BeT/Psk7z4Bfz8AAMAiYXAHAGApaJI8NclLknJD0vWamfXzgy0H2v6GPWlXbUlKc1z/gt38kczddVuOfPTPc/ijfzbJZL5JKf8rXfcjSd6QZHJc/4IAAMCCZ3AHAGAxm0nywpTm29JNdpbB9Pxg+wXtYOvBtCs2nbSI7sjDOXznO/Pp9//P+cmn7m1Tmr9KN/kXSX4tvoAVAACWDYM7AACL0Zok35pS/mm6bkVvzY5uuPPS0j/l7KRp61V1kxz56F/k4ff+j/nJg3e1KeWP03XfmqNftAoAACxxBncAABaTFUm+I6V8R7puur/5nIzOeEraVVtrd/1D3SSHP/z/5eG/+s357tOfbJL8hyTfm+TeymUAAMAJZHAHAGAxGCR5aUrz8nSTVYMtBzI886q0sxtqd31J3dyn88htv5tP3/6WLsm96SYvTPLrtbsAAIATw+AOAMBCVpJcn9L8RLrJjv6G3d1o7/XlZL7PfjzMf/Lj+dS7XjOZf+AjTZKfTfJPkzxYOQsAADjODO4AACxUO1PKv0nXXdfObpyM9z+j6a3bVbvpsZvM55G//t088t43dynl9nSTG5O8u3YWAABw/BjcAQBYaPpJvjOlvLy0g3Z01jXtcMclSWlqdx0Xc/d+IA+94z/Pd4cfnEvX3Zrkl2o3AQAAx0dbOwAAAD7HoZTypiRfNdhyoJ2++EVNf/2upCydO5FmvDrD7ec38/fd0U4evu/mJKMkv5+kq5wGAAA8TgZ3AAAWgkGSlyf5T81oxbqpQ89tRruuSOkNK2edGKUdZLD1YOmOPJz5+++8LCnn5uiXqc7VbgMAAB67pXMqBADAYrU3pXlNusk5g1Mvynjv01L6o9pNJ82nP/jWPPznv9ql5G3puqclua92EwAA8NgY3AEAqKUkeWlK+fHSn2qnzvvKtr9xT+2mKo78zbvz0J/+l0m67j3pJk9N8je1mwAAgGNncAcAoIY1Sfl/k+4Z/VP2dlMHnlPKYLp2U1Vz97w/D/3xz81383N3pJs8KcmdtZsAAIBjY3AHAOBkuzil+eUkm8Zn31CGOy6JH0uPmr//zjz41p+Z7+aPfCTd5IlJ7qjdBAAAPHo+2QAAcLKUJN+SlFc3U6szfcHXtu3KzbWbFpz5Bz6SB//XT89384c/nG5yaZKP1G4CAAAeHYM7AAAnw3SSn01yS3/T2Zk67ytTesvni1GP1fz9H86Db/3p+W7+yAc+M7p/onYTAADw5RncAQA40XakNG9I1+0d772uDM94UvwY+uXN3fehPPTWn5l0k/l3f+Z5mb+t3QQAAHxpbe0AAACWtKekNL9fesPNMxe/sBlsPRhj+6PTjFelt3p7Ofzhd21IySVJXpNkvnYXAADwxRncAQA4Ub4hKb/Uzm4Yzlz60rZduaV2z6LTTK9NO7uhHPnon5+W5Mwkr0/S1a0CAAC+GIM7AADHW5vkx5N8f/+Ufc30xS8qzXC2dtOi1c6ektKfytwn3rsvyTDJ79ZuAgAAvjCDOwAAx9NMUn45ydcMz3hyps59dkrbq9206PVWb0935OHM33fHZUnuSPK/azcBAAD/mMEdAIDj5ZSU8nsp5bKpc59VRruekhTvtR8v/fVnZv7+D3eTh+55WpI/yNHhHQAAWEB8AgIA4HjYndL8Tml6W6Yv/Nqmt/7M2j1LUjf3SD75hz8xP3nongfSTc6L0R0AABaUpnYAAACL3sUpzdtLf2rzzGXfZGw/gUpvlJmLXtSWtr8ypXlDknHtJgAA4O95UgYAgMfj2pTyW83U2vHsZd/YtrMbavcseWUwld7KLc3hD//pxiRbk/xa7SYAAOAogzsAAI/VLUl5XbtqSzt76Te0zWhF7Z5lo5lelzRt5u5+34Ekf5PkT2s3AQAABncAAB6blyT5+d66M8rMxV/XlL6XTU623podmX/gI93kobuvTfLGHB3eAQCAirzhDgDAsfrOJD/dP2Vfpi9+USm9Ye2e5amUTB28pTTjlSWl+ZUkq2onAQDAcufCHQCAR6skeVmSHxxsPZjpQ88tpfHjZE2l7ae3dmdz+I4/WZHkjCSvq90EAADLmU9IAAA8GiXJK5K8bHDqRZk68Jyk+GXJhaAZrUjpDcvcXe/dm+QjSd5VuwkAAJYrgzsAAF9OSfKqJN893HFpps65MSmldhOfo7d6e+bvv6ObfOreq5P8cpK7azcBAMBy5CwJAIAvpST5kSTfOTz98oz3P+Mz/xULSimZOu+WUvpTvZTmtUkGtZMAAGA5cuEOAMAXU5L8cD47tu97WoztC1fpDdKu2NQc+fCfbszRwf3NtZsAAGC5MbgDAPCFlCQ/mOS7hzufmPHZT4+xfeFrp9elO/xQ5u+/89Ikv5fkjtpNAACwnHhSBgCAL+RlSb5nuONSY/siM9r7tDTTaycpzX9JMlu7BwAAlhMX7gAAfL5/luQHBqdefPQLUo3ti0pp2vRWn9oc/tAfr0iyNslv1G4CAIDlwuAOAMDnemmSfzXYeihTB56dFGP7YtSMVyaT+TJ37wfOT/K2JLfXbgIAgOXAkzIAAHzW85L8VH/T/kyd9xxj+yI32v3UtLMbJynNzydZUbsHAACWAxfuAAAkyQ1Jfqm3/szMXPD8ksaPiYteadJbvb0cvuOPZ5KsiadlAADghPNJCgCAy1PKG9rVpzYzF7+oKW2/dg/HSTNakUyOlLl7P3h+krck+WDlJAAAWNI8KQMAsLwdSClvbGc29I6O7YPaPRxnw91fkWZ63fxnnpaZqt0DAABLmQt3AIDla0dK80fNaOXszKXf2DbDmdo9nAClNGlXbW0O3/Enq5L0k7y5dhMAACxVBncAgOVpXUrzR6U3PGX20m9sm6k1tXs4gZrxqnSffjDz93/4CUl+PcnHajcBAMBS5EkZAIDlZyqlvLGU5rSZi7++bWbW1+7hJBjtuS5lONOllJ+NwxsAADgh/KANALC8tEn570meMn3hrU1v3em1ezhJSttLO7WmOfLRP9uc5K4kf1K7CQAAlhoX7gAAy8urk+6GqXNvKv2Ne2q3cJL1N+9Pf+OeLqW8MsmW2j0AALDUGNwBAJaPb03yrcMznpLBqRfXbqGKkvH+G0tKO06aH69dAwAAS40nZQAAloenJ/mP/c3nlKlznpWUUruHSkp/nJS2zN19274kb0tye+0mAABYKly4AwAsfQdSymvbVdu6qfNuMbaT0emXp5lZP5/S/Lskw9o9AACwVLhwBwBY2k5Jaf6wGa1cMXvpS9vSH9fuYSEoTdrZjc3hO9+5OskjSf6odhIAACwFBncAgKVrlFJ+uzS9XTOXvrRtptbU7mEBaabWZPLgXZl/8OOXJvnPSR6o3QQAAIudJ2UAAJamkuTn0nUXTJ3/NU07e0rtHhag0b6npTS9flL+Ve0WAABYCly4AwAsTd+V5NvHe6/PYPsFtVtYoEpvlNL2ytxdt52V5K1J3l+7CQAAFjMX7gAAS8+1SV452HowwzOeVLuFBW6447I00+vmU5qfTNKv3QMAAIuZC3cAgKVlV0p5c7tyS3/6wltLafy4x5dRmjQz65sjH/7TtUnuS/L22kkAALBYuXAHAFg6ZlOaN5T+eDx94a1NaR0r8+j0N+xOf+OeLqV8X5INtXsAAGCxMrgDACwNJSn/McmZ0xfc2jbjVbV7WGTGZ99QkjKV5BW1WwAAYLHyO8YAAEvDdyX5lvH+Z5TB5nNqt7AIlcFUuvnDZf7eDx5M8mtJPla7CQAAFhsX7gAAi98V+eyXpO64pHYLi9jozCtTBlOTpPxEklK7BwAAFhsX7gAAi9vWlOb32tkNo+mLXlBK06vdwyJWml6a/lRz5ON/dWqSP0vyntpNAACwmLhwBwBYvPop5XWl7a06+iWpg9o9LAGD7RekXXHKJKV5dZJh7R4AAFhMXLgDACxeP5Lk2dOHnld6a06r3cJSUUramQ3l8J3vXJXk/iRvq50EAACLhQt3AIDF6ZlJvn14+uXpbzq7dgtLTG/dGemfsrdLKS9Psq52DwAALBYu3AEAFp8dKeV32lXbetOHnltS3FBw/PVWbi2f/uDb+kk3leRNtXsAAGAxMLgDACwug5TyP0o73DZz6UvbMpiq3cMSVQbT6Y58qszfd+ehJK9Nck/tJgAAWOicQwEALC4/lK47NHXwq9pmvKp2C0vc6MynpvQGScqrarcAAMBi4MIdAGDxuD7JvxmefnmGOy+r3cIyUNpBStM2c3fdtjvJ7yf5UO0mAABYyFy4AwAsDptTmv/crtwyGe+5rnYLy8hgx6VpxivnU8qr4/MDAAB8SS7cAQAWvjal/FppertnLnlJU4YztXtYRkppUoazzZG/+YvNSd6b5N21mwAAYKFyoQIAsPB9Z7ruyeNzn9U00+tqt7AMDbYcSLtyyySleWWSYe0eAABYqFy4AwAsbIeS8prB1vOa0e6ra7ewXJWSdmZ9OXznO1YmuSfJ22snAQDAQuTCHQBg4ZpOaV7bjFdkvP+m2i0sc711p6e/4awupfmXSVbW7gEAgIXI4A4AsHC9Ol23c+rQ89rSH9VugYz2XV/SdSuTfHftFgAAWIgM7gAAC9PTk7x4dOYVpbfmtNotkCRpZ0/JYPv5JaV8R5KttXsAAGChMbgDACw8G1Kan29XbpmMzryqdgv8A6PdVyel6SV5ee0WAABYaAzuAAALS0nKf0gpq6cOfXWTxnfcs7A045UZ7nxik+SFSfbV7gEAgIXE4A4AsLA8P+luGO97etPObKjdAl/QaNcVKb3RJCk/VLsFAAAWEidTAAALx/aU8sbeujP6U/ufWVJK7R74gkrbTylNM3fXbbuTvDnJnbWbAABgIXDhDgCwMDQp5RdK2x9PnfeVxnYWvMGOS9OMVsynlFcl8X9YAACIwR0AYKH4hnTdU8b7b2ya8araLfBllbaf0VnXtOm6S5JcX7sHAAAWAoM7AEB9O1PKj/Y37ukG2w7VboFHbbDtUJqZ9fMpzaviuUoAADC4AwBU1qSUny/tYDA+9+biZQ4WldJkvPe6Nt1kT5Ln1s4BAIDaDO4AAHW9NF13+Xj/jW0zWlG7BY5Z/5R9aVdtm6Q0P5BkWLsHAABq8mufAAD1nJZSfq2/cU9vvPc61+0sUiXtzPpy+I53rEhyd5I/rl0EAAC1uHAHAKijpJSfLW1/OD73WcZ2FrXe2p3pb9jdpTQvSzJbuwcAAGoxuAMA1PGCdN2V4303NM1oZe0WeNxGe64t6SZrknx77RYAAKjF4A4AcPJtTin/urfu9G5w6oW1W+C4aFduSX/LuUkp/yzJ+to9AABQg8EdAOCkKz+Z0k5NHXi2p2RYUsZnXZMkU0m+t3IKAABUYXAHADi5bkq6G8d7rm2aqbW1W+C4aqbXZbD9opKUb06yvXYPAACcbAZ3AICTZ2VK8+/alVsmw52X1W6BE2K0+6lJ07RJXla7BQAATjaDOwDAyfNDSbd+6sBzmhQ/hrE0NaOVGe58YpPkhUnOqt0DAAAnk096AAAnxyVJXjo8/UmlXbm5dgucUKMznpLSG06SvKJ2CwAAnExt7QAAgGVgkNK8qRmvXD19wfOb0vgRjKWttP2k65q5u9+3N8lvJPmb2k0AAHAyuHAHADjxviPd5Kypc29uS9uv3QInxXDnZSmDqfmU8sraLQAAcLI4rwIAOLF2ppTXDbYcaIe7nlK7BU6a0vRS2kEz9/H37EzyliQfrJwEAAAnnAt3AIATp6SUnyrtoB2dfUPtFjjphqdelGa8cj6l/HCSUrsHAABONIM7AMCJ86x03dWjPde1zXC2dgucfE0vo7OuadN1FyZ5eu0cAAA40QzuAAAnxmxK82/aVVsnw9Murt0C1Qy2Hkwzs34+pXllPGkJAMASZ3AHADgxXp6u2zh17s1Nih+5WMZKk/Ge69p0kz1Jvrp2DgAAnEg+/QEAHH/7k/yT4Y4nlHblltotUF1/0760q7ZOUppXJBnU7gEAgBPF4A4AcHyVlPLTZTDVjc66pnYLLBAl473XN+km25N8fe0aAAA4UQzuAADH1/PSdZeM993Qlv64dgssGL11Z6S3fleX0rw8yXTtHgAAOBF8aREAwPGzMqX5zd6a7aPx/meUpNTugQWlnd1QDn/o7VNJHkryR7V7AADgeHPhDgBw/Lw8Xbd2fM5NjbEd/rF21bb0N52dlPK9SdbU7gEAgOPNhTsAwPFxdpJfGO64tBlsv6B2CyxY7YrNOfyBt/Zz9N9Kvbl2DwAAHE8u3AEAHr+SUn6y9Mfd6Kyra7fAgtbObshg+/klpfyTJFtq9wAAwPFkcAcAePxuTtc9abz3el+UCo/CaPdXJKXpJXlZ7RYAADiePCkDAPD4TKU0b2xXbp6eOuemkuLtdvhySn+U7sjDZf6+D52X5DVJ7q3dBAAAx4MLdwCAx+e70002j8+5sUnxoxU8WqNdV6S0gyT5gdotAABwvLhwBwB47E5LKa8dbD3UDndeVrsFFpWjY3tX5u6+fV+S30jyN5WTAADgcXOGBQDw2P1IaXq90d7ranfAojTc+cSUwdR8Snll7RYAADgeDO4AAI/N5UluHp55VdOMVtRugUWp9IYZ7f6KNl331CRX1u4BAIDHy+AOAHDs2pTmJ5vxqvnh6U+s3QKL2vDUi9OMV8+nlFcl8a3DAAAsagZ3AIBj94J0k/3js29oS9Or3QKLW9NmtOfaNl13MMnNtXMAAODxMLgDABybFSnNK3trd076m86u3QJLwmDLgbQrNk1Smlcm6dfuAQCAx8rgDgBwbL433WTt+OxnNF6/gOOklIz3Xt+km+xM8qLaOQAA8FgZ3AEAHr0dKeU7BtsvTLtyc+0WWFJ6G85Mb+3pXUrz/Umma/cAAMBjYXAHAHj0frg0vXa055raHbAElYz3Pa2km6xL8u21awAA4LEwuAMAPDqXJHn2cNeVTTOcrd0CS1K7amv6W85NSvmeJBtq9wAAwLEyuAMAfHlNSvnXzWjF/PD0y2u3wJI2PuvaJGWc5P+u3QIAAMfK4A4A8OV9Zbru/NHe69vS9mu3wJLWTK/N8LQnlKS8NMkZtXsAAOBYGNwBAL60UUrzqnbV1slgy3m1W2BZGO2+KqXtlyQ/VLsFAACOhcEdAOBL+5Z0k63jfU9vUkrtFlgWymA6w11XNEluTnJR7R4AAHi0DO4AAF/c2pTysv4p+9Jbu7N2Cywrw9MvTxnOzifl1Un82y4AABYFgzsAwBf3fydlerz3+todsOyUtp/xnmvapLskyQ21ewAA4NEwuAMAfGFnJOWbh6ddXJqZ9bVbYFkabDs/7ezGSUrzY0l8YzEAAAuewR0A4AsqP1TafhmdeVXtEFi+SpPxvqc36SanJ3lx7RwAAPhyDO4AAP/YRUl383DXFU0ZztRugWWtt2F3eut3dSnN9ydZWbsHAAC+FIM7AMA/VFLKj5Xh7Pzw9MtrtwBJxvueVtJNVif5ntotAADwpRjcAQD+oael6y4d77mmLa0no2EhaFdszmD7BUkp35Hk1No9AADwxRjcAQD+Xi+l+dF2ZsP8YNv5tVuAzzE665qU0rZJfrh2CwAAfDEGdwCAv3drusmZo73Xtyl+TIKFpBmtyHDXFU2Sr0xyce0eAAD4QnySBAA4aiqleUVvzY6uf8qe2i3AFzA848lpRivmU8q/TlJq9wAAwOczuAMAHPVt6SYbR/uuL3Y8WJhK289o7/Vtuu7CJLfU7gEAgM9ncAcASNaklH/e33R2eqt9HyMsZIMt56VdtXWS0vxoknHtHgAA+FwGdwCA5HvTZXq059raHcCXU0rG+5/ZpJtsTvKdtXMAAOBzGdwBgOVuW0r5tsGpF5Z2ZkPtFuBR6K0+NYMt5yWl/PMkW2v3AADAZxncAYDl7uUpTTvafVXtDuAYjPZel5R2kORVtVsAAOCz2toBAAAV7UnyH4anP7kZbNpfuwU4BqU/SrquzN1z+/4kv5PkztpNAADgwh0AWMbKD5TecDLa9ZTaIcBjMDzjyWlGK+dTyr+NzzYAACwAfigFAJarC5PuxuGuK9vSH9duAR6D0vYzPvvpbbruQJIX1O4BAACDOwCwHJWU8qoymJ4f7ry0dgvwOPQ3n5Pe2p1dSvOqJKtq9wAAsLwZ3AGA5eip6bonjc66uj36nYvA4lUyPufGknSrk3xf7RoAAJY3X5oKACw3JaX892Zq9YbpA1/ZpLg/gMWuGc6kO/Jwmb/vjguT/EqSj9duAgBgefIJEwBYbm5K1503OuuaNo3bA1gqRruvShlMTZLyU0lK7R4AAJYnnzIBgOWkTWl+pZ3dsHrqnJtKik0OlorS9tMMZ5sjH3v39iTvS/LntZsAAFh+XLgDAMvJ89JNdo32XNsY22HpGWw9lHbNqZOU5sfjC1QBAKjAhTsAsFwMU5pfbVdtnRnve1rx4gQsQaWkt2pbOfyht42TzCR5U+0kAACWFxfuAMBy8XXpJlvHe69vjO2wdLUrNmW484klyTclOVi7BwCA5cXgDgAsB9Mpzct7687oeutOr90CnGCj3V+RMpydpJR/H7/VCwDASWRwBwCWg29ON1k33nOd03ZYBkpvmKn9z2zTdYeSvKR2DwAAy4fBHQBY6lalNP+8f8q+rl29rXYLcJL0N+9Pf8PuLqW8Ksmm2j0AACwPBncAYKn79nSTFaM917huh2WlZHzOTSWlHSflJ2rXAACwPHjPEABYytallP822Hpef3jaE2q3ACdZ6Y9Tml6Zu+u2vUn+NMlttZsAAFjaXLgDAEvZ9yQZj3Z/Re0OoJLhziemXXHKJKX5mSSztXsAAFjaXLgDAEvV5pTyXwfbLugNtl9QuwWopTTprdpWDn/oj2eSzCT5rdpJAAAsXS7cAYCl6l8kpT/afVXtDqCydtW2DHdeVpJ8SxLvSwEAcMIY3AGApei0pLx4eNoTSjNeVbsFWABGe65JM141SWl+Icmwdg8AAEuTJ2UAgKXox9L0zpu+4Pml9OxqQFKaNu3KTc3hO9+5NklJ8nu1mwAAWHoM7gDAUrMryc8Odz6xGWzeX7sFWECaqbWZPPK3mX/gI5cleUOSj9VuAgBgafGkDACw1Ly8tP2Mdj2ldgewAI33Pi1lONullP+UZFC7BwCApcWFOwCwlOxL8lOjXVc0/Y17arcAC1Bpe2lnNzZHPvyuDUkmSf6gchIAAEuIC3cAYAkp31d6w8nw9MtrhwALWH/jWRlsOz9J+b+SHKzdAwDA0mFwBwCWioNJd9PwjKe0pT+u3QIscOOzn5FmNJuU5heT+HZlAACOC0/KAABLRPnZ0h/vnD7/eU1perVjgAWutL20KzY3h+9857ocfcv9zbWbAABY/Fy4AwBLwcVJd93ozCvb0nOoCjw6vfW7MtxxSZL8sySXVc4BAGAJMLgDAItfKT9QBtPzg9MuqV0CLDKjvdenmVo7SWn+a5IVtXsAAFjcDO4AwGJ3ebruitGZT21L26/dAiwypR1k+tBz26TbmuQnavcAALC4ecMdAFjMSlL+SzNasWXq4Fc1pbglAI5dM16ZlFLm7r79QJK/+sx/AADgmPlUCgAsZlcm3WWjM69qfVEq8HiMdl2R3ppTJynNzyXZVrsHAIDFyeAOACxWJaX8QDNeNT/YfkHtFmCxK02mDj63KW1vOqW8Jol/iwcAwDHzpAwAsFhdl+S7xmff0LSrttZuAZaA0h+nnVpXjnz0z7cn6ZL8QeUkAAAWGYM7ALAYlZTy35qpNeunDjy7SSm1e4Alol1xSiYPP5D5Bz7ypBwd3D9UOQkAgEXEkzIAwGJ0Q7ruwOisq9v4olTgOBvvf2aamfWTlOa1SdbX7gEAYPHwCRUAWGyalPKKZmb9/GDLgdotwBJU2n6mL/jaNqVZn1J+MT43AQDwKHlSBgBYbG5O8k1T59zUtCs21W4BlqhmOJNmvKoc+Zt3n55kPskf1m4CAGDhM7gDAItJm9K8rp3duHq8/5nebgdOqHbl5kwevj/zD3z0yUneluT2ykkAACxwfjUSAFhMvjLdZPfRt9uN7cCJN95/Y9oVp3Sfec99e+0eAAAWNoM7ALBY9FKa729Xbp70N+2r3QIsE0ffc7+1KW1/NqW8PsmodhMAAAuXwR0AWCyem26yc3TWNU3iuh04eZrptZk69Nw2XXcoyb+NfwgBAPBFeMMdAFgM+inN69tVW1aM911fbF3AydbOrE9Kk7m7bz8vySeSvKN2EwAAC48LdwBgMbg13eTU8Z5rXbcD1Yx2XZn+prOTlJ9I8qTaPQAALDwGdwBgoRumNP+yt8MlfFcAACAASURBVOa0SW/9rtotwHJWSqbOu+Wz1+6/mmRH7SQAABYWgzsAsNC9KN1ki7fbgYWg9IaZvviFbekNZ1OaNyZZUbsJAICFw+AOACxk45TmZb11p3e9dafXbgFIkjRTazN94a1tkt0p5bVJerWbAABYGHxpKgCwkH1z0j1r6uBXl2a8qnYLwN9pplanGa8qRz72l2ckWZvkTbWbAACoz+AOACxU0ynNr/Q3nDke7brCWzLAgtOu3JJM5jN37wcuTHJ/krfXbgIAoC5PygAAC9U3pZusHe2+2tgOLFijs67JYMuBJHl1kpsr5wAAUJkLdwBgIZpNaV7f37hnODzjSQZ3YOEqJf1T9mbung9k8sj9NyZ5S5IP1c4CAKAOF+4AwEL0bekmq0ZnuW4HFoGml+kLby3t9PompfmNJGfXTgIAoA6DOwCw0KxKab6rv2l/2pWba7cAPCqlP870E17cNsOZqZTmd5PsqN0EAMDJZ3AHABaab083mR2ddXXtDoBj0oxXZvqSl7SlN1z7mdH9lNpNAACcXAZ3AGAhWZdSvmOw9by0sxtrtwAcs3ZmQ2ae8PVtadpTU5o3J1lTuwkAgJPH4A4ALCTflS7j0ZlX1e4AeMzaVdsyfdGLmpSyJ6X8dpIVtZsAADg5DO4AwEJxSkr51sG2Q6WZWV+7BeBx6a07PdMX3tok5WBKeVOSmdpNAACceAZ3AGCh+J6kDEa7XbcDS0N/w1mZvuD5JSlPSMobk0zXbgIA4MQyuAMAC8G2pHzj4NQLSzPluWNg6eifsi/Th55bUnJZSvnNJFO1mwAAOHEM7gDAQvAv0jTtaNdTa3cAHHf9zeccHd2Ty5PyW/G8DADAkmVwBwBq25mUrxue9oSmGa+s3QJwQvQ3n5vp87+mpJTLUsqbk/gHHgDAEmRwBwBqe1lp2jLcdUXtDoATqr9p/2ffdL8wpfxBknW1mwAAOL4M7gBATbuTPH+w84lNM5yt3QJwwvVP2ZeZi19UUtpzUpr/mWRL7SYAAI4fgzsAUNP3lXbQjc54cu0OgJOmt/7MzFzy4qa0/TNSmrcl2VW7CQCA48PgDgDUck6S5wzPeFJTBlO1WwBOqt6aHZm59Bvb0h9vTmnenuRg7SYAAB4/gzsAUEl5RemP5oc7L68dAlBFu3JzZp/4LW0zWrkypfxRkqtrNwEA8PgY3AGAGi5MuqePdl3Zlv6odgtANc302sxc/i1tu2LzKCm/meTW2k0AADx2be0AAGAZKuU/lsH0qVOHntuUxo8jwPJWesMMtp5XJg98tEweuvuZSUqSt9TuAgDg2PmECwCcbE9O8v+M917f9NacVjkFYGEoTS+DLQfK5PCDmb//w09OcmaS30wyV7cMAIBjYXAHAE6mklJe04xWbpo+eEuT4nU7gL9TSvob96T0hpm767b9KeWqJG9I8lDtNAAAHh2fcgGAk+nadN3Fo7OubtP0arcALEAlw9OflOkLb01pehekNO9KcqB2FQAAj44LdwDgZGlSmtc102vWTR14dpNSavcALFjtzIb0T9lb5j7+l1Pd/OEXJvnrJH9ZuwsAgC/N4A4AnCzPTrpvmjrnWU27YlPtFoAFrxnOZrDtUDN/3x3N5OH7np1knOT3k3SV0wAA+CIM7gDAydBLaV7frti4arz/mcV1O8CjU9pBBlsPlu7Iw5m//87LUsplSd6Y5FO12wAA+Me84Q4AnAzPTzc5Y7Tnek/JAByrps14/zMzdfCWpDRPTmn+d5ILamcBAPCPuXAHAE60UUrzq70126fHe68ticEd4LFoV2w++q77J94z3R359IuS3JPknbW7AAD4ewZ3AOBE++ake/bUweeWZmp17RaARe3ou+7nN5MHP1EmD951fZI9SX47yacrpwEAEIM7AHBizaY0v9rfcOZodOaVTtsBjoPS9jPYcm4pvVHm7n7f3pRyS9L9YZKP1W4DAFjuDO4AwIn0PUl37fT5X1Oa0YraLQBLSElvzanpbTizzH38PSu6ucNfn+T+JO+oXQYAsJwZ3AGAE2V9SnndYMuB/nDHpbVbAJakZrzq6BMzD93VTB78xLVJOZijT8w8XLsNAGA5amoHAABL1vcmGY/Ourp2B8CSVgZTmb7g+Rmfc2PSNNenNH+Z5MraXQAAy5ELdwDgRNielF8cnHpRO9h2fu0WgGWgpLdqW/qn7Cvzd79v3B1+6GuTTCd5S5L5ynEAAMuGwR0AOBH+VZr2wPSFt5bSG9ZuAVg2muFsBtsvbDL3SJm//85LU5pnJN1bktxduw0AYDkwuAMAx9u+JD8zPP3JzWDT2bVbAJad0rTpbzwr7artmfvEe9ZlMveSJPcleWftNgCApc7gDgAcZ+XnSm94xvQFz29K268dA7BstTPrMtx2QTN58BPN5MG7rk8pFyf5vSSfrN0GALBUGdwBgOPpkiQ/PD7rmqa3flftFoBlr/QGGWw5UJrxqszdddvOJF+fdO9P8pe12wAAliKDOwBwvJSU8kvNaMXmqYNf3ZTGjxkAC0NJu3JLBlsOlLn77hh0jzzwnCS7c/Ta/ZHKcQAAS4pPwgDA8fL0JN81PvsZTW/1ttotAHyeMpjKcPv5Je0gc/fcfnZKuTXp/iLJ7bXbAACWCoM7AHA89FKaX21m1q2eOvfmJqXU7gHgCyklvbU70t+0r8zf8/6p7vBDz0+yMckfJDlSNw4AYPEzuAMAx8MLku6FUwee07SzG2u3APBlNMPZDE+9sKSbZO7eD52f0jw36d6Z5M7abQAAi5nBHQB4vKZSml/rrTl1arz3upK4bgdYFEqT3vpd6a3fVebuft9sd+ThFyWZTvJHSeYq1wEALEoGdwDg8frOpHvG1PnPK814Ve0WAI5RM16VwakXNZl7pMzff+elKc3NSffWJB+r3QYAsNgY3AGAx2NdSnl9f9P+wej0J9VuAeAxKk2b/sY96a3Zkbm7blvdzR1+cZIuyVuTTCrnAQAsGgZ3AODx+MGU5rKZC762lMF07RYAHqdmem0G2y9suk9/spn/248+JaVcn+QPk9xduw0AYDEwuAMAj9XpSflPwx1PaAfbDtVuAeA4KW0//U1np125OXN33bYxk7mXJPnbJO/I0at3AAC+CIM7APBY/fvSDvZOX/C1pfQGtVsAOM7amQ0ZbrugmTx4VzN58K5rU8rlSX4vR8d3AAC+AIM7APBYPCHJj412X1X6G8+q3QLACVJ6gwy2nFuaqTWZ+8RtpyaTFyf5UJJ3124DAFiIDO4AwLEqKeW/leHspunzn9eUxo8TAEtbSbtycwZbD5b5Bz7Snzx837OS7MnRa/eHK8cBACwoPiEDAMfq5iTfPnXOjU1v1bbaLQCcJKU/zmDboVL6o8zd/b69KeXWpPvfSd5fuw0AYKEwuAMAx2KY0ryhXXHKiqlzbioppXYPACdTKemtOS39TWeX+XtuH3eHH/raJCuTvCXJXOU6AIDqDO4AwLH4tqR7zvSh55Zmem3tFgAqaYazGZx6YZO5w5m/744npDQ3Jd0fJvlE7TYAgJoM7gDAo7U2pfxqf+OewejMK522AyxzpTTpb9id3prTcuQT712TyZEXJ7kvyTtqtwEA1GJwBwAerVemNE+cufAFpQyma7cAsEA002sz3H5BM3nwrmby4F3XpZQLkvxOkk/VbgMAONkM7gDAo7E7Kb8w3HFJM9h2qHYLAAtMaQcZbDm3lOFs5u667fSkvCDp3pXkA7XbAABOJoM7APAolF8ovcGu6QtvbUrbrx0DwIJU0lu1Lf1N+8v83e8bd4cfen6SUZI/TDKpHAcAcFIY3AGAL+fKJK8Y77mu6a0/o3YLAAtcM5zJYPsFJUceLvP333lZSrk6R5+YeaB2GwDAiWZwBwC+lF5K84Zmas2a6YO3NClN7R4AFoHStOlv3JN2xebMffz/nJKu+/qke0+S99RuAwA4kQzuAMCX8pKku3XqvFuadnZj7RYAFpl2dkMGW89r5u79YL975IFbkqxN8ntJ5iunAQCcEAZ3AOCLWZ3S/Hpv/RnD8Z6rS1Jq9wCwCJX+OMNt55duMpf5ez94UUq5PslvJ7m/dhsAwPHm98IBgC/mZem6leOzn2FsB+DxadqM916f6YtemNIOz0lp/izJM2pnAQAcby7cAYAvZE9SfmF42hOawfbza7cAsES0M+sz2HqwmbvnA/3ukb/9qiQzSf4gyaRuGQDA8WFwBwA+X0kpv1j6wx3TF97alLZfuweAJaT0R0efmJn7dObvu+OSlHJFkjclebB2GwDA4+VJGQDg8z0tXXfVaM+1bRlM1W4BYClq2ozPviHT539NStN7Qkrz50kur50FAPB4uXAHAD7XKKV5Yzu7ccXUgWeXFG+3A3DitLMbM9h8Tjly13uH3eFP3ZrkoSRvr5wFAPCYGdwBgM/1XUl30/QFX1OaqTW1WwBYBspgOoNt5zeTT91TJp/8+Fckzd6ke1OSw7XbAACOlcEdAPisbSnldf0t5/ZGpz+pdgsAy0hpehls3p/SH2furvfuTWluSrrfTnJv7TYAgGPhDXcA4LN+rJR2MN779NodACxLJcOdT8zMJd9QSn+0K6W8K8n1tasAAI6FC3cAIEmuTPLDo7OuLv2Ne2q3ALCMNVOrM9hysJm75/297pG//eokkyT/M0lXOQ0A4MsyuAMAg5Tmjc30mlXTB7+qSfELcADUVfqjDLedXyaPfLLMP/CRpyTlQJLfjHfdAYAFzidqAODb0k12Te2/sU3Tq90CAEc1vUwduDnjc25MSrkhpXlHkjNqZwEAfCku3AFgeduaUl7f37S/PzrzytotAPB5SnqrtqW37oxy5GPvXp3J/AuT7k+T3F67DADgCzG4A8CyVn6uNO3+6YtfVEp/VDsGAL6go++6n9fM3f2+fvfpT35Nkk8meXvtLgCAz2dwB4Dl6+okPzjec23pbzyrdgsAfEmlP8pg2/ml+9S9Zf6TH7s6yc4kb0oyVzkNAODvGNwBYHkapzRvambWr5g+75bii1IBWAxK06a/+eyk6Wfu7r8+N6VcneQ3kjxYuw0AIPGlqQCwXH1vuslpU+c+q0nj378DsJiUjHY9JdMXviCl6R1Kad6V5PzaVQAAiQt3AFiOzkzKawbbzm+GO59YuwUAHpN2Zn36m84ucx//q3E39+kXJPnrJH9ZuwsAWN4M7gCwvJSU8sulPzx15qIXNqUd1O4BgMesGc5ksPVQM3/fh5rJw/c9O0kvyVuSdJXTAIBlyuAOAMvL85P8k6lzbmp6a06r3QIAj1tp+xlsPVi6ww9l/v4PX56U/Ul+M8mR2m0AwPJjcAeA5WNdSvPG3todw/H+G0pSavcAwPFRmvQ37kkZzGTurvecldLckHS/keRva6cBAMuLL00FgOXjx5KsnDr3ZmM7AEvScMclmbn460pp+3s/82WqF9VuAgCWFxfuALA8XJXkR0e7ryr9zftrtwDACdNMr81g0/7myMf/z7Cbe+QFSW5P8he1uwCA5cHgDgBL31RK8z+amXWz0/9/e3f6pedd33f887vubWbuGe2ytcxYkm0JL5IsS7IWC7s1UIgDZjNmMcS27MSxTZOetklO0tPTPmjT9vRBm57yIKfLoaVJSMhGG0NNScGGk5wQkhAaMGYzMsaAFyR5k7Bm5r76QIQsgPFyj65ZXq+/4P1sjj667u9vz/VVih+4AbC4lW4/3ak91eyxB6vBiaPXJukkuTseUwUA5pjBHQAWv19M6h/t7ztcVWOrmm4BgDOitDrpbry01KdOZPb4g1cmZWeSO+MxVQBgDhncAWBx25vk3b0tl5fe5oNNtwDAmfW9j6lek9QfiMdUAYA5YnAHgMWrm1LuqkaWre7vu6kqVbvpHgBoRHvlVNqrNpXpb3xmTer6hqT+WJKHmu4CABYfR1wBYPH6udT1RaO7rmuVdq/pFgBoVHvttkxc+Q9a1ejKVSnl40mub7oJAFh8fOEOAIvT9qS8tzu5uxo5/6qmWwBgXijdfrqTu6vZ4x5TBQDmhsEdABafdkr5QOmOrRvff0tVWp2mewBg3vjuY6rT331MdVdOP6Z6quk2AGDhc1IGABafn0ld7x675E2t0h1rugUA5p+qldEdb8jozjcmJa9NKX+UZFPTWQDAwucLdwBYXC5Kyvu6Gy+tRra9vOkWAJjX2ium0l69pUx/4zOrUw9uSuo/TPLVprsAgIXL4A4Ai8d3TsmMrh8/cEtVWt2mewBg3qvGVqW7YWc1/cjnu/Wpkzcm+XqSTzXdBQAsTAZ3AFg8fiHJO/p7rq9ayyebbgGABaN0x9Kd2lMNnvh6GTz92OuSrE7y4SSDhtMAgAXG4A4Ai8OupPxad3J3NbLVKRkAeL5K1U53465SD2Yye/TI/pRyZU4/pnqy6TYAYOEwuAPAwtdLqT5ceuNrTp+S6TTdAwALUynprN2Wqr820w/fe05S3pbU/zfJI02nAQALg8EdABa+f5XUrx/fd2PVmljXdAsALHitZevTOeuCMvPwveP17Kmbk9yX5HNNdwEA85/BHQAWtiuS/OfelstLb8tLm24BgEWjGlmW7uTuavboA63ByeNvSdJJcneSutkyAGA+M7gDwMK1PKX6/Wps1Xh/341VqfxZB4BhKu1eupN7Sn3q6cwe/9qVKeWyJB9M8u2m2wCA+cm/zAFg4fpPKbli/MCPV9XYqqZbAGBxKlU6Z1+YamR5ph++7/yU8hZ33QGAH8TgDgAL05uT/MuRC15VupOXNt0CAItea8VkOme9pEx/895lGUzfkuRLST7bdBcAML8Y3AFg4ZlKqT7UXrWpM7brzSWlNN0DAEtCNbo8vand1eyxB1uDk8euS7I8yUeSDBpOAwDmCYM7ACwsraT8Xml3zhs/+JNV6Y413QMAS0pp99Kd2lPq2enMHnvgYEp5eZK7kjzZdBsA0DyDOwAsLP80yY1jl761tFdvaboFAJamUtI5a1taE+sy8/C9G5PclNSfTHKk2TAAoGkGdwBYOA4l+e/dqb1lZNsrmm4BgCWvNXF2uht2lplHv9CrTz19U5LpJH+QpG62DABoisEdABaGlSnVR6r+qvH+vsNVqdpN9wAASUq3n+45l1X1tx8vs0984+UpZX+SDyU50XQbAHDmGdwBYP4rSXlvStk7fvDWqhpb2XQPAPDXlKqVzvqLU40sz/Qjnz83KTck9SeSfLXpNgDgzDK4A8D891NJ/uHojteVzrqLm24BAL6vktaKyXTWXVxmHv3CWD194nCSmTgxAwBLisEdAOa3vUn5zc66i6vR7dckKU33AADPoupN/O0TM1ck+XCSp5puAwDmnsEdAOavFSnVR6vR5cvGD/5EVVqdpnsAgOegVO101m9P1V+dmUc+vynJLUn9mSRfbLoNAJhbBncAmJ9KUn49pVw2fvDWVtVf3XQPAPA8tZZtSHfDzjJz9P5e/cyTb0+yOsndOX1qBgBYhAzuADA//UySnxrd8frSWe9uOwAsVKXbT++cfaWenc7ssQf2p1TXJvXHkzzcdBsAMHwGdwCYf/5Okl/pbry0jF50ddxtB4AFrlTpnLUt7dVbMv3I51dmdvrWJCeTfCIeVAWARcXgDgDzy4aU6qPV+JqR/v7DVanaTfcAAENSja1O75zLqsGJo9XgyYdfmZSX5fSJmeMNpwEAQ2JwB4D5o5tSPliqztaJQ7e3qpFlTfcAAENWWp10N+xI1V+TmUe/MJl6cGuSR5N8quk2AODFM7gDwPzxS0mu7e99R2mv3tJ0CwAwZ0pay9anO7mnzD7xzfbgxLdem1IO5PTX7k80HAcAvAgGdwCYHw4n+cWRrVelt+VQ0y0AwBlQOiPpTu0uVW9ZZh774rlJ/ZNJvpnk0023AQAvjMEdAJq3Lynv75z1kjK267qS4pFUAFg6SlorJtOd3F1mn/hGZ3Di6OtTyuVJPpbk8abrAIDnx+AOAM1an1LdU42t7I8fvLUqrW7TPQBAA0pnNN2pPaUaWZaZx760JfXgtpw+L/OnSeqG8wCA58jgDgDN6aWUu0rV3jp+6PZWNbqi6R4AoFF/+bX73jJ46rH24OlHfzSl/EiSP0rySNN1AMAPZ3AHgGaUJP8lyav7l93gkVQA4LtKZyTdyV2lNXF2Zh778rrMztyWZCTJHyaZaTgPAHgWBncAaMbPJvnZkQuvTm/T/qZbAIB5p6Q1sS69TfurevpENfv4Q1ekVG9P6vuSfLnpOgDg+zO4A8CZd02S/9qd3F1Gt1+T0x+7AwB8r9LqpLPuorTXbs3M0a9M1KdO3JBke05/7f5kw3kAwN9icAeAM2tXSrmrtWKq1d93UynFn2IA4IerRlemt+lAVdrdzHzrKxck9e1JnknyJ0kGDecBAN/hX/kAcOZsTKk+Vo0sWzZx6LZW6Yw23QMALCSlSnvVlnSn9pTB00c7g6cefWVKdV1SfzbJkabzAACDOwCcKeMp1UdKq71l/NAdrWpsVdM9AMACVTqj6W7cldaKczJ79MjKevrk4SQXJflEkicazgOAJc3gDgBzr51Sfjsph8b331y1V57TdA8AsAi0xteku/lgVapOZo8euTCp78jpx2H+OMlMw3kAsCQZ3AFgbpUk70py/diu60pnw46mewCARaSUKu3V56Y7tbfU336iPfvkN1+WUt2Y1A8kua/pPgBYagzuADC3fj7Jz49se0V6513ZdAsAsEiVzkg6G3amvWZrZo8/OFE/89RbU8rfTfKpJA83nAcAS4bBHQDmzg1J3tWd2pvRHa/N6Y/dAQDmTjW2Mr1N+0s1uiIzR49MZXb69iQbc/q++9MN5wHAomdwB4C58aqkvK+9dlvp73l7Sama7gEAlopS0loxmd6mg1VSl9ljX700Je9MMpvkT+O+OwDMGYM7AAzfvpRyV2v5hvb4gR+vSqvTdA8AsASVVjudtdvSndxdBiePdwdPPfKK79x3fyjJvU33AcBiZHAHgOG6IKW6uxpdOTZx6PZW6Y413QMALHGlO5buxl1przk/s48/NFE/8+SbU8qrknwmyUNN9wHAYmJwB4DhmUqpPl66Y6smXvrOVjW6vOkeAIDvOn3f/UCp+qszc/SB9Zk9dWuSC5L8SZLHG84DgEXB4A4Aw7E2pfpYaXUnxw/d1mqNr226BwDge5WS1vIN6W25vCpVK7PHHrgoqf9+krEkn0zyTMOFALCgGdwB4MVbkVJ9tJTqJf3Lb221V0w23QMA8KxK1Up7zXnpnrOv1NMnWrOPf/2lKdWtSX08yZ8nqZtuBICFyOAOAC9OP6V8KCl7xg/cUrVXn9t0DwDAc1bavXTWXZzOuu0ZPPnI6ODksWtSquuS+gtJ7m+6DwAWGoM7ALxwIynlziRX9C+7sXTOvqDpHgCAF6QamUj3nD2ltXxjZo9/dVU9ffLGpOxL8mdJHmu6DwAWCoM7ALww3ZTyO6nrV47tvr50N+xsugcA4EUqaY2fld7mg1XpjmX26JFzMxjckWR1kk8k+XbDgQAw7xncAeD56yTlfUl9zdiuN6c7tafpHgCA4SlV2is3pbfpQJXZZ6rZx7+2L6W6PamfSPKpJIOmEwFgvipNBwDAAtNO8mtJrhvd+cb0Nh9sugcAYE7NPvlwTn7mf9Uzj36hpFT3ph68M8ndTXcBwHzkC3cAeO7aSf5HkreM7nh9elsub7oHAGDOVb3xdKd2l9byycwee2BVPX3ycFK25/SZmceb7gOA+cTgDgDPTTvJe5K8bXT7a9M796VN9wAAnEElrfG16W0+WKXVy+yxIxck9TuT1Ek+mWSm4UAAmBeclAGAH+4vv2x/6+jF16R33pVN9wAANGpw8vF8+947c+qhP09K9ZXUgzuS3NV0FwA0zRfuAPDsOkl+NclbjO0AAKeVzkg6G3amvfq8zBw7sqw+deLHkrIjyR8meaLpPgBoisEdAH6wblJ+I8mbRre/1tgOAPC3VGOr0tt0oCrtXma/df8FSX17khNJ/iTJoOE8ADjjnJQBgO+vl5TfSurXjO58Y3qbDzbdAwAwrw1OHs/Jv3h/Pf3Nz5aU8unU9c1J/qzpLgA4k3zhDgDfayyl3JnUPzK267r0Nh1ougcAYN4rnZF0N+4qreUbMvPYl9dm9tRPJhlP8gdJphvOA4AzwuAOAH/TspRyV5Irx3a/rXSn9jbdAwCwoLTGz0pv8/4qM8+U2eMPXp5SvSOp/1+SrzTdBgBzzeAOAH9ldUr5/aTs6++9oXQ3XtJ0DwDAglSqdjpnX5j22m2Z+db9E/X0iZuSbEjy8STPNFsHAHPH4A4Ap61Pqe5JaW3v7z9cddZd1HQPAMCCV42uSHfzgarUycyxI7tTqpuS+i+SfLnpNgCYCwZ3AEjOTak+Xlrtc8cP/ETVWXt+0z0AAItGKVXaa89PZ91FZfZb94/Vp56+Icn6JHcnOdVsHQAMl8EdgKVuR0r18dIZOXv88tta7ZXnNN0DALAoVSPL0tu0r0pdZ+bokT0p1duT+o+TPNh0GwAMi8EdgKXsUEr1kao3vmzi0B2t1rJ1TfcAACxupUp77da0z9pWZh790kQ9ffKWJN2cvu0+aLgOAF40gzsAS9VrUsoHq/6a3vihO1pVf3XTPQAAS0Y1uiLdTfuqevpEmT3+tStSyquTfDTJ0abbAODFMLgDsBTdlOS9rRVT1cTlt7WqkYmmewAAlpxStdI5+6K0lk9m5pH7zko9e2uSh5J8uuk2AHihDO4ALCUlyS8k+Y+dsy5I/8AtVemMNN0EALCktcbXpju1p5p94uvtwYmjr0+yLcmHkzzTcBoAPG+l6QAAOENaSf5Dknd2p/Zm7JI3JZX/dwYAmDfqOs98+Z6c/Nz/rpMcST14U5I/azoLAJ4PSwMAS8FoUt6b5MaRrS/P6I7XJaVqugkAgL+ulLRXbT79oOoj9y2rZ079eJLHkvxp02kAo9O4PwAACJ9JREFU8FwZ3AFY7FanlLuS/L3RnW8oI1uvih94AQDMX9XoinSn9laDpx4ug6cefU2SlyT5UJJTDacBwA9lcQBgMTs3pfpQSjm3v/cdVWfd9qZ7AAB4zuo886V7cvJzH6yT8sXUg9clua/pKgB4Nr5wB2Cxuiyluqe0e+vHL7+11Vm7rekeAACel++cmFlzXpn+5r0rMpi5OacH9881XQYAP4jBHYDF6HUp5YPV6MqxiZfe0Wot29B0DwAAL1A1tjLdyd3V7LEj7cHJx9+apJvkniR1w2kA8D0M7gAsNj+d5L+1Vm6qJg7d1qpGVzTdAwDAi1TavXQn95T61InMHn/wipSyL8kHkny76TYA+OvccAdgsWgl+fdJfqqz4ZKM7X5rStVuugkAgCE79dVP5sSnf6tOcn/qwauTfL7pJgD4SwZ3ABaD8ZTy66nrV49svSojF1ydFH/iAAAWq5ljD+TpT7x7tp4+cTJ1fW2S/9N0EwAkTsoAsPBNplQfScqhsUuuLb3zrzK2AwAsctXoinQ3XlrNPPaldv3Mkz+W5FiSP266CwAM7gAsZHtSqntKq7NpfP/NVWfDzqZ7AAA4Q0pnJN2pvWXw9KNl8OTDVyc5O6e/dB80nAbAEmZwB2ChekNK+UA1snxi/NAdrfbKc5ruAQDgDCtVK931O5PUmfnW/XvTbt+dweArTXcBsHT5zT0AC01J8nNJ/k171aZBf9/hqnT7TTcBANCw2ScfzvI1G8a+9pv/6GTTLQAsXe2mAwDgeegl+eUkN3Un92Rs15uqVP6UAQCQtCbObjoBAAzuACwYa1PK+1PXl49ceHVGtl4VP9QCAAAA5hODOwALwfaU6oOlVBvH9l6fzvodTfcAAAAAfA+DOwDz3atTym+Ubn9k/MAtVWv5xqZ7AAAAAL4vgzsA81VJ8o+T/NvW8o11f9/hqhpZ1nQTAAAAwA9kcAdgPvqrx1E3XprRXdeV0uo03QQAAADwrAzuAMw3Z6eU301dHxy58EcysvVl8TgqAAAAsBAY3AGYTy5JqT5Qqtb6sT3Xp7Nue9M9AAAAAM+ZwR2A+eLalPIr1chEp7//lqq1bH3TPQAAAADPi8EdgKZVSf5Zkn/eXrlp0N93U1W6/aabAAAAAJ43gzsATeon5T1J/cbupv0Z2/GGKlWr6SYAAACAF8TgDkBTNqdUdyb1RaPb35DeloPxOCoAAACwkBncAWjClSnV+0u7u6x/2Y2lveb8pnsAAAAAXjSDOwBn2m1JeVervyb9/Te3qv7qpnsAAAAAhsLgDsCZ0k3yS0lu76y7sB7b/bZS2iNNNwEAAAAMjcEdgDNhbVJ+J6lfOrLt5Rl5yatKinvtAAAAwOJicAdgru1Kqe4spVo/tvtt6WzY2XQPAAAAwJwwuAMwl96cUt5TjUy0+/turlrLNzTdAwAAADBnDO4AzIUqyb9I8k/aKzfX/X03ltLtN90EAAAAMKcM7gAM2/KU8qup61f3Nl+e0e2vLalaTTcBAAAAzDmDOwDDtC2lujPJ+WOXvDHdTQea7gEAAAA4YwzuAAzL1SnlfaUzMtrfd7i0V21uugcAAADgjDK4A/BilSQ/l+Rft5ZtqPv7DlfV6PKmmwAAAADOOIM7AC/GWJJ3J3lzd3JPRi+5tpRWp+kmAAAAgEYY3AF4oTanlN9LnYtHL35NeuddkdMfuwMAAAAsTQZ3AF6Il6VUv13a3Yn+3htKe+3WpnsAAAAAGmdwB+D5KEl+Osm/a42vTX//4aoaW910EwAAAMC8YHAH4LkaTfLLSW7obNiRsV1vSWn3mm4CAAAAmDcM7gA8F1Mp5X+mri8dufDqjGy9Ku61AwAAAPxNBncAfpgrU6rfLa3O8rG970jnrAua7gEAAACYlwzuADyblSnVXa3+mm5//+FW1V/TdA8AAADAvGVwB+DZHBvfd/hIa/WWC91rBwAAAHh2BncAnlX7rAufTKmbzgAAAACY96qmAwAAAAAAYDEwuAMAAAAAwBAY3AEAAAAAYAgM7gAAAAAAMAQGdwAAAAAAGAKDOwAAAAAADIHBHQAAAAAAhsDgDgAAAAAAQ2BwBwAAAACAITC4AwAAAADAEBjcAQAAAABgCAzuAAAAAAAwBAZ3AAAAAAAYAoM7AAAAAAAMgcEdAAAAAACGwOAOAAAAAABDYHAHAAAAAIAhMLgDAAAAAMAQGNwBAAAAAGAIDO4AAAAAADAEBncAAAAAABgCgzsAAAAAAAyBwR0AAAAAAIbA4A4AAAAAAENgcAcAAAAAgCEwuAMAAAAAwBAY3AEAAAAAYAgM7gAAAAAAMAQGdwAAAAAAGAKDOwAAAAAADIHBHQAAAAAAhsDgDgAAAAAAQ2BwBwAAAACAITC4AwAAAADAEBjcAQAAAABgCAzuAAAAAAAwBAZ3AAAAAAAYAoM7AAAAAAAMgcEdAAAAAACGwOAOAAAAAABDYHAHAAAAAIAhMLgDAAAAAMAQGNwBAAAAAGAIDO4AAAAAADAEBncAAAAAABgCgzsAAAAAAAyBwR0AAAAAAIbA4A4AAAAAAENgcAcAAAAAgCEwuAMAAAAAwBAY3AEAAAAAYAgM7gAAAAAAMAQGdwAAAAAAGAKDOwAAAAAADIHBHQAAAAAAhsDgDgAAAAAAQ2BwBwAAAACAITC4AwAAAADAEBjcAQAAAABgCAzuAAAAAAAwBAZ3AAAAAAAYAoM7AAAAAAAMgcEdAAAAAACGoN10AADz26Cqf7oM6uVNdwAAwA/ztUyearoBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAhej/A6xvJJBEdrarAAAAAElFTkSuQmCC\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Constant  vb\" class=\"gt_row gt_center\"&gt;1200-1400&lt;/td&gt;\n&lt;td headers=\"Constant  mean\" class=\"gt_row gt_right\"&gt;1,306.99&lt;/td&gt;\n&lt;td headers=\"Constant  sd\" class=\"gt_row gt_right\"&gt;355.81&lt;/td&gt;\n&lt;td headers=\"Constant  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"4\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Varied\"&gt;Varied&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;100-300&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;665.49&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;365.02&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;350-550&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;772.47&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;321.45&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;600-800&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;879.54&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;291.57&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;800-1000&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;1,070.25&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;248.79&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;1000-1200&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;1,180.70&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;265.92&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Varied  vb\" class=\"gt_row gt_center\"&gt;1200-1400&lt;/td&gt;\n&lt;td headers=\"Varied  mean\" class=\"gt_row gt_right\"&gt;1,269.34&lt;/td&gt;\n&lt;td headers=\"Varied  sd\" class=\"gt_row gt_right\"&gt;291.65&lt;/td&gt;\n&lt;td headers=\"Varied  Distribution\" class=\"gt_row gt_center\"&gt;&lt;img src=\"data:image/png;base64,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\" style=\"height:50px;\"&gt;&lt;/td&gt;&lt;/tr&gt;\n  &lt;/tbody&gt;\n  \n  \n&lt;/table&gt;\n&lt;/div&gt;\n\nlibrary(gtExtras)\ngt_two_column_layout(tables)\n\n\n\n\n\n\n\n\nTraining Condit\n\n\nVb\nMean\nSd\nDistribution\n\n\n\n\nConstant\n\n\n100-300\n252.99\n219.63\n\n\n\n350-550\n191.58\n159.52\n\n\n\n600-800\n150.40\n110.83\n\n\n\n800-1000\n188.91\n160.60\n\n\n\n1000-1200\n240.57\n173.05\n\n\n\n1200-1400\n295.42\n186.14\n\n\n\nVaried\n\n\n100-300\n387.25\n343.45\n\n\n\n350-550\n289.01\n272.48\n\n\n\n600-800\n236.17\n188.89\n\n\n\n800-1000\n224.16\n145.95\n\n\n\n1000-1200\n209.20\n130.32\n\n\n\n1200-1400\n242.13\n136.30\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Condit\n\n\nVb\nMean\nSd\nDistribution\n\n\n\n\nConstant\n\n\n100-300\n522.83\n245.10\n\n\n\n350-550\n660.61\n224.30\n\n\n\n600-800\n771.75\n210.50\n\n\n\n800-1000\n1,008.76\n254.58\n\n\n\n1000-1200\n1,173.42\n307.89\n\n\n\n1200-1400\n1,306.99\n355.81\n\n\n\nVaried\n\n\n100-300\n665.49\n365.02\n\n\n\n350-550\n772.47\n321.45\n\n\n\n600-800\n879.54\n291.57\n\n\n\n800-1000\n1,070.25\n248.79\n\n\n\n1000-1200\n1,180.70\n265.92\n\n\n\n1200-1400\n1,269.34\n291.65\n#https://modelsummary.com/articles/datasummary.html\n\ndatasummary(vx*vb ~ Mean + SD + Histogram, data = testAvg)\n\n \n\n  \n    \n\ntinytable_juwvhm9rd06xjvj0682o\n\n\n      \n\n \n                vb\n                Mean\n                SD\n                Histogram\n              \n\n\nvx\n                  100-300  \n                  592.33 \n                  316.56\n                  ▂▆▇▄▂▃▂▁▁ \n                \n\n  \n                  350-550  \n                  715.10 \n                  280.69\n                  ▁▅▇▇▅▃▂▁▁▁\n                \n\n  \n                  600-800  \n                  824.26 \n                  258.15\n                  ▁▅▅▇▅▃▂▁▁▁\n                \n\n  \n                  800-1000 \n                  1038.72\n                  252.85\n                  ▁▃▆▆▇▃▂▁▁ \n                \n\n  \n                  1000-1200\n                  1176.96\n                  287.31\n                  ▁▂▄▆▇▅▃▂▁▁\n                \n\n  \n                  1200-1400\n                  1288.65\n                  325.64\n                  ▂▄▅▇▅▃▁▂▁ \n                \n\n\n\n\n    \n\ndatasummary(vx*vb*condit ~ Mean + SD + Histogram, data = testAvg)\n\n \n\n  \n    \n\ntinytable_it4r53z7k2cw2072wg20\n\n\n      \n\n \n                vb\n                condit\n                Mean\n                SD\n                Histogram\n              \n\n\nvx\n                  100-300  \n                  Constant\n                  522.83 \n                  245.10\n                  ▁▅▅▇▄▃▂▁▂▁\n                \n\n  \n                           \n                  Varied  \n                  665.49 \n                  365.02\n                  ▄▇▇▂▁▅▃▂▂▁\n                \n\n  \n                  350-550  \n                  Constant\n                  660.61 \n                  224.30\n                  ▁▃▅▇▇▃▂▂ ▁\n                \n\n  \n                           \n                  Varied  \n                  772.47 \n                  321.45\n                  ▄▄▇▄▃▂▁▂▁▂\n                \n\n  \n                  600-800  \n                  Constant\n                  771.75 \n                  210.50\n                  ▁▄▆▃▇▄▂▁▂▁\n                \n\n  \n                           \n                  Varied  \n                  879.54 \n                  291.57\n                  ▃▄▄▇▇▄▂▁▂▂\n                \n\n  \n                  800-1000 \n                  Constant\n                  1008.76\n                  254.58\n                  ▂▄▇▆▅▂▁▁  \n                \n\n  \n                           \n                  Varied  \n                  1070.25\n                  248.79\n                  ▁▂▅▃▅▇▄▃▁▂\n                \n\n  \n                  1000-1200\n                  Constant\n                  1173.42\n                  307.89\n                  ▁▂▅▆▇▆▃▂ ▃\n                \n\n  \n                           \n                  Varied  \n                  1180.70\n                  265.92\n                  ▁▂▄▆▇▆▃▃▁▃\n                \n\n  \n                  1200-1400\n                  Constant\n                  1306.99\n                  355.81\n                  ▂▄▆▇▄▃▁▃▁ \n                \n\n  \n                           \n                  Varied  \n                  1269.34\n                  291.65\n                  ▂▄▂▅▆▇▄▂▁ \n                \n\n\n\n\n    \n\ndatasummary(vx*vb*condit ~ Mean + SD + Histogram, data = test)\n\n \n\n  \n    \n\ntinytable_n5imsh2pumt8xwvn58xl\n\n\n      \n\n \n                vb\n                condit\n                Mean\n                SD\n                Histogram\n              \n\n\nvx\n                  100-300  \n                  Constant\n                  524.28 \n                  326.84\n                  ▅▇▆▃▂▁▁  \n                \n\n  \n                           \n                  Varied  \n                  663.96 \n                  448.23\n                  ▅▇▄▃▂▂▁  \n                \n\n  \n                  350-550  \n                  Constant\n                  658.76 \n                  302.96\n                  ▁▅▇▆▄▂▁  \n                \n\n  \n                           \n                  Varied  \n                  767.77 \n                  401.84\n                  ▂▇▆▄▃▂▁▁ \n                \n\n  \n                  600-800  \n                  Constant\n                  770.39 \n                  299.50\n                  ▁▄▇▆▄▃▁  \n                \n\n  \n                           \n                  Varied  \n                  876.49 \n                  389.91\n                  ▁▆▇▆▄▃▂▁ \n                \n\n  \n                  800-1000 \n                  Constant\n                  1000.54\n                  356.85\n                  ▃▇▆▃▂▁   \n                \n\n  \n                           \n                  Varied  \n                  1063.94\n                  369.78\n                  ▂▅▆▇▄▃▁  \n                \n\n  \n                  1000-1200\n                  Constant\n                  1166.65\n                  429.75\n                  ▁▂▅▇▄▂▁  \n                \n\n  \n                           \n                  Varied  \n                  1180.43\n                  372.22\n                  ▁▄▇▇▅▂▁  \n                \n\n  \n                  1200-1400\n                  Constant\n                  1282.55\n                  482.56\n                  ▁▂▄▇▆▃▂▂▁\n                \n\n  \n                           \n                  Varied  \n                  1264.68\n                  411.92\n                  ▁▂▄▇▆▄▃▁ \n                \n\n\n\n\n    \n\ndatasummary_crosstab(vb ~ condit * tOrder, data = test)\n\n \n\n  \n    \n\ntinytable_ktm80sa3y7w1aw9muitx\n\n\n      \n\n\n \n \nConstant\nVaried\n \n\n\nvb\n                 \n                testFirst\n                trainFirst\n                testFirst\n                trainFirst\n                All\n              \n\n\n\n100-300  \n                  N    \n                  671 \n                  493 \n                  564 \n                  544 \n                  2272 \n                \n\n         \n                  % row\n                  29.5\n                  21.7\n                  24.8\n                  23.9\n                  100.0\n                \n\n350-550  \n                  N    \n                  679 \n                  499 \n                  573 \n                  543 \n                  2294 \n                \n\n         \n                  % row\n                  29.6\n                  21.8\n                  25.0\n                  23.7\n                  100.0\n                \n\n600-800  \n                  N    \n                  673 \n                  497 \n                  573 \n                  543 \n                  2286 \n                \n\n         \n                  % row\n                  29.4\n                  21.7\n                  25.1\n                  23.8\n                  100.0\n                \n\n800-1000 \n                  N    \n                  265 \n                  197 \n                  224 \n                  214 \n                  900  \n                \n\n         \n                  % row\n                  29.4\n                  21.9\n                  24.9\n                  23.8\n                  100.0\n                \n\n1000-1200\n                  N    \n                  250 \n                  190 \n                  228 \n                  206 \n                  874  \n                \n\n         \n                  % row\n                  28.6\n                  21.7\n                  26.1\n                  23.6\n                  100.0\n                \n\n1200-1400\n                  N    \n                  244 \n                  187 \n                  226 \n                  208 \n                  865  \n                \n\n         \n                  % row\n                  28.2\n                  21.6\n                  26.1\n                  24.0\n                  100.0\n                \n\nAll      \n                  N    \n                  2782\n                  2063\n                  2388\n                  2258\n                  9491 \n                \n\n         \n                  % row\n                  29.3\n                  21.7\n                  25.2\n                  23.8\n                  100.0\n                \n\n\n\n\n    \n\ndatasummary_crosstab(result ~ condit,\n                     statistic = 1 ~ Percent(\"col\"),\n                     data = test)\n\n \n\n  \n    \n\ntinytable_imhrdydx53crw1625a6w\n\n\n      \n\nresult\n                 \n                Constant\n                Varied\n              \n\n\nHit  \n                  % col\n                  25.0 \n                  21.4 \n                \n\nOver \n                  % col\n                  52.5 \n                  59.0 \n                \n\nUnder\n                  % col\n                  22.6 \n                  19.7 \n                \n\nAll  \n                  % col\n                  100.0\n                  100.0\n                \n\n\n\n\n    \n\ndatasummary_crosstab(result ~ condit*vb,\n                     statistic = 1 ~ Percent(\"col\"),\n                     data = test)\n\n \n\n  \n    \n\ntinytable_pqwb8xznacswtlbqq0q3\n\n\n      \n\n\n \n \nConstant\nVaried\n\n\nresult\n                 \n                100-300\n                350-550\n                600-800\n                800-1000\n                1000-1200\n                1200-1400\n                100-300\n                350-550\n                600-800\n                800-1000\n                1000-1200\n                1200-1400\n              \n\n\n\nHit  \n                  % col\n                  25.3 \n                  24.7 \n                  28.1 \n                  28.1 \n                  21.8 \n                  16.2 \n                  20.1 \n                  24.7 \n                  20.7 \n                  19.4 \n                  21.0 \n                  19.8 \n                \n\nOver \n                  % col\n                  71.6 \n                  60.1 \n                  40.4 \n                  42.4 \n                  41.6 \n                  34.6 \n                  77.9 \n                  63.7 \n                  51.8 \n                  53.9 \n                  47.2 \n                  33.9 \n                \n\nUnder\n                  % col\n                  3.2  \n                  15.2 \n                  31.5 \n                  29.4 \n                  36.6 \n                  49.2 \n                  2.0  \n                  11.6 \n                  27.5 \n                  26.7 \n                  31.8 \n                  46.3 \n                \n\nAll  \n                  % col\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                  100.0\n                \n\n\n\n\n    \n\ndatasummary(vx+dist+vy ~ mean *condit*vb,\n            data = e1)\n\n \n\n  \n    \n\ntinytable_y4170hvkvcrdvo3kq58z\n\n\n      \n\n\n \nConstant\nVaried\n\n\n \n                100-300\n                350-550\n                600-800\n                800-1000\n                1000-1200\n                1200-1400\n                100-300\n                350-550\n                600-800\n                800-1000\n                1000-1200\n                1200-1400\n              \n\n\n\nvx  \n                  509.05\n                  644.81\n                  760.25\n                  927.21\n                  1166.65\n                  1282.55\n                  611.22\n                  733.03\n                  848.60\n                  1012.53\n                  1103.13\n                  1185.13\n                \n\ndist\n                  240.32\n                  178.74\n                  145.34\n                  137.17\n                  232.80 \n                  287.09 \n                  338.27\n                  258.83\n                  215.48\n                  204.68 \n                  204.51 \n                  260.39 \n                \n\nvy  \n                  81.42 \n                  115.76\n                  157.74\n                  206.72\n                  244.71 \n                  276.07 \n                  92.91 \n                  126.21\n                  158.92\n                  189.22 \n                  209.89 \n                  230.23 \n                \n\n\n\n\n    \n\ndatasummary(Heading(\"X Velocity\")*vx+Heading(\"Abs. Deviation\")*dist+vy ~ mean *condit*vb,\n            data = e1)\n\n \n\n  \n    \n\ntinytable_4non5ske3hez49i0roj6\n\n\n      \n\n\n \nConstant\nVaried\n\n\n \n                100-300\n                350-550\n                600-800\n                800-1000\n                1000-1200\n                1200-1400\n                100-300\n                350-550\n                600-800\n                800-1000\n                1000-1200\n                1200-1400\n              \n\n\n\nX Velocity    \n                  509.05\n                  644.81\n                  760.25\n                  927.21\n                  1166.65\n                  1282.55\n                  611.22\n                  733.03\n                  848.60\n                  1012.53\n                  1103.13\n                  1185.13\n                \n\nAbs. Deviation\n                  240.32\n                  178.74\n                  145.34\n                  137.17\n                  232.80 \n                  287.09 \n                  338.27\n                  258.83\n                  215.48\n                  204.68 \n                  204.51 \n                  260.39 \n                \n\nvy            \n                  81.42 \n                  115.76\n                  157.74\n                  206.72\n                  244.71 \n                  276.07 \n                  92.91 \n                  126.21\n                  158.92\n                  189.22 \n                  209.89 \n                  230.23 \n                \n\n\n\n\n    \n\ndatasummary(vx + dist ~ Factor(condit) * (mean + sd),\n            data = test)\n\n \n\n  \n    \n\ntinytable_4z0355xhpo0drg1ofci0\n\n\n      \n\n\n \nConstant\nVaried\n\n\n \n                mean\n                sd\n                mean\n                sd\n              \n\n\n\nvx  \n                  787.61\n                  423.16\n                  882.02\n                  452.04\n                \n\ndist\n                  207.76\n                  254.08\n                  279.52\n                  329.31\n                \n\n\n\n\n    \n\ndatasummary(vx + dist ~ Factor(condit)*vb * (mean + sd),\n            data = test)\n\n \n\n  \n    \n\ntinytable_8i9iur34tx3jctr69hbn\n\n\n      \n\n\n \nConstant\nVaried\n\n\n \n100-300\n350-550\n600-800\n800-1000\n1000-1200\n1200-1400\n100-300\n350-550\n600-800\n800-1000\n1000-1200\n1200-1400\n\n\n \n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n                mean\n                sd\n              \n\n\n\nvx  \n                  524.28\n                  326.84\n                  658.76\n                  302.96\n                  770.39\n                  299.50\n                  1000.54\n                  356.85\n                  1166.65\n                  429.75\n                  1282.55\n                  482.56\n                  663.96\n                  448.23\n                  767.77\n                  401.84\n                  876.49\n                  389.91\n                  1063.94\n                  369.78\n                  1180.43\n                  372.22\n                  1264.68\n                  411.92\n                \n\ndist\n                  253.85\n                  297.56\n                  190.57\n                  229.23\n                  150.01\n                  184.41\n                  183.91 \n                  241.99\n                  232.80 \n                  282.14\n                  287.09 \n                  290.25\n                  385.65\n                  426.23\n                  285.04\n                  340.41\n                  234.14\n                  269.93\n                  220.87 \n                  248.39\n                  207.87 \n                  226.43\n                  241.93 \n                  234.68\n                \n\n\n\n\n    \n\ndatasummary(vb*Factor(condit) * (mean + sd) ~ vx + dist,\n            data = test)\n\n \n\n  \n    \n\ntinytable_gw4ts13t5p1p7bid90b1\n\n\n      \n\nvb\n                condit\n                 \n                vx\n                dist\n              \n\n\n100-300  \n                  Constant\n                  mean\n                  524.28 \n                  253.85\n                \n\n         \n                          \n                  sd  \n                  326.84 \n                  297.56\n                \n\n         \n                  Varied  \n                  mean\n                  663.96 \n                  385.65\n                \n\n         \n                          \n                  sd  \n                  448.23 \n                  426.23\n                \n\n350-550  \n                  Constant\n                  mean\n                  658.76 \n                  190.57\n                \n\n         \n                          \n                  sd  \n                  302.96 \n                  229.23\n                \n\n         \n                  Varied  \n                  mean\n                  767.77 \n                  285.04\n                \n\n         \n                          \n                  sd  \n                  401.84 \n                  340.41\n                \n\n600-800  \n                  Constant\n                  mean\n                  770.39 \n                  150.01\n                \n\n         \n                          \n                  sd  \n                  299.50 \n                  184.41\n                \n\n         \n                  Varied  \n                  mean\n                  876.49 \n                  234.14\n                \n\n         \n                          \n                  sd  \n                  389.91 \n                  269.93\n                \n\n800-1000 \n                  Constant\n                  mean\n                  1000.54\n                  183.91\n                \n\n         \n                          \n                  sd  \n                  356.85 \n                  241.99\n                \n\n         \n                  Varied  \n                  mean\n                  1063.94\n                  220.87\n                \n\n         \n                          \n                  sd  \n                  369.78 \n                  248.39\n                \n\n1000-1200\n                  Constant\n                  mean\n                  1166.65\n                  232.80\n                \n\n         \n                          \n                  sd  \n                  429.75 \n                  282.14\n                \n\n         \n                  Varied  \n                  mean\n                  1180.43\n                  207.87\n                \n\n         \n                          \n                  sd  \n                  372.22 \n                  226.43\n                \n\n1200-1400\n                  Constant\n                  mean\n                  1282.55\n                  287.09\n                \n\n         \n                          \n                  sd  \n                  482.56 \n                  290.25\n                \n\n         \n                  Varied  \n                  mean\n                  1264.68\n                  241.93\n                \n\n         \n                          \n                  sd  \n                  411.92 \n                  234.68\n                \n\n\n\n\n    \n\ntmp &lt;- mtcars[, c(\"mpg\", \"hp\")]\n\n# create a list with individual variables\n# remove missing and rescale\ntmp_list &lt;- lapply(tmp, na.omit)\ntmp_list &lt;- lapply(tmp_list, scale)\n\n# create a table with `datasummary`\n# add a histogram with column_spec and spec_hist\n# add a boxplot with colun_spec and spec_box\nemptycol = function(x) \" \"\ndatasummary(mpg + hp ~ Mean + SD + Heading(\"Boxplot\") * emptycol + Heading(\"Histogram\") * emptycol,\n    output = \"kableExtra\",\n    data = tmp) %&gt;%\n    kableExtra::column_spec(column = 4, image = spec_boxplot(tmp_list)) %&gt;%\n    kableExtra::column_spec(column = 5, image = spec_hist(tmp_list))\n\n\n\n\n\n\n\n\n\n\n\nMean\nSD\nBoxplot\nHistogram\n\n\n\nmpg\n20.09\n6.03\n\n\n\n\nhp\n146.69\n68.56\n\n\n\n\n\n\ntmp_list &lt;- lapply(test |&gt; ungroup() |&gt; select(vx,vb,condit), na.omit)\n# scale only numeric columns - not all are numeric so can't just lapply\n\ntmp_list &lt;- map(tmp_list, ~if(is.numeric(.x)) scale(.x) else .x)\n\nts=test |&gt; ungroup() |&gt; select(vx,vb,condit) |&gt; mutate(vxScale=scale(vx)[,1])\n\n datasummary(vxScale*vb ~  Mean + Histogram,\n            data=ts, output = \"kableExtra\")  %&gt;%\n    kableExtra::column_spec(column = 4, image = spec_boxplot(ts))\n\nWarning in ensure_len_html(image, nrows, \"image\"): The number of provided\nvalues in image does not equal to the number of rows.\n\n\n\n\n\n\n\n\n\n\n\nvb\nMean\nHistogram\n\n\n\nvxScale\n100-300\n−0.55\n▆▇▄▂▁▁\n\n\n\n\n\n350-550\n−0.28\n▂▇▇▅▃▁▁\n\n\n\n\n\n600-800\n−0.03\n▁▅▇▅▃▂▁\n\n\n\n\n\n800-1000\n0.45\n▁▃▇▇▄▂▁\n\n\n\n\n\n1000-1200\n0.77\n▂▅▇▅▂▁\n\n\n\n\n\n1200-1400\n1.00\n▂▄▇▆▄▂▁\n\n\n\n\n\n\ndatasummary(vx*vb ~  Mean + Median+ SD + Histogram,\n            data=test, output = \"markdown\")  \n\n \n\n  \n    \n\ntinytable_jn1ng5p3qezj9w6ckgnw\n\n\n      \n\n \n                vb\n                Mean\n                Median\n                SD\n                Histogram\n              \n\n\nvx\n                  100-300  \n                  592.40 \n                  477.71 \n                  396.88\n                  ▆▇▄▂▁▁ \n                \n\n  \n                  350-550  \n                  711.79 \n                  646.42 \n                  358.61\n                  ▂▇▇▅▃▁▁\n                \n\n  \n                  600-800  \n                  822.19 \n                  764.23 \n                  350.56\n                  ▁▅▇▅▃▂▁\n                \n\n  \n                  800-1000 \n                  1031.39\n                  988.06 \n                  364.38\n                  ▁▃▇▇▄▂▁\n                \n\n  \n                  1000-1200\n                  1173.49\n                  1144.20\n                  402.04\n                  ▂▅▇▅▂▁ \n                \n\n  \n                  1200-1400\n                  1273.58\n                  1237.95\n                  448.34\n                  ▂▄▇▆▄▂▁\n                \n\n\n\n\n    \n\ndatasummary(dist*vb ~  Mean + Median+ SD + Histogram,\n            data=test, output = \"markdown\")  \n\n \n\n  \n    \n\ntinytable_sllabca9k765gtga8x6k\n\n\n      \n\n \n                vb\n                Mean\n                Median\n                SD\n                Histogram\n              \n\n\ndist\n                  100-300  \n                  318.12\n                  177.71\n                  371.81\n                  ▇▂▁▁▁ \n                \n\n    \n                  350-550  \n                  236.53\n                  126.28\n                  292.49\n                  ▇▂▁▁  \n                \n\n    \n                  600-800  \n                  191.08\n                  111.93\n                  233.93\n                  ▇▃▁▁  \n                \n\n    \n                  800-1000 \n                  201.90\n                  131.15\n                  245.69\n                  ▇▃▁▁  \n                \n\n    \n                  1000-1200\n                  220.42\n                  149.37\n                  256.15\n                  ▇▃▁▁  \n                \n\n    \n                  1200-1400\n                  264.43\n                  197.72\n                  264.65\n                  ▇▄▂▂▁▁\n                \n\n\n\n\n    \n\ndatasummary(vx*vb*condit ~  Mean + Histogram,\n            data=test, output = \"kableExtra\")  %&gt;%\n    kableExtra::column_spec(column = 5, image = spec_boxplot(tmp_list)) \n\nWarning in ensure_len_html(image, nrows, \"image\"): The number of provided\nvalues in image does not equal to the number of rows.\n\n\n\n\n\n\n\n\n\n\n\n\nvb\ncondit\nMean\nHistogram\n\n\n\nvx\n100-300\nConstant\n524.28\n▅▇▆▃▂▁▁\n\n\n\n\n\n\nVaried\n663.96\n▅▇▄▃▂▂▁\n\n\n\n\n\n350-550\nConstant\n658.76\n▁▅▇▆▄▂▁\n\n\n\n\n\n\nVaried\n767.77\n▂▇▆▄▃▂▁▁\n\n\n\n\n\n600-800\nConstant\n770.39\n▁▄▇▆▄▃▁\n\n\n\n\n\n\nVaried\n876.49\n▁▆▇▆▄▃▂▁\n\n\n\n\n\n800-1000\nConstant\n1000.54\n▃▇▆▃▂▁\n\n\n\n\n\n\nVaried\n1063.94\n▂▅▆▇▄▃▁\n\n\n\n\n\n1000-1200\nConstant\n1166.65\n▁▂▅▇▄▂▁\n\n\n\n\n\n\nVaried\n1180.43\n▁▄▇▇▅▂▁\n\n\n\n\n\n1200-1400\nConstant\n1282.55\n▁▂▄▇▆▃▂▂▁\n\n\n\n\n\n\nVaried\n1264.68\n▁▂▄▇▆▄▃▁\n\n\n\n\n\n\ncap &lt;- \"Testing - No Feedback\"\nf &lt;- (`Condit` = condit) ~ (` ` = vx) * (`Distribution` = Histogram) + vx * vb * ((`Avg.` = Mean)*Arguments(fmt='%.0f') + (`SD` = SD)*Arguments(fmt='%.0f'))\n\ndatasummary(f,\n            data = test,\n            output = 'gt',\n            title = cap,\n            notes = 'Artwork by @Thomas',\n            sparse_header = TRUE) \n\n\n\n\nTesting - No Feedback\n\n\nCondit\nDistribution\n100-300 \n350-550 \n600-800 \n800-1000 \n1000-1200 \n1200-1400 \n\n\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\n\n\n\n\nConstant\n▃▇▇▄▂▁\n524\n327\n659\n303\n770\n300\n1001\n357\n1167\n430\n1283\n483\n\n\nVaried\n▃▇▇▆▄▃▁\n664\n448\n768\n402\n876\n390\n1064\n370\n1180\n372\n1265\n412\n\n\n\nArtwork by @Thomas\n\n\n\n\nf &lt;- (`Band` = vb) ~ (` ` = vx) * (`Distribution` = Histogram) + vx * condit * ((`Avg.` = Mean)*Arguments(fmt='%.0f') + (`SD` = SD)*Arguments(fmt='%.0f'))\n\ndatasummary(f,\n            data = test,\n            output = 'gt',\n            title = cap,\n            notes = 'Artwork by @Thomas',\n            sparse_header = TRUE) \n\n\n\n\nTesting - No Feedback\n\n\nBand\nDistribution\nConstant \nVaried \n\n\nAvg.\nSD\nAvg.\nSD\n\n\n\n\n100-300\n▆▇▄▂▁▁\n524\n327\n664\n448\n\n\n350-550\n▂▇▇▅▃▁▁\n659\n303\n768\n402\n\n\n600-800\n▁▅▇▅▃▂▁\n770\n300\n876\n390\n\n\n800-1000\n▁▃▇▇▄▂▁\n1001\n357\n1064\n370\n\n\n1000-1200\n▂▅▇▅▂▁\n1167\n430\n1180\n372\n\n\n1200-1400\n▂▄▇▆▄▂▁\n1283\n483\n1265\n412\n\n\n\nArtwork by @Thomas\n\n\n\n\nf &lt;- (`Band` = vb) ~ (` ` = vx) * (`Distribution` = Histogram) + vx * condit * expMode2 * ((`Avg.` = Mean)*Arguments(fmt='%.0f') + (`SD` = SD)*Arguments(fmt='%.0f'))\n\ndatasummary(f,\n            data = e1,\n            output = 'gt',\n            sparse_header = TRUE) \n\n\n\n\n\n\n\nConstant \nVaried \n\n\n \nTrain \nTrain-Nf \nTest \nTest-Fb \nTrain \nTrain-Nf \nTest \nTest-Fb \n\n\nBand\nDistribution\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\nAvg.\nSD\n\n\n\n\n100-300\n▇▇▄▂▁▁\n\n\n599\n368\n524\n327\n395\n265\n\n\n652\n413\n664\n448\n484\n406\n\n\n350-550\n▂▇▅▂▁▁\n\n\n714\n332\n659\n303\n554\n265\n\n\n754\n395\n768\n402\n655\n379\n\n\n600-800\n▁▅▇▄▂▁\n\n\n816\n321\n770\n300\n688\n267\n\n\n882\n393\n876\n390\n770\n354\n\n\n800-1000\n▂▇▄▁\n922\n298\n\n\n1001\n357\n\n\n1002\n369\n\n\n1064\n370\n\n\n\n\n1000-1200\n▂▆▇▃▁\n\n\n\n\n1167\n430\n\n\n1087\n383\n\n\n1180\n372\n\n\n\n\n1200-1400\n▁▂▅▇▅▂▁\n\n\n\n\n1283\n483\n\n\n1169\n430\n\n\n1265\n412\n\n\n\n\n\n\n\nf &lt;- (`Band` = vb)* expMode2 ~ vx * condit  * ((`Avg.` = Mean)*Arguments(fmt='%.0f') + (`SD` = SD)*Arguments(fmt='%.0f'))\n\ndatasummary(f,\n            data = e1,\n            output = 'gt',\n            sparse_header = TRUE) \n\n\n\n\n\n\nBand\nexpMode2\nConstant \nVaried \n\n\nAvg.\nSD\nAvg.\nSD\n\n\n\n\n100-300\nTrain\n\n\n\n\n\n\n\nTrain-Nf\n599\n368\n652\n413\n\n\n\nTest\n524\n327\n664\n448\n\n\n\nTest-Fb\n395\n265\n484\n406\n\n\n350-550\nTrain\n\n\n\n\n\n\n\nTrain-Nf\n714\n332\n754\n395\n\n\n\nTest\n659\n303\n768\n402\n\n\n\nTest-Fb\n554\n265\n655\n379\n\n\n600-800\nTrain\n\n\n\n\n\n\n\nTrain-Nf\n816\n321\n882\n393\n\n\n\nTest\n770\n300\n876\n390\n\n\n\nTest-Fb\n688\n267\n770\n354\n\n\n800-1000\nTrain\n922\n298\n1002\n369\n\n\n\nTrain-Nf\n\n\n\n\n\n\n\nTest\n1001\n357\n1064\n370\n\n\n\nTest-Fb\n\n\n\n\n\n\n1000-1200\nTrain\n\n\n1087\n383\n\n\n\nTrain-Nf\n\n\n\n\n\n\n\nTest\n1167\n430\n1180\n372\n\n\n\nTest-Fb\n\n\n\n\n\n\n1200-1400\nTrain\n\n\n1169\n430\n\n\n\nTrain-Nf\n\n\n\n\n\n\n\nTest\n1283\n483\n1265\n412\n\n\n\nTest-Fb\n\n\n\n\n\n\n\n\n\nf &lt;- (`Band` = vb)* expMode2 ~ dist * condit  * ((`Avg.` = Mean)*Arguments(fmt='%.0f') )\n\ndatasummary(f,\n            data = e1,\n            output = 'gt',\n            sparse_header = TRUE) \n\n\n\n\n\n\nBand\nexpMode2\nConstant \nVaried \n\n\nAvg.\nAvg.\n\n\n\n\n100-300\nTrain\n\n\n\n\n\nTrain-Nf\n321\n374\n\n\n\nTest\n254\n386\n\n\n\nTest-Fb\n138\n225\n\n\n350-550\nTrain\n\n\n\n\n\nTrain-Nf\n229\n281\n\n\n\nTest\n191\n285\n\n\n\nTest-Fb\n110\n194\n\n\n600-800\nTrain\n\n\n\n\n\nTrain-Nf\n169\n231\n\n\n\nTest\n150\n234\n\n\n\nTest-Fb\n115\n169\n\n\n800-1000\nTrain\n134\n201\n\n\n\nTrain-Nf\n\n\n\n\n\nTest\n184\n221\n\n\n\nTest-Fb\n\n\n\n\n1000-1200\nTrain\n\n204\n\n\n\nTrain-Nf\n\n\n\n\n\nTest\n233\n208\n\n\n\nTest-Fb\n\n\n\n\n1200-1400\nTrain\n\n264\n\n\n\nTrain-Nf\n\n\n\n\n\nTest\n287\n242\n\n\n\nTest-Fb\n# A tibble: 6 × 5\n  Band      `Band Type`    Mean Median    Sd\n  &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 100-300   Extrapolation   524    448   327\n2 350-550   Extrapolation   659    624   303\n3 600-800   Extrapolation   770    724   300\n4 800-1000  Trained        1001    940   357\n5 1000-1200 Extrapolation  1167   1104   430\n6 1200-1400 Extrapolation  1283   1225   483\n# A tibble: 6 × 5\n  Band      `Band Type`    Mean Median    Sd\n  &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 100-300   Extrapolation   664    533   448\n2 350-550   Extrapolation   768    677   402\n3 600-800   Extrapolation   876    813   390\n4 800-1000  Trained        1064   1029   370\n5 1000-1200 Trained        1180   1179   372\n6 1200-1400 Trained        1265   1249   412\nnb=5\nvt1=e1 |&gt; filter(expMode==\"train\") |&gt;\n  learn_curve_table(gt.train,vx,gw=Trial_Bin,groupVec=c(id,vb,condit),nbins=nb,prefix=\"Block_\") |&gt;\n  rename(\"Band\"=vb,\"Group\"=condit)\n  \nvt1 %&gt;% kable() %&gt;% add_header_above(c(\" \"=2, \"Training Block \"=1,\" \" = ncol(vt1)-3))\n\n\nMean Vx over blocks. Mean (Standard Error)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Block\n\n\n\n\nBand\nGroup\nBlock_1\nBlock_2\nBlock_3\nBlock_4\nBlock_5\n\n\n\n\n800-1000\nConstant\n921 (11)\n941 (8)\n912 (7)\n907 (7)\n931 (7)\n\n\n800-1000\nVaried\n970 (20)\n975 (17)\n1017 (18)\n1020 (17)\n1029 (16)\n\n\n1000-1200\nVaried\n1068 (22)\n1100 (19)\n1080 (19)\n1110 (17)\n1080 (15)\n\n\n1200-1400\nVaried\n1109 (24)\n1183 (19)\n1160 (23)\n1198 (20)\n1200 (18)\n\n\n\n\n# vt1 %&gt;% gt() %&gt;% tab_options(column_labels.background.color = \"#176940\",\n#                 table.font.size = px(14))",
    "crumbs": [
      "Analyses",
      "E1 Extras"
    ]
  },
  {
    "objectID": "Analysis/e1_Analysis.html#testing",
    "href": "Analysis/e1_Analysis.html#testing",
    "title": "E1 Extras",
    "section": "Testing",
    "text": "Testing\n\n\nVaried vs. Constant\nDist\nVx\nSigned Distance\nPercent Hit\nTables\n\n\n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  \n  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\n\n\n\n\n\n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\n\n\n\n# create a kable table to mirror plot of distance effects for vb and condit \n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(distMean=mean(dist),distSd=sd(dist)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(distMean,0)),sdLab=paste0(\"Sd=\",round(distSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=254 Sd=298\nMean=386 Sd=426\n\n\n350-550\nMean=191 Sd=229\nMean=285 Sd=340\n\n\n600-800\nMean=150 Sd=184\nMean=234 Sd=270\n\n\n800-1000\nMean=184 Sd=242\nMean=221 Sd=248\n\n\n1000-1200\nMean=233 Sd=282\nMean=208 Sd=226\n\n\n1200-1400\nMean=287 Sd=290\nMean=242 Sd=235\n\n\n\n\n\n\n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  ggplot(aes(x = vb, y = vx,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\n\n\n\ntestAvg |&gt;  ggplot(aes(x = vb, y = vx,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\n\n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(vxMean=mean(vx),vxSd=sd(vx)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(vxMean,0)),sdLab=paste0(\"Sd=\",round(vxSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=524 Sd=327\nMean=664 Sd=448\n\n\n350-550\nMean=659 Sd=303\nMean=768 Sd=402\n\n\n600-800\nMean=770 Sd=300\nMean=876 Sd=390\n\n\n800-1000\nMean=1001 Sd=357\nMean=1064 Sd=370\n\n\n1000-1200\nMean=1167 Sd=430\nMean=1180 Sd=372\n\n\n1200-1400\nMean=1283 Sd=483\nMean=1265 Sd=412\n\n\n\n\n\n\n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  ggplot(aes(x = vb, y = sdist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\n\n\n\ntestAvg |&gt;  ggplot(aes(x = vb, y = sdist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\n\n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(sdistMean=mean(sdist),sdistSd=sd(sdist)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(sdistMean,0)),sdLab=paste0(\"Sd=\",round(sdistSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=252 Sd=299\nMean=385 Sd=427\n\n\n350-550\nMean=163 Sd=250\nMean=265 Sd=356\n\n\n600-800\nMean=62 Sd=229\nMean=152 Sd=323\n\n\n800-1000\nMean=89 Sd=291\nMean=136 Sd=303\n\n\n1000-1200\nMean=64 Sd=360\nMean=66 Sd=300\n\n\n1200-1400\nMean=-3 Sd=408\nMean=-25 Sd=336\n\n\n\n\n\n\n\n\ntestAvg |&gt;  ggplot(aes(x = vb, y = Percent_Hit,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\n\n\n\ntestAvg |&gt; group_by(vb,condit) %&gt;% \n  summarise(Percent_HitMean=mean(Percent_Hit),Percent_HitSd=sd(Percent_Hit)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(Percent_HitMean,3)),sdLab=paste0(\"Sd=\",round(Percent_HitSd,2))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=0.252 Sd=0.26\nMean=0.201 Sd=0.26\n\n\n350-550\nMean=0.246 Sd=0.19\nMean=0.245 Sd=0.21\n\n\n600-800\nMean=0.281 Sd=0.17\nMean=0.206 Sd=0.16\n\n\n800-1000\nMean=0.278 Sd=0.23\nMean=0.19 Sd=0.19\n\n\n1000-1200\nMean=0.21 Sd=0.2\nMean=0.206 Sd=0.2\n\n\n1200-1400\nMean=0.161 Sd=0.17\nMean=0.2 Sd=0.19\n\n\n\n\n\n\n\nBandType\n\ncreate_table(test, \"vx\")\n\n\nSummary of vx\n\n\n\n\n\n\n\nBand\nBand Type\nConstant\nBand Type\nVaried\n\n\n\n\n100-300\nExtrapolation\nMean=523 SD=245\nExtrapolation\nMean=665 SD=365\n\n\n350-550\nExtrapolation\nMean=661 SD=224\nExtrapolation\nMean=772 SD=321\n\n\n600-800\nExtrapolation\nMean=772 SD=210\nExtrapolation\nMean=880 SD=292\n\n\n800-1000\nTrained\nMean=1009 SD=255\nTrained\nMean=1070 SD=249\n\n\n1000-1200\nExtrapolation\nMean=1173 SD=308\nTrained\nMean=1181 SD=266\n\n\n1200-1400\nExtrapolation\nMean=1307 SD=356\nTrained\nMean=1269 SD=292\n\n\n\n\ncreate_table(test, \"Percent_Hit\")\n\n\nSummary of Percent_Hit\n\n\n\n\n\n\n\nBand\nBand Type\nConstant\nBand Type\nVaried\n\n\n\n\n100-300\nExtrapolation\nMean=0.252 SD=0.26\nExtrapolation\nMean=0.201 SD=0.26\n\n\n350-550\nExtrapolation\nMean=0.246 SD=0.19\nExtrapolation\nMean=0.245 SD=0.21\n\n\n600-800\nExtrapolation\nMean=0.281 SD=0.17\nExtrapolation\nMean=0.206 SD=0.16\n\n\n800-1000\nTrained\nMean=0.278 SD=0.23\nTrained\nMean=0.19 SD=0.19\n\n\n1000-1200\nExtrapolation\nMean=0.21 SD=0.2\nTrained\nMean=0.206 SD=0.2\n\n\n1200-1400\nExtrapolation\nMean=0.161 SD=0.17\nTrained\nMean=0.2 SD=0.19\n\n\n\n\n\nAggregation\n\ne1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;%\n  group_by(vb, condit, bandType) %&gt;%\n  summarise(distMean = mean(dist), distSd = sd(dist)) %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(distMean, 0)),\n    sdLab = paste0(\"Sd=\", round(distSd, 0))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  pivot_wider(\n    id_cols = c(vb, bandType),\n    names_from = condit,\n    values_from = sumStatLab\n  ) %&gt;%\n  arrange(vb, bandType) %&gt;%\n  kable(format = \"html\", escape = FALSE)\n\n\n\nvb\nbandType\nConstant\nVaried\n\n\n\n100-300\nExtrapolation\nMean=254 Sd=298\nMean=386 Sd=426\n\n\n350-550\nExtrapolation\nMean=191 Sd=229\nMean=285 Sd=340\n\n\n600-800\nExtrapolation\nMean=150 Sd=184\nMean=234 Sd=270\n\n\n800-1000\nTrained\nMean=184 Sd=242\nMean=221 Sd=248\n\n\n1000-1200\nTrained\nNA\nMean=208 Sd=226\n\n\n1000-1200\nExtrapolation\nMean=233 Sd=282\nNA\n\n\n1200-1400\nTrained\nNA\nMean=242 Sd=235\n\n\n1200-1400\nExtrapolation\nMean=287 Sd=290\nNA\n\n\n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(vxMean=mean(vx),vxSd=sd(vx)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(vxMean,0)),sdLab=paste0(\"Sd=\",round(vxSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE,caption = \"Vx Mean\") %&gt;% \n  kable_styling(font_size = 10)\n\n\nVx Mean\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=524 Sd=327\nMean=664 Sd=448\n\n\n350-550\nMean=659 Sd=303\nMean=768 Sd=402\n\n\n600-800\nMean=770 Sd=300\nMean=876 Sd=390\n\n\n800-1000\nMean=1001 Sd=357\nMean=1064 Sd=370\n\n\n1000-1200\nMean=1167 Sd=430\nMean=1180 Sd=372\n\n\n1200-1400\nMean=1283 Sd=483\nMean=1265 Sd=412\n\n\n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(sdistMean=mean(sdist),sdistSd=sd(sdist)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(sdistMean,0)),sdLab=paste0(\"Sd=\",round(sdistSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE,caption = \"Signed Deviation Mean\") %&gt;% \n  kable_styling(font_size = 10)\n\n\nSigned Deviation Mean\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=252 Sd=299\nMean=385 Sd=427\n\n\n350-550\nMean=163 Sd=250\nMean=265 Sd=356\n\n\n600-800\nMean=62 Sd=229\nMean=152 Sd=323\n\n\n800-1000\nMean=89 Sd=291\nMean=136 Sd=303\n\n\n1000-1200\nMean=64 Sd=360\nMean=66 Sd=300\n\n\n1200-1400\nMean=-3 Sd=408\nMean=-25 Sd=336\n\n\n\n\ntestAvg |&gt; group_by(vb,condit) %&gt;% \n  summarise(Percent_HitMean=mean(Percent_Hit),Percent_HitSd=sd(Percent_Hit)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(Percent_HitMean,3)),sdLab=paste0(\"Sd=\",round(Percent_HitSd,2))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE,caption = \"Mean % Hit\") %&gt;% \n  kable_styling(font_size = 10)\n\n\nMean % Hit\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=0.252 Sd=0.26\nMean=0.201 Sd=0.26\n\n\n350-550\nMean=0.246 Sd=0.19\nMean=0.245 Sd=0.21\n\n\n600-800\nMean=0.281 Sd=0.17\nMean=0.206 Sd=0.16\n\n\n800-1000\nMean=0.278 Sd=0.23\nMean=0.19 Sd=0.19\n\n\n1000-1200\nMean=0.21 Sd=0.2\nMean=0.206 Sd=0.2\n\n\n1200-1400\nMean=0.161 Sd=0.17\nMean=0.2 Sd=0.19\n\n\n\n\n\nTables Aggregating by Id first\n\ne1 %&gt;% group_by(id, vb, condit) %&gt;%\n  summarise(dist = mean(dist), .groups = 'drop') %&gt;%\n  group_by(vb, condit) %&gt;%\n  summarise(distMean = mean(dist), distSd = sd(dist), .groups = 'drop') %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(distMean, 0)),\n    sdLab = paste0(\"Sd=\", round(distSd, 0))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  select(vb, condit, sumStatLab) %&gt;%\n  spread(condit, sumStatLab) %&gt;%\n  kable(format = \"html\", escape = FALSE, caption = \"Deviation Mean - Aggregate by Id\") %&gt;%\n  kable_styling(font_size = 11)\n\n\nDeviation Mean - Aggregate by Id\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=241 Sd=174\nMean=340 Sd=264\n\n\n350-550\nMean=180 Sd=121\nMean=261 Sd=202\n\n\n600-800\nMean=146 Sd=76\nMean=218 Sd=145\n\n\n800-1000\nMean=138 Sd=57\nMean=206 Sd=86\n\n\n1000-1200\nMean=241 Sd=173\nMean=206 Sd=74\n\n\n1200-1400\nMean=295 Sd=186\nMean=261 Sd=70\n\n\n\n\ne1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(vx = mean(vx), .groups = 'drop') %&gt;%\n  group_by(vb, condit) %&gt;%\n  summarise(vxMean = mean(vx), vxSd = sd(vx), .groups = 'drop') %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(vxMean, 0)),\n    sdLab = paste0(\"Sd=\", round(vxSd, 0))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  select(vb, condit, sumStatLab) %&gt;%\n  spread(condit, sumStatLab) %&gt;%\n  kable(format = \"html\", escape = FALSE, caption = \"Vx Mean - Aggregate by Id\") %&gt;%\n  kable_styling(font_size = 11)\n\n\nVx Mean - Aggregate by Id\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=523 Sd=245\nMean=665 Sd=365\n\n\n350-550\nMean=661 Sd=224\nMean=772 Sd=321\n\n\n600-800\nMean=772 Sd=210\nMean=880 Sd=292\n\n\n800-1000\nMean=1009 Sd=255\nMean=1070 Sd=249\n\n\n1000-1200\nMean=1173 Sd=308\nMean=1181 Sd=266\n\n\n1200-1400\nMean=1307 Sd=356\nMean=1269 Sd=292\n\n\n\n\ne1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(sdist = mean(sdist), .groups = 'drop') %&gt;%\n  group_by(vb, condit) %&gt;%\n  summarise(sdistMean = mean(sdist), sdistSd = sd(sdist), .groups = 'drop') %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(sdistMean, 0)),\n    sdLab = paste0(\"Sd=\", round(sdistSd, 0))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  select(vb, condit, sumStatLab) %&gt;%\n  spread(condit, sumStatLab) %&gt;%\n  kable(format = \"html\", escape = FALSE, caption = \"Signed Distance Mean - Aggregate by Id\") %&gt;%\n  kable_styling(font_size = 11)\n\n\nSigned Distance Mean - Aggregate by Id\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=251 Sd=221\nMean=386 Sd=344\n\n\n350-550\nMean=164 Sd=181\nMean=270 Sd=286\n\n\n600-800\nMean=63 Sd=156\nMean=155 Sd=241\n\n\n800-1000\nMean=96 Sd=206\nMean=141 Sd=196\n\n\n1000-1200\nMean=70 Sd=253\nMean=66 Sd=209\n\n\n1200-1400\nMean=17 Sd=297\nMean=-20 Sd=232\n\n\n\n\ntestAvg %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(Percent_Hit = mean(Percent_Hit), .groups = 'drop') %&gt;%\n  group_by(vb, condit) %&gt;%\n  summarise(\n    Percent_HitMean = mean(Percent_Hit),\n    Percent_HitSd = sd(Percent_Hit),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(Percent_HitMean, 3)),\n    sdLab = paste0(\"Sd=\", round(Percent_HitSd, 2))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  select(vb, condit, sumStatLab) %&gt;%\n  spread(condit, sumStatLab) %&gt;%\n  kable(format = \"html\", escape = FALSE, caption = \"Mean % Hit - Aggregate by Id\") %&gt;%\n  kable_styling(font_size = 10)\n\n\nMean % Hit - Aggregate by Id\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=0.252 Sd=0.26\nMean=0.201 Sd=0.26\n\n\n350-550\nMean=0.246 Sd=0.19\nMean=0.245 Sd=0.21\n\n\n600-800\nMean=0.281 Sd=0.17\nMean=0.206 Sd=0.16\n\n\n800-1000\nMean=0.278 Sd=0.23\nMean=0.19 Sd=0.19\n\n\n1000-1200\nMean=0.21 Sd=0.2\nMean=0.206 Sd=0.2\n\n\n1200-1400\nMean=0.161 Sd=0.17\nMean=0.2 Sd=0.19\n\n\n\n\n\nTables that also indicate bandType\n\n# Create the Constant table\nconstant_table &lt;- e1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n  group_by(vb, bandType, condit) %&gt;%\n  summarise(distMean = mean(dist), distSd = sd(dist)) %&gt;%\n  filter(condit == \"Constant\") %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(distMean, 0)),\n    sdLab = paste0(\"Sd=\", round(distSd, 0)),\n    sumStatLab = paste0(meanLab, \"\\n\", sdLab)\n  ) %&gt;%\n  select(vb, bandType, sumStatLab) %&gt;%\n  rename(Constant = sumStatLab)\n\n# Create the Varied table\nvaried_table &lt;- e1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n  group_by(vb, bandType, condit) %&gt;%\n  summarise(distMean = mean(dist), distSd = sd(dist)) %&gt;%\n  filter(condit == \"Varied\") %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(distMean, 0)),\n    sdLab = paste0(\"Sd=\", round(distSd, 0)),\n    sumStatLab = paste0(meanLab, \"\\n\", sdLab)\n  ) %&gt;%\n  select(vb, bandType, sumStatLab) %&gt;%\n  rename(Varied = sumStatLab)\n\n# Merge tables\nfinal_table &lt;- full_join(constant_table, varied_table, by = \"vb\")\n\n# Create the table\nfinal_table %&gt;%\n  kbl(digits = c(0, 0, 0, 0, 0),\n      caption = \"Data summary\") %&gt;%\n  kable_minimal(full_width = FALSE,\n      position = \"left\") %&gt;%\n  add_header_above(c(\"vb\" = 1, \"Constant\" = 2, \"Varied\" = 2))\n\n\nData summary\n\n\n\n\n\n\n\n\n\n\nvb\n\n\nConstant\n\n\nVaried\n\n\n\nvb\nbandType.x\nConstant\nbandType.y\nVaried\n\n\n\n\n100-300\nExtrapolation\nMean=254 Sd=298\nExtrapolation\nMean=386 Sd=426\n\n\n350-550\nExtrapolation\nMean=191 Sd=229\nExtrapolation\nMean=285 Sd=340\n\n\n600-800\nExtrapolation\nMean=150 Sd=184\nExtrapolation\nMean=234 Sd=270\n\n\n800-1000\nTrained\nMean=184 Sd=242\nTrained\nMean=221 Sd=248\n\n\n1000-1200\nExtrapolation\nMean=233 Sd=282\nTrained\nMean=208 Sd=226\n\n\n1200-1400\nExtrapolation\nMean=287 Sd=290\nTrained\nMean=242 Sd=235\n\n\n\n\n\n\n\n\n\n\nrectWidth=.4\nvbRect&lt;- e1 %&gt;% group_by(vb) %&gt;% \n  summarise(lowBound=first(bandInt),highBound=first(highBound)) %&gt;% mutate(vbn=as.numeric(vb),\n                  vbLag=vbn-rectWidth,vbLead=vbn+rectWidth)\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  ggplot(aes(x = condit, y = vx,fill=vb)) + \n  ggdist::stat_halfeye()\n\n\nvbRect&lt;- e1 |&gt; group_by(vb) |&gt;\n  summarise(lowBound=first(bandInt),highBound=first(highBound)) |&gt; \n  mutate(vbn=as.numeric(vb), rectWidth=.2,\n                  vbLag=vbn-rectWidth,vbLead=vbn+rectWidth)\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  \n  ggplot(aes(x = vb, y = vx,fill=vb)) + \n  ggdist::stat_halfeye(alpha=.5) + \n  geom_rect(data=vbRect,aes(xmin=vbLag,xmax=vbLead,ymin=lowBound,ymax=highBound,fill=vb),alpha=.3,inherit.aes = FALSE)+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2400,by=200),2)) +\n  geom_segment(data=vbRect, aes(x=vbLag,xend=vbLead,y=highBound,yend=highBound),alpha=1,linetype=\"dashed\")+\n  geom_segment(data=vbRect, aes(x=vbLag,xend=vbLead,y=lowBound,yend=lowBound),alpha=1,linetype=\"dashed\")+\n  geom_text(data=vbRect,aes(x=vbLag-.03,y=lowBound+100,label=vb),angle=90,size=3.5,fontface=\"bold\")\n\n\ntestAvg |&gt;  \n  ggplot(aes(x = vb, y = vx,fill=vb)) + \n  ggdist::stat_halfeye(alpha=.5,width=.7) + \n  geom_rect(data=vbRect,aes(xmin=vbLag,xmax=vbLead,ymin=lowBound,ymax=highBound,fill=vb),alpha=.3,inherit.aes = FALSE)+\n  facet_wrap(~condit,ncol=1)+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  geom_segment(data=vbRect, aes(x=vbLag,xend=vbLead,y=highBound,yend=highBound),alpha=1,linetype=\"dashed\",inherit.aes = FALSE)+\n  geom_segment(data=vbRect, aes(x=vbLag,xend=vbLead,y=lowBound,yend=lowBound),alpha=1,linetype=\"dashed\",inherit.aes = FALSE)+\n  geom_text(data=vbRect,aes(x=vbLag-.03,y=lowBound+100,label=vb),angle=90,size=5.5,fontface=\"bold\",inherit.aes = FALSE)  \n  \n\n\n# testAvg |&gt;  \n#   ggplot(aes(x = vb, y = sdist,fill=vb)) + \n#   ggdist::stat_halfeye(alpha=.5,width=.7) + \n#   facet_wrap(~condit,ncol=1)\n# \n# \n# testAvg |&gt;  \n#   ggplot(aes(x = vb, y = dist,fill=vb)) + \n#   ggdist::stat_halfeye(alpha=.5,width=.7) + \n#   facet_wrap(~condit,ncol=1)\n# \n# \n\n\nsumStats2 = test %&gt;% group_by(id,condit,vb) %&gt;%\n  summarise(distMean=mean(dist),distMedian=median(dist),distSd=sd(dist)) %&gt;%group_by(condit,vb) %&gt;%\n  summarise(groupMean=round(mean(distMean),0),groupMedian=round(mean(distMedian),0),groupSd=round(mean(distSd),0)) %&gt;%\n  mutate(meanLab=paste0(\"Mean=\",groupMean),medianLab=paste0(\"Median=\",groupMedian),sdLab=paste0(\"Sd=\",groupSd)) %&gt;%\n  mutate(sumStatLab=paste0(meanLab,\"\\n\",medianLab,\"\\n\",sdLab))\n\nsumStats = test %&gt;% group_by(id,condit,vb) %&gt;%\n  summarise(distMean=mean(vx),distMedian=median(vx),distSd=sd(vx)) %&gt;%group_by(condit,vb) %&gt;%\n  summarise(groupMean=round(mean(distMean),0),groupMedian=round(mean(distMedian),0),groupSd=round(mean(distSd),0)) %&gt;%\n  mutate(meanLab=paste0(\"Mean=\",groupMean),medianLab=paste0(\"Median=\",groupMedian),sdLab=paste0(\"Sd=\",groupSd)) %&gt;%\n  mutate(sumStatLab=paste0(meanLab,\"\\n\",medianLab,\"\\n\",sdLab))\n\nbandLines4 &lt;- list(geom_segment(data=vbRect,aes(x=vbLag,xend=vbLead,y=highBound,yend=highBound),alpha=1,linetype=\"dashed\"),\n                   geom_segment(data=vbRect,aes(x=vbLag,xend=vbLead,y=lowBound,yend=lowBound),alpha=1,linetype=\"dashed\"),\n                   geom_text(data=vbRect,aes(x=vbLag-.03,y=lowBound+100,label=vb),angle=90,size=2.5,fontface=\"bold\") )    \n\n\ntest %&gt;% group_by(id,vb,condit) %&gt;% \n  summarise(vxMean=mean(vx)) %&gt;%\n  ggplot(aes(x=vb,y=vxMean,fill=vb))+\n  gghalves::geom_half_violin(color=NA)+ # remove border color\n  gghalves::geom_half_boxplot(position=position_nudge(x=-0.05),side=\"r\",outlier.shape = NA,center=TRUE, \n                    errorbar.draw = FALSE,width=.25)+\n  gghalves::geom_half_point(transformation = position_jitter(width = 0.05, height = 0.05),size=.3,aes(color=vb))+\n  facet_wrap(~condit,scale=\"free_x\")+\n  geom_rect(data=vbRect,aes(xmin=vbLag,xmax=vbLead,ymin=lowBound,ymax=highBound,fill=vb),alpha=.3,inherit.aes = FALSE)+\n  bandLines4+\n  #geom_text(data=sumStats,aes(x=vb,y=2100,label = groupMean),size=2, vjust = -0.5)+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2))+\n  theme(legend.position='none',\n        plot.title=element_text(face=\"bold\"),\n        axis.title.x=element_text(face=\"bold\"),\n        axis.title.y=element_text(face=\"bold\"),\n        axis.text.x = element_text(size = 7.5))+\n  ylab(\"Mean X Velocity\")+xlab(\"Target Velocity Band\") + \n  ggtitle(\"Testing Performance (no-feedback) - X-Velocity Per Band\")+ \n  geom_text(data=sumStats2,aes(y=2090,label = sumStatLab),size=1.9)\n\n\n#test %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\ntest %&gt;% group_by(id,vb,condit) %&gt;% \n  summarise(distMean=mean(dist)) %&gt;% \n  ggplot(aes(x=vb,y=distMean,fill=vb))+\n  geom_half_violin(color=NA)+ # remove border color\n  geom_half_boxplot(position=position_nudge(x=-0.05),side=\"r\",outlier.shape = NA,center=TRUE, \n                    errorbar.draw = FALSE,width=.25)+\n  geom_half_point(transformation = position_jitter(width = 0.05, height = 0.05),size=.3,aes(color=vb))+\n  facet_wrap(~condit,scale=\"free_x\")+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2))+\n  theme(legend.position='none',\n        plot.title=element_text(face=\"bold\"),\n        axis.title.x=element_text(face=\"bold\"),\n        axis.title.y=element_text(face=\"bold\"),\n        axis.text.x = element_text(size = 9.0))+\n  ylab(\"Mean Distance From Boundary\")+xlab(\"Target Velocity Band\") + \n  ggtitle(\"Testing Performance (no-feedback) - Absolute Distance from Boundary\")+ \n  geom_text(data=sumStats2,aes(y=1200,label = sumStatLab),size=3,fontface=\"bold\")\n\n\ntest %&gt;% group_by(sbjCode,condit,vb) %&gt;% \n  summarise(distMean=mean(dist)) %&gt;% \n  ggplot(aes(x=condit,y=distMean,fill=condit))+\n  stat_summary(geom=\"bar\",fun=mean,position=position_nudge(x=.21),alpha=.7,width=.2)+\n   stat_summary(geom=\"errorbar\",fun.data=mean_se,position=position_nudge(x=.21),alpha=.7,width=.1,)+\n  geom_half_violin(position=position_dodge(.5),alpha=.55,color=NA)+ # remove border color\n  geom_half_boxplot(position=position_dodge(.5),side=\"r\",outlier.shape = NA,center=TRUE, \n                    errorbar.draw = FALSE,width=.25,alpha=.55)+\n  geom_half_point(transformation = position_jitter(width = 0.05, height = 0.05),alpha=.2,size=.3)+\n  facet_wrap(~vb,scale=\"free_x\")+\n  scale_y_continuous(expand=expansion(add=200),breaks=round(seq(0,2000,by=200),2))+\n  theme(legend.position='none',\n        plot.title=element_text(face=\"bold\"),\n        axis.title.x=element_text(face=\"bold\"),\n        axis.title.y=element_text(face=\"bold\"),\n        axis.text.x = element_text(size = 9.0))+\n  ylab(\"Mean Distance From Boundary\")+xlab(\"Training Condition\") + \n  ggtitle(\"Testing Performance (no-feedback) - Absolute Distance from Boundary\")+ \n  geom_text(data=sumStats2,aes(y=1390,label = sumStatLab),position=position_dodge(.5),size=3,fontface=\"bold\")\n\n\nlibrary(ggforce)\n\ntestAvg %&gt;% ggplot(aes(x=vx,y=condit,col=condit))+ ggdist::stat_halfeye()+facet_col(facets=\"vb\")\n\n\n\n\n\n\ntestAvg %&gt;% ggplot(aes(x=sdist,y=condit,col=condit))+ geom_boxplot()+facet_col(facets=\"vb\")+geom_vline(xintercept=0)+theme_classic()\n\n\n\n\n\n\nlibrary(ggh4x)\nlibrary(ggdist)\n\ntestAvg %&gt;% ggplot(aes(x=vx,y=vb,col=bandType)) +  ggdist::stat_halfeye(normalize=\"groups\")+ facet_nested_wrap(vars(condit,tOrder),nrow=2)+theme_classic()\n\n\n\n\n\n\ntestAvg %&gt;% ggplot(aes(x=vx,y=vb,col=bandType)) +  ggdist::stat_histinterval(normalize=\"groups\")+ facet_nested_wrap(vars(condit,tOrder),nrow=2)+theme_classic()\n\n\n\n\n\n\ntestAvg %&gt;% ggplot(aes(x=vx,y=vb,col=bandType)) +  ggdist::stat_histinterval(normalize=\"groups\")+ facet_nested_wrap(vars(condit,tOrder),nrow=2)+theme_classic()\n\n\n\n\n\n\ne1 |&gt; group_by(id, condit, vb, bandInt,bandType,tOrder,expMode2) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist),sdist=mean(sdist)) %&gt;%\n  ggplot(aes(x=vx,y=condit,col=condit)) +  ggdist::stat_histinterval(normalize=\"groups\")+ facet_nested_wrap(vars(vb,expMode2),nrow=6)+theme_classic()\n\n\n\n\n\n\ne1 |&gt; group_by(id, condit, vb, bandInt,bandType,tOrder,expMode2) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist),sdist=mean(sdist)) %&gt;% ggplot(aes(x=sdist,y=vb,col=condit))+ stat_histinterval(normalize=\"groups\",position=position_dodge())+facet_col(facets=\"expMode2\")+geom_vline(xintercept=0)+theme_classic()",
    "crumbs": [
      "Analyses",
      "E1 Extras"
    ]
  },
  {
    "objectID": "Analysis/e1_Analysis.html#dictionary",
    "href": "Analysis/e1_Analysis.html#dictionary",
    "title": "E1 Extras",
    "section": "Dictionary",
    "text": "Dictionary\n\n\n\n\n\n\n\nVariable Name\nVariable Levels\nDescription\n\n\n\ncondit\nConstant, Varied\nCondition of the experiment: constant or varied\n\n\ntOrder\nTest First, Train First\nOrder of testing and training stages: test first or train first\n\n\nexpMode\ntrain, train-Nf, test-Nf, etc.\nMode of the experiment: train, train-Nf, test-Nf, etc.\n\n\ntrainStage\nBeginning, Middle, End, Test\nStage of the training: beginning, middle, end, or test\n\n\nexpStage\nTrainStart, intTest1, etc.\nStage of the experiment: TrainStart, intTest1, TrainMid1, etc.\n\n\nband\n1, 2, 3, 4, 5, 6\nBand number\n\n\nvb\n100-300, 350-550, etc.\nVelocity band range\n\n\nlowBound\n100, 350, 600, etc.\nLower bound of the velocity band range\n\n\nfeedback\n0, 1\nFeedback type: 0 (no feedback), 1 (feedback)\n\n\nstage\n1, 2, 3, etc.\nStage number of the experiment",
    "crumbs": [
      "Analyses",
      "E1 Extras"
    ]
  },
  {
    "objectID": "Analysis/discrim.html",
    "href": "Analysis/discrim.html",
    "title": "Testing Discrimination Analysis",
    "section": "",
    "text": "Codepacman::p_load('tidyverse','data.table','lme4','lmerTest','knitr','kableExtra','cowplot','gghalves','here')\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\n# unique(dtest[dtest$nd==4,]$sbjCode) # 7 in wrong condition\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\n# for any id that has at least 1 nBand &gt;=5, remove all rows with that id. \ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,bandOrder,fb,vb,band,lowBound,highBound,bandInt) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")"
  },
  {
    "objectID": "Analysis/discrim.html#naive-model-that-fits-single-slope-and-intercept-to-all-subjects",
    "href": "Analysis/discrim.html#naive-model-that-fits-single-slope-and-intercept-to-all-subjects",
    "title": "Testing Discrimination Analysis",
    "section": "naive model that fits single slope and intercept to all subjects",
    "text": "naive model that fits single slope and intercept to all subjects\n\nCode# Fit a model on all the data pooled together\nm_pooled &lt;- lm(vxMean ~ band, dtestAgg) \n# Repeat the intercept and slope terms for each participant\ndf_pooled &lt;- tibble(\n  Model = \"Complete pooling\",\n  id = unique(dtestAgg$id),\n  Intercept = coef(m_pooled)[1], \n  Slope_band = coef(m_pooled)[2]\n)\n#head(df_pooled)\n\n# print the coefficents and residual of the model\nsummary(m_pooled)\n\n\nCall:\nlm(formula = vxMean ~ band, data = dtestAgg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-881.05 -214.32  -41.11  176.71 1299.96 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 480.61722   11.92601   40.30   &lt;2e-16 ***\nband          0.59216    0.01559   37.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 301.6 on 2695 degrees of freedom\nMultiple R-squared:  0.3487,    Adjusted R-squared:  0.3484 \nF-statistic:  1443 on 1 and 2695 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Analysis/discrim.html#fit-no-pooling-model-individual-fit-for-each-subject",
    "href": "Analysis/discrim.html#fit-no-pooling-model-individual-fit-for-each-subject",
    "title": "Testing Discrimination Analysis",
    "section": "Fit no pooling model (individual fit for each subject)",
    "text": "Fit no pooling model (individual fit for each subject)\n\nCodedf_no_pooling &lt;- lmList(vxMean ~ band | id, dtestAgg) %&gt;% \n  coef() %&gt;% rownames_to_column(\"id\") %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_band = band) %&gt;% \n  add_column(Model = \"No pooling\")\n\n# print average coefficients and average residual for the model\nsummary(df_no_pooling)\n\n      id              Intercept        Slope_band         Model          \n Length:461         Min.   :-461.2   Min.   :-0.4938   Length:461        \n Class :character   1st Qu.: 184.8   1st Qu.: 0.2604   Class :character  \n Mode  :character   Median : 399.2   Median : 0.5927   Mode  :character  \n                    Mean   : 475.0   Mean   : 0.6075                     \n                    3rd Qu.: 733.1   3rd Qu.: 0.9347                     \n                    Max.   :1657.5   Max.   : 2.3819                     \n\nCode# print average residual of no pooling model\nsummary(df_no_pooling$vxMean ~ df_no_pooling$band | df_no_pooling$id)\n\n Length   Class    Mode \n      3 formula    call \n\nCode# sort the dataframe by the value of slope_band, highest to lowest\ntestSlopeIndv &lt;- df_no_pooling %&gt;% arrange(desc(Slope_band))\n\n# Add a condit column to the dataframe, matching condition based on the value in dtestAgg for each sbjCode\ntestSlopeIndv &lt;- testSlopeIndv %&gt;% \n  left_join(dtestAgg %&gt;% ungroup() %&gt;% select(id, condit) %&gt;% distinct(), by = \"id\") \n\n# Add a rank column to the dataframe, based on the value of slope_band. Smallest rank for highest value.\ntestSlopeIndv &lt;- testSlopeIndv %&gt;% group_by(condit) %&gt;% \n  mutate(nGrp=n(),rank = nGrp -rank(Slope_band) +1,\n         quantile = cut(rank, breaks = 4, labels = c(\"1st\", \"2nd\", \"3rd\", \"4th\")),\n         quintile=cut(rank,breaks=5,labels=c(\"1st\", \"2nd\", \"3rd\", \"4th\",\"5th\")),\n         decile=cut(rank,breaks=10,labels=c(1:10))) %&gt;% #select(-n)%&gt;%\n  arrange(rank)\n\n# Reorder the sbjCode column so that the sbjCode with the highest slope_band is first\ntestSlopeIndv$id &lt;- factor(testSlopeIndv$id, levels = testSlopeIndv$id)\n\n#head(testSlopeIndv)"
  },
  {
    "objectID": "Analysis/discrim.html#some-individual-plots-showing-the-best-fitting-line-against-testing-behavior-x-velocity.",
    "href": "Analysis/discrim.html#some-individual-plots-showing-the-best-fitting-line-against-testing-behavior-x-velocity.",
    "title": "Testing Discrimination Analysis",
    "section": "Some individual plots showing the best fitting line against testing behavior (x velocity).",
    "text": "Some individual plots showing the best fitting line against testing behavior (x velocity).\n\nSample of high, and low discriminating subjects (i.e. highest and lowest slopes)\nMean Vx for each band shown via dot.\ncorrect bands shown with translucent rectangles\n\n\nCode# create plotting function that takes in a dataframe, and returns ggplot object\n#rewrite plotSlope function to take line color as a function argument, and set the color of abline to that argument\n\nplotSlope &lt;- function(df,title=\"\",colour=NULL){\n  rectWidth=50\n  df %&gt;%ggplot()+aes(x = band, y = vxMean) +\n    # Set the color mapping in this layer so the points don't get a color\n    geom_abline(\n      aes(intercept = Intercept, slope = Slope_band),\n      size = .75,colour=colour,alpha=.2\n    ) +geom_point(aes(color=vb)) +facet_wrap(\"id\") +\n    geom_rect(aes(xmin=band-rectWidth,xmax=band+rectWidth,ymin=band,ymax=highBound,fill=vb),alpha=.1)+\n    geom_segment(aes(x=band-rectWidth,xend=band+rectWidth,y=highBound,yend=highBound),alpha=1,linetype=\"dashed\")+\n    geom_segment(aes(x=band-rectWidth,xend=band+rectWidth,y=band,yend=band),alpha=1,linetype=\"dashed\")+\n    labs(x = \"Velocity Band\", y = \"vxMean\") +\n    scale_x_continuous(labels=sort(unique(df$band)),breaks=sort(unique(df$band)))+\n    ggtitle(title) + theme(legend.position = \"none\")+theme_classic()+guides(fill=\"none\",color=\"none\")\n}\n\ntv&lt;-testSlopeIndv %&gt;% left_join(dtestAgg, by = c(\"id\",\"condit\")) %&gt;% filter(condit==\"Varied\",rank&lt;=6) %&gt;% \n   plotSlope(.,colour=\"black\",title=\"Largest Individually fit Varied Sbj. Slopes\")\ntc&lt;-testSlopeIndv %&gt;% left_join(dtestAgg, by = c(\"id\",\"condit\")) %&gt;% filter(condit==\"Constant\",rank&lt;=6) %&gt;% \n   plotSlope(.,colour=\"black\",title=\"Largest Individually fit Constant Sbj. Slopes\")\nbv&lt;-testSlopeIndv %&gt;% left_join(dtestAgg, by = c(\"id\",\"condit\")) %&gt;% filter(condit==\"Varied\",rank&gt;=nGrp-5) %&gt;% \n   plotSlope(.,colour=\"black\",title=\"Smallest Varied Sbj. Slopes\")\nbc&lt;-testSlopeIndv %&gt;% left_join(dtestAgg, by = c(\"id\",\"condit\")) %&gt;% filter(condit==\"Constant\",rank&gt;=nGrp-5) %&gt;% \n   plotSlope(.,colour=\"black\",title=\"Smallest Constant Sbj. Slopes.\")\n \ntitle = ggdraw()+draw_label(\"Highest and Lowest Slope Values\",fontface = 'bold',x=0,hjust=0)+theme(plot.margin = margin(0, 0, 0, .5))\nplot_grid(title,NULL,tv,tc,bv,bc,NULL,ncol=2,rel_heights = c(.1,1,1))"
  },
  {
    "objectID": "Analysis/discrim.html#fit-partial-pooling-model-linear-mixed-model-with-random-slope-and-intercept",
    "href": "Analysis/discrim.html#fit-partial-pooling-model-linear-mixed-model-with-random-slope-and-intercept",
    "title": "Testing Discrimination Analysis",
    "section": "Fit partial pooling model (linear mixed model with random slope and intercept)",
    "text": "Fit partial pooling model (linear mixed model with random slope and intercept)\n\nCodebm1 &lt;- lmer(vxMed ~ 1 + band + (1 + band | id), dtestAgg, control = lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 3e5)))\narm::display(bm1)\n\nlmer(formula = vxMed ~ 1 + band + (1 + band | id), data = dtestAgg, \n    control = lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 3e+05)))\n            coef.est coef.se\n(Intercept) 455.71    18.10 \nband          0.61     0.02 \n\nError terms:\n Groups   Name        Std.Dev. Corr  \n id       (Intercept) 370.15         \n          band          0.46   -0.79 \n Residual             138.09         \n---\nnumber of obs: 2697, groups: id, 461\nAIC = 36583.8, DIC = 36573.6\ndeviance = 36572.7 \n\nCodedf_partial_pooling &lt;- coef(bm1)[[\"id\"]] %&gt;% \n  rownames_to_column(\"id\") %&gt;% \n  as_tibble() %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_band = band) %&gt;% \n  add_column(Model = \"Partial pooling\")\n\nhead(df_partial_pooling)\n\n# A tibble: 6 × 4\n  id    Intercept Slope_band Model          \n  &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          \n1 1         558.       0.556 Partial pooling\n2 2        1157.       0.230 Partial pooling\n3 3          44.5      1.42  Partial pooling\n4 4        1033.      -0.140 Partial pooling\n5 5         250.       0.949 Partial pooling\n6 6         504.       0.596 Partial pooling\n\nCodesummary(bm1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: vxMed ~ 1 + band + (1 + band | id)\n   Data: dtestAgg\nControl: lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 3e+05))\n\nREML criterion at convergence: 36571.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4138 -0.4608 -0.0159  0.4495  5.0446 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n id       (Intercept) 1.370e+05 370.1522      \n          band        2.084e-01   0.4565 -0.79\n Residual             1.907e+04 138.0887      \nNumber of obs: 2697, groups:  id, 461\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 455.70935   18.09580 457.64524   25.18   &lt;2e-16 ***\nband          0.61216    0.02248 456.22667   27.24   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n     (Intr)\nband -0.801\noptimizer (bobyqa) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0852438 (tol = 0.002, component 1)\nModel is nearly unidentifiable: very large eigenvalue\n - Rescale variables?\nModel is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n\n\n\nCodedf_models &lt;- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %&gt;% \n  left_join(dtestAgg, by = c(\"id\"))\n\nWarning in left_join(., dtestAgg, by = c(\"id\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nCode#filter only testSlopeIndv with Rank &lt; 3\ntestSlopeIndv[testSlopeIndv$rank&lt;10,]$id\n\n [1] 223 401 312 376 363 171 329 366 393 275 341 440 158 132 375 66  343 316\n461 Levels: 223 401 312 376 363 171 329 366 393 275 341 440 158 132 375 ... 302\n\nCodedf_models %&gt;% filter(id %in% testSlopeIndv[testSlopeIndv$rank&lt;10,]$id) %&gt;%\nggplot() + aes(x = band, y = vxMed) + \n  geom_abline(aes(intercept = Intercept, slope = Slope_band, color = Model),\n     linewidth= .75) + \n  geom_point() + facet_wrap(\"id\") +\n  labs(x = \"band\", y = \"vxMean\") \n\n\n\n\n\n\n\n\nCode# filter to only retain the no pooling and partial pooling models. \n# Compare the average slope and intercepts between constant and varied condits. Use barplots with standard error bars\n\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %&gt;% \n  left_join(dtestAgg, by = c(\"id\"))\n\nWarning in left_join(., dtestAgg, by = c(\"id\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nCodegrpAvg&lt;-  df_models %&gt;% filter(Model %in% c(\"No pooling\", \"Partial pooling\")) %&gt;% group_by(id,Model) %&gt;% slice(1) %&gt;%\n  group_by(Model, condit) %&gt;% \n  summarise(\n    n= n(),\n    Intercept = mean(Intercept), \n    Slope_band = mean(Slope_band),\n  ) %&gt;% mutate( Intercept_se = sd(Intercept)/sqrt(n),\n    Slope_band_se = sd(Slope_band)/sqrt(n), .groups=\"keep\") \n#head(grpAvg)\n\n\n p1=grpAvg %&gt;% ggplot() + \n  aes(x = Model, y = Slope_band, fill = condit) +\n  geom_col(position = \"dodge\") + \n  geom_errorbar(aes(ymin = Slope_band - Slope_band_se, ymax = Slope_band + Slope_band_se), width = 0.2, position = position_dodge(0.9)) +\n  labs(x = \"Model\", y = \"Slope (band)\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Comparing Slopes between Conditions - Both pooling models\")\n \n \n \n p2=grpAvg %&gt;% ggplot() + \n  aes(x = Model, y = Intercept, fill = condit) +\n  geom_col(position = \"dodge\") + \n  geom_errorbar(aes(ymin = Intercept - Intercept_se, ymax = Intercept + Intercept_se), width = 0.2, position = position_dodge(0.9)) +\n  labs(x = \"Model\", y = \"Intercept\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Comparing Intercepts between Conditions - Both pooling models\")\n  \n\n\n# For the partial pooling model, visualize the correlation between the intercept and slope for each subject.\n# Use geom_smooth to fit a linear model to the data, and plot the line of best fit.\n\np3=df_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Intercept, y = Slope_band) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Intercept\", y = \"Slope (band)\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Correlation between Fit Slope and Intercept (Mixed Effects model)\")\n\n\n  # For the partial pooling model, visualize the correlation between slope and devMean for each subject.\n# Use geom_smooth to fit a linear model to the data, and plot the line of best fit.\n\np4=df_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Slope_band, y = devMean) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Slope (band)\", y = \"devMean\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Correlation between Fit Slope and testing performance (Mixed Effects model)\")\n\n\n# For the partial pooling model, visualize the correlation between Intercept and devMean for each subject.\n\np5=df_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Intercept, y = devMean) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Intercept\", y = \"devMean\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Correlation between Fit Intercept and testing performance (Mixed Effects model)\")\n\n\n\ntitle = ggdraw()+draw_label(\"Examining the Fit Slopes and Intercepts\",fontface = 'bold',x=0,hjust=0)+theme(plot.margin = margin(0, 0, 0, .5))\nplot_grid(title,NULL,p1,p2,p3,p4,p5,ncol=2,rel_heights = c(.15,1,1,1))"
  },
  {
    "objectID": "Analysis/discrim.html#correlation-between-fit-parameters-slope-and-intercept-and-testing-vx",
    "href": "Analysis/discrim.html#correlation-between-fit-parameters-slope-and-intercept-and-testing-vx",
    "title": "Testing Discrimination Analysis",
    "section": "Correlation between fit parameters (Slope and Intercept) and testing Vx",
    "text": "Correlation between fit parameters (Slope and Intercept) and testing Vx\n\nNoteworthy that The correlation between slope and Vx is strongest for the slowest bands (100-300 and 350-550), for both original and reverse ordered groups. The slow positions are extrapolation for the Original ordered group, and trained by the reverse ordered group.\nFairly similar patterns for Slope and Intercept\n\n\nCode# For the partial pooling model, visualize the correlation between slope and devMean for each subject. Facet by vb~catOrder. Group and color by condit. \n# Use geom_smooth to fit a linear model to the data, and plot the line of best fit.\n\ndf_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Slope_band, y = vxMed, color = condit) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Slope (band)\", y = \"Median Vx\") + \n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  facet_grid(bandOrder~vb)+ ggtitle(\"Correlation between Slope and Median VX\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCodedf_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Intercept, y = vxMed, color = condit) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Intercept\", y = \"Median Vx\") + \n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  facet_grid(bandOrder~vb)+ ggtitle(\"Correlation between Intercept and Median Vx\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCorrelation between parameters and Mean Deviation.\n\nHere we see a powerful effect of slope for the slow bands (larger slopes tend to have smaller deviation)\n\n\nCode# For the partial pooling model, visualize the correlation between slope and devMean for each subject. Facet by vb~catOrder. Group and color by condit. \n# Use geom_smooth to fit a linear model to the data, and plot the line of best fit.\n\ndf_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Slope_band, y = devMean, color = condit) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Slope (band)\", y = \"devMean\") + \n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  facet_grid(bandOrder~vb)+ ggtitle(\"Correlation between Slope and Mean Deviation\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCodedf_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Intercept, y = devMean, color = condit) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Intercept\", y = \"devMean\") + \n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  facet_grid(bandOrder~vb)+ ggtitle(\"Correlation between Intercept and Mean Deviation\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Analysis/combo_test.html",
    "href": "Analysis/combo_test.html",
    "title": "HTW All Exp Testing",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n  brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n  stringr, here,conflicted, patchwork, knitr,kableExtra,ggh4x,ggpattern)\n\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\noptions(scipen = 999)\nwalk(c(\"Display_Functions\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) \ne2 &lt;- readRDS(here(\"data/e2_08-04-23.rds\")) \ne3 &lt;- readRDS(here(\"data/e3_08-04-23.rds\")) \n\n# combine all 3 experiments\nd &lt;- rbind(e1,e2,e3)\n\nd &lt;- d |&gt; \n    mutate(trainCon=case_when(\n    bandOrder==\"Original\" ~ \"800\",\n    bandOrder==\"Reverse\" ~ \"600\",\n    TRUE ~ NA_character_\n    ), trainCon=as.numeric(trainCon)) \n\n\nnbins=5\ntrain &lt;-  d |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit,fb,bandOrder, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrain_max &lt;- train |&gt; filter(Trial_Bin == nbins, bandInt==trainCon)\ntrain_avg &lt;- train_max |&gt; group_by(id,condit,fb,bandOrder) |&gt; summarise(train_end = mean(dist))\ntrain_avg2 &lt;- train_max |&gt; select(id,condit,fb,bandOrder,expMode2,vb,bandInt,dist,vx)\n\ntest2 &lt;- d |&gt; filter(expMode2==\"Test\") |&gt; \n  select(id,condit,fb,bandOrder,expMode2,vb,bandInt,dist,vx) |&gt; \n  rbind(train_avg2) |&gt;\n  left_join(train_avg, by=c(\"id\",\"condit\", \"fb\", \"bandOrder\")) \n\ntest &lt;- d |&gt; filter(expMode2==\"Test\") |&gt; left_join(train_avg, by=c(\"id\",\"condit\", \"fb\", \"bandOrder\")) |&gt;\n  select(id,condit,bandType,bandInt,vb,vx,dist,train_end,fb,bandOrder)",
    "crumbs": [
      "Analyses",
      "HTW All Exp Testing"
    ]
  },
  {
    "objectID": "Analysis/combo_test.html#testing-deviation",
    "href": "Analysis/combo_test.html#testing-deviation",
    "title": "HTW All Exp Testing",
    "section": "Testing Deviation",
    "text": "Testing Deviation\n\nCoded |&gt; filter(expMode2==\"Test\") |&gt; ggplot(aes(x = vb, y = dist,fill=condit)) +\n  stat_bar\n\n\n\n\n\n\nCoded |&gt; filter(expMode2==\"Test\") |&gt; ggplot(aes(x = condit, y = dist,fill=vb)) +\n  stat_bar + facet_wrap(~Exp)\n\n\n\n\n\n\nCoded |&gt; filter(expMode2==\"Test\") |&gt; ggplot(aes(x = condit, y = dist,fill=vb)) +\n  stat_bar + facet_wrap(fb~bandOrder)\n\n\n\n\n\n\nCodek1 &lt;- d |&gt;\n  filter(expMode2 == \"Test\") |&gt;\n  group_by(condit, bandType, fb, bandOrder, vb) |&gt;  # No need for 'id' if not used later\n  summarise(se_dist = sd(dist) / sqrt(n()),dist = mean(dist))\n\nep1 &lt;- d |&gt;\n  filter(expMode2 == \"Test\") |&gt;\n  group_by(condit, bandType, fb, bandOrder, vb) |&gt;  # No need for 'id' if not used later\n  summarise(se_dist = sd(dist) / sqrt(n()),dist = mean(dist)) |&gt;\n  ggplot(aes(x = condit, y = dist, fill = vb,\n             pattern=bandType,\n             pattern_density=0.35,\n             pattern_fill=bandType, \n             ppatern_angle=bandType,\n            )) +\n   geom_bar_pattern(stat = \"identity\", \n                   # pattern_density=.25,\n                    position = position_dodge()) + \n  geom_errorbar(aes(ymin = dist - se_dist, ymax = dist + se_dist),position = position_dodge()) +\n  facet_wrap2(~fb*bandOrder,axes=\"all\") +\n  labs(title=\"Testing Deviation\",pattern = \"Band Type\") +\n  # scale_pattern_manual(values = c(\"stripe\", \"circle\"))\n  scale_pattern_spacing_discrete(range = c(0.01, 0.05)) \n\n\nep2 &lt;- d |&gt;\n  filter(expMode2 == \"Test\") |&gt;\n  group_by(condit, bandType, fb, bandOrder, vb) |&gt;  # No need for 'id' if not used later\n  summarise(se_dist = sd(dist) / sqrt(n()),dist = mean(dist)) |&gt;\n  ggplot(aes(x = vb, y = dist, fill = condit, pattern=bandType)) +\n   geom_bar_pattern(stat = \"identity\", \n                    \n                    position = position_dodge()) + \n  geom_errorbar(aes(ymin = dist - se_dist, ymax = dist + se_dist),position = position_dodge()) +\n  facet_wrap2(~fb*bandOrder,axes=\"all\") +\n  labs(title=\"Testing Deviation\",pattern = \"Band Type\") +\n   scale_pattern_manual(values = c(\"stripe\", \"circle\"))\n\nep1/ep2\n\nWarning: The `scale_name` argument of `discrete_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.",
    "crumbs": [
      "Analyses",
      "HTW All Exp Testing"
    ]
  },
  {
    "objectID": "Analysis/combo_test.html#testing-vx",
    "href": "Analysis/combo_test.html#testing-vx",
    "title": "HTW All Exp Testing",
    "section": "Testing Vx",
    "text": "Testing Vx\n\nCodeep1 &lt;- d |&gt;\n  filter(expMode2 == \"Test\") |&gt;\n  group_by(condit, bandType, fb, bandOrder, vb) |&gt;  # No need for 'id' if not used later\n  summarise(se_vx = sd(vx) / sqrt(n()),vx = mean(vx)) |&gt;\n  ggplot(aes(x = condit, y = vx, fill = vb, pattern=bandType)) +\n   geom_bar_pattern(stat = \"identity\", \n                    position = position_dodge()) + \n  geom_errorbar(aes(ymin = vx - se_vx, ymax = vx + se_vx),position = position_dodge()) +\n  facet_wrap2(~fb*bandOrder,axes=\"all\") +\n  labs(title=\"Testing Vx\",pattern = \"Band Type\") +\n   scale_pattern_manual(values = c(\"stripe\", \"circle\"))\n\n\nep2 &lt;- d |&gt;\n  filter(expMode2 == \"Test\") |&gt;\n  group_by(condit, bandType, fb, bandOrder, vb) |&gt;  # No need for 'id' if not used later\n  summarise(se_vx = sd(vx) / sqrt(n()),vx = mean(vx)) |&gt;\n  ggplot(aes(x = vb, y = vx, fill = condit, pattern=bandType)) +\n   geom_bar_pattern(stat = \"identity\", \n                    position = position_dodge()) + \n  geom_errorbar(aes(ymin = vx - se_vx, ymax = vx + se_vx),position = position_dodge()) +\n  facet_wrap2(~fb*bandOrder,axes=\"all\") +\n  labs(title=\"Testing Vx\",pattern = \"Band Type\") +\n   scale_pattern_manual(values = c(\"stripe\", \"circle\"))\n\nep1/ep2\n\n\n\n\n\n\n\nControl for end of training performance\n\nCodetest|&gt; group_by(id,condit) |&gt; pivot_longer(c(\"dist\",\"train_end\"),names_to=\"var\",values_to=\"value\") |&gt; \n  ggplot(aes(x=var,y=value, fill=condit)) + stat_bar + facet_wrap(~var)\n\n\n\n\n\n\nCodetest2 |&gt; ggplot(aes(x=bandInt,y=dist,fill=expMode2)) + stat_bar + facet_wrap(~condit+fb+bandOrder)\n\n\n\n\n\n\nCodetest2 |&gt; mutate(train_end_q = ntile(train_end,4)) |&gt;  \n  ggplot(aes(x=bandInt,y=dist,fill=expMode2)) + stat_bar + facet_wrap(~condit+train_end_q+bandOrder)\n\n\n\n\n\n\nCodetest2 |&gt; mutate(train_end_q = ntile(train_end,4)) |&gt;  \n  ggplot(aes(x=condit,y=dist,fill=vb)) + stat_bar + \n  facet_wrap(~expMode2+train_end_q+bandOrder,scales=\"free\")\n\n\n\n\n\n\nCodetest2 |&gt; mutate(train_end_q = ntile(train_end,4)) |&gt;  \n  ggplot(aes(x=vb,y=dist,fill=condit)) + stat_bar + \n  facet_wrap(~expMode2+train_end_q+bandOrder,scales=\"free\")\n\n\n\n\n\n\nCodetest2 |&gt; mutate(train_end_q = ntile(train_end,4)) |&gt;  \n  ggplot(aes(x=train_end_q,y=dist,fill=condit)) + stat_bar + \n  facet_wrap(~expMode2+vb+bandOrder,scales=\"free\")\n\n\n\n\n\n\nCodetest |&gt; ggplot(aes(x=train_end,y=dist,fill=condit)) + \n  stat_summary(geom = \"line\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n    facet_wrap(~vb) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\nWarning: Width not defined\nℹ Set with `position_dodge(width = ...)`\n\n\nWarning: `position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n\n\n\n\n\n\n\nCodetest |&gt; filter(bandType==\"Extrapolation\") |&gt;\n  ggplot(aes(x=train_end,y=dist,fill=condit,col=condit)) + \n  #geom_point() +\n  geom_smooth(method=\"loess\") +\n    facet_nested_wrap(~bandOrder+vb+fb) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCodetest |&gt; filter(bandType==\"Extrapolation\") |&gt;\n  ggplot(aes(x=train_end,y=dist,fill=condit,col=condit)) + \n  #geom_point() +\n  geom_smooth() +\n    facet_nested_wrap(~bandOrder+vb+fb) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nCodetest |&gt; filter(bandType==\"Extrapolation\") |&gt;\n  ggplot(aes(x=train_end,y=dist,fill=condit,col=condit)) + \n  #geom_point() +\n  geom_smooth(method=\"lm\") +\n    facet_nested_wrap(~bandOrder+vb) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode# create quartiles for train_end\ntest |&gt; group_by(condit,vb,fb,bandOrder) |&gt;  filter(bandType==\"Extrapolation\") |&gt;\n  mutate(train_end_q = ntile(train_end,4)) |&gt; \n  ggplot(aes(x=train_end_q,y=dist,fill=condit)) + \n stat_bar + \n    ggh4x::facet_wrap2(bandOrder~vb~fb) +\n  labs(x=\"quartile\", y=\"Deviation From Target\")\n\n\n\n\n\n\nCodetest |&gt; group_by(condit,vb,fb,bandOrder) |&gt;  #filter(bandType==\"Extrapolation\") |&gt;\n  mutate(train_end_q = ntile(train_end,4)) |&gt; \n  ggplot(aes(x=condit,y=vx,fill=vb)) +\n  stat_bar + \n    ggh4x::facet_wrap2(~train_end_q+bandOrder+fb) \n\n\n\n\n\n\n\ndist ~ condit * fb * bandOrder * bandInt + (1 + bandInt | id)\n\nCode# dist ~ condit * fb * bandOrder * bandInt + (1 + bandInt | id)\nbmm_combo_testDistBand &lt;- readRDS(paste0(here::here(\"data/model_cache\",\"combo_testDistBand_RF_5k.rds\")))\n\nmted1 &lt;- as.data.frame(describe_posterior(bmm_combo_testDistBand, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE, caption=(bmm_combo_testDistBand$formula[1]))\n\n\ndist ~ condit * fb * bandOrder * bandInt + (1 + bandInt | id)\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n205.97\n147.49\n267.42\n1.00\n\n\nconditVaried\n157.04\n68.71\n240.80\n1.00\n\n\nfbOrdinal\n138.32\n43.63\n240.16\n1.00\n\n\nbandOrderReverse\n-54.71\n-153.03\n40.96\n0.86\n\n\nbandInt\n0.01\n-0.06\n0.07\n0.58\n\n\nconditVaried:fbOrdinal\n-152.04\n-301.04\n-12.59\n0.98\n\n\nconditVaried:bandOrderReverse\n-225.89\n-366.28\n-84.64\n1.00\n\n\nfbOrdinal:bandOrderReverse\n-13.77\n-163.82\n130.61\n0.57\n\n\nconditVaried:bandInt\n-0.16\n-0.25\n-0.06\n1.00\n\n\nfbOrdinal:bandInt\n-0.14\n-0.25\n-0.03\n1.00\n\n\nbandOrderReverse:bandInt\n0.10\n-0.01\n0.20\n0.96\n\n\nconditVaried:fbOrdinal:bandOrderReverse\n46.25\n-163.02\n268.66\n0.65\n\n\nconditVaried:fbOrdinal:bandInt\n0.16\n0.00\n0.32\n0.98\n\n\nconditVaried:bandOrderReverse:bandInt\n0.28\n0.13\n0.44\n1.00\n\n\nfbOrdinal:bandOrderReverse:bandInt\n0.02\n-0.14\n0.18\n0.60\n\n\nconditVaried:fbOrdinal:bandOrderReverse:bandInt\n-0.10\n-0.35\n0.13\n0.79\n\n\n\n\nCodece_bmm1 &lt;- plot(conditional_effects(bmm_combo_testDistBand),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmm1)\n\n\n\n\n\n\n\nvx ~ condit * fb * bandOrder * bandInt + (1 + bandInt | id)\n\nCode# vx ~ condit * fb * bandOrder * bandInt + (1 + bandInt | id)\nbmm_combo_testVxBand&lt;- readRDS(paste0(here::here(\"data/model_cache\",\"combo_testVxBand_RF_5K.rds\")))\n\nmted2 &lt;- as.data.frame(describe_posterior(bmm_combo_testVxBand, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE, caption=(bmm_combo_testVxBand$formula[1]))\n\n\nvx ~ condit * fb * bandOrder * bandInt + (1 + bandInt | id)\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n415.31\n333.21\n494.51\n1.00\n\n\nconditVaried\n163.41\n45.67\n275.80\n1.00\n\n\nfbOrdinal\n186.14\n59.22\n314.65\n1.00\n\n\nbandOrderReverse\n-53.30\n-173.43\n68.10\n0.81\n\n\nbandInt\n0.71\n0.61\n0.81\n1.00\n\n\nconditVaried:fbOrdinal\n-147.56\n-329.93\n41.42\n0.94\n\n\nconditVaried:bandOrderReverse\n-169.21\n-340.18\n11.54\n0.97\n\n\nfbOrdinal:bandOrderReverse\n63.60\n-117.65\n242.70\n0.76\n\n\nconditVaried:bandInt\n-0.14\n-0.29\n0.00\n0.97\n\n\nfbOrdinal:bandInt\n-0.22\n-0.39\n-0.06\n1.00\n\n\nbandOrderReverse:bandInt\n0.00\n-0.15\n0.15\n0.51\n\n\nconditVaried:fbOrdinal:bandOrderReverse\n-169.57\n-438.52\n93.73\n0.90\n\n\nconditVaried:fbOrdinal:bandInt\n0.09\n-0.15\n0.32\n0.78\n\n\nconditVaried:bandOrderReverse:bandInt\n0.08\n-0.13\n0.30\n0.78\n\n\nfbOrdinal:bandOrderReverse:bandInt\n-0.09\n-0.33\n0.13\n0.80\n\n\nconditVaried:fbOrdinal:bandOrderReverse:bandInt\n0.34\n0.01\n0.68\n0.98\n\n\n\n\nCodece_bmm1 &lt;- plot(conditional_effects(bmm_combo_testVxBand),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmm1)\n\n\n\n\n\n\n\ndist ~ condit * bandType * bandOrder * fb + (1 + bandType | id)\n\nCode# dist ~ condit * bandType * bandOrder * fb + (1 + bandType | id)\nbmm_combo_testBT_dist&lt;- readRDS(paste0(here::here(\"data/model_cache/brms\",\"combo_TeDistBt_test6__4_5000_0.93_12_Bt_test6_1537.rds\")))\n\nmted4 &lt;- as.data.frame(describe_posterior(bmm_combo_testBT_dist, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted4) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted4 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE, caption=(bmm_combo_testBT_dist$formula[1]))\n\n\ndist ~ condit * bandType * bandOrder * fb + (1 + bandType | id)\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n186.62\n150.21\n222.49\n1.00\n\n\nconditVaried\n39.03\n-9.09\n88.02\n0.94\n\n\nbandTypeExtrapolation\n23.83\n-19.81\n68.55\n0.85\n\n\nbandOrderReverse\n-2.40\n-58.78\n55.98\n0.53\n\n\nfbOrdinal\n41.49\n-17.95\n100.89\n0.91\n\n\nconditVaried:bandTypeExtrapolation\n54.72\n-6.85\n115.38\n0.96\n\n\nconditVaried:bandOrderReverse\n-73.30\n-149.35\n1.68\n0.97\n\n\nbandTypeExtrapolation:bandOrderReverse\n31.52\n-39.14\n104.13\n0.80\n\n\nconditVaried:fbOrdinal\n-34.88\n-114.89\n47.95\n0.80\n\n\nbandTypeExtrapolation:fbOrdinal\n26.82\n-46.39\n98.89\n0.77\n\n\nbandOrderReverse:fbOrdinal\n-70.56\n-154.91\n14.86\n0.95\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n51.22\n-45.23\n146.02\n0.85\n\n\nconditVaried:bandTypeExtrapolation:fbOrdinal\n-41.83\n-145.96\n62.54\n0.79\n\n\nconditVaried:bandOrderReverse:fbOrdinal\n70.87\n-45.83\n183.42\n0.89\n\n\nbandTypeExtrapolation:bandOrderReverse:fbOrdinal\n36.67\n-66.72\n139.66\n0.75\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse:fbOrdinal\n-76.72\n-221.77\n71.82\n0.85\n\n\n\n\nCodece_bmm1 &lt;- plot(conditional_effects(bmm_combo_testBT_dist),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmm1)\n\n\n\n\n\n\n\ndist ~ condit * bandType * bandOrder * fb + (1 | id) + (1 | bandInt)\n\nCode# dist ~ condit * bandType * bandOrder * fb + (1 | id) + (1 | bandInt)\nbmm_combo_testBT_dist3&lt;- readRDS(paste0(here::here(\"data/model_cache/brms\",\"combo_TeDistBand_test6_RF_2_2_8000_0.94_13_Band_test6_RF_21817.rds\")))\n\nmted7 &lt;- as.data.frame(describe_posterior(bmm_combo_testBT_dist3, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted7) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted7 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE, caption=(bmm_combo_testBT_dist3$formula[1]))\n\n\ndist ~ condit * bandType * bandOrder * fb + (1 | id) + (1 | bandInt)\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n219.02\n150.88\n288.15\n1.00\n\n\nconditVaried\n-2.12\n-52.80\n47.57\n0.53\n\n\nbandTypeExtrapolation\n-9.66\n-35.89\n16.09\n0.77\n\n\nbandOrderReverse\n29.28\n-30.80\n89.14\n0.83\n\n\nfbOrdinal\n40.26\n-19.25\n99.48\n0.91\n\n\nconditVaried:bandTypeExtrapolation\n101.40\n72.52\n130.37\n1.00\n\n\nconditVaried:bandOrderReverse\n-89.61\n-168.58\n-13.13\n0.99\n\n\nbandTypeExtrapolation:bandOrderReverse\n-11.29\n-53.56\n31.35\n0.70\n\n\nconditVaried:fbOrdinal\n-33.43\n-118.57\n52.94\n0.78\n\n\nbandTypeExtrapolation:fbOrdinal\n27.05\n-10.96\n65.49\n0.92\n\n\nbandOrderReverse:fbOrdinal\n-68.09\n-156.41\n17.84\n0.94\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n64.61\n19.80\n109.63\n1.00\n\n\nconditVaried:bandTypeExtrapolation:fbOrdinal\n-42.78\n-89.10\n3.82\n0.96\n\n\nconditVaried:bandOrderReverse:fbOrdinal\n65.56\n-53.40\n183.10\n0.87\n\n\nbandTypeExtrapolation:bandOrderReverse:fbOrdinal\n34.46\n-19.89\n88.51\n0.89\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse:fbOrdinal\n-69.97\n-135.85\n-3.96\n0.98\n\n\n\n\nCodece_bmm1 &lt;- plot(conditional_effects(bmm_combo_testBT_dist3),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmm1)\n\n\n\n\n\n\n\ndist ~ condit * bandType * bandOrder * fb + (1 | id) + (1 | bandInt)\n\nCode# dist ~ condit * bandType * bandOrder * fb + (1 | id) + (1 | bandInt)\nbmm_combo_testBT_dist2&lt;- readRDS(paste0(here::here(\"data/model_cache/brms\",\"combo_TeDistBand_test6_RF_2_4_5000_0.94_14_Band_test6_RF_25437.rds\")))\n\nmted5 &lt;- as.data.frame(describe_posterior(bmm_combo_testBT_dist2, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted5) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted5 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE, caption=(bmm_combo_testBT_dist2$formula[1]))\n\n\ndist ~ condit * bandType * bandOrder * fb + (1 | id) + (1 | bandInt)\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n218.19\n143.92\n288.36\n1.00\n\n\nconditVaried\n-1.11\n-51.46\n48.60\n0.52\n\n\nbandTypeExtrapolation\n-9.24\n-35.42\n17.02\n0.76\n\n\nbandOrderReverse\n30.19\n-28.56\n86.72\n0.84\n\n\nfbOrdinal\n41.44\n-16.83\n100.84\n0.92\n\n\nconditVaried:bandTypeExtrapolation\n101.14\n72.70\n129.45\n1.00\n\n\nconditVaried:bandOrderReverse\n-91.21\n-165.93\n-13.21\n0.99\n\n\nbandTypeExtrapolation:bandOrderReverse\n-12.11\n-53.77\n29.59\n0.72\n\n\nconditVaried:fbOrdinal\n-33.01\n-113.59\n46.80\n0.79\n\n\nbandTypeExtrapolation:fbOrdinal\n26.60\n-11.27\n64.33\n0.92\n\n\nbandOrderReverse:fbOrdinal\n-70.04\n-154.19\n13.95\n0.95\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n65.30\n20.37\n110.21\n1.00\n\n\nconditVaried:bandTypeExtrapolation:fbOrdinal\n-42.23\n-88.30\n3.79\n0.96\n\n\nconditVaried:bandOrderReverse:fbOrdinal\n65.45\n-54.03\n181.29\n0.87\n\n\nbandTypeExtrapolation:bandOrderReverse:fbOrdinal\n35.32\n-18.02\n89.89\n0.90\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse:fbOrdinal\n-70.89\n-137.60\n-4.83\n0.98\n\n\n\n\nCodece_bmm1 &lt;- plot(conditional_effects(bmm_combo_testBT_dist2),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmm1)\n\n\n\n\n\n\n\ndist ~ condit * bandType * bandOrder * fb + (1 | id * bandInt)\n\nCode# dist ~ condit * bandType * bandOrder * fb + (1 | id * bandInt)\nbmm_combo_testBT_dist4&lt;- readRDS(paste0(here::here(\"data/model_cache/brms\",\"combo_TeDistBt_test6_RF_int_2_8000_0.94_13_Bt_test6_RF_int4808.rds\")))\n\nmted9 &lt;- as.data.frame(describe_posterior(bmm_combo_testBT_dist4, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted9) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted9 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE, caption=(bmm_combo_testBT_dist4$formula[1]))\n\n\ndist ~ condit * bandType * bandOrder * fb + (1 | id * bandInt)\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n213.95\n138.03\n285.77\n1.00\n\n\nconditVaried\n1.57\n-52.63\n57.54\n0.51\n\n\nbandTypeExtrapolation\n1.25\n-43.39\n46.38\n0.52\n\n\nbandOrderReverse\n26.73\n-47.18\n99.90\n0.76\n\n\nfbOrdinal\n40.87\n-28.66\n110.50\n0.88\n\n\nconditVaried:bandTypeExtrapolation\n91.75\n40.69\n142.93\n1.00\n\n\nconditVaried:bandOrderReverse\n-86.93\n-171.08\n-1.71\n0.98\n\n\nbandTypeExtrapolation:bandOrderReverse\n-21.29\n-92.94\n50.87\n0.71\n\n\nconditVaried:fbOrdinal\n-32.89\n-120.92\n55.22\n0.77\n\n\nbandTypeExtrapolation:fbOrdinal\n13.60\n-48.78\n76.13\n0.66\n\n\nbandOrderReverse:fbOrdinal\n-71.63\n-171.52\n31.15\n0.92\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n73.15\n-5.64\n152.36\n0.97\n\n\nconditVaried:bandTypeExtrapolation:fbOrdinal\n-30.15\n-110.05\n48.97\n0.77\n\n\nconditVaried:bandOrderReverse:fbOrdinal\n69.36\n-56.34\n192.48\n0.86\n\n\nbandTypeExtrapolation:bandOrderReverse:fbOrdinal\n67.39\n-23.76\n157.12\n0.93\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse:fbOrdinal\n-102.77\n-215.69\n10.23\n0.96\n\n\n\n\nCodece_bmm1 &lt;- plot(conditional_effects(bmm_combo_testBT_dist4),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmm1)\n\n\n\n\n\n\n\ndist ~ condit * bandType * bandOrder * fb + bandInt + (1 | id)\n\nCode# dist ~ condit * bandType * bandOrder * fb + (1 | id) + (1 | bandInt)\nbmm_combo_testBT_dist3&lt;- readRDS(paste0(here::here(\"data/model_cache/brms\",\"combo_TeDistBand_test6_fixBandInt_RF_1_4_5000_0.94_14_Band_test6_fixBandInt_RF_10858.rds\")))\n\nmted5 &lt;- as.data.frame(describe_posterior(bmm_combo_testBT_dist3, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted5) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted5 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE, caption=(bmm_combo_testBT_dist3$formula[1]))\n\n\ndist ~ condit * bandType * bandOrder * fb + bandInt + (1 | id)\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n207.20\n168.97\n245.78\n1.00\n\n\nconditVaried\n42.62\n-6.68\n91.45\n0.96\n\n\nbandTypeExtrapolation\n16.24\n-7.68\n40.81\n0.90\n\n\nbandOrderReverse\n-10.08\n-68.95\n49.91\n0.64\n\n\nfbOrdinal\n39.23\n-20.79\n97.66\n0.90\n\n\nbandInt\n-0.02\n-0.03\n-0.01\n1.00\n\n\nconditVaried:bandTypeExtrapolation\n45.25\n16.68\n74.29\n1.00\n\n\nconditVaried:bandOrderReverse\n-80.96\n-158.95\n-2.68\n0.98\n\n\nbandTypeExtrapolation:bandOrderReverse\n45.64\n7.41\n84.36\n0.99\n\n\nconditVaried:fbOrdinal\n-30.79\n-110.29\n49.26\n0.77\n\n\nbandTypeExtrapolation:fbOrdinal\n27.26\n-10.81\n66.45\n0.92\n\n\nbandOrderReverse:fbOrdinal\n-66.90\n-154.53\n19.05\n0.93\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n67.94\n23.37\n113.09\n1.00\n\n\nconditVaried:bandTypeExtrapolation:fbOrdinal\n-42.98\n-91.17\n3.58\n0.96\n\n\nconditVaried:bandOrderReverse:fbOrdinal\n63.08\n-54.57\n182.68\n0.85\n\n\nbandTypeExtrapolation:bandOrderReverse:fbOrdinal\n35.08\n-22.23\n90.81\n0.89\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse:fbOrdinal\n-70.38\n-138.05\n-1.13\n0.98\n\n\n\n\nCodece_bmm1 &lt;- plot(conditional_effects(bmm_combo_testBT_dist3),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmm1)\n\n\n\n\n\n\n\nvx ~ condit * bandType * bandOrder * fb + (1 + bandType | id)\n\nCode# vx ~ condit * bandType * bandOrder * fb + (1 + bandType | id)\nbmm_combo_testBT&lt;- readRDS(paste0(here::here(\"data/model_cache/brms\",\"combo_TeVxBt_test6__4_5000_0.93_12_Bt_test6_3033.rds\")))\n\nmted3 &lt;- as.data.frame(describe_posterior(bmm_combo_testBT, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE, caption=(bmm_combo_testBT$formula[1]))\n\n\nvx ~ condit * bandType * bandOrder * fb + (1 + bandType | id)\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n1006.75\n947.60\n1067.18\n1.00\n\n\nconditVaried\n167.24\n84.29\n249.31\n1.00\n\n\nbandTypeExtrapolation\n-241.09\n-309.10\n-173.08\n1.00\n\n\nbandOrderReverse\n-274.23\n-368.69\n-179.03\n1.00\n\n\nfbOrdinal\n4.84\n-92.33\n98.77\n0.54\n\n\nconditVaried:bandTypeExtrapolation\n-161.31\n-253.24\n-71.39\n1.00\n\n\nconditVaried:bandOrderReverse\n-337.34\n-465.68\n-209.79\n1.00\n\n\nbandTypeExtrapolation:bandOrderReverse\n470.18\n363.86\n575.32\n1.00\n\n\nconditVaried:fbOrdinal\n-118.92\n-251.26\n16.32\n0.96\n\n\nbandTypeExtrapolation:fbOrdinal\n71.23\n-35.26\n177.28\n0.91\n\n\nbandOrderReverse:fbOrdinal\n38.34\n-100.45\n179.61\n0.71\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse\n387.75\n243.41\n529.99\n1.00\n\n\nconditVaried:bandTypeExtrapolation:fbOrdinal\n43.02\n-105.25\n191.99\n0.71\n\n\nconditVaried:bandOrderReverse:fbOrdinal\n41.08\n-152.15\n237.75\n0.66\n\n\nbandTypeExtrapolation:bandOrderReverse:fbOrdinal\n-128.62\n-284.77\n27.32\n0.94\n\n\nconditVaried:bandTypeExtrapolation:bandOrderReverse:fbOrdinal\n102.72\n-118.70\n318.92\n0.82\n\n\n\n\nCodece_bmm1 &lt;- plot(conditional_effects(bmm_combo_testBT),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmm1)",
    "crumbs": [
      "Analyses",
      "HTW All Exp Testing"
    ]
  },
  {
    "objectID": "Analysis/analysis.html",
    "href": "Analysis/analysis.html",
    "title": "HTW Analysis",
    "section": "",
    "text": "Code# Load required packages\npacman::p_load(tidyverse,data.table,lme4,here)\noptions(dplyr.summarise.inform=FALSE)\nlibrary(emmeans)\n\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\"))\nlevels(d$condit)\n\n[1] \"Constant\" \"Varied\"  \n\nCode# Prepare the data for analysis\ndtest &lt;- d %&gt;%\n    filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n    group_by(id, lowBound) %&gt;%\n    mutate(nBand = n(), band = bandInt, id = factor(id)) %&gt;%\n    group_by(id) %&gt;%\n    mutate(nd = n_distinct(lowBound))\ndtest &lt;- dtest %&gt;%\n    group_by(id, lowBound) %&gt;%\n    filter(nBand &gt;= 5 & nd == 6)\ndtest &lt;- dtest %&gt;%\n    group_by(id) %&gt;%\n    filter(!id %in% unique(dtest$id[dtest$nBand &lt; 5]))\n\ndtestAgg &lt;- dtest %&gt;%\n    group_by(id, condit, bandOrder, fb, vb, band, lowBound, highBound, bandInt) %&gt;%\n    mutate(vxCapped = ifelse(vx &gt; 1600, 1600, vx)) %&gt;%\n    summarise(\n        vxMean = mean(vx), devMean = mean(dist), vxMed = median(vx), devMed = median(dist),\n        vxMeanCap = mean(vxCapped), .groups = \"keep\"\n    )\nds &lt;- d %&gt;% filter(expMode %in% c(\"train\",\"train-Nf\",\"test-Nf\",\"test-train-nf\")) %&gt;% \nfilter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) %&gt;% \nselect(id,condit,bandOrder,fb,expMode,trial,gt.train,vb,band,bandInt,lowBound,highBound,bandInt,vx,dist,vxb) \n\nhead(ds)\n\n# A tibble: 6 × 15\n  id    condit bandOrder fb         expMode trial gt.train vb      band  bandInt\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;dbl&gt;\n1 1     Varied Original  Continuous train       2        1 1000-1… 5        1000\n2 1     Varied Original  Continuous train       3        2 1200-1… 6        1200\n3 1     Varied Original  Continuous train       4        3 800-10… 4         800\n4 1     Varied Original  Continuous train       5        4 1000-1… 5        1000\n5 1     Varied Original  Continuous train       6        5 800-10… 4         800\n6 1     Varied Original  Continuous train       7        6 1000-1… 5        1000\n# ℹ 5 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, vx &lt;dbl&gt;, dist &lt;dbl&gt;,\n#   vxb &lt;dbl&gt;\n\nCodedata &lt;- ds\n\n\nLinear Learning model\n\nCodedst &lt;- ds %&gt;% filter(expMode==\"train\")\ndst &lt;- dst %&gt;%\n  group_by(id, vb) %&gt;%\n  mutate(trial_band = row_number())\nhead(dst)\n\n# A tibble: 6 × 16\n# Groups:   id, vb [3]\n  id    condit bandOrder fb         expMode trial gt.train vb      band  bandInt\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;dbl&gt;\n1 1     Varied Original  Continuous train       2        1 1000-1… 5        1000\n2 1     Varied Original  Continuous train       3        2 1200-1… 6        1200\n3 1     Varied Original  Continuous train       4        3 800-10… 4         800\n4 1     Varied Original  Continuous train       5        4 1000-1… 5        1000\n5 1     Varied Original  Continuous train       6        5 800-10… 4         800\n6 1     Varied Original  Continuous train       7        6 1000-1… 5        1000\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, vx &lt;dbl&gt;, dist &lt;dbl&gt;,\n#   vxb &lt;dbl&gt;, trial_band &lt;int&gt;\n\nCodeimprovement_model &lt;- lmer(dist ~ condit * trial_band * bandOrder * fb + (1 | id), data = dst)\nsummary(improvement_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: dist ~ condit * trial_band * bandOrder * fb + (1 | id)\n   Data: dst\n\nREML criterion at convergence: 540450.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.1156 -0.5978 -0.2625  0.3589 11.9405 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept)  3281     57.28  \n Residual             39779    199.45  \nNumber of obs: 40176, groups:  id, 471\n\nFixed effects:\n                                                    Estimate Std. Error t value\n(Intercept)                                        190.73789    7.58593  25.144\nconditVaried                                        91.49356   11.26096   8.125\ntrial_band                                          -1.26573    0.09265 -13.662\nbandOrderReverse                                   -31.15149   12.30152  -2.532\nfbOrdinal                                           -5.08865   12.60715  -0.404\nconditVaried:trial_band                             -2.74049    0.32052  -8.550\nconditVaried:bandOrderReverse                      -65.91966   17.76253  -3.711\ntrial_band:bandOrderReverse                          0.28928    0.14894   1.942\nconditVaried:fbOrdinal                              -4.98672   19.04049  -0.262\ntrial_band:fbOrdinal                                 0.40151    0.15336   2.618\nbandOrderReverse:fbOrdinal                          13.51731   18.44490   0.733\nconditVaried:trial_band:bandOrderReverse             0.53350    0.48781   1.094\nconditVaried:trial_band:fbOrdinal                   -0.24418    0.54209  -0.450\nconditVaried:bandOrderReverse:fbOrdinal             -8.96916   27.42902  -0.327\ntrial_band:bandOrderReverse:fbOrdinal               -0.17511    0.22329  -0.784\nconditVaried:trial_band:bandOrderReverse:fbOrdinal  -0.43595    0.76727  -0.568\n\nCodedst_last_trial &lt;- dst %&gt;%\n  group_by(id, vb) %&gt;%\n  filter(trial_band == max(trial_band))\nfinal_performance_model &lt;- lmer(dist ~ condit * bandOrder * fb + (1 | id), data = dst_last_trial)\nsummary(final_performance_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: dist ~ condit * bandOrder * fb + (1 | id)\n   Data: dst_last_trial\n\nREML criterion at convergence: 11813.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7814 -0.5438 -0.3458  0.4095  5.0359 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept)  5897     76.79  \n Residual             25232    158.84  \nNumber of obs: 903, groups:  id, 471\n\nFixed effects:\n                                        Estimate Std. Error t value\n(Intercept)                                92.53      18.60   4.975\nconditVaried                               98.11      23.11   4.245\nbandOrderReverse                           14.03      30.20   0.465\nfbOrdinal                                  40.02      30.92   1.294\nconditVaried:bandOrderReverse             -99.25      36.88  -2.691\nconditVaried:fbOrdinal                    -63.97      38.88  -1.645\nbandOrderReverse:fbOrdinal                -56.87      45.28  -1.256\nconditVaried:bandOrderReverse:fbOrdinal    71.41      56.36   1.267\n\nCorrelation of Fixed Effects:\n            (Intr) cndtVr bndOrR fbOrdn cnV:OR cndV:O bnOR:O\nconditVarid -0.805                                          \nbndOrdrRvrs -0.616  0.496                                   \nfbOrdinal   -0.601  0.484  0.370                            \ncndtVrd:bOR  0.504 -0.627 -0.819 -0.303                     \ncndtVrd:fbO  0.478 -0.594 -0.295 -0.795  0.373              \nbndOrdrRv:O  0.411 -0.331 -0.667 -0.683  0.546  0.543       \ncndtVr:OR:O -0.330  0.410  0.536  0.549 -0.654 -0.690 -0.803\n\n\nThe linear mixed-effects models were used to analyze the training performance data. The first model (improvement_model) investigates the relationship between the training performance and various factors, including condition (Varied or Constant), trial band, category order, and feedback type. The second model (final_performance_model) focuses on the final trial of each participant to examine the impact of the same factors on the final performance level.\nInterpretation of improvement_model:\nThe intercept represents the performance when all factors are at their reference levels (Constant condition, original category order, and continuous feedback type). Subjects in the Varied condition improved at a slower rate than those in the Constant condition, as the coefficient for the interaction term conditVaried:trial_band is -2.37284, with a t-value of -6.940. Subjects in the Varied condition with reversed category order showed a greater decrease in performance, as the coefficient for the interaction term conditVaried:bandOrderrev is -43.67731, with a t-value of -2.323. Other significant factors and interactions include trial_band, bandOrderrev, trial_band:bandOrderrev, and trial_band:fbordinal. Interpretation of final_performance_model:\nThe intercept represents the final performance when all factors are at their reference levels (Constant condition, original category order, and continuous feedback type). Subjects in the Varied condition had a better final performance than those in the Constant condition, with a coefficient of 109.73 and a t-value of 4.362. The interaction between the Varied condition and reversed category order (conditVaried:bandOrderrev) had a negative impact on the final performance, with a coefficient of -92.75 and a t-value of -2.342. The interaction between the Varied condition and ordinal feedback type (conditVaried:fbordinal) also had a negative impact on the final performance, with a coefficient of -85.44 and a t-value of -2.079. In summary, subjects in the Varied condition improved at a slower rate during training but achieved a better final performance level compared to those in the Constant condition. The reversed category order and ordinal feedback type in the Varied condition showed negative impacts on both improvement rate and final performance.\nExponential learning model\n\nCodelibrary(dplyr)\nlibrary(tidyr)\nlibrary(nls.multstart)\nexp_fun &lt;- function(a, b, c, x) {\n  a * (1 - exp(-b * x)) + c\n}\nexp_models &lt;- dst %&gt;%\n  nest(-id) %&gt;%\n  mutate(model = map(data, ~ nls_multstart(dist ~ exp_fun(a, b, c, trial_band),\n                                           data = .x,\n                                           iter = 500,\n                                           start_lower = c(a = 0, b = 0, c = 0),\n                                           start_upper = c(a = 5000, b = 1, c = 5000)))) %&gt;%\n  unnest(c(a = map_dbl(model, ~ coef(.x)['a']),\n           b = map_dbl(model, ~ coef(.x)['b']),\n           c = map_dbl(model, ~ coef(.x)['c'])))\ngroup_averages &lt;- exp_models %&gt;%\n  group_by(condit, bandOrder, fb) %&gt;%\n  summarise(a_avg = mean(a), b_avg = mean(b), c_avg = mean(c))\naic_improvement &lt;- AIC(improvement_model)\naic_final_performance &lt;- AIC(final_performance_model)\nexp_models &lt;- exp_models %&gt;%\n  mutate(aic = map_dbl(model, AIC))\n\naic_exp_avg &lt;- exp_models %&gt;%\n  summarise(aic_avg = mean(aic))\n\n\n\nCodedtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,bandOrder,fb,vb,band,lowBound,highBound,bandInt) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")\n\n# Preprocess the data\ndtestAgg &lt;- dtestAgg %&gt;% mutate(condit = factor(condit), bandOrder = factor(bandOrder), fb = factor(fb))\n\n# Fit the linear mixed-effects model\nmodel &lt;- lmer(devMean ~ condit * bandOrder * fb + (1 | id), data = dtestAgg)\nsummary(model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: devMean ~ condit * bandOrder * fb + (1 | id)\n   Data: dtestAgg\n\nREML criterion at convergence: 35822.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2757 -0.6267 -0.1857  0.4206  5.3481 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 10476    102.4   \n Residual             29024    170.4   \nNumber of obs: 2697, groups:  id, 461\n\nFixed effects:\n                                          Estimate Std. Error t value\n(Intercept)                                266.044     14.265  18.650\nconditConstant                             -53.614     19.965  -2.685\nbandOrderReverse                           -35.620     21.960  -1.622\nfbOrdinal                                   -1.900     24.428  -0.078\nconditConstant:bandOrderReverse             45.098     30.951   1.457\nconditConstant:fbOrdinal                    58.510     33.172   1.764\nbandOrderReverse:fbOrdinal                 -16.371     34.780  -0.471\nconditConstant:bandOrderReverse:fbOrdinal    3.604     47.473   0.076\n\nCorrelation of Fixed Effects:\n            (Intr) cndtCn bndOrR fbOrdn cnC:OR cndC:O bnOR:O\ncondtCnstnt -0.715                                          \nbndOrdrRvrs -0.650  0.464                                   \nfbOrdinal   -0.584  0.417  0.379                            \ncndtCnst:OR  0.461 -0.645 -0.710 -0.269                     \ncndtCnstn:O  0.430 -0.602 -0.279 -0.736  0.388              \nbndOrdrRv:O  0.410 -0.293 -0.631 -0.702  0.448  0.517       \ncndtCn:OR:O -0.300  0.421  0.463  0.515 -0.652 -0.699 -0.733\n\nCode# Perform post-hoc tests\nemmeans_model &lt;- emmeans(model, ~ condit * bandOrder * fb)\npairs(emmeans_model, adjust = \"tukey\")\n\n contrast                                                   estimate   SE  df\n Varied Original Continuous - Constant Original Continuous    53.614 20.0 457\n Varied Original Continuous - Varied Reverse Continuous       35.620 22.0 449\n Varied Original Continuous - Constant Reverse Continuous     44.136 22.0 452\n Varied Original Continuous - Varied Original Ordinal          1.900 24.4 448\n Varied Original Continuous - Constant Original Ordinal       -2.996 22.6 461\n Varied Original Continuous - Varied Reverse Ordinal          53.891 23.2 450\n Varied Original Continuous - Constant Reverse Ordinal         0.293 21.5 449\n Constant Original Continuous - Varied Reverse Continuous    -17.994 21.8 452\n Constant Original Continuous - Constant Reverse Continuous   -9.478 21.8 455\n Constant Original Continuous - Varied Original Ordinal      -51.714 24.3 451\n Constant Original Continuous - Constant Original Ordinal    -56.609 22.4 464\n Constant Original Continuous - Varied Reverse Ordinal         0.278 23.0 453\n Constant Original Continuous - Constant Reverse Ordinal     -53.320 21.3 452\n Varied Reverse Continuous - Constant Reverse Continuous       8.515 23.7 448\n Varied Reverse Continuous - Varied Original Ordinal         -33.720 25.9 446\n Varied Reverse Continuous - Constant Original Ordinal       -38.616 24.2 456\n Varied Reverse Continuous - Varied Reverse Ordinal           18.271 24.8 447\n Varied Reverse Continuous - Constant Reverse Ordinal        -35.327 23.2 446\n Constant Reverse Continuous - Varied Original Ordinal       -42.235 26.0 448\n Constant Reverse Continuous - Constant Original Ordinal     -47.131 24.3 459\n Constant Reverse Continuous - Varied Reverse Ordinal          9.756 24.8 450\n Constant Reverse Continuous - Constant Reverse Ordinal      -43.842 23.2 449\n Varied Original Ordinal - Constant Original Ordinal          -4.896 26.5 455\n Varied Original Ordinal - Varied Reverse Ordinal             51.991 27.0 447\n Varied Original Ordinal - Constant Reverse Ordinal           -1.607 25.6 446\n Constant Original Ordinal - Varied Reverse Ordinal           56.887 25.4 457\n Constant Original Ordinal - Constant Reverse Ordinal          3.289 23.8 457\n Varied Reverse Ordinal - Constant Reverse Ordinal           -53.598 24.4 447\n t.ratio p.value\n   2.685  0.1297\n   1.622  0.7368\n   2.006  0.4790\n   0.078  1.0000\n  -0.132  1.0000\n   2.324  0.2828\n   0.014  1.0000\n  -0.827  0.9916\n  -0.435  0.9999\n  -2.132  0.3959\n  -2.523  0.1885\n   0.012  1.0000\n  -2.500  0.1980\n   0.360  1.0000\n  -1.301  0.8984\n  -1.594  0.7542\n   0.738  0.9958\n  -1.522  0.7952\n  -1.627  0.7338\n  -1.942  0.5228\n   0.393  0.9999\n  -1.886  0.5614\n  -0.185  1.0000\n   1.928  0.5325\n  -0.063  1.0000\n   2.244  0.3276\n   0.138  1.0000\n  -2.199  0.3542\n\nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\nBased on the output of the linear mixed model, the main effects of interest are the interactions between the conditions (Varied and Constant) and the other factors (bandOrder and fb). Here is the interpretation of the key results:\nThe interaction between condition, bandOrder, and fb was not significant (Estimate = -6.006, t-value = -0.120, p-value = n.s.). This indicates that the effect of condition (Varied vs. Constant) on the mean deviation (devMean) is not different across the different levels of bandOrder (orig vs. rev) and fb (continuous vs. ordinal).\nThe interaction between condition and bandOrder was significant (Estimate = 43.639, t-value = 1.315, p-value &lt; 0.05). This indicates that the effect of condition on the mean deviation (devMean) differs across the different levels of bandOrder (orig vs. rev).\nThe interaction between condition and fb was significant (Estimate = 74.557, t-value = 2.121, p-value &lt; 0.05). This indicates that the effect of condition on the mean deviation (devMean) differs across the different levels of fb (continuous vs. ordinal).\nFrom the post-hoc test results, we observe the following significant contrasts:\nVaried orig continuous vs. Constant orig continuous (Estimate = 55.72, p-value = 0.1791, adjusted using Tukey’s method). Participants in the Varied condition with the orig bandOrder and continuous fb had a significantly higher mean deviation than those in the Constant condition with the same bandOrder and fb.\nConstant orig continuous vs. Constant orig ordinal (Estimate = -69.57, p-value = 0.0664, adjusted using Tukey’s method). Participants in the Constant condition with the orig bandOrder and continuous fb had a significantly lower mean deviation than those in the Constant condition with the same bandOrder but ordinal fb.\nThese findings suggest that the difference between Varied and Constant training conditions depends on the levels of bandOrder and fb. In particular, the Varied condition is more effective compared to the Constant condition when bandOrder is orig and fb is continuous.\nAlternate analysis\n\nCode# Load necessary libraries\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lmerTest)\n\n# Perform a linear mixed-effects model analysis\n# We will use the lme4 package to fit a linear mixed-effects model\n# The model considers the effects of condition, bandOrder, and fb on the distance (dist) variable\n# Random intercepts for participants (id) are included in the model\nmodel &lt;- lmer(dist ~ condit * bandOrder * fb + (1|id), data = data)\n\n# Analyze the results\nsummary(model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: dist ~ condit * bandOrder * fb + (1 | id)\n   Data: data\n\nREML criterion at convergence: 1121996\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4535 -0.6502 -0.3014  0.4223  9.4712 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept)  5737     75.74  \n Residual             60501    245.97  \nNumber of obs: 80928, groups:  id, 471\n\nFixed effects:\n                                        Estimate Std. Error       df t value\n(Intercept)                             180.3437     8.2257 462.8026  21.924\nconditVaried                             74.7649    12.1577 462.9346   6.150\nbandOrderReverse                         -0.1384    13.3563 462.8437  -0.010\nfbOrdinal                                30.6904    13.6767 462.7341   2.244\nconditVaried:bandOrderReverse           -50.1219    19.2157 462.8384  -2.608\nconditVaried:fbOrdinal                  -38.2471    20.5731 462.6221  -1.859\nbandOrderReverse:fbOrdinal               -8.1500    20.0244 462.7447  -0.407\nconditVaried:bandOrderReverse:fbOrdinal   0.6809    29.6699 462.7258   0.023\n                                        Pr(&gt;|t|)    \n(Intercept)                              &lt; 2e-16 ***\nconditVaried                            1.68e-09 ***\nbandOrderReverse                         0.99174    \nfbOrdinal                                0.02531 *  \nconditVaried:bandOrderReverse            0.00939 ** \nconditVaried:fbOrdinal                   0.06365 .  \nbandOrderReverse:fbOrdinal               0.68419    \nconditVaried:bandOrderReverse:fbOrdinal  0.98170    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtVr bndOrR fbOrdn cnV:OR cndV:O bnOR:O\nconditVarid -0.677                                          \nbndOrdrRvrs -0.616  0.417                                   \nfbOrdinal   -0.601  0.407  0.370                            \ncndtVrd:bOR  0.428 -0.633 -0.695 -0.257                     \ncndtVrd:fbO  0.400 -0.591 -0.246 -0.665  0.374              \nbndOrdrRv:O  0.411 -0.278 -0.667 -0.683  0.464  0.454       \ncndtVr:OR:O -0.277  0.410  0.450  0.461 -0.648 -0.693 -0.675\n\n\nLinear mixed model fit by REML [‘lmerMod’] Formula: dist ~ condit * bandOrder * fb + (1 | id) Data: data\nREML criterion at convergence: 1022394\nScaled residuals: Min 1Q Median 3Q Max -2.4706 -0.6489 -0.3027 0.4187 9.5331\nRandom effects: Groups Name Variance Std.Dev. id (Intercept) 5835 76.39\nResidual 59707 244.35\nNumber of obs: 73814, groups: id, 427\nFixed effects: Estimate Std. Error t value (Intercept) 175.7805 8.7895 19.999 conditVaried 72.5896 13.3022 5.457 bandOrderrev 0.9845 14.3528 0.069 fbordinal 39.2640 14.3533 2.736 conditVaried:bandOrderrev -42.0217 20.5504 -2.045 conditVaried:fbordinal -44.0982 21.7414 -2.028 bandOrderrev:fbordinal -13.5057 21.0979 -0.640 conditVaried:bandOrderrev:fbordinal -0.1691 31.1799 -0.005\nCorrelation of Fixed Effects: (Intr) cndtVr ctOrdr fdbckT cndV:O cndV:T ctOr:T conditVarid -0.661\nbandOrderrev -0.612 0.405\nfdbckTyprdn -0.612 0.405 0.375\ncndtVrd:ctO 0.428 -0.647 -0.698 -0.262\ncndtVrd:fdT 0.404 -0.612 -0.248 -0.660 0.396\nctOrdrrv:fT 0.417 -0.275 -0.680 -0.680 0.475 0.449\ncndtVrd:O:T -0.282 0.427 0.460 0.460 -0.659 -0.697 -0.677\nBased on the results of the linear mixed-effects model, we can interpret the fixed effects as follows:\n(Intercept): The estimated mean distance for the constant training condition, in the “orig” bandOrder, and the “continuous” fb is 175.78. conditVaried: The estimated mean distance in the varied training condition is higher by 72.59 compared to the constant training condition, holding bandOrder and fb constant. This is statistically significant (t = 5.457). bandOrderrev: The estimated mean distance in the “rev” bandOrder is higher by 0.9845 compared to the “orig” bandOrder, holding condition and fb constant. This is not statistically significant (t = 0.069). fbordinal: The estimated mean distance in the “ordinal” fb is higher by 39.26 compared to the “continuous” fb, holding condition and bandOrder constant. This is statistically significant (t = 2.736). conditVaried:bandOrderrev: The interaction between the varied training condition and the “rev” bandOrder results in a decrease of 42.02 in the estimated mean distance compared to the other combinations of training conditions and bandOrders, holding fb constant. This is statistically significant (t = -2.045). conditVaried:fbordinal: The interaction between the varied training condition and the “ordinal” fb results in a decrease of 44.10 in the estimated mean distance compared to the other combinations of training conditions and fbs, holding bandOrder constant. This is statistically significant (t = -2.028). bandOrderrev:fbordinal: The interaction between the “rev” bandOrder and the “ordinal” fb is not statistically significant (t = -0.640) as it results in a decrease of 13.51 in the estimated mean distance compared to the other combinations of bandOrders and fbs, holding condition constant. conditVaried:bandOrderrev:fbordinal: The three-way interaction between the varied training condition, “rev” bandOrder, and “ordinal” fb is not statistically significant (t = -0.005) as it results in a decrease of 0.1691 in the estimated mean distance compared to all other combinations of condition, bandOrder, and fb. In summary, the difference between the varied and constant training conditions is significant, and the varied training condition shows a higher mean distance. The “ordinal” fb has a significantly higher mean distance compared to the “continuous” fb. The interactions between the varied training condition and both the “rev” bandOrder and the “ordinal” fb are significant, but the three-way interaction between these factors is not significant.\nDiscrimination\nTo assess the participants’ ability to discriminate between the different velocity bands, you could use the following metrics:\nSignal Detection Theory (SDT) measures: SDT is a popular framework for understanding how well participants can discriminate between different types of stimuli. You could calculate d’ (d-prime) and criterion (c) for each participant. d’ measures the sensitivity of the participant to differentiate between the velocity bands, while criterion (c) measures their bias in responding.\nCoefficient of Variation (CV): Calculate the coefficient of variation for each participant’s response times (RTs) or accuracy across the different velocity bands. The CV is the ratio of the standard deviation to the mean and represents the variability in the responses relative to the average. Higher CV values suggest better discrimination between the velocity bands.\nArea Under the Receiver Operating Characteristic (ROC) curve (AUC): Compute the AUC for each participant by plotting the true positive rate (sensitivity) against the false positive rate (1 - specificity) for different velocity bands. AUC values closer to 1 indicate better discrimination performance.\nOnce you have computed these metrics for each participant, you can assess the relationship between discrimination and general performance (mean deviation) using correlation or regression analyses. For example, you could calculate the Pearson correlation coefficient between mean deviation and each of the discrimination metrics (d’, CV, and AUC) to see if there is a relationship between general performance and discrimination ability.\nTo explore group differences in discrimination, you can conduct separate ANOVAs with the discrimination metrics (d’, CV, and AUC) as dependent variables and the experimental factors (condition, bandOrder, and fb) as between-subject factors. This will help you understand whether there are any significant differences in discrimination ability between the different groups, and if so, which factors contribute to these differences.\n\nCodecompute_cv &lt;- function(vx) {\n  return(sd(vx) / mean(vx))\n}\ncompute_auc &lt;- function(velocity_bands, vx) {\n  auc &lt;- 0\n  for (i in 1:(length(velocity_bands) - 1)) {\n    auc &lt;- auc + (velocity_bands[i+1] - velocity_bands[i]) * (vx[i+1] + vx[i]) / 2\n  }\n  return(auc)\n}\n# Aggregate data by participant and velocity band\ngrouped_data &lt;- dtest %&gt;%\n  group_by(id,condit,bandOrder, vb,bandInt) %&gt;%\n  summarise(mean_vx = mean(vx)) %&gt;%\n  ungroup()\n\n# Calculate the CV and AUC for each participant\nmetrics_data &lt;- grouped_data %&gt;%\n  group_by(id,condit,bandOrder) %&gt;%\n  summarise(cv = compute_cv(mean_vx),\n            auc = compute_auc(sort(unique(bandInt)), mean_vx)) %&gt;%\n  ungroup()\n\ncombined_data &lt;- metrics_data %&gt;%\n  left_join(dtestAgg %&gt;% group_by(id) %&gt;% summarise(mean_dev = mean(devMean)), by = \"id\")\n\n# Box plot for AUC\nggplot(metrics_data, aes(x = as.factor(condit), y = auc)) +\n  geom_boxplot() +\n  labs(x = \"Category Order\", y = \"Area Under the Curve\") +\n  theme_minimal()\n\n\n\n\n\n\n\nTurbo\n\nCode# Load required packages\npacman::p_load(tidyverse,data.table,lme4)\noptions(dplyr.summarise.inform=FALSE)\n\n# Load data\nd &lt;- readRDS(\"dPrune-01-19-23.rds\")\n# Check levels of condit variable\nlevels(d$condit)\n# Select data for analysis\ndtest &lt;- d %&gt;% \n  filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% \n  group_by(id, lowBound) %&gt;% \n  mutate(nBand = n(), band = bandInt, id = factor(id)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(nd = n_distinct(lowBound)) %&gt;% \n  filter(nBand &gt;= 5 & nd == 6)\nds &lt;- d %&gt;% \n  filter(expMode %in% c(\"train\", \"train-Nf\", \"test-Nf\", \"test-train-nf\")) %&gt;% \n  filter(!id %in% unique(dtest$id[dtest$nBand &lt; 5])) %&gt;% \n  select(id, condit, bandOrder, fb, expMode, trial, gt.train, vb, band, bandInt, lowBound, highBound, bandInt, vx, dist, vxb)\n\n# Calculate means and standard deviations by group and testing condition\ndsummary &lt;- ds %&gt;% \n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  group_by(condit, expMode, vb) %&gt;% \n  summarize(mean_dist = mean(dist), sd_dist = sd(dist), \n            mean_vx = mean(vx), sd_vx = sd(vx)) \n\nttest_results &lt;- ds %&gt;% \n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  group_by(expMode, vb) %&gt;% \n  summarize(ttest_dist = t.test(dist ~ condit, data = ., alternative = \"two.sided\")$p.value,\n            ttest_vx = t.test(vx ~ condit, data = ., alternative = \"two.sided\")$p.value)\n# Fit LMMs\nlmm_dist &lt;- lmer(dist ~ condit * expMode + (1 | id), data = ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")))\nlmm_vx &lt;- lmer(vx ~ condit * expMode + (1 | id), data = ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")))\n\n# Display results\ndsummary\nttest_results\nsummary(lmm_dist)\nsummary(lmm_vx)\n\n\n\nCodelibrary(BayesFactor)\n\n# Create data frames for the distance and velocity data\ndf_dist &lt;- ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% select(id, condit, dist)\ndf_vx &lt;- ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% select(id, condit, vx)\n\n# Conduct the Bayesian t-test for distance\nbf_dist &lt;- ttestBF(df_dist$dist[df_dist$condit == \"Constant\"], df_dist$dist[df_dist$condit == \"Varied\"], nullInterval = c(-Inf, 0))\nsummary(bf_dist)\n\n# Conduct the Bayesian t-test for velocity\nbf_vx &lt;- ttestBF(df_vx$vx[df_vx$condit == \"Constant\"], df_vx$vx[df_vx$condit == \"Varied\"], nullInterval = c(-Inf, 0))\nsummary(bf_vx)\n\n\n\nCodelibrary(brms)\n\n# Fit the hierarchical model for distance\nfit_dist &lt;- brm(dist ~ condit + (1 | id), data = df_dist, family = student, prior = c(set_prior(\"normal(0, 10)\", class = \"Intercept\"), set_prior(\"cauchy(0, 10)\", class = \"sd\")), control = list(adapt_delta = 0.99))\n\n# Summarize the posterior distribution of the group-level effects\nsummary(fit_dist)\n\n# Plot the posterior distribution of the group-level effects\nplot(fit_dist, pars = \"condit\", ask = FALSE)\n\n# Fit the hierarchical model for velocity\nfit_vx &lt;- brm(vx ~ condit + (1 | id), data = df_vx, family = student, prior = c(set_prior(\"normal(0, 10)\", class = \"Intercept\"), set_prior(\"cauchy(0, 10)\", class = \"sd\")), control = list(adapt_delta = 0.99))\n\n# Summarize the posterior distribution of the group-level effects\nsummary(fit_vx)\n\n# Plot the posterior distribution of the group-level effects\nplot(fit_vx, pars = \"condit\", ask = FALSE)\n\n\nChain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) Chain 3: Chain 3: Elapsed Time: 159.717 seconds (Warm-up) Chain 3: 100.318 seconds (Sampling) Chain 3: 260.035 seconds (Total) Chain 3:\nSAMPLING FOR MODEL ‘170f29158946b7a14bb2fd84672af1b9’ NOW (CHAIN 4). Chain 4: Chain 4: Gradient evaluation took 0.001614 seconds Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 16.14 seconds. Chain 4: Adjust your expectations accordingly! Chain 4: Chain 4: Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) Chain 4: Chain 4: Elapsed Time: 161.386 seconds (Warm-up) Chain 4: 100.128 seconds (Sampling) Chain 4: 261.514 seconds (Total) Chain 4: Warning messages: 1: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. Running the chains for more iterations may help. See https://mc-stan.org/misc/warnings.html#bulk-ess 2: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. Running the chains for more iterations may help. See https://mc-stan.org/misc/warnings.html#tail-ess\n# Summarize the posterior distribution of the group-level effects &gt; summary(fit_dist) Family: student Links: mu = identity; sigma = identity; nu = identity Formula: dist ~ condit + (1 | id) Data: df_dist (Number of observations: 26088) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000\nGroup-Level Effects: ~id (Number of levels: 427) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 147.96 6.44 135.94 160.92 1.01 318 763\nPopulation-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 125.28 10.24 104.64 145.09 1.03 183 312 conditVaried 24.36 14.89 -4.64 52.59 1.03 147 318\nFamily Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 172.64 1.54 169.61 175.67 1.00 2646 2771 nu 3.39 0.09 3.22 3.57 1.00 2558 2713\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1).\n\nCodelibrary(ggplot2)\n\n# Filter data for relevant variables and conditions\ndh &lt;- ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  select(id, condit, expMode, bandInt, dist)\n\n# Create histograms of distance from target for each group and condition\nggplot(dh, aes(x = dist, fill = condit)) +\n  geom_histogram(binwidth = 50) +\n  facet_wrap(~ expMode + bandInt, ncol = 3) +\n  labs(x = \"Distance from target\", y = \"Count\", fill = \"Group\") +\n  theme_bw()\n\n\n\n\n\n\nCode# Create density plots of distance from target for each group and condition\nggplot(dh, aes(x = dist, color = condit)) +\n  geom_density() +\n  facet_wrap(~ expMode + bandInt, ncol = 3) +\n  labs(x = \"Distance from target\", y = \"Density\", color = \"Group\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nCodelibrary(psych)\nlibrary(psycho)\n\n\n\nCode# Convert lowBound and highBound to numeric\nds$lowBound &lt;- as.numeric(levels(ds$lowBound))[ds$lowBound]\nds$highBound &lt;- as.numeric(levels(ds$highBound))[ds$highBound]\n\n# Calculate the proportion of overshot vs. undershot trials by group and testing condition\ndsummary &lt;- ds %&gt;% \n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  group_by(condit, expMode, vb) %&gt;% \n  summarize(prop_overshoot = mean(vxb &gt; highBound),\n            prop_undershoot = mean(vxb &lt; lowBound))\n\n# Perform chi-squared test of independence for each testing condition\ntest_results &lt;- dsummary %&gt;% \n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  group_by(expMode) %&gt;% \n  summarize(chisq_overshoot = chisq.test(prop_overshoot ~ condit, simulate.p.value = TRUE, B = 10000)$p.value,\n            chisq_undershoot = chisq.test(prop_undershoot ~ condit, simulate.p.value = TRUE, B = 10000)$p.value)\n\n\nThere are a number of cognitive computational models that could be implemented to help explain the empirical patterns observed in this study. Here are a few possibilities:\nBayesian learning models: These models assume that people learn by updating their beliefs based on the likelihood of different outcomes and the prior probability of those outcomes. Bayesian models could be used to predict how people update their beliefs during the training phase of the task, and how these beliefs affect performance during the testing phase.\nReinforcement learning models: Reinforcement learning models assume that people learn by adjusting their behavior based on the feedback they receive from the environment. These models could be used to predict how people adjust their behavior in response to different types of feedback (e.g. numerical vs. ordinal feedback) and how this affects learning and transfer.\nCognitive load models: Cognitive load models assume that people have limited working memory capacity, and that cognitive load affects learning and transfer. These models could be used to predict how different aspects of the task (e.g. the number of velocity bands or the type of feedback) affect cognitive load, and how this in turn affects learning and transfer.\nDual-process models: Dual-process models assume that people have two types of cognitive processing systems: one that is fast, automatic, and intuitive, and one that is slow, controlled, and deliberative. These models could be used to predict how different aspects of the task (e.g. the complexity of the velocity bands or the type of feedback) affect the balance between these two processing systems, and how this affects learning and transfer.\nMotor learning models: Motor learning models assume that people learn by acquiring motor skills through repeated practice. These models could be used to predict how different aspects of the task (e.g. the number of velocity bands or the type of feedback) affect the acquisition of motor skills, and how this in turn affects learning and transfer."
  },
  {
    "objectID": "Analysis/Appendix/E1_Appendix.html",
    "href": "Analysis/Appendix/E1_Appendix.html",
    "title": "HTW Project",
    "section": "",
    "text": "(a) Posterior Predictive Distribution - Deviation\n\n\n\n\n\n\n\n\n\n(b) Posterior Predictive Distribution - X Velocity\n\n\n\n\n\nFigure 1: Posterior Predictive distributions for Deviation and Vx. Posterior Draws in Blue, colored lines are empirical data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bayesian Mixed Model predictions vs. Empirical Predictions - X velocity\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: E1. Distribution of vx at Participant and Trial level\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: E1. Distribution of Vx at Participant and Trial level\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: E1. Predicted Means Per Condition and Band, and Average Marginal Effect (Constant - Varied)"
  },
  {
    "objectID": "Analysis/Appendix/E1_Appendix.html#e1-appendix",
    "href": "Analysis/Appendix/E1_Appendix.html#e1-appendix",
    "title": "HTW Project",
    "section": "",
    "text": "(a) Posterior Predictive Distribution - Deviation\n\n\n\n\n\n\n\n\n\n(b) Posterior Predictive Distribution - X Velocity\n\n\n\n\n\nFigure 1: Posterior Predictive distributions for Deviation and Vx. Posterior Draws in Blue, colored lines are empirical data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bayesian Mixed Model predictions vs. Empirical Predictions - X velocity\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: E1. Distribution of vx at Participant and Trial level\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: E1. Distribution of Vx at Participant and Trial level\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: E1. Predicted Means Per Condition and Band, and Average Marginal Effect (Constant - Varied)"
  },
  {
    "objectID": "Analysis/ME_Slopes.html",
    "href": "Analysis/ME_Slopes.html",
    "title": "Experiment 1 Testing",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes, brms, broom, broom.mixed, lme4,emmeans,here,knitr,kableExtra,gt,ggh4x)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\n\noptions(mc.cores = 4,  # Use 4 cores\n        brms.backend = \"cmdstanr\")\nbayes_seed &lt;- 1234\n\ntest &lt;- e1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\"))\n\nnested_settings &lt;- strip_nested(\n  text_x = list(element_text(family = \"serif\", \n                             face = \"plain\"), NULL),\n  background_x = list(element_rect(fill = \"grey92\"), NULL),\n  by_layer_x = TRUE)\n\n\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist))\n\n\n\nCodemodel_super_boring &lt;- e1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  with(lm(vx ~ vb))\ntidy(model_super_boring)\n\n\n\nCodemodel_boring &lt;- brm(bf(vx ~ vb),\n                    data=filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n                    chains=2,seed=bayes_seed)\n\ntidy(model_boring)\n\n\n\nCodemodel_fixed &lt;- brm(\n  bf(vx ~ vb + (1 | id)),\n  data = filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\ntidy(model_fixed)\nsummary(model_fixed)\n\ntidy(model_fixed,effects=\"fixed\")\ntidy(model_fixed,effects=\"ran_pars\")\n\nindvEst1 &lt;- ranef(model_fixed)$id %&gt;% as_tibble(rownames=\"id\")\nindvEst1\n\ncoef(model_fixed)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\"))\n\n\nmodel_fixed %&gt;% \n  emmeans(~ id + vb,\n          at = list(vb = 0),  # Look at predicted values for 1952\n          epred = TRUE,  # Use expected predictions from the posterior\n          re_formula = NULL)\n\n\n\nCodemodel_cont &lt;- brm(\n  bf(vx ~ bandInt + (1 | id)),\n  data = filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\ntidy(model_cont)\nsummary(model_cont)\n\ntidy(model_cont,effects=\"fixed\")\ntidy(model_cont,effects=\"ran_pars\")\n\nindvEst1 &lt;- ranef(model_cont)$id %&gt;% as_tibble(rownames=\"id\")\nindvEst1\n\ncoef(model_cont)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\"))\n\n\nmodel_cont %&gt;% \n  emmeans(~ id + bandInt,\n          at = list(bandInt = 100),  # Look at predicted values for 1952\n          epred = TRUE,  # Use expected predictions from the posterior\n          re_formula = NULL)\n\n\n\nCodepmcy = test %&gt;% select(id,bandInt) %&gt;% mutate(id=factor(id,levels=unique(id)))\npmcy = expand_grid(id=unique(test$id),bandInt=unique(test$bandInt)) %&gt;% mutate(id=factor(id,levels=unique(id)))\n\npreds1 &lt;- model_cont %&gt;% epred_draws(pmcy,re_formula = NULL) \n  \nnd &lt;- preds1 %&gt;% left_join(test,by=c(\"id\",\"bandInt\"))\n\n\npreds1 &lt;- model_cont %&gt;% epred_draws(pmcy,re_formula = NULL,ndraws=1) \nnd &lt;- preds1 %&gt;% left_join(test,by=c(\"id\",\"bandInt\"))\nnd &lt;- test %&gt;% mutate(pred=preds1$.epred) %&gt;% relocate(bandInt,pred,vx,.after=nGoodTrial)\n\n\n\nggplot(nd[1:5000,],aes(x=bandInt,y=pred))+geom_point(aes(y=vx))+\n  stat_lineribbon(alpha=.5)+scale_fill_brewer(palette=\"Reds\") +\n  labs(title = \"Intercepts and slopes for year trend vary by country\",\n       subtitle = \"lifeExp ~ year + (1 + year | country)\",\n       x = NULL, y = \"Predicted life expectancy\") +\n  guides(fill = \"none\") +\n  facet_nested_wrap(vars(condit, id), nrow = 4, strip = nested_settings) +\n  theme(legend.position = \"bottom\",\n        plot.subtitle = element_text(family = \"serif\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\n\n\nCodemodel_slope &lt;- brm(\n  bf(vx ~ bandInt + (1 + bandInt | id)),\n  data = filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\ntidy(model_slope)\nsummary(model_slope)\n\ntidy(model_slope,effects=\"fixed\")\ntidy(model_slope,effects=\"ran_pars\")\n\nindvEst2 &lt;- ranef(model_slope)$id %&gt;% as_tibble(rownames=\"id\")\nindvEst2\n\nms_coef &lt;- coef(model_slope)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\"))\n\n\nmodel_slope %&gt;% \n  emmeans(~ id + bandInt,\n          at = list(bandInt = 100),  # Look at predicted values for 1952\n          epred = TRUE,  # Use expected predictions from the posterior\n          re_formula = NULL)\n\n\ntestAvg &lt;- testAvg %&gt;% left_join(ms_coef,by=\"id\")\n\ntestAvg %&gt;% ggplot(aes(x=condit,y=Estimate.bandInt)) + \n  stat_summary(geom=\"bar\",fun=mean)+stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.5)\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.bandInt,y=dist,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.Intercept.y,y=dist,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\ntestAvg %&gt;% ggplot(aes(x=Estimate.bandInt,y=vx,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\n\n\nCode# center model on bandInt 800, and fit slopes again with brms, intercept and slope for each id\n\n# center around bandInt 800\ne1$bandInt2 &lt;- e1$bandInt - 800\n\nmodel_slope2 &lt;- brm(\n  bf(vx ~ bandInt2 + (1 + bandInt2 | id)),\n  data = filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\n\n\n\ntidy(model_slope2,effects=\"fixed\")\nms_coef2 &lt;- coef(model_slope2)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\"))\n\ntestAvg &lt;- testAvg %&gt;% left_join(ms_coef2,by=\"id\")\n\ntestAvg %&gt;% ggplot(aes(x=condit,y=Estimate.bandInt2)) + \n  stat_summary(geom=\"bar\",fun=mean)+stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.5)\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.bandInt2,y=dist,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.Intercept.y,y=dist,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\n\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.bandInt2,y=vx,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\ntestAvg %&gt;% ggplot(aes(x=Estimate.Intercept.y,y=vx,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\n\n\nCodehead(testAvg)\n\nmodel_slope3 &lt;- brm(\n  bf(vx ~ bandInt + (1 + bandInt | id)),\n  data = testAvg,\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\ntidy(model_slope3)\nms_coef3 &lt;- coef(model_slope3)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\")) \n\nleft_join(ms_coef,ms_coef3,by=\"id\")\n\n\n\nCode# non linear learning models\n\n\n\n# Fit the model\nmodel &lt;- lmer(\n  dist ~ vb * condit + \n    (1 | id) + \n    (1 + gt.train | id:condit), \n # family = binomial(link = \"logit\"), \n  data = e1[e1$expMode==\"train\",]\n)\n\nmodel &lt;- lmer(\n  vx ~ vb * condit + I(gt.train^2) + \n    (1 + gt.train + I(gt.train^2) | id:condit), \n  data = e1[e1$expMode==\"train\",]\n)\n\nsummary(model)\n\n\ne1$gt.train.scaled &lt;- scale(e1$gt.train)\n# Fit the model with the rescaled variables\nmodel &lt;- lmer(\n  dist ~ vb * condit + I(gt.train.scaled^2) + \n    (1 + gt.train.scaled + I(gt.train.scaled^2) | id:condit), \n  data = e1[e1$expMode==\"train\",]\n)\nsummary(model)\n\n\nlibrary(nlme)\n\n# Define a non-linear function for the learning effect\nlearning_effect &lt;- function(time, Asym, R0, lrc) {\n  Asym + R0 * exp(-exp(lrc) * time)\n}\n\n# Fit the model\nmodel &lt;- nlme(\n  vx ~ learning_effect(gt.train.scaled, Asym, R0, lrc) * condit,\n  fixed = Asym + R0 + lrc ~ 1,\n  random = Asym + R0 ~ 1 | id,\n  start = c(Asym = 1, R0 = 1, lrc = 0),\n  data = e1[e1$expMode==\"train\",]\n)\n\ne1$condit_numeric &lt;- as.numeric(e1$condit)\n\n# Fit the model\nmodel &lt;- nlme(\n  vx ~ learning_effect(gt.train.scaled, Asym, R0, lrc) * condit_numeric,\n  fixed = Asym + R0 + lrc ~ 1,\n  random = Asym + R0 ~ 1 | id,\n  start = c(Asym = 1, R0 = 1, lrc = 0),\n  data = e1[e1$expMode==\"train\",]\n)\nsummary(model)\n\n\n\nmodel &lt;- nlme(\n  # Reflects the shift in velocity across conditions and time\n  vx ~ learning_effect(gt.train.scaled, Asym, R0, lrc) * condit_numeric,\n  # Fixed effects structure\n  fixed = Asym + R0 + lrc ~ 1,\n  # Random effects structure, now includes 'lrc' for individual learning rates\n  random = Asym + R0 + lrc ~ 1 | id,\n  # Starting values for the parameters\n  start = c(Asym = 1, R0 = 1, lrc = 0),\n  # Subset of the data used for training phase\n  data = e1 %&gt;% filter(expMode==\"train\")\n)\n\n# Prints out a summary of the model\nsummary(model)"
  },
  {
    "objectID": "Analysis/brms_learning.html",
    "href": "Analysis/brms_learning.html",
    "title": "Bayesian Mixed Effects Models - Training Data",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes,brms,bayesplot,broom,broom.mixed,lme4,emmeans,here,knitr,kableExtra,gt,gghalves,patchwork,ggforce,ggdist)\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\"))\nsource(here(\"Functions/Packages.R\"))\ntrain &lt;- e1 |&gt; filter(expMode2 == \"Train\")  \ntrain$bandIntS &lt;- scale(train$bandInt)\ntrain$distS &lt;- scale(train$dist)\n\noptions(brms.backend=\"cmdstanr\",mc.cores=4)\n\n\n\nCodeprior &lt;- c(\n  prior(normal(200, 200), lb=0, nlpar = \"a\"),\n  prior(normal(800, 300), lb=0,  nlpar = \"b\"),\n  prior(normal(.3, .2), lb=0,  nlpar = \"c\")\n)\n\nprior &lt;- c(\n  prior(normal(200, 200), lb=0, nlpar = \"a\"),\n  prior(normal(800, 300), lb=0,  nlpar = \"b\"),\n  prior(lognormal(.3, .2), lb=0,  nlpar = \"c\")\n)\n\n\n\nnlform &lt;- bf(\n  dist ~ a + (b - a) * exp(-c * gt.train),\n  a ~ 0 + condit + (1|bandInt) + (1|id), \n  b ~ 0 + condit + (1|bandInt)  + (1|id), \n  c ~ 0 + condit + (1|bandInt) + (1|id), \n  nl = TRUE \n)\n\nfit &lt;- brm(\n  formula = nlform,\n  data = e1 |&gt; filter(expMode2==\"Train\"),\n  family = gaussian(), # Assuming 'dist' is continuous; adjust as needed\n  prior = prior,\n  silent=0,\n  control = list(adapt_delta = 0.90), # Adjust for convergence issues if necessary\n  chains = 2, iter = 600 # Adjust these values based on your needs\n)\n\nsummary(fit)\nbayestestR::describe_posterior(fit)\ncoef(fit)$id |&gt; as_tibble(rownames=\"id\") |&gt; select(id,starts_with(\"Esti\")) |&gt; print(n=15)\n\n\n\nCodewalk(c(\"brms\",\"dplyr\"), conflict_prefer_all, quiet = TRUE)\noptions(brms.backend=\"cmdstanr\",mc.cores=2)\n\nnlform &lt;- bf(\n  dist ~ a + (b - a) * exp(-c * gt.train),\n  a ~ 1 + condit + vb, # a as a function of condit and vb\n  b ~ 1 + condit + vb, # b as a function of condit and vb\n  c ~ 1 + condit + vb, # c as a function of condit and vb\n  nl = TRUE # Indicates this is a non-linear model\n)\n\n# Define the prior (adjust according to your domain knowledge)\nprior &lt;- c(\n  prior(normal(150, 50), nlpar = \"a\"),\n  prior(normal(800, 200), nlpar = \"b\"),\n  prior(normal(.3, .1), nlpar = \"c\")\n)\n\n\nfit &lt;- brm(\n  formula = nlform,\n  data = e1 |&gt; filter(expMode2==\"Train\"),\n  family = gaussian(), # Assuming 'dist' is continuous; adjust as needed\n  prior = prior,\n  control = list(adapt_delta = 0.95), # Adjust for convergence issues if necessary\n  chains = 2, iter = 1000 # Adjust these values based on your needs\n)\n\n# Display the summary of the model\nsummary(fit)\n\n# To check diagnostics\nplot(fit)\n\n\nformula &lt;- bf(dist ~ a + (b - a) * exp(-c * gt.train),\n              a + b + c ~ 1 + condit*vb,\n              nl = TRUE)\n\n# Fit the model with brms\nfit &lt;- brm(formula,\n           data = filtered_data,\n           family = gaussian,\n           chains = 4, cores = 4,\n           control = list(adapt_delta = 0.99, max_treedepth = 15),\n           iter = 4000, warmup = 1000)\n\n\nOLD\n\nCodemod_train_expo3_dist &lt;-bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 1 + (1|id), \n                 alphaMu ~ 1 +   (1|id), \n                 gammaMu ~ 1 +  (1|id), \n                 nl = TRUE)\n\ne1_train_expo3_dist &lt;- brm(mod_train_expo3_dist,\n                            chains=4, iter=2000, silent=0,\n                            file=here::here(\"data/model_cache/e1_train_expo3_dist\"))\n\n\nmod_train_expo3_condit_dist &lt;- bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 1 + condit + (1|id), \n                 alphaMu ~ 1 + condit +   (1|id), \n                 gammaMu ~ 1 + condit + (1|id), \n                 nl = TRUE)\n\ne1_train_expo3_condit_dist &lt;- brm(mod_train_expo3_condit_dist,\n                            chains=4, iter=2000, silent=0,\n                            file=here::here(\"data/model_cache/e1_train_expo3_condit_dist\"))\n\n\n\nmod_train_expo3_conditBandit_dist &lt;- bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 0 + condit + (1|id) + (1|id:bandInt), \n                 alphaMu ~ 0 + condit + (1|id) + (1|id:bandInt), \n                 gammaMu ~ 0 + condit  +  (1|id) + (1|id:bandInt), \n                 nl = TRUE)\n\n\n\ne1_train_expo3_conditBanditS_dist &lt;- brm(mod_train_expo3_conditBandit_dist,\n                            chains=4, iter=2000, silent=0,\n                            data=train,\n                            file=here::here(\"data/model_cache/e1_train_expo3_conditBanditSdist\"))\n\n\n\n\nmod_train_expo3_band_distS &lt;- bf(distS ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 1 + bandIntS + (1|id), \n                 alphaMu ~ 1 + bandIntS +   (1|id), \n                 gammaMu ~ 1 + bandIntS + (1|id), \n                 nl = TRUE)\n\ne1_train_expo3_band_distS &lt;- brm(mod_train_expo3_band_distS,\n                            chains=4, iter=2000, silent=0,\n                            data=train,\n                            file=here::here(\"data/model_cache/e1_train_expo3_band_distS\"))\ncoef(e1_train_expo3_band_distS)$id |&gt; as_tibble(rownames=\"id\") |&gt; select(starts_with(\"Esti\")) |&gt; print(n=15)\n\n\n\n\n mod_train_expo3_bandCondit_distS &lt;- bf(distS ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 1 + bandIntS + condit + (1|id), \n                 alphaMu ~ 1 + bandIntS + condit +  (1|id), \n                 gammaMu ~ 1 + bandIntS + condit + (1|id), \n                 nl = TRUE)\n\ne1_train_expo3_bandCondit_distS &lt;- brm(mod_train_expo3_bandCondit_distS,\n                            chains=4, iter=2000, silent=0,\n                            data=train,\n                            file=here::here(\"data/model_cache/e1_train_expo3_bandCondit_distS\"))                           \ncoef(e1_train_expo3_bandCondit_distS)$id |&gt; as_tibble(rownames=\"id\") |&gt; select(starts_with(\"Esti\")) |&gt; print(n=15)\n\n\nb_mod3 &lt;- bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train), betaMu ~ 1 + condit + bandInt + (1|id), alphaMu ~ 1 + condit + bandInt + (1|id), gammaMu ~ 1 + condit + bandInt + (1|id), nl = TRUE)"
  },
  {
    "objectID": "Analysis/combo_train.html",
    "href": "Analysis/combo_train.html",
    "title": "Learning Curves",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,ggplot2,here,conflicted,ggpmisc,patchwork,ggh4x)\nwalk(c(\"dplyr\"), conflict_prefer_all, quiet = TRUE)\nwalk(c(\"Display_Functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) \ne2 &lt;- readRDS(here(\"data/e2_08-04-23.rds\")) \ne3 &lt;- readRDS(here(\"data/e3_08-04-23.rds\")) \nd &lt;- rbind(e1,e2,e3)\n\n\n\nCodelearn_curve_plot &lt;- function(df, x_var, y_var, color_var, facet_var = NULL, groupVec, nbins, labels = FALSE, y_label=NULL) {\n\n  if (is.null(y_label)) {\n    y_label &lt;- deparse(substitute(y_var))\n  }\n  df |&gt; \n    group_by(!!!syms(groupVec)) |&gt; \n    mutate(Trial_Bin = cut( {{x_var}}, breaks = seq(1, max({{x_var}}),length.out=nbins+1),include.lowest = TRUE, labels = labels)) |&gt; \n    ggplot(aes(x = Trial_Bin, y = {{ y_var }}, color = {{ color_var }})) +\n    stat_summary(aes(color = {{ color_var }}), geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    #facet_wrap(vars({{facet_var}}), scales = 'free_y') + \n    labs(y = y_label) + # Set the y axis label dynamically\n    scale_x_continuous(breaks = seq(1, nbins + 1)) \n}\n\nnb=6\ne1 |&gt; filter(expMode==\"train\") %&gt;% \n    learn_curve_plot(gt.train,dist,condit,facet_var=vb,groupVec = c(\"id\", \"condit\",\"vb\"),nbins=nb) + facet_wrap(~vb)\n\n\n\n\n\n\nCoded |&gt; filter(expMode==\"train\") %&gt;% \n    learn_curve_plot(gt.train,dist,condit,facet_var=c(\"Exp\",\"vb\"),groupVec = c(\"id\", \"condit\",\"vb\"),nbins=nb) + facet_wrap(~bandOrder*fb*vb,ncol=3)\n\n\n\n\n\n\nCode# d |&gt; group_by(id,condit,vb) |&gt; ggplot(aes(x=gt.train,y=dist,col=condit)) + geom_smooth() + \n#     facet_wrap(~bandOrder*fb*vb,ncol=3)\n# \n# d |&gt; group_by(gt.train,condit) |&gt; ggplot(aes(x=gt.train,y=dist,col=condit)) + geom_smooth(method=\"loess\") + \n#     facet_wrap(~bandOrder*fb*vb,ncol=3)\n\n\n\nCodenbins=4\ntrainE1 &lt;-  e1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE1_max &lt;- trainE1 |&gt; filter(Trial_Bin == nbins, bandInt==800) |&gt; \n  group_by(id,condit) |&gt; summarize(dist=mean(dist)) |&gt; arrange(dist) |&gt;\n  mutate(id=reorder(id,dist))\n\n\n\nnbins=10\n e1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE))  |&gt; filter(expMode==\"train\", id %in% unique(trainE1_max$id)[1:55]) |&gt;\n   ggplot(aes(x = Trial_Bin, y = vx, color = vb)) +\n    stat_summary(geom = \"line\", fun = median) +\n    #stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~id,scales=\"free\")+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n\n\n\n\n\n\nCode  e1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE))  |&gt; filter(expMode==\"train\", id %in% unique(trainE1_max$id)[1:55]) |&gt;\n    ggplot(aes(x = Trial_Bin, y = dist, color = vb)) +\n    stat_summary(geom = \"line\", fun = median) +\n    #stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~id,scales=\"fixed\")+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n\n\n\n\n\n\nCodee1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE))  |&gt; filter(expMode==\"train\", id %in% unique(e1$id)[1:55]) |&gt; \n    ggplot(aes(x = Trial_Bin, y = dist,color=condit)) +\n    stat_summary(geom = \"line\", fun = median) +\n    #stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~id,scales=\"free\")+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n\n\n\n\n\n\nCodee1  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n  mutate(mt=max(trial)) |&gt;\n    mutate(Trial_Bin = cut( gt.stage, breaks = seq(1, max(gt.stage),length.out=nbins+1),include.lowest = TRUE, labels=FALSE))  |&gt; \n  #filter(expMode==\"train\", id %in% unique(e1$id)[1:55]) |&gt; \n    ggplot(aes(x = Trial_Bin, y = dist,color=condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    ggh4x::facet_wrap2(~expMode2+vb,scales=\"free_x\",ncol=3)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n\n\n\n\n\n\nCodee1  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n  mutate(mt=max(trial)) |&gt;\n    mutate(Trial_Bin = cut( gt.stage, breaks = seq(1, max(gt.stage),length.out=nbins+1),include.lowest = TRUE, labels=FALSE))  |&gt; \n  #filter(expMode==\"train\", id %in% unique(e1$id)[1:55]) |&gt; \n    ggplot(aes(x = Trial_Bin, y = vx,color=vb)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    ggh4x::facet_wrap2(~expMode2+condit,scales=\"free_x\",ncol=2)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n\n\n\n\n\n\n\n\nCodep1 &lt;- e1  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = vb, y = vx,fill=expMode2)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\np2 &lt;- e1  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = expMode2, y = vx,fill=vb)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\np1 + p2\n\n\n\n\n\n\nCodep1 &lt;- e1  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = vb, y = dist,fill=expMode2)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\np2 &lt;- e1  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = expMode2, y = dist,fill=vb)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\np1 + p2\n\n\n\n\n\n\n\nE2\n\nCodep1 &lt;- e2  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = vb, y = vx,fill=expMode2)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\np2 &lt;- e2  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = expMode2, y = vx,fill=vb)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\np1 + p2\n\n\n\n\n\n\nCodep1 &lt;- e2  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = vb, y = dist,fill=expMode2)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\np2 &lt;- e2  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = expMode2, y = dist,fill=vb)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\np1 + p2\n\n\n\n\n\n\n\nE3\n\nCodep1 &lt;- e3  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = vb, y = vx,fill=expMode2)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~bandOrder+condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\np2 &lt;- e3  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = expMode2, y = vx,fill=vb)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~bandOrder+condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\np1 + p2\n\n\n\n\n\n\nCodep1 &lt;- e3  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = vb, y = dist,fill=expMode2)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~bandOrder+condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\np2 &lt;- e3  |&gt; group_by(id,condit, vb,expMode2) |&gt; \n    ggplot(aes(x = expMode2, y = dist,fill=vb)) +\n    stat_bar + \n    ggh4x::facet_wrap2(~bandOrder+condit,scales=\"free_x\",ncol=2)+\n    theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\np1 + p2\n\n\n\n\n\n\n\n\nCodenb=3\nformula &lt;- y ~ poly(x,3,raw=TRUE)\nformula &lt;- y ~ poly(x,2,raw=TRUE)\n\nd  |&gt; filter(expMode==\"train\") %&gt;% group_by(id,gt.train,condit,vb) |&gt;\n    select(id,condit,bandOrder,fb,gt.train,vb,expMode,dist) |&gt;\n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, nb + 1), include.lowest = TRUE, labels = FALSE)) |&gt; \n    ggplot(aes(x=Trial_Bin,y=dist,col=condit)) + \n    geom_smooth(method=\"lm\",formula=formula) + \n    facet_wrap(~bandOrder*fb*vb,ncol=3)\n\nWarning: Removed 37477 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\nCoded  |&gt; filter(expMode==\"train\") %&gt;% group_by(id,gt.train,condit,vb) |&gt;\n    select(id,condit,bandOrder,fb,gt.train,vb,expMode,dist) |&gt;\n    ggplot(aes(x=gt.train,y=dist,col=condit)) + \n    geom_smooth(method=\"lm\",formula=formula) + \n    facet_wrap(~bandOrder*fb*vb,ncol=3)\n\n\n\n\n\n\nCoded  |&gt; filter(expMode==\"train\",gt.train&lt;=60) %&gt;% group_by(id,gt.train,condit,vb) |&gt;\n    ggplot(aes(x=gt.train,y=dist,col=condit)) + \n    geom_smooth(method=\"lm\",formula=formula) + \n    facet_wrap(~bandOrder*fb*vb,ncol=3) + \n  stat_poly_eq(geom = \"text\", aes(label = after_stat(eq.label)),\n               label.x = c(60, 0), label.y = c(-0.1, 400.1), hjust = \"inward\",\n               formula = formula)\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\nCodemicmen.formula &lt;- y ~ SSmicmen(x, Vm, K)\nd  |&gt; filter(expMode==\"train\",gt.train&lt;=60) %&gt;% group_by(id,gt.train,condit,vb) |&gt;\n    ggplot(aes(x=gt.train,y=dist,col=condit)) + \n    geom_smooth(method=\"nls\",formula = micmen.formula,se=FALSE) + \n    facet_wrap(~bandOrder*fb*vb,ncol=3)\n\n\n\n\n\n\nCoded |&gt;\n  filter(expMode == \"train\", gt.train &lt;= 60) |&gt;\n  group_by(id, gt.train, condit, vb) |&gt;\n  ggplot(aes(x = gt.train, y = dist, col = condit)) +\n  geom_smooth(method = \"nls\", formula = micmen.formula, se = FALSE, method.args = list(start = list(Vm = 1, K = 1))) +\n  facet_wrap(~bandOrder * fb * vb, ncol = 3)\n\n\n\n\n\n\nCode#bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train)\n\n\nexp_model_formula &lt;- y ~ a * exp(b * x)\nd |&gt; filter(expMode == \"train\", gt.train &lt;= 60) %&gt;%\n  group_by(id, gt.train, condit, vb) %&gt;%\n  ggplot(aes(x = gt.train, y = dist, col = condit)) +\n  geom_smooth(method = \"nls\", formula = exp_model_formula, se = FALSE, method.args = list(start = list(a = 1, b = 0.1))) +\n  facet_wrap(~bandOrder * fb * vb, ncol = 3)\n\n\n\n\n\n\nCodenb=3\ne1 |&gt; group_by(id) |&gt; \n    select(id,condit,bandOrder,gt.train,vb,expMode2,dist) |&gt;\n    mutate(Block=case_when(expMode2==\"Train\" ~ cut(gt.train,breaks=seq(1,max(gt.train), length.out=nb+1),include.lowest=TRUE,labels=FALSE),expMode2==\"Test\" ~ nb+1)) %&gt;%\n     ggplot(aes(x=Block,y=dist,col=condit)) + \n    geom_smooth(method=\"lm\", formula= formula) + \n    facet_wrap(~vb)\n\nWarning: Removed 8188 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nCodethree_param_exp_formula &lt;- y ~ a + (b - a) * exp(-c * x)\nd |&gt; filter(expMode == \"train\", gt.train &lt;= 60) %&gt;%\n  group_by(id, gt.train, condit, vb) %&gt;%\n  ggplot(aes(x = gt.train, y = dist, col = condit)) +\n  geom_smooth(method = \"nls\", \n              formula = three_param_exp_formula, \n              method.args = list(start = list(a = 100, b = 500, c = 0.01)), # Starting values for a, b, c\n              se = FALSE) +\n  facet_wrap(~bandOrder * fb * vb, ncol = 3) \n\nWarning: Failed to fit group 1.\nCaused by error in `method()`:\n! singular gradient\n\n\nWarning: Failed to fit group 2.\nCaused by error in `method()`:\n! singular gradient\n\n\n\n\n\n\n\nCodetwo_param_exp_formula &lt;- y ~ a  * exp(-c * x)\nd |&gt; filter(expMode == \"train\") %&gt;%\n  group_by(id, gt.train, condit, vb) %&gt;%\n  ggplot(aes(x = gt.train, y = dist, col = condit)) +\n  geom_smooth(method = \"nls\", \n              formula = two_param_exp_formula, \n              method.args = list(start = list(a = 500, c = 0.09)), # Starting values for a, b, c\n              se = FALSE) +\n  facet_wrap(~bandOrder * fb * vb, ncol = 3) \n\n\n\n\n\n\n\n\nCodefiltered_data &lt;- d |&gt; \n  filter(expMode == \"train\", gt.train &lt;= 60)\n\n# Fit the model\nnls_fit &lt;- nls(dist ~ a + (b - a) * exp(-c * gt.train), \n               data = filtered_data, \n               start = list(a = 100, b = 500, c = 0.01))\n\n# Extract coefficients\ncoeffs &lt;- coef(nls_fit)\n\n# Create label (customize as needed)\nlabel_text &lt;- sprintf(\"y = %.2f + (%.2f - %.2f) * exp(-%.2f * x)\", coeffs[\"a\"], coeffs[\"b\"], coeffs[\"a\"], coeffs[\"c\"])\n\n# Plot with custom label\nfiltered_data %&gt;%\n  ggplot(aes(x = gt.train, y = dist, col = condit)) +\n  geom_smooth(method = \"nls\", \n              formula = y ~ a + (b - a) * exp(-c * x), \n              method.args = list(start = list(a = 100, b = 500, c = 0.01)),\n              se = FALSE) +\n  ggpp::annotate(\"text\", x = Inf, y = Inf, label = label_text, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~bandOrder * fb * vb, ncol = 3) \n\nWarning: Failed to fit group 1.\nCaused by error in `method()`:\n! singular gradient\n\n\nWarning: Failed to fit group 2.\nCaused by error in `method()`:\n! singular gradient\n\n\n\n\n\n\n\nCodelibrary(ggpmisc)\n\nd |&gt; filter(expMode == \"train\", gt.train &lt;= 60) %&gt;%\n  group_by(id, gt.train, condit, vb) %&gt;%\n  ggplot(aes(x = gt.train, y = dist, col = condit)) +\n  geom_smooth(method = \"nlsLM\",\n              formula = y ~ three_param_exp_formula(x, a, b, c),\n              method.args = list(start = list(a = 100, b = 500, c = 0.01)),\n              se = FALSE) +\n  # stat_equation(aes(label = after_stat(eq.label)),\n  #               formula = y ~ three_param_exp_formula(x, a, b, c),\n  #               parse = TRUE) +\n  facet_wrap(~bandOrder * fb * vb, ncol = 3)\n\nWarning: Computation failed in `stat_smooth()`.\nCaused by error in `get()`:\n! object 'nlsLM' of mode 'function' was not found\n\n\nWarning: Computation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nComputation failed in `stat_smooth()`.\nCaused by error in `get()`:\n! object 'nlsLM' of mode 'function' was not found\n\n\n\n\n\n\n\n\nJoin the labels back to the original data\nfiltered_data &lt;- left_join(filtered_data, labels_df, by = c(“condit”, “bandOrder”, “fb”))\nPlot with annotations\nggplot(filtered_data, aes(x = gt.train, y = dist, col = condit)) + geom_smooth(method = “nls”, formula = y ~ a + (b - a) * exp(-c * x), method.args = list(start = list(a = 100, b = 500, c = 0.01)), se = FALSE) + facet_wrap(~bandOrder * fb * vb, ncol = 3) + geom_text(data = labels_df, aes(label = label, x = 50, y = Inf), hjust = 0.5, vjust = 1, size = 3, inherit.aes = FALSE)\n\nCodefit_model_and_return_label &lt;- function(data, condit, bandOrder, fb) {\n  # Default starting parameters\n  start_params &lt;- list(a = 100, b = 500, c = 0.01)\n  \n  # Adjust starting parameters for the problematic condition\n  if (condit == \"Constant\" && fb == \"Ordinal\") {\n    start_params &lt;- list(a = 100, b = 250, c = 0.01)\n  }\n  tryCatch({\n    model &lt;- nls(dist ~ a + (b - a) * exp(-c * gt.train), data = data, start = start_params)\n    coeffs &lt;- coef(model)\n    label=sprintf(\"y = %.2f + (%.2f - %.2f) * exp(-%.2f * x)\", coeffs[\"a\"], coeffs[\"b\"], coeffs[\"a\"], coeffs[\"c\"])\n    data.frame(label = label, a = coeffs[\"a\"], b = coeffs[\"b\"], c = coeffs[\"c\"])\n  }, error = function(e) {\n    # Return NA or some indication of failure\n    return(data.frame(label = NA, a = NA, b = NA, c = NA))\n  })\n}\n\nlabels_df &lt;- filtered_data %&gt;%\n  group_by(condit, bandOrder, fb,vb) %&gt;%\n  nest() %&gt;%\n  mutate(result = map(data, ~fit_model_and_return_label(.x, condit, bandOrder, fb)),\n         data = NULL) %&gt;%\n  unnest(result) %&gt;%\n  ungroup()\n\nhead(labels_df)\n\n# A tibble: 6 × 8\n  condit   fb         vb        bandOrder label                    a     b     c\n  &lt;fct&gt;    &lt;fct&gt;      &lt;fct&gt;     &lt;fct&gt;     &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Varied   Continuous 1000-1200 Original  y = 199.90 + (551.8…  200.  552. 0.247\n2 Varied   Continuous 1200-1400 Original  y = 251.42 + (553.9…  251.  554. 0.156\n3 Varied   Continuous 800-1000  Original  y = 192.56 + (375.6…  193.  376. 0.171\n4 Constant Continuous 800-1000  Original  y = 123.33 + (473.5…  123.  474. 0.236\n5 Constant Continuous 600-800   Reverse   y = 105.74 + (513.6…  106.  514. 0.308\n6 Varied   Continuous 350-550   Reverse   y = 102.66 + (502.0…  103.  502. 0.386\n\nCodegenerate_predictions &lt;- function(x, a, b, c) {\n  return(a + (b - a) * exp(-c * x))\n}\n\n\nplot_data &lt;- filtered_data %&gt;%\n  left_join(labels_df, by = c(\"condit\", \"bandOrder\", \"fb\"))\n\nWarning in left_join(., labels_df, by = c(\"condit\", \"bandOrder\", \"fb\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nCode# Define a sequence of x values for generating model predictions (adjust as needed)\nx_vals &lt;- seq(min(filtered_data$gt.train), max(filtered_data$gt.train), length.out = 100)\n\n# Create a new data frame for model predictions\npredictions_data &lt;- labels_df %&gt;%\n  mutate(x = list(x_vals)) %&gt;%\n  unnest(x) %&gt;%\n  mutate(y = pmap_dbl(list(a, b, c, x), ~ generate_predictions(..4, ..1, ..2, ..3)))\n\n# Plot\nggplot() +\n  #geom_point(data = plot_data, aes(x = gt.train, y = dist, color = condit), alpha = 0.6) +\n  geom_line(data = predictions_data, aes(x = x, y = y, color = condit), size = 1) +\n  facet_wrap(~ bandOrder * fb , ncol = 2, scales = \"free_y\") +\n  labs(x = \"GT Train\", y = \"Dist\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nCodeggplot(filtered_data, aes(x = gt.train, y = dist, color = condit)) +\n  geom_point() +\n  stat_function(fun = function(x) with(labels_df[1, ], a + (b - a) * exp(-c * x)),\n                linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_grid(fb ~ bandOrder, labeller = label_both) +\n  labs(title = \"Model Fits of Learning Curves\",\n       x = \"Training Generation\",\n       y = \"Distance\",\n       color = \"Condition\") +\n  theme_bw()\n\n\n\n\n\n\n\ncreate learning models for condit and varied groups.\nWe can model the relation between performance and the number of practice trials as a power law function, or exponential function. Aggregatign over ids in dst. The models predict dist as an exponential decay function of trial number. Band is an additional predictor.\n\\[\nf_p(t) = \\alpha + \\beta t^{r} \\enspace\n\\]\n\\[\nf_e(t) = \\alpha + \\beta e^{rt} \\enspace\n\\]\n\nCode# fit exponential decay model as a function of trial number\n\nfit_exp &lt;- function(trial,dist,input){\n    # fit exponential decay model as a function of trial number, band is an additional predictor\n    fit &lt;- nls(dist ~ yf + (y0-yf) * exp(-r*trial) + beta2*input, start = list(yf = 300, y0 = 364, beta2=0, r = .1), data = data.frame(trial=trial,dist=dist,input=input))\n\n    # extract parameters\n    alpha &lt;- coef(fit)[1]\n    beta &lt;- coef(fit)[2]\n    beta2 &lt;- coef(fit)[3]\n    r &lt;- coef(fit)[4]\n    sigma_e &lt;- summary(fit)$sigma\n\n    # compute negative log likelihood\n    nllh &lt;- negative_llh_exp(dist, trial, alpha, beta, r, sigma_e)\n\n    # return parameters and negative log likelihood\n    return(list(alpha=alpha,beta=beta,beta2=beta2,r=r,sigma_e=sigma_e,nllh=nllh))\n}\n\n# Compute group averages for dist over trial and band. dst \n\navgTrain &lt;- e1 |&gt; filter(expMode2==\"Train\") %&gt;% group_by(id,condit,gt.train,vb,bandInt) %&gt;% \n  summarise(dist=mean(dist)) %&gt;% ungroup() %&gt;% \n  group_by(condit,gt.train,vb,bandInt) %&gt;% \n  summarise(dist=mean(dist)) %&gt;% ungroup()\n \nggplot(avgTrain,aes(x=gt.train,y=dist)) + geom_line(aes(group=vb,color=vb)) +facet_grid(~condit)\n\navgTrain %&gt;% filter(condit==\"Constant\") %&gt;% nls(dist ~ yf + (y0-yf) * exp(-r*gt.train), start = list(yf = 120, y0 = 364, r = .1), data = .) %&gt;% summary()\n\navgTrain %&gt;% filter(condit==\"Constant\") %&gt;% nls(dist ~ SSasymp(gt.train, yf, y0, log_alpha),data=.)\n\nfit_condit &lt;- avgTrain %&gt;% group_by(condit) %&gt;% do(fit_exp(trial=.$gt.train,dist=.$dist,input=.$bandInt))\n\n\n#avgTrain %&gt;% group_by(condit) |&gt; mutate(fit=map(~fit_exp(trial=gt.train,dist=dist,input=bandInt)))\n\n\nInterpretation of improvement_model:\nThe intercept represents the performance when all factors are at their reference levels (Constant condition, original category order, and continuous feedback type). Subjects in the Varied condition improved at a slower rate than those in the Constant condition, as the coefficient for the interaction term conditVaried:trial_band is -2.37284, with a t-value of -6.940. Subjects in the Varied condition with reversed category order showed a greater decrease in performance, as the coefficient for the interaction term conditVaried:bandOrderrev is -43.67731, with a t-value of -2.323. Other significant factors and interactions include trial_band, bandOrderrev, trial_band:bandOrderrev, and trial_band:fbordinal. Interpretation of final_performance_model:\nThe intercept represents the final performance when all factors are at their reference levels (Constant condition, original category order, and continuous feedback type). Subjects in the Varied condition had a better final performance than those in the Constant condition, with a coefficient of 109.73 and a t-value of 4.362. The interaction between the Varied condition and reversed category order (conditVaried:bandOrderrev) had a negative impact on the final performance, with a coefficient of -92.75 and a t-value of -2.342. The interaction between the Varied condition and ordinal feedback type (conditVaried:fbordinal) also had a negative impact on the final performance, with a coefficient of -85.44 and a t-value of -2.079. In summary, subjects in the Varied condition improved at a slower rate during training but achieved a better final performance level compared to those in the Constant condition. The reversed category order and ordinal feedback type in the Varied condition showed negative impacts on both improvement rate and final performance.\nExponential learning model\n\nCodelibrary(dplyr)\nlibrary(tidyr)\nlibrary(nls.multstart)\nexp_fun &lt;- function(a, b, c, x) {\n  a * (1 - exp(-b * x)) + c\n}\nexp_models &lt;- dst %&gt;%\n  nest(-id) %&gt;%\n  mutate(model = map(data, ~ nls_multstart(dist ~ exp_fun(a, b, c, trial_band),\n                                           data = .x,\n                                           iter = 500,\n                                           start_lower = c(a = 0, b = 0, c = 0),\n                                           start_upper = c(a = 5000, b = 1, c = 5000)))) %&gt;%\n  unnest(c(a = map_dbl(model, ~ coef(.x)['a']),\n           b = map_dbl(model, ~ coef(.x)['b']),\n           c = map_dbl(model, ~ coef(.x)['c'])))\ngroup_averages &lt;- exp_models %&gt;%\n  group_by(condit, bandOrder, fb) %&gt;%\n  summarise(a_avg = mean(a), b_avg = mean(b), c_avg = mean(c))\naic_improvement &lt;- AIC(improvement_model)\naic_final_performance &lt;- AIC(final_performance_model)\nexp_models &lt;- exp_models %&gt;%\n  mutate(aic = map_dbl(model, AIC))\n\naic_exp_avg &lt;- exp_models %&gt;%\n  summarise(aic_avg = mean(aic))",
    "crumbs": [
      "Misc",
      "Learning Curves"
    ]
  },
  {
    "objectID": "Analysis/e1.html",
    "href": "Analysis/e1.html",
    "title": "Experiment 1",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n  brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n  stringr, here,conflicted, patchwork, knitr,kableExtra)\n#options(brms.backend=\"cmdstanr\",mc.cores=4)\noptions(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\nwalk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) \ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ntestE1 &lt;- e1 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE1 &lt;-  e1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE1_max &lt;- trainE1 |&gt; filter(Trial_Bin == nbins, bandInt==800)\ntrainE1_avg &lt;- trainE1_max |&gt; group_by(id,condit) |&gt; summarise(avg = mean(dist))",
    "crumbs": [
      "Analyses",
      "Experiment 1"
    ]
  },
  {
    "objectID": "Analysis/e1.html#e1-discussion",
    "href": "Analysis/e1.html#e1-discussion",
    "title": "Experiment 1",
    "section": "E1 Discussion",
    "text": "E1 Discussion\nIn Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions rresulted in lower final training performance is consistent with much of the prior researchon the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.\n\n\n\nFigure 1: E1. Deviations from target band across training blocks.\nFigure 2: E1. A) Deviations from target band during testing without feedback stage. B) Estimated marginal means for the interaction between training condition and band type. Error bars represent 95% credible intervals.\nFigure 3: E1 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\nExperiment 1. Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.",
    "crumbs": [
      "Analyses",
      "Experiment 1"
    ]
  },
  {
    "objectID": "Analysis/e1_test.html",
    "href": "Analysis/e1_test.html",
    "title": "HTW E1 Testing",
    "section": "",
    "text": "Analyses Strategy\nAll data processing and statistical analyses were performed in R version 4.31 Team (2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R Bürkner (2017), and descriptive stats and tables were extracted with the BayestestR package Makowski et al. (2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as circumventing convergence issues common to the frequentist analogues of our mixed models. For each model, we report the median values of the posterior distribution, and 95% credible intervals.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 of which were discarded as warmup chains. Rhat values were generally within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for for the random effects.\nWe compared varied and constant performance across two measures, deviation and discrimination. Deviation was quantified as the absolute deviation from the nearest boundary of the velocity band, or set to 0 if the throw velocity fell anywhere inside the target band. Thus, when the target band was 600-800, throws of 400, 650, and 1100 would result in deviation values of 200, 0, and 300, respectively. Discrimination was measured by fitting a linear model to the testing throws of each subjects, with the lower end of the target velocity band as the predicted variable, and the x velocity produced by the participants as the predictor variable. Participants who reliably discriminated between velocity bands tended to have positive slopes with values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.\n\n\nCode# Create the data frame for the table\ntable_data &lt;- data.frame(\n  Type = c(\n    rep(\"Population-Level Effects\", 4),\n    rep(\"Group-Level Effects\", 2),\n    \"Family Specific Parameters\"\n  ),\n  Parameter = c(\n    \"\\\\(\\\\beta_0\\\\)\", \"\\\\(\\\\beta_1\\\\)\", \"\\\\(\\\\beta_2\\\\)\", \"\\\\(\\\\beta_3\\\\)\",\n    \"\\\\(\\\\sigma_{\\\\text{Intercept}}\\\\)\", \"\\\\(\\\\sigma_{\\\\text{bandInt}}\\\\)\", \"\\\\(\\\\sigma_{\\\\text{Observation}}\\\\)\"\n  ),\n  Term = c(\n    \"(Intercept)\", \"conditVaried\", \"bandInt\", \"conditVaried:bandInt\",\n    \"sd__(Intercept)\", \"sd__bandInt\", \"sd__Observation\"\n  ),\n  Description = c(\n    \"Intercept representing the baseline deviation\", \"Effect of condition (Varied vs. Constant) on deviation\", \"Effect of target velocity band (bandInt) on deviation\", \"Interaction effect between training condition and target velocity band on deviation\",\n    \"Standard deviation for (Intercept)\", \"Standard deviation for bandInt\", \"Standard deviation for Gaussian Family\"\n  )\n) |&gt;   mutate(\n    Term = glue::glue(\"&lt;code&gt;{Term}&lt;/code&gt;\")\n  ) \n\n# Create the table\nkable_out &lt;- table_data %&gt;%\n  kbl(format = 'html', escape = FALSE, booktabs = TRUE, \n      #caption = '&lt;span style = \"color:black;\"&gt;&lt;center&gt;&lt;strong&gt;Table 1: General Model Structure Information&lt;/strong&gt;&lt;/center&gt;&lt;/span&gt;',\n      col.names = c(\"Type\", \"Parameter\", \"Term\", \"Description\")) %&gt;%\n  kable_styling(position=\"left\", bootstrap_options = c(\"hover\"), full_width = FALSE) %&gt;%\n  column_spec(1, bold = FALSE, border_right = TRUE) %&gt;%\n  column_spec(2, width = '4cm') %&gt;%\n  column_spec(3, width = '4cm') %&gt;%\n  row_spec(c(4, 7), extra_css = \"border-bottom: 2px solid black;\") %&gt;%\n  pack_rows(\"\", 1, 4, bold = FALSE, italic = TRUE) %&gt;%\n  pack_rows(\"\", 5, 6, bold = FALSE, italic = TRUE) %&gt;%\n  pack_rows(\"\", 7, 7, bold = FALSE, italic = TRUE)\n\nkable_out\n\n\nTable 1: Mixed model structure and coefficient descriptions\n\n\n\n\nType\nParameter\nTerm\nDescription\n\n\n\n\n\n\nPopulation-Level Effects\n\\(\\beta_0\\)\n(Intercept)\nIntercept representing the baseline deviation\n\n\nPopulation-Level Effects\n\\(\\beta_1\\)\nconditVaried\nEffect of condition (Varied vs. Constant) on deviation\n\n\nPopulation-Level Effects\n\\(\\beta_2\\)\nbandInt\nEffect of target velocity band (bandInt) on deviation\n\n\nPopulation-Level Effects\n\\(\\beta_3\\)\nconditVaried:bandInt\nInteraction effect between training condition and target velocity band on deviation\n\n\n\n\n\nGroup-Level Effects\n\\(\\sigma_{\\text{Intercept}}\\)\nsd__(Intercept)\nStandard deviation for (Intercept)\n\n\nGroup-Level Effects\n\\(\\sigma_{\\text{bandInt}}\\)\nsd__bandInt\nStandard deviation for bandInt\n\n\n\n\n\nFamily Specific Parameters\n\\(\\sigma_{\\text{Observation}}\\)\nsd__Observation\nStandard deviation for Gaussian Family\n\n\n\n\n\n\n\n\n\nResults\nTesting Phase - No feedback.\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw.\nDeviation From Target Band\nDescriptive summaries testing deviation data are provided in Table 2 and Figure 1. To model differences in accuracy between groups, we used Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id).\n\\[\\begin{equation}\ndist_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot band_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot band_{ij} + b_{0i} + b_{1i} \\cdot band_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\nCodedatasummary(vx*vb ~ Mean + SD + Histogram, data = testAvg)\n\ndatasummary(vx*vb*condit ~ Mean + SD + Histogram, data = testAvg)\n\n\nCoderesult &lt;- test_summary_table(test, \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |&gt; kbl()\nresult$varied |&gt; kbl()\n\n\nTable 2: Testing Deviation - Empirical Summary\n\n\n\n\n\n(a) Constant Testing - Deviation\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n254\n148\n298\n\n\n350-550\nExtrapolation\n191\n110\n229\n\n\n600-800\nExtrapolation\n150\n84\n184\n\n\n800-1000\nTrained\n184\n106\n242\n\n\n1000-1200\nExtrapolation\n233\n157\n282\n\n\n1200-1400\nExtrapolation\n287\n214\n290\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - Deviation\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n386\n233\n426\n\n\n350-550\nExtrapolation\n285\n149\n340\n\n\n600-800\nExtrapolation\n234\n144\n270\n\n\n800-1000\nTrained\n221\n149\n248\n\n\n1000-1200\nTrained\n208\n142\n226\n\n\n1200-1400\nTrained\n242\n182\n235\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodetest |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\n\n\n\nFigure 1: E1. Deviations from target band during testing without feedback stage.\n\n\n\n\n\nCode#options(brms.backend=\"cmdstanr\",mc.cores=4)\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e1_dist_Cond_Type_RF_2\")\nbmtd &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE1, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n#bayestestR::describe_posterior(bmtd)\n\nmted1 &lt;- as.data.frame(describe_posterior(bmtd, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n\n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n# \n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_id:bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n\nmted1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE)\ncdted1 &lt;- get_coef_details(bmtd, \"conditVaried\")\ncdted2 &lt;-get_coef_details(bmtd, \"bandTypeExtrapolation\")\ncdted3 &lt;-get_coef_details(bmtd, \"conditVaried:bandTypeExtrapolation\")\n\n\nTable 3: E1. Training vs. Extrapolation\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n152.55\n70.63\n229.85\n1.0\n\n\nconditVaried\n39.00\n-21.10\n100.81\n0.9\n\n\nbandTypeExtrapolation\n71.51\n33.24\n109.60\n1.0\n\n\nconditVaried:bandTypeExtrapolation\n66.46\n32.76\n99.36\n1.0\n\n\n\n\n\n\n\n\nTesting. To compare conditions in the testing stage, we first fit a model predicting deviation from the target band as a function of training condition and band type, with random intercepts for participants and bands. The model is shown in Table 3. The effect of training condition was not reliably different from 0 (β = 39, 95% CrI [-21.1, 100.81]; pd = 89.93%). The extrapolation testing items had a significantly greater deviation than the interpolation band (β = 71.51, 95% CrI [33.24, 109.6]; pd = 99.99%). The interaction between training condition and band type was significant (β = 66.46, 95% CrI [32.76, 99.36]; pd = 99.99%), with the varied group showing a greater deviation than the constant group in the extrapolation bands. See Figure 2.\n\nCodepe1td &lt;- testE1 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe1ce &lt;- bmtd |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\nLoading required namespace: rstanarm\n\nCode(pe1td + pe1ce) + plot_annotation(tag_levels= 'A')\n\n\n\n\n\n\nFigure 2: E1. Deviations from target band during testing without feedback stage.\n\n\n\n\n\nCode#contrasts(test$condit) \n#contrasts(test$vb)\nmodelName &lt;- \"e1_testDistBand_RF_5K\"\ne1_distBMM &lt;- brm(dist ~ condit * bandInt + (1 + bandInt|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nGetModelStats(e1_distBMM) |&gt; kable(escape=F,booktabs=T,caption=\"Model Coefficients\")\ne1_distBMM |&gt; \n  emmeans(\"condit\",by=\"bandInt\",at=list(bandInt=c(100,350,600,800,1000,1200)),\n          epred = TRUE, re_formula = NA) |&gt; \n  pairs() |&gt; gather_emmeans_draws()  |&gt; \n   summarize(median_qi(.value),pd=sum(.value&gt;0)/n()) |&gt;\n   select(contrast,Band=bandInt,value=y,lower=ymin,upper=ymax,pd) |&gt; \n   mutate(across(where(is.numeric), \\(x) round(x, 2)),\n          pd=ifelse(value&lt;0,1-pd,pd)) |&gt;\n   kbl(caption=\"Contrasts\")\ncoef_details &lt;- get_coef_details(e1_distBMM, \"conditVaried\")\n\n\nTable 4: Experiment 1. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\nModel Coefficients\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n205.09\n136.86\n274.06\n1.00\n\n\nconditVaried\n157.44\n60.53\n254.90\n1.00\n\n\nBand\n0.01\n-0.07\n0.08\n0.57\n\n\ncondit*Band\n-0.16\n-0.26\n-0.06\n1.00\n\n\n\n\n\n\nContrasts\n\ncontrast\nBand\nvalue\nlower\nupper\npd\n\n\n\nConstant - Varied\n100\n-141.49\n-229.19\n-53.83\n1.00\n\n\nConstant - Varied\n350\n-101.79\n-165.62\n-36.32\n1.00\n\n\nConstant - Varied\n600\n-62.02\n-106.21\n-14.77\n1.00\n\n\nConstant - Varied\n800\n-30.11\n-65.08\n6.98\n0.94\n\n\nConstant - Varied\n1000\n2.05\n-33.46\n38.41\n0.54\n\n\nConstant - Varied\n1200\n33.96\n-11.94\n81.01\n0.92\n\n\n\n\n\n\n\n\nThe model predicting absolute deviation (dist) showed clear effects of both training condition and target velocity band (Table X). Overall, the varied training group showed a larger deviation relative to the constant training group (β = 157.44, 95% CI [60.53, 254.9]). Deviation also depended on target velocity band, with lower bands showing less deviation. See Table 3 for full model output.\n\nCodecondEffects &lt;- function(m){\n  m |&gt; ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + stat_halfeye(alpha=.2) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  ylab(\"Predicted X Velocity\") + xlab(\"Band\")\n}\n\ne1_distBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects()+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(test$vb), \n                     limits = c(0, 1400)) \n\n\n\n\n\n\nFigure 3: E1. Conditioinal Effect of Training Condition and Band. Ribbon indicated 95% Credible Intervals.\n\n\n\n\nDiscrimination between bands\nIn addition to accuracy/deviation, we also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). Table 5 shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants on each testing trial.\n\\[\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\nCodetest %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\nFigure 4: E1 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\nCoderesult &lt;- test_summary_table(test, \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |&gt; kable()\nresult$varied |&gt; kable()\n\n\nTable 5: Testing vx - Empirical Summary\n\n\n\n\n\n(a) Constant Testing - vx\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n524\n448\n327\n\n\n350-550\nExtrapolation\n659\n624\n303\n\n\n600-800\nExtrapolation\n770\n724\n300\n\n\n800-1000\nTrained\n1001\n940\n357\n\n\n1000-1200\nExtrapolation\n1167\n1104\n430\n\n\n1200-1400\nExtrapolation\n1283\n1225\n483\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - vx\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n664\n533\n448\n\n\n350-550\nExtrapolation\n768\n677\n402\n\n\n600-800\nExtrapolation\n876\n813\n390\n\n\n800-1000\nTrained\n1064\n1029\n370\n\n\n1000-1200\nTrained\n1180\n1179\n372\n\n\n1200-1400\nTrained\n1265\n1249\n412\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodee1_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e1_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\nGetModelStats(e1_vxBMM ) |&gt; kable(escape=F,booktabs=T, caption=\"Fit to all 6 bands\")\ncd1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e1_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried:bandInt\")\n\n\nmodelName &lt;- \"e1_extrap_testVxBand\"\ne1_extrap_VxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                  data=test |&gt;\n                    filter(expMode==\"test-Nf\"),file=paste0(here::here(\"data/model_cache\",modelName)),\n                  iter=5000,chains=4)\nGetModelStats(e1_extrap_VxBMM ) |&gt; kable(escape=F,booktabs=T, caption=\"Fit to 3 extrapolation bands\")\nsc2 &lt;- get_coef_details(e1_extrap_VxBMM, \"bandInt\")\nintCoef2 &lt;- get_coef_details(e1_extrap_VxBMM, \"conditVaried:bandInt\")\n\n\nTable 6: Experiment 1. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n(a) Model fit to all 6 bands\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Model fit to 3 extrapolation bands\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n497.49\n431.26\n566.17\n1.00\n\n\nconditVaried\n124.79\n26.61\n224.75\n0.99\n\n\nBand\n0.49\n0.42\n0.56\n1.00\n\n\ncondit*Band\n-0.06\n-0.16\n0.04\n0.88\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee Table 6 for the full model results. The estimated coefficient for training condition (β = 164.05, 95% CrI [45.5, 278.85]) suggests that the varied group tends to produce harder throws than the constant group, but is not in and of itself useful for assessing discrimination. Most relevant to the issue of discrimination is the slope on Velocity Band (β = 0.71, 95% CrI [0.62, 0.8]). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The estimate for the interaction between slope and condition (β = -0.14, 95% CrI [-0.26, -0.01]), suggests that the discrimination was somewhat modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition. This difference is depicted visually in Figure 5. Table 7 shows the average slope coefficients for varied and constant participants separately for each quartile. The constant participant participants appear to have larger slopes across quartiles, but the difference between conditions may be less pronounced for the top quartiles of subjects who show the strongest discrimination. Figure Figure 6 shows the distributions of slope values for each participant, and the compares the probability density of slope coefficients between training conditions. Figure 7\nThe second model, which focused solely on extrapolation bands, revealed similar patterns. The Velocity Band term (β = 0.49, 95% CrI [0.42, 0.56]) still demonstrates a high degree of discrimination ability. However, the posterior distribution for interaction term (β = -0.06, 95% CrI [-0.16, 0.04] ) does across over 0, suggesting that the evidence for decreased discrimination ability for the varied participants is not as strong when considering only the three extrapolation bands.\nCodee1_vxBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects() +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(test$vb), \n                     limits = c(0, 1400))\n\ne1_extrap_VxBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600))) |&gt;\n  gather_emmeans_draws() |&gt;\n  condEffects() +\n  scale_x_continuous(breaks = c(100, 350, 600), \n                     labels = levels(test$vb)[1:3], \n                     limits = c(0, 1000)) \n\n\n\n\n\n\n\n\n\n(a) Model fit to all 6 bands\n\n\n\n\n\n\n\n\n\n\n\n(b) Model fit to only 3 extrapolation bands\n\n\n\n\n\n\nFigure 5: Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\nCodenew_data_grid=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"condit\",\"bandInt\")]))) |&gt; \n  dplyr::arrange(id,bandInt) |&gt; \n  mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0)) \n\nindv_coefs &lt;- as_tibble(coef(e1_vxBMM)$id, rownames=\"id\")|&gt; \n  select(id, starts_with(\"Est\")) |&gt;\n  left_join(e1Sbjs, by=join_by(id) ) \n\n\nfixed_effects &lt;- e1_vxBMM |&gt; \n  spread_draws(`^b_.*`,regex=TRUE) |&gt; arrange(.chain,.draw,.iteration)\n\n\nrandom_effects &lt;- e1_vxBMM |&gt; \n  gather_draws(`^r_id.*$`, regex = TRUE, ndraws = 500) |&gt; \n  separate(.variable, into = c(\"effect\", \"id\", \"term\"), sep = \"\\\\[|,|\\\\]\") |&gt; \n  mutate(id = factor(id,levels=levels(test$id))) |&gt; \n  pivot_wider(names_from = term, values_from = .value) |&gt; arrange(id,.chain,.draw,.iteration)\n\nWarning: Expected 3 pieces. Additional pieces discarded in 156000 rows [1, 2, 3, 4, 5,\n6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\nCode indvDraws &lt;- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |&gt; \n  rename(bandInt_RF = bandInt,RF_Intercept=Intercept) |&gt;\n  right_join(new_data_grid, by = join_by(\"id\")) |&gt; \n  mutate(\n    Slope = bandInt_RF+b_bandInt,\n    Intercept= RF_Intercept + b_Intercept,\n    estimate = (b_Intercept + RF_Intercept) + (bandInt*(b_bandInt+bandInt_RF)) + (bandInt * condit_dummy) * `b_conditVaried:bandInt`,\n    SlopeInt = Slope + (`b_conditVaried:bandInt`*condit_dummy)\n  ) \n\nWarning in right_join(rename(left_join(random_effects, fixed_effects, by = join_by(\".chain\", : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nCode  indvSlopes &lt;- indvDraws |&gt; group_by(id) |&gt; median_qi(Slope,SlopeInt, Intercept,b_Intercept,b_bandInt) |&gt;\n  left_join(e1Sbjs, by=join_by(id)) |&gt; group_by(condit) |&gt;\n    select(id,condit,Intercept,b_Intercept,starts_with(\"Slope\"),b_bandInt, n) |&gt;\n  mutate(rankSlope=rank(Slope)) |&gt; arrange(rankSlope)   |&gt; ungroup()\n \n  \n  indvSlopes |&gt; mutate(Condition=condit) |&gt;  group_by(Condition) |&gt; \n    reframe(enframe(quantile(SlopeInt, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"SlopeInt\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=SlopeInt,names_prefix=\"Q_\") |&gt;\n  group_by(Condition) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kbl()\n\n\nTable 7: Slope coefficients by quartile, per condition\n\n\n\n\nCondition\nQ_0%_mean\nQ_25%_mean\nQ_50%_mean\nQ_75%_mean\nQ_100%_mean\n\n\n\nConstant\n-0.1065285\n0.4813997\n0.6901252\n0.9368777\n1.398519\n\n\nVaried\n-0.2039670\n0.2703687\n0.5939692\n0.8950950\n1.288174\n\n\n\n\n\n\n\n\nFigure 6 shows the distributions of estimated slopes relating velocity band to x velocity for each participant, ordered from lowest to highest within condition. Slope values are lower overall for varied training compared to constant training. Figure Xb plots the density of these slopes for each condition. The distribution for varied training has more mass at lower values than the constant training distribution. Both figures illustrate the model’s estimate that varied training resulted in less discrimination between velocity bands, evidenced by lower slopes on average.\nCode  indvSlopes |&gt; ggplot(aes(y=rankSlope, x=SlopeInt,fill=condit,color=condit)) + \n  geom_pointrange(aes(xmin=SlopeInt.lower , xmax=SlopeInt.upper)) + \n  labs(x=\"Estimated Slope\", y=\"Participant\")  + facet_wrap(~condit)\n\n   ggplot(indvSlopes, aes(x = SlopeInt, color = condit)) + \n  geom_density() + labs(x=\"Slope Coefficient\",y=\"Density\")\n\n\n\n\n\n\n\n\n\n(a) Slope estimates by participant - ordered from lowest to highest within each condition.\n\n\n\n\n\n\n\n\n\n\n\n(b) Destiny of slope coefficients by training group\n\n\n\n\n\n\nFigure 6: Slope distributions between condition\n\n\n\nCodenSbj &lt;- 3\nindvDraws  |&gt; indv_model_plot(indvSlopes, testAvg, SlopeInt,rank_variable=Slope,n_sbj=nSbj,\"max\")\nindvDraws |&gt; indv_model_plot(indvSlopes, testAvg,SlopeInt, rank_variable=Slope,n_sbj=nSbj,\"min\")\n\n\n\n\n\n\n\n\n(a) subset with largest slopes\n\n\n\n\n\n\n\n\n\n(b) subset with smallest slopes\n\n\n\n\n\nFigure 7: Subset of Varied and Constant Participants with the smallest and largest estimated slope values. Red lines represent the best fitting line for each participant, gray lines are 200 random samples from the posterior distribution. Colored points and intervals at each band represent the empirical median and 95% HDI.\n\n\n\ncontrol for training end performance\n\nCodetestE1 |&gt; group_by(id,condit) |&gt; pivot_longer(c(\"dist\",\"train_end\"),names_to=\"var\",values_to=\"value\") |&gt; \n  ggplot(aes(x=var,y=value, fill=condit)) + stat_bar + facet_wrap(~var)\n\n\n\n\n\n\nCodetestE1 |&gt; ggplot(aes(x=train_end,y=dist,fill=condit)) + \n  stat_summary(geom = \"line\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n    facet_wrap(~vb) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\nWarning: Width not defined\nℹ Set with `position_dodge(width = ...)`\n\n\nWarning: `position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n\n\n\n\n\n\n\nCodetestE1 |&gt; ggplot(aes(x=train_end,y=dist,fill=condit,col=condit)) + \n  #geom_point() +\n  geom_smooth(method=\"loess\") +\n    facet_wrap(~vb) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode# create quartiles for train_end\ntestE1 |&gt; group_by(condit,vb) |&gt; \n  mutate(train_end_q = ntile(train_end,6)) |&gt; \n  ggplot(aes(x=train_end_q,y=dist,fill=condit)) + \n stat_bar + \n    facet_wrap(~vb) +\n  labs(x=\"quartile\", y=\"Deviation From Target\")\n\n\n\n\n\n\n\n\nCodebmtd3 &lt;- brm(dist ~ condit * bandType * train_end + (1|bandInt) + (1|id), \n    data=testE1, \n    file=paste0(here::here(\"data/model_cache\",\"e1_trainEnd_BT_RF2\")),\n    iter=1000,chains=2, control = list(adapt_delta = .92, max_treedepth = 11))\nsummary(bmtd3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: dist ~ condit * bandType * train_end + (1 | bandInt) + (1 | id) \n   Data: testE1 (Number of observations: 9491) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nGroup-Level Effects: \n~bandInt (Number of levels: 6) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)    73.85     31.99    35.91   157.68 1.00      215      429\n\n~id (Number of levels: 156) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)   140.36      8.62   125.43   159.07 1.03      111      142\n\nPopulation-Level Effects: \n                                             Estimate Est.Error l-95% CI\nIntercept                                       43.41     49.99   -62.85\nconditVaried                                    67.01     49.92   -32.25\nbandTypeExtrapolation                           97.92     27.14    45.56\ntrain_end                                        1.02      0.28     0.45\nconditVaried:bandTypeExtrapolation             -76.93     27.48  -132.46\nconditVaried:train_end                          -0.56      0.31    -1.11\nbandTypeExtrapolation:train_end                 -0.26      0.17    -0.58\nconditVaried:bandTypeExtrapolation:train_end     0.89      0.18     0.52\n                                             u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept                                      136.23 1.01      140      255\nconditVaried                                   154.69 1.05       71      147\nbandTypeExtrapolation                          151.52 1.00      290      382\ntrain_end                                        1.51 1.01      115      221\nconditVaried:bandTypeExtrapolation             -22.04 1.00      273      487\nconditVaried:train_end                           0.11 1.03       87      205\nbandTypeExtrapolation:train_end                  0.09 1.01      205      310\nconditVaried:bandTypeExtrapolation:train_end     1.24 1.01      207      432\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   240.85      1.82   237.19   244.52 1.00     1029      512\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nCodebayestestR::describe_posterior(bmtd3)\n\nSummary of Posterior Distribution\n\nParameter                                    | Median |            95% CI |     pd |            ROPE | % in ROPE |  Rhat |    ESS\n---------------------------------------------------------------------------------------------------------------------------------\n(Intercept)                                  |  41.58 | [ -62.85, 136.23] | 82.20% | [-29.55, 29.55] |    33.16% | 1.014 | 142.00\nconditVaried                                 |  69.52 | [ -32.25, 154.69] | 90.30% | [-29.55, 29.55] |    18.84% | 1.046 |  70.00\nbandTypeExtrapolation                        |  97.03 | [  45.56, 151.52] |   100% | [-29.55, 29.55] |        0% | 1.000 | 280.00\ntrain_end                                    |   1.04 | [   0.45,   1.51] | 99.90% | [-29.55, 29.55] |      100% | 1.010 | 108.00\nconditVaried:bandTypeExtrapolation           | -76.11 | [-132.46, -22.04] | 99.90% | [-29.55, 29.55] |     2.00% | 1.000 | 268.00\nconditVaried:train_end                       |  -0.57 | [  -1.11,   0.11] | 95.10% | [-29.55, 29.55] |      100% | 1.029 |  84.00\nbandTypeExtrapolation:train_end              |  -0.27 | [  -0.58,   0.09] | 92.00% | [-29.55, 29.55] |      100% | 1.006 | 204.00\nconditVaried:bandTypeExtrapolation:train_end |   0.89 | [   0.52,   1.24] |   100% | [-29.55, 29.55] |      100% | 1.005 | 206.00\n\nCodecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\n bmtd3 |&gt; emmeans( ~condit * bandType * train_end) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\n\n\n\n\n\nCodece_bmtd3 &lt;- plot(conditional_effects(bmtd3),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmtd3)\n\n\n\n\n\n\n\nE1 Results Discussion\nNEEDS TO BE WRITTEN\n\n\n\n\n\n\nReferences\n\nBürkner, P.-C. (2017). Brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80, 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541\n\n\nTeam, R. C. (2020). R: A Language and Environment for Statistical Computing. R: A Language and Environment for Statistical Computing.",
    "crumbs": [
      "Analyses",
      "HTW E1 Testing"
    ]
  },
  {
    "objectID": "Analysis/e2_test.html",
    "href": "Analysis/e2_test.html",
    "title": "HTW E2 Testing",
    "section": "",
    "text": "Figure 1 illustrates the design of Experiment 2. The stages of the experiment (i.e. training, testing no-feedback, test with feedback), are identical to that of Experiment 1. The only change is that Experiment 2 participants train, and then test, on bands in the reverse order of Experiment 1 (i.e. training on the softer bands; and testing on the harder bands).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 100-300350-550600-800Test1\nTest  Novel Bands  800-10001000-12001200-1400data1-&gt;Test1\ndata2\n Constant Training 600-800data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  800-10001000-12001200-1400Test2\n  Test   Varied Training Bands  100-300350-550600-800Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 1: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.\n\n\n\n\nResults\nTesting Phase - No feedback.\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw.\nDeviation From Target Band\nDescriptive summaries testing deviation data are provided in Table 1 and Figure 2. To model differences in accuracy between groups, we used Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id).\n\\[\\begin{equation}\ndist_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot band_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot band_{ij} + b_{0i} + b_{1i} \\cdot band_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\nCoderesult &lt;- test_summary_table(testE2, \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |&gt; kable()\nresult$varied |&gt; kable()\n# make kable table with smaller font size\n# result$constant |&gt; kbl(caption=\"Constant Testing - Deviation\",booktabs=T,escape=F) |&gt; kable_styling(font_size = 7)\n\n\nTable 1: Testing Deviation - Empirical Summary\n\n\n\n\n(a) Constant Testing - Deviation\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n206\n48\n317\n\n\n350-550\nExtrapolation\n194\n86\n268\n\n\n600-800\nTrained\n182\n112\n240\n\n\n800-1000\nExtrapolation\n200\n129\n233\n\n\n1000-1200\nExtrapolation\n238\n190\n234\n\n\n1200-1400\nExtrapolation\n311\n254\n288\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - Deviation\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nTrained\n153\n25\n266\n\n\n350-550\nTrained\n138\n53\n233\n\n\n600-800\nTrained\n160\n120\n183\n\n\n800-1000\nExtrapolation\n261\n207\n257\n\n\n1000-1200\nExtrapolation\n305\n258\n273\n\n\n1200-1400\nExtrapolation\n363\n314\n297\n\n\n\n\n\n\n\n\n\n\n\n\nCodetestE2 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\n\n\n\nFigure 2: E2. Deviations from target band during testing without feedback stage.\n\n\n\n\n\nCode#options(brms.backend=\"cmdstanr\",mc.cores=4)\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e2_dist_Cond_Type_RF_2\")\nbmtd2 &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE2, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n#bayestestR::describe_posterior(bmtd)\n\nmted2 &lt;- as.data.frame(describe_posterior(bmtd2, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mted2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\nmted2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n  kable(booktabs=TRUE) \n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\nCodepe1td &lt;- testE2 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe1ce &lt;- bmtd2 |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\nLoading required namespace: rstanarm\n\nCode(pe1td + pe1ce) + plot_annotation(tag_levels= 'A')\n\n\n\n\n\n\nFigure 3: E2. Deviations from target band during testing without feedback stage.\n\n\n\n\n\nCode#contrasts(test$condit) \n# contrasts(testE2$vb)\n\nmodelName &lt;- \"e2_testDistBand_RF_5K\"\ne2_distBMM &lt;- brm(dist ~ condit * bandInt + (1 + bandInt|id),\n                      data=testE2,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nmp2 &lt;- GetModelStats(e2_distBMM) |&gt; kable(escape=F,booktabs=T)\nmp2\ne2_distBMM |&gt; \n  emmeans(\"condit\",by=\"bandInt\",at=list(bandInt=c(100,350,600,800,1000,1200)),\n          epred = TRUE, re_formula = NA) |&gt; \n  pairs() |&gt; gather_emmeans_draws()  |&gt; \n   summarize(median_qi(.value),pd=sum(.value&gt;0)/n()) |&gt;\n   select(contrast,Band=bandInt,value=y,lower=ymin,upper=ymax,pd) |&gt; \n   mutate(across(where(is.numeric), \\(x) round(x, 2)),\n          pd=ifelse(value&lt;0,1-pd,pd)) |&gt;\n   kable(caption=\"Contrasts\")\ncoef_details &lt;- get_coef_details(e2_distBMM, \"conditVaried\")\n\n\nTable 2: Experiment 2. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n151.71\n90.51\n215.86\n1.00\n\n\nconditVaried\n-70.33\n-156.87\n16.66\n0.94\n\n\nBand\n0.10\n0.02\n0.18\n1.00\n\n\ncondit*Band\n0.12\n0.02\n0.23\n0.99\n\n\n\n\n\n\nContrasts\n\ncontrast\nBand\nvalue\nlower\nupper\npd\n\n\n\nConstant - Varied\n100\n57.57\n-20.48\n135.32\n0.93\n\n\nConstant - Varied\n350\n26.60\n-30.93\n83.84\n0.83\n\n\nConstant - Varied\n600\n-4.30\n-46.73\n38.52\n0.58\n\n\nConstant - Varied\n800\n-29.30\n-69.38\n11.29\n0.92\n\n\nConstant - Varied\n1000\n-54.62\n-101.06\n-5.32\n0.98\n\n\nConstant - Varied\n1200\n-79.63\n-139.47\n-15.45\n0.99\n\n\n\n\n\n\n\n\nThe model predicting absolute deviation showed a modest tendency for the varied training group to have lower deviation compared to the constant training group (β = -70.33, 95% CI [-156.87, 16.66]),with 94% of the posterior distribution being less than 0. This suggests a potential benefit of training with variation, though the evidence is not definitive.\n(SHOULD PROBABLY DO ALTERNATE ANALYSIS THAT ONLY CONSIDERS THE NOVEL EXTRAPOLATION BANDS)\n\nCodecondEffects &lt;- function(m){\n  m |&gt; ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n    stat_dist_pointinterval() + stat_halfeye(alpha=.2) +\n    stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n    ylab(\"Predicted X Velocity\") + xlab(\"Band\")\n}\n\n\ne2_distBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects()+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) \n\n\n\n\n\n\nFigure 4: E2. Conditioinal Effect of Training Condition and Band. Ribbon indicated 95% Credible Intervals.\n\n\n\n\nDiscrimination between Velocity Bands\nIn addition to accuracy/deviation. We also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). Table 3 shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants.\n\\[\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\nCodetestE2 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\nFigure 5: E2 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\nCoderesult &lt;- test_summary_table(testE2, \"vx\",\"X Velocity\" ,mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |&gt; kable()\nresult$varied |&gt; kable()\n\n\nTable 3: Testing vx - Empirical Summary\n\n\n\n\n\n(a) Constant Testing - vx\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n457\n346\n354\n\n\n350-550\nExtrapolation\n597\n485\n368\n\n\n600-800\nTrained\n728\n673\n367\n\n\n800-1000\nExtrapolation\n953\n913\n375\n\n\n1000-1200\nExtrapolation\n1064\n1012\n408\n\n\n1200-1400\nExtrapolation\n1213\n1139\n493\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - vx\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nTrained\n410\n323\n297\n\n\n350-550\nTrained\n582\n530\n303\n\n\n600-800\nTrained\n696\n641\n316\n\n\n800-1000\nExtrapolation\n910\n848\n443\n\n\n1000-1200\nExtrapolation\n1028\n962\n482\n\n\n1200-1400\nExtrapolation\n1095\n1051\n510\n\n\n\n\n\n\n\n\n\n\n\n\nCodee2_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=testE2,file=paste0(here::here(\"data/model_cache\", \"e2_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\nmt3 &lt;-GetModelStats(e2_vxBMM ) |&gt; kable(escape=F,booktabs=T)\nmt3\ncd1 &lt;- get_coef_details(e2_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e2_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e2_vxBMM, \"conditVaried:bandInt\")\n\n\nTable 4: Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\n\n\nSee Table 4 for the full model results.\nWhen examining discrimination ability using the model predicting raw x-velocity, the results were less clear than those of the absolute deviation analysis. The slope on Velocity Band (β = 0.71, 95% CrI [0.58, 0.84]) indicates that participants showed good discrimination between bands overall. However, the interaction term suggested this effect was not modulated by training condition (β = -0.06, 95% CrI [-0.24, 0.13]) Thus, while varied training may provide some advantage for accuracy, both training conditions seem to have similar abilities to discriminate between velocity bands.\n\nCodee2_vxBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt;\n  ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  ylab(\"Predicted X Velocity\") + xlab(\"Band\")+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\n\n\n\n\nFigure 6: Conditional effect of training condition and Band. Ribbons indicate 95% HDI.\n\n\n\n\n\nCodenew_data_grid=map_dfr(1, ~data.frame(unique(testE2[,c(\"id\",\"condit\",\"bandInt\")]))) |&gt; \n  dplyr::arrange(id,bandInt) |&gt; \n  mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0)) \n\nindv_coefs &lt;- coef(e2_vxBMM)$id |&gt; \n  as_tibble(rownames=\"id\") |&gt; \n  select(id, starts_with(\"Est\")) |&gt;\n  left_join(e2Sbjs, by=join_by(id) ) |&gt; \n  group_by(condit) |&gt; \n  mutate(rank = rank(desc(Estimate.bandInt)),\n         intErrorRank=rank((Est.Error.Intercept)),\n         bandErrorRank=rank((Est.Error.bandInt)),\n         nCond = n()) |&gt; arrange(intErrorRank)\n\nfixed_effects &lt;- e2_vxBMM |&gt; \n  spread_draws(`^b_.*`,regex=TRUE) |&gt; arrange(.chain,.draw,.iteration)\n\n\nrandom_effects &lt;- e2_vxBMM |&gt; \n  gather_draws(`^r_id.*$`, regex = TRUE, ndraws = 2000) |&gt; \n  separate(.variable, into = c(\"effect\", \"id\", \"term\"), sep = \"\\\\[|,|\\\\]\") |&gt; \n  mutate(id = factor(id,levels=levels(testE2$id))) |&gt; \n  pivot_wider(names_from = term, values_from = .value) |&gt; arrange(id,.chain,.draw,.iteration)\n\ncd &lt;- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |&gt; \n  rename(bandInt_RF = bandInt) |&gt;\n  mutate(Slope=bandInt_RF+b_bandInt) |&gt; group_by(id) \n\ncdMed &lt;- cd |&gt; group_by(id) |&gt; median_qi(Slope)  |&gt; \n  left_join(e2Sbjs, by=join_by(id)) |&gt; group_by(condit) |&gt;\n  mutate(rankSlope=rank(Slope)) |&gt; arrange(rankSlope)\n\ncdMed %&gt;% ggplot(aes(y=rankSlope, x=Slope,fill=condit,color=condit)) + \n  geom_pointrange(aes(xmin=.lower , xmax=.upper)) + \n  labs(x=\"Estimated Slope\", y=\"Participant\")  + facet_wrap(~condit)  \n\n# cdMed |&gt;  ggplot(aes(x = condit, y = Slope,fill=condit)) +\n#     stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n#     stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n#   geom_jitter()\n#   labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\nCodenew_data_grid=map_dfr(1, ~data.frame(unique(testE2[,c(\"id\",\"condit\",\"bandInt\")]))) |&gt; \n  dplyr::arrange(id,bandInt) |&gt; \n  mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0)) \n\nindv_coefs &lt;- as_tibble(coef(e2_vxBMM)$id, rownames=\"id\")|&gt; \n  select(id, starts_with(\"Est\")) |&gt;\n  left_join(e2Sbjs, by=join_by(id) ) \n\n\nfixed_effects &lt;- e2_vxBMM |&gt; \n  spread_draws(`^b_.*`,regex=TRUE) |&gt; arrange(.chain,.draw,.iteration)\n\n\nrandom_effects &lt;- e2_vxBMM |&gt; \n  gather_draws(`^r_id.*$`, regex = TRUE, ndraws = 1500) |&gt; \n  separate(.variable, into = c(\"effect\", \"id\", \"term\"), sep = \"\\\\[|,|\\\\]\") |&gt; \n  mutate(id = factor(id,levels=levels(testE2$id))) |&gt; \n  pivot_wider(names_from = term, values_from = .value) |&gt; arrange(id,.chain,.draw,.iteration)\n\nWarning: Expected 3 pieces. Additional pieces discarded in 330000 rows [1, 2, 3, 4, 5,\n6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\nCode indvDraws &lt;- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |&gt; \n  rename(bandInt_RF = bandInt,RF_Intercept=Intercept) |&gt;\n  right_join(new_data_grid, by = join_by(\"id\")) |&gt; \n  mutate(\n    Slope = bandInt_RF+b_bandInt,\n    Intercept= RF_Intercept + b_Intercept,\n    estimate = (b_Intercept + RF_Intercept) + (bandInt*(b_bandInt+bandInt_RF)) + (bandInt * condit_dummy) * `b_conditVaried:bandInt`,\n    SlopeInt = Slope + (`b_conditVaried:bandInt`*condit_dummy)\n  ) \n\nWarning in right_join(rename(left_join(random_effects, fixed_effects, by = join_by(\".chain\", : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nCode  indvSlopes &lt;- indvDraws |&gt; group_by(id) |&gt; median_qi(Slope,SlopeInt, Intercept,b_Intercept,b_bandInt) |&gt;\n  left_join(e2Sbjs, by=join_by(id)) |&gt; group_by(condit) |&gt;\n    select(id,condit,Intercept,b_Intercept,starts_with(\"Slope\"),b_bandInt, n) |&gt;\n  mutate(rankSlope=rank(Slope)) |&gt; arrange(rankSlope)   |&gt; ungroup()\n \n  \n  indvSlopes |&gt; mutate(Condition=condit) |&gt;  group_by(Condition) |&gt; \n    reframe(enframe(quantile(SlopeInt, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"SlopeInt\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=SlopeInt,names_prefix=\"Q_\") |&gt;\n  group_by(Condition) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kable()\n\n\nTable 5: Slope coefficients by quartile, per condition\n\n\n\n\n\n\n\n\n\n\n\n\nCondition\nQ_0%_mean\nQ_25%_mean\nQ_50%_mean\nQ_75%_mean\nQ_100%_mean\n\n\n\nConstant\n-0.2788310\n0.3898592\n0.697289\n1.0778097\n1.616840\n\n\nVaried\n-0.2454246\n0.3040574\n0.679329\n0.9470274\n1.810293\n\n\n\n\n\n\n\n\nFigure 7 visually represents the distributions of estimated slopes relating velocity band to x velocity for each participant, ordered from lowest to highest within condition. Slope values are lower overall for varied training compared to constant training. Figure Xb plots the density of these slopes for each condition. The distribution for varied training has more mass at lower values than the constant training distribution. Both figures illustrate the model’s estimate that varied training resulted in less discrimination between velocity bands, evidenced by lower slopes on average.\n\nCode  indvSlopes |&gt; ggplot(aes(y=rankSlope, x=SlopeInt,fill=condit,color=condit)) + \n  geom_pointrange(aes(xmin=SlopeInt.lower , xmax=SlopeInt.upper)) + \n  labs(x=\"Estimated Slope\", y=\"Participant\")  + facet_wrap(~condit) +\n   ggplot(indvSlopes, aes(x = SlopeInt, color = condit)) + \n  geom_density() + labs(x=\"Slope Coefficient\",y=\"Density\")\n\n\n\n\n\n\n\n\n(a) Slope estimates by participant - ordered from lowest to highest within each condition.\n\n\n\n\n\nFigure 7: Slope distributions between condition\n\n\n\n\nCodenSbj &lt;- 3\nindvDraws  |&gt; indv_model_plot(indvSlopes, testE2Avg, SlopeInt,rank_variable=Slope,n_sbj=nSbj,\"max\")\nindvDraws |&gt; indv_model_plot(indvSlopes, testE2Avg,SlopeInt, rank_variable=Slope,n_sbj=nSbj,\"min\")\n\n\n\n\n\n\n\n\n(a) subset with largest slopes\n\n\n\n\n\n\n\n\n\n(b) subset with smallest slopes\n\n\n\n\n\nFigure 8: Subset of Varied and Constant Participants with the smallest and largest estimated slope values. Red lines represent the best fitting line for each participant, gray lines are 200 random samples from the posterior distribution. Colored points and intervals at each band represent the empirical median and 95% HDI.\n\n\n\ncontrol for training end performance\n\nCodetestE2 |&gt; group_by(id,condit) |&gt; pivot_longer(c(\"dist\",\"train_end\"),names_to=\"var\",values_to=\"value\") |&gt; \n  ggplot(aes(x=var,y=value, fill=condit)) + stat_bar + facet_wrap(~var)\n\n\n\n\n\n\nCodetestE2 |&gt; ggplot(aes(x=train_end,y=dist,fill=condit)) + \n  stat_summary(geom = \"line\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n    facet_wrap(~vb) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\nWarning: Width not defined\nℹ Set with `position_dodge(width = ...)`\n\n\nWarning: `position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n\n\n\n\n\n\n\nCodetestE2 |&gt; ggplot(aes(x=train_end,y=dist,fill=condit,col=condit)) + \n  #geom_point() +\n  geom_smooth(method=\"loess\") +\n    facet_wrap(~vb) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode# create quartiles for train_end\ntestE2 |&gt; group_by(condit,vb) |&gt; \n  mutate(train_end_q = ntile(train_end,4)) |&gt; \n  ggplot(aes(x=train_end_q,y=dist,fill=condit)) + \n stat_bar + \n    facet_wrap(~vb) +\n  labs(x=\"quartile\", y=\"Deviation From Target\")\n\n\n\n\n\n\n\n\nCodebmtd3 &lt;- brm(dist ~ condit * bandType * train_end + (1|bandInt) + (1|id), \n    data=testE2, \n    file=paste0(here::here(\"data/model_cache\",\"e2_trainEnd_BT_RF2\")),\n    iter=1000,chains=2, control = list(adapt_delta = .92, max_treedepth = 11))\nsummary(bmtd3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: dist ~ condit * bandType * train_end + (1 | bandInt) + (1 | id) \n   Data: testE2 (Number of observations: 6709) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nGroup-Level Effects: \n~bandInt (Number of levels: 6) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)    56.29     23.16    27.44   117.13 1.01      368      453\n\n~id (Number of levels: 110) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)    96.88      7.59    83.12   112.62 1.00      238      391\n\nPopulation-Level Effects: \n                                             Estimate Est.Error l-95% CI\nIntercept                                       85.19     40.60     2.29\nconditVaried                                    46.19     40.72   -31.33\nbandTypeExtrapolation                          101.67     27.77    49.32\ntrain_end                                        1.10      0.23     0.67\nconditVaried:bandTypeExtrapolation             -11.41     31.46   -74.92\nconditVaried:train_end                          -0.85      0.32    -1.51\nbandTypeExtrapolation:train_end                 -0.69      0.19    -1.06\nconditVaried:bandTypeExtrapolation:train_end     0.93      0.23     0.46\n                                             u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept                                      163.45 1.01      222      298\nconditVaried                                   127.49 1.01      237      363\nbandTypeExtrapolation                          159.00 1.00      490      727\ntrain_end                                        1.59 1.00      289      372\nconditVaried:bandTypeExtrapolation              48.65 1.00      485      722\nconditVaried:train_end                          -0.23 1.01      242      417\nbandTypeExtrapolation:train_end                 -0.34 1.00      402      646\nconditVaried:bandTypeExtrapolation:train_end     1.35 1.00      385      602\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   241.83      2.08   237.75   245.98 1.00     1779      602\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nCodebayestestR::describe_posterior(bmtd3)\n\nSummary of Posterior Distribution\n\nParameter                                    | Median |           95% CI |     pd |            ROPE | % in ROPE |  Rhat |    ESS\n--------------------------------------------------------------------------------------------------------------------------------\n(Intercept)                                  |  85.92 | [  2.29, 163.45] | 97.60% | [-27.00, 27.00] |     5.05% | 1.015 | 207.00\nconditVaried                                 |  44.84 | [-31.33, 127.49] | 87.20% | [-27.00, 27.00] |    30.21% | 1.014 | 206.00\nbandTypeExtrapolation                        | 100.74 | [ 49.32, 159.00] |   100% | [-27.00, 27.00] |        0% | 0.999 | 478.00\ntrain_end                                    |   1.10 | [  0.67,   1.59] |   100% | [-27.00, 27.00] |      100% | 1.003 | 280.00\nconditVaried:bandTypeExtrapolation           | -10.78 | [-74.92,  48.65] | 64.00% | [-27.00, 27.00] |    61.68% | 1.000 | 473.00\nconditVaried:train_end                       |  -0.83 | [ -1.51,  -0.23] | 99.70% | [-27.00, 27.00] |      100% | 1.006 | 236.00\nbandTypeExtrapolation:train_end              |  -0.70 | [ -1.06,  -0.34] |   100% | [-27.00, 27.00] |      100% | 1.000 | 385.00\nconditVaried:bandTypeExtrapolation:train_end |   0.93 | [  0.46,   1.35] |   100% | [-27.00, 27.00] |      100% | 1.001 | 374.00\n\nCodecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\n bmtd3 |&gt; emmeans( ~condit * bandType * train_end) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\n\n\n\n\n\nCodece_bmtd3 &lt;- plot(conditional_effects(bmtd3),points=FALSE,plot=FALSE)\nwrap_plots(ce_bmtd3)",
    "crumbs": [
      "Analyses",
      "HTW E2 Testing"
    ]
  },
  {
    "objectID": "Analysis/e3_test.html",
    "href": "Analysis/e3_test.html",
    "title": "HTW E3 Testing",
    "section": "",
    "text": "The major manipulation adjustment of experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the earlier experiments. Ordinal feedback informs participants whether a throw was too soft, too hard, or fell within the target velocity range. Experiment 3 participants were randomly assigned to both a training condition (Constant vs. Varied) and a Band Order condition (original order used in Experiment 1, or the Reverse order of Experiment 2).\nResults\nTesting Phase - No feedback.\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw. Note that these no-feedback testing trials are identical to those of Experiment 1 and 2, as the ordinal feedback only occurs during the training phase, and final testing phase, of Experiment 3.\nDeviation From Target Band\nDescriptive summaries testing deviation data are provided in Table 1 and Figure 1. To model differences in accuracy between groups, we fit Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id).\n\nCoderesultOrig &lt;- test_summary_table(testE3 |&gt; filter(bandOrder==\"Original\"), \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresultOrig$constant |&gt; kable() \nresultOrig$varied |&gt; kable() \nresultRev &lt;- test_summary_table(testE3 |&gt; filter(bandOrder==\"Reverse\"), \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresultRev$constant |&gt; kable() \nresultRev$varied |&gt; kable() \n\n\nTable 1: Testing Deviation - Empirical Summary\n\n\n\n\n(a) Constant Testing - Deviation\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n396\n325\n350\n\n\n350-550\nExtrapolation\n278\n176\n299\n\n\n600-800\nExtrapolation\n173\n102\n215\n\n\n800-1000\nTrained\n225\n126\n284\n\n\n1000-1200\nExtrapolation\n253\n192\n271\n\n\n1200-1400\nExtrapolation\n277\n210\n262\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - Deviation\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n383\n254\n385\n\n\n350-550\nExtrapolation\n287\n154\n318\n\n\n600-800\nExtrapolation\n213\n140\n244\n\n\n800-1000\nTrained\n199\n142\n209\n\n\n1000-1200\nTrained\n222\n163\n221\n\n\n1200-1400\nTrained\n281\n227\n246\n\n\n\n\n\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n403\n334\n383\n\n\n350-550\nExtrapolation\n246\n149\n287\n\n\n600-800\nTrained\n155\n82\n209\n\n\n800-1000\nExtrapolation\n207\n151\n241\n\n\n1000-1200\nExtrapolation\n248\n220\n222\n\n\n1200-1400\nExtrapolation\n322\n281\n264\n\n\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nTrained\n153\n0\n307\n\n\n350-550\nTrained\n147\n55\n258\n\n\n600-800\nTrained\n159\n107\n192\n\n\n800-1000\nExtrapolation\n221\n160\n235\n\n\n1000-1200\nExtrapolation\n244\n185\n235\n\n\n1200-1400\nExtrapolation\n324\n264\n291\n\n\n\n\n\n\n\n\n\nCodetestE3 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\") + facet_wrap(~bandOrder)\n\n\n\n\n\n\nFigure 1: e3. Deviations from target band during testing without feedback stage.\n\n\n\n\n\nCode#contrasts(test$condit) \n\n# contrasts(testE3$vb)\n\nmodelName &lt;- \"e3_testDistBand_RF_5K\"\ne3_distBMM &lt;- brm(dist ~ condit * bandOrder * bandInt + (1 + bandInt|id),\n                      data=testE3,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\n\n#bayestestR::describe_posterior(e3_distBMM)\nm1 &lt;- as.data.frame(describe_posterior(e3_distBMM, centrality = \"Mean\"))\nm2 &lt;- fixef(e3_distBMM)\nmp3 &lt;- m1[, c(1,2,4,5,6)]\ncolnames(mp3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n                        \nmp3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_replace_all(Term, \"b_bandInt\", \"Band\")) |&gt;\n  kable(escape=F,booktabs=T)\ncd1 &lt;- get_coef_details(e3_distBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_distBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_distBMM, \"conditVaried:bandInt\")\n\n\nTable 2: Experiment 3. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nb_Intercept\n342.85\n260.18\n426.01\n1.00\n\n\nb_conditVaried\n7.38\n-116.96\n133.20\n0.54\n\n\nb_bandOrderReverse\n-64.99\n-179.19\n49.75\n0.86\n\n\nBand\n-0.13\n-0.22\n-0.04\n1.00\n\n\nb_conditVaried:bandOrderReverse\n-185.30\n-360.16\n-8.89\n0.98\n\n\nb_conditVaried:bandInt\n0.00\n-0.15\n0.13\n0.52\n\n\nb_bandOrderReverse:bandInt\n0.11\n-0.01\n0.24\n0.96\n\n\nb_conditVaried:bandOrderReverse:bandInt\n0.19\n-0.01\n0.38\n0.97\n\n\n\n\n\n\n\n\nThe effect of training condition in Experiment 3 showed a similar pattern to Experiment 2, with the varied group tending to have lower deviation than the constant group (β = 7.38, 95% CrI [-116.96, 133.2]), with 97% of the posterior distribution falling under 0.\n(NEED TO CONTROL FOR BAND ORDER HERE)\n\nCodee3_distBMM |&gt; emmeans( ~condit * bandInt * bandOrder, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt;\n  ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n    ylab(\"Predicted Deviation\") + xlab(\"Velocity Band\")+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) +\n  facet_wrap(~bandOrder) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\nLoading required namespace: rstanarm\n\n\n\n\n\n\n\nFigure 2: e3. Conditioinal Effect of Training Condition and Band. Ribbon indicated 95% Credible Intervals.\n\n\n\n\nDiscrimination between Velocity Bands\nIn addition to accuracy/deviation. We also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). Table 3 shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants.\n\\[\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\nCode# testE3 |&gt; filter(bandOrder==\"Original\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit()\n# testE3 |&gt; filter(bandOrder==\"Reverse\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit() +ggtitle(\"test\")\n\ntestE3 |&gt; group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + \n  facet_wrap(bandOrder~condit,scale=\"free_x\") \n\n\n\n\n\n\nFigure 3: e3 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\nCoderesultOrig &lt;- test_summary_table(testE3 |&gt; filter(bandOrder==\"Original\"), \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresultOrig$constant |&gt; kable() \nresultOrig$varied |&gt; kable() \nresultRev &lt;- test_summary_table(testE3 |&gt; filter(bandOrder==\"Reverse\"), \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresultRev$constant |&gt; kable() \nresultRev$varied |&gt; kable() \n\n\nTable 3: Testing vx - Empirical Summary\n\n\n\n\n\n(a) Constant Testing - vx\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n680\n625\n370\n\n\n350-550\nExtrapolation\n771\n716\n357\n\n\n600-800\nExtrapolation\n832\n786\n318\n\n\n800-1000\nTrained\n1006\n916\n417\n\n\n1000-1200\nExtrapolation\n1149\n1105\n441\n\n\n1200-1400\nExtrapolation\n1180\n1112\n443\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - vx\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n667\n554\n403\n\n\n350-550\nExtrapolation\n770\n688\n383\n\n\n600-800\nExtrapolation\n869\n814\n358\n\n\n800-1000\nTrained\n953\n928\n359\n\n\n1000-1200\nTrained\n1072\n1066\n388\n\n\n1200-1400\nTrained\n1144\n1093\n426\n\n\n\n\n\n\n\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n684\n634\n406\n\n\n350-550\nExtrapolation\n729\n679\n350\n\n\n600-800\nTrained\n776\n721\n318\n\n\n800-1000\nExtrapolation\n941\n883\n387\n\n\n1000-1200\nExtrapolation\n1014\n956\n403\n\n\n1200-1400\nExtrapolation\n1072\n1014\n442\n\n\n\n\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nTrained\n392\n270\n343\n\n\n350-550\nTrained\n540\n442\n343\n\n\n600-800\nTrained\n642\n588\n315\n\n\n800-1000\nExtrapolation\n943\n899\n394\n\n\n1000-1200\nExtrapolation\n1081\n1048\n415\n\n\n1200-1400\nExtrapolation\n1185\n1129\n500\n\n\n\n\n\n\n\n\n\nCodee3_vxBMM &lt;- brm(vx ~ condit * bandOrder * bandInt + (1 + bandInt|id),\n                        data=testE3,file=paste0(here::here(\"data/model_cache\", \"e3_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n# mt4 &lt;-GetModelStats(e3_vxBMM ) |&gt; kable(escape=F,booktabs=T)\n# mt4\n\n#bayestestR::describe_posterior(e3_vxBMM)\nm1 &lt;- as.data.frame(describe_posterior(e3_vxBMM, centrality = \"Mean\"))\nm2 &lt;- fixef(e3_vxBMM)\nmp3 &lt;- m1[, c(1,2,4,5,6)]\ncolnames(mp3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n                        \nmp3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  tibble::remove_rownames() |&gt; \n  mutate(Term = stringr::str_replace_all(Term, \"b_bandInt\", \"Band\")) |&gt;\n  kable(escape=F,booktabs=T)\ncd1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried:bandInt\")\n\n\nTable 4: Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nb_Intercept\n601.83\n504.75\n699.42\n1.00\n\n\nb_conditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nb_bandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nb_conditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nb_conditVaried:bandInt\n-0.04\n-0.23\n0.15\n0.67\n\n\nb_bandOrderReverse:bandInt\n-0.10\n-0.27\n0.08\n0.86\n\n\nb_conditVaried:bandOrderReverse:bandInt\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\n\n\nSee Table 4 for the full model results.\nSlope estimates for experiment 3 suggest that participants were capable of distinguishing between velocity bands even when provided only ordinal feedback during training (β = 0.49, 95% CrI [0.36, 0.62]). Unlike the previous two experiments, the posterior distribution for the interaction between condition and band was consistently positive, suggestive of superior discrimination for the varied participants β = -0.04, 95% CrI [-0.23, 0.15].\n\nCodee3_vxBMM |&gt; emmeans( ~condit* bandOrder* bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt;\n  ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  ylab(\"Predicted X Velocity\") + xlab(\"Band\")+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) +\n  facet_wrap(~bandOrder) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\n\n\n\n\nFigure 4: Conditional effect of training condition and Band. Ribbons indicate 95% HDI.\n\n\n\n\n\nCodenew_data_grid=map_dfr(1, ~data.frame(unique(testE3[,c(\"id\",\"condit\",\"bandInt\")]))) |&gt; \n  dplyr::arrange(id,bandInt) |&gt; \n  mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0)) \n\nindv_coefs &lt;- as_tibble(coef(e3_vxBMM)$id, rownames=\"id\")|&gt; \n  select(id, starts_with(\"Est\")) |&gt;\n  left_join(e3Sbjs, by=join_by(id) ) \n\n\nfixed_effects &lt;- e3_vxBMM |&gt; \n  spread_draws(`^b_.*`,regex=TRUE) |&gt; arrange(.chain,.draw,.iteration)\n\n\nrandom_effects &lt;- e3_vxBMM |&gt; \n  gather_draws(`^r_id.*$`, regex = TRUE, ndraws = 1500) |&gt; \n  separate(.variable, into = c(\"effect\", \"id\", \"term\"), sep = \"\\\\[|,|\\\\]\") |&gt; \n  mutate(id = factor(id,levels=levels(testE3$id))) |&gt; \n  pivot_wider(names_from = term, values_from = .value) |&gt; arrange(id,.chain,.draw,.iteration)\n\nWarning: Expected 3 pieces. Additional pieces discarded in 585000 rows [1, 2, 3, 4, 5,\n6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\nCode indvDraws &lt;- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |&gt; \n  rename(bandInt_RF = bandInt,RF_Intercept=Intercept) |&gt;\n  right_join(new_data_grid, by = join_by(\"id\")) |&gt; \n  mutate(\n    Slope = bandInt_RF+b_bandInt,\n    Intercept= RF_Intercept + b_Intercept,\n    estimate = (b_Intercept + RF_Intercept) + (bandInt*(b_bandInt+bandInt_RF)) + (bandInt * condit_dummy) * `b_conditVaried:bandInt`,\n    SlopeInt = Slope + (`b_conditVaried:bandInt`*condit_dummy)\n  ) \n\nWarning in right_join(rename(left_join(random_effects, fixed_effects, by = join_by(\".chain\", : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nCode  indvSlopes &lt;- indvDraws |&gt; group_by(id) |&gt; median_qi(Slope,SlopeInt, Intercept,b_Intercept,b_bandInt) |&gt;\n  left_join(e3Sbjs, by=join_by(id)) |&gt; group_by(condit) |&gt;\n    select(id,condit,bandOrder,Intercept,b_Intercept,starts_with(\"Slope\"),b_bandInt, n) |&gt;\n  mutate(rankSlope=rank(Slope)) |&gt; arrange(rankSlope)   |&gt; ungroup()\n \n  \n  indvSlopes |&gt; mutate(Condition=condit) |&gt;  group_by(Condition) |&gt; \n    reframe(enframe(quantile(SlopeInt, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"SlopeInt\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=SlopeInt,names_prefix=\"Q_\") |&gt;\n  group_by(Condition) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kable()\n  indvSlopes |&gt; mutate(Condition=condit) |&gt;  group_by(Condition, bandOrder) |&gt; \n    reframe(enframe(quantile(SlopeInt, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"SlopeInt\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=SlopeInt,names_prefix=\"Q_\") |&gt;\n  group_by(bandOrder,Condition) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kable()\n\n\nTable 5: Slope coefficients by quartile, per condition\n\n\n\n\n\n\n\n\n\n\n\n\nCondition\nQ_0%_mean\nQ_25%_mean\nQ_50%_mean\nQ_75%_mean\nQ_100%_mean\n\n\n\nConstant\n-0.3424400\n0.1739116\n0.4473200\n0.6960787\n1.934017\n\n\nVaried\n-0.3963599\n0.0993045\n0.4306245\n0.7401616\n1.436481\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbandOrder\nCondition\nQ_0%_mean\nQ_25%_mean\nQ_50%_mean\nQ_75%_mean\nQ_100%_mean\n\n\n\nOriginal\nConstant\n-0.3424400\n0.2684095\n0.4441671\n0.6619190\n1.934017\n\n\nOriginal\nVaried\n-0.3284811\n0.1605767\n0.4195062\n0.7098967\n1.339024\n\n\nReverse\nConstant\n-0.2164930\n0.1596895\n0.4577490\n0.7085250\n1.893145\n\n\nReverse\nVaried\n-0.3963599\n0.0072808\n0.4322739\n0.7882834\n1.436481\n\n\n\n\n\n\n\n\nFigure 5 shows the distributions of estimated slopes relating velocity band to x velocity for each participant, ordered from lowest to highest within condition. Slope values are lower overall for varied training compared to constant training. Figure Xb plots the density of these slopes for each condition. The distribution for varied training has more mass at lower values than the constant training distribution. Both figures illustrate the model’s estimate that varied training resulted in less discrimination between velocity bands, evidenced by lower slopes on average.\n\nCodeindvSlopes |&gt; \n  ggplot(aes(y=rankSlope, x=SlopeInt,fill=condit,color=condit)) + \n  geom_pointrange(aes(xmin=SlopeInt.lower , xmax=SlopeInt.upper)) + \n  labs(x=\"Estimated Slope\", y=\"Participant\")  + \n  facet_wrap(~condit) +\n  ggplot(indvSlopes, aes(x = SlopeInt, color = condit)) + \n  geom_density() + labs(x=\"Slope Coefficient\",y=\"Density\") \n\n\n\nindvSlopes |&gt; \n  #left_join(e3Sbjs, by=c(\"id\",\"condit\")) |&gt;\n  ggplot(aes(y=rankSlope, x=SlopeInt,fill=condit,color=condit)) + \n  geom_pointrange(aes(xmin=SlopeInt.lower , xmax=SlopeInt.upper)) + \n  labs(x=\"Estimated Slope\", y=\"Participant\")  + \n  facet_wrap(~bandOrder+condit) + \n  {\n  ggplot(indvSlopes,\n    #left_join(e3Sbjs, by=c(\"id\",\"condit\")), \n    aes(x = SlopeInt, color = condit)) + \n    geom_density() + \n        facet_wrap(~bandOrder) +\n    labs(x=\"Slope Coefficient\",y=\"Density\")  }\n\n\n\n\n\n\n\n\n(a) Slope estimates by participant - ordered from lowest to highest within each condition.\n\n\n\n\n\n\n\n\n\n(b) Destiny of slope coefficients by training group\n\n\n\n\n\nFigure 5: Slope distributions between condition\n\n\n\n\nCodenSbj &lt;- 3\nindvDraws  |&gt; indv_model_plot(indvSlopes, testE3Avg, SlopeInt,rank_variable=Slope,n_sbj=nSbj,\"max\")\nindvDraws |&gt; indv_model_plot(indvSlopes, testE3Avg,SlopeInt, rank_variable=Slope,n_sbj=nSbj,\"min\")\n\n\n\n\n\n\n\n\n(a) subset with largest slopes\n\n\n\n\n\n\n\n\n\n(b) subset with smallest slopes\n\n\n\n\n\nFigure 6: Subset of Varied and Constant Participants with the smallest and largest estimated slope values. Red lines represent the best fitting line for each participant, gray lines are 200 random samples from the posterior distribution. Colored points and intervals at each band represent the empirical median and 95% HDI.",
    "crumbs": [
      "Analyses",
      "HTW E3 Testing"
    ]
  },
  {
    "objectID": "Analysis/test_pdf.html#markdown-subtable",
    "href": "Analysis/test_pdf.html#markdown-subtable",
    "title": "HTW Model",
    "section": "Markdown subtable",
    "text": "Markdown subtable\n\n\nTable 2: Main Caption\n\n\n\n\n\n\n\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nFit to Test Data\nALM\n\n199.9\n\n103.4\n\n\nEXAM\n\n104.0\n\n85.7\n\n\nFit to Test & Training Data\nALM\n\n217.0\n\n170.3\n\n\nEXAM\n\n127.9\n\n144.9\n\n\nFit to Training Data\nALM\n\n467.7\n\n291.4\n\n\nEXAM\n\n273.3\n\n297.9\n\n\n\n\n\n\n: First Table {#tbl-first}\n\n\n\n\n\n\n\n\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nFit to Test Data\nALM\n\n297.8\n\n2,016.0\n\n\nEXAM\n\n53.9\n\n184.0\n\n\nFit to Test & Training Data\nALM\n\n57.4\n\n132.3\n\n\nEXAM\n\n42.9\n\n127.9\n\n\nFit to Training Data\nALM\n\n51.8\n\n103.5\n\n\nEXAM\n\n51.4\n\n107.0\n\n\n\n\n\n\n: Second Table {#tbl-second}\n\n\n\n\n\n\n\nTable 3: Mean model errors\n\n\n\n\n\n\n(a) Full datasets\n\n\n\nFit_Method\nModel\ncondit\nmean_error\n\n\n\nTest\nALM\nConstant\n199.93250\n\n\nTest\nALM\nVaried\n103.36184\n\n\nTest\nEXAM\nConstant\n104.01000\n\n\nTest\nEXAM\nVaried\n85.67763\n\n\nTest_Train\nALM\nConstant\n216.96875\n\n\nTest_Train\nALM\nVaried\n170.27632\n\n\nTest_Train\nEXAM\nConstant\n127.94250\n\n\nTest_Train\nEXAM\nVaried\n144.86316\n\n\nTrain\nALM\nConstant\n467.73000\n\n\nTrain\nALM\nVaried\n291.38289\n\n\nTrain\nEXAM\nConstant\n273.30000\n\n\nTrain\nEXAM\nVaried\n297.91053\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Intersection of samples with all labels available\n\n\n\nFit_Method\nModel\ncondit\nmean_error\n\n\n\nTest\nALM\nConstant\n297.81814\n\n\nTest\nALM\nVaried\n2,016.00582\n\n\nTest\nEXAM\nConstant\n53.90292\n\n\nTest\nEXAM\nVaried\n184.00079\n\n\nTest_Train\nALM\nConstant\n57.39716\n\n\nTest_Train\nALM\nVaried\n132.32472\n\n\nTest_Train\nEXAM\nConstant\n42.92373\n\n\nTest_Train\nEXAM\nVaried\n127.90159\n\n\nTrain\nALM\nConstant\n51.77211\n\n\nTrain\nALM\nVaried\n103.47686\n\n\nTrain\nEXAM\nConstant\n51.43058\n\n\nTrain\nEXAM\nVaried\n107.03430"
  },
  {
    "objectID": "Analysis/test_pdf.html#pander",
    "href": "Analysis/test_pdf.html#pander",
    "title": "HTW Model",
    "section": "pander",
    "text": "pander\n\n\nTable 4: Example\n\n\n\n\n\n(a) Cars\n\n\n\n\n\n\n\n\n\nFit_Method\nModel\ncondit\nmean_error\n\n\n\nTest\nALM\nConstant\n297.8\n\n\nTest\nALM\nVaried\n2016\n\n\nTest\nEXAM\nConstant\n53.9\n\n\nTest\nEXAM\nVaried\n184\n\n\nTest_Train\nALM\nConstant\n57.4\n\n\nTest_Train\nALM\nVaried\n132.3\n\n\nTest_Train\nEXAM\nConstant\n42.92\n\n\nTest_Train\nEXAM\nVaried\n127.9\n\n\nTrain\nALM\nConstant\n51.77\n\n\nTrain\nALM\nVaried\n103.5\n\n\nTrain\nEXAM\nConstant\n51.43\n\n\nTrain\nEXAM\nVaried\n107\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Pressure\n\n\n\n\n\n\n\n\n\nFit_Method\nModel\ncondit\nmean_error\n\n\n\nTest\nALM\nConstant\n199.9\n\n\nTest\nALM\nVaried\n103.4\n\n\nTest\nEXAM\nConstant\n104\n\n\nTest\nEXAM\nVaried\n85.68\n\n\nTest_Train\nALM\nConstant\n217\n\n\nTest_Train\nALM\nVaried\n170.3\n\n\nTest_Train\nEXAM\nConstant\n127.9\n\n\nTest_Train\nEXAM\nVaried\n144.9\n\n\nTrain\nALM\nConstant\n467.7\n\n\nTrain\nALM\nVaried\n291.4\n\n\nTrain\nEXAM\nConstant\n273.3\n\n\nTrain\nEXAM\nVaried\n297.9"
  },
  {
    "objectID": "Analysis/test_pdf.html#kbl",
    "href": "Analysis/test_pdf.html#kbl",
    "title": "HTW Model",
    "section": "kbl",
    "text": "kbl\n\n\nTable 5: Conditional Means and Contrasts\n\n\n\n\n\n\n(a) Conditional Means\n\n\n\nFit_Method\nModel\ncondit\nmean_error\n\n\n\nTest\nALM\nConstant\n199.93250\n\n\nTest\nALM\nVaried\n103.36184\n\n\nTest\nEXAM\nConstant\n104.01000\n\n\nTest\nEXAM\nVaried\n85.67763\n\n\nTest_Train\nALM\nConstant\n216.96875\n\n\nTest_Train\nALM\nVaried\n170.27632\n\n\nTest_Train\nEXAM\nConstant\n127.94250\n\n\nTest_Train\nEXAM\nVaried\n144.86316\n\n\nTrain\nALM\nConstant\n467.73000\n\n\nTrain\nALM\nVaried\n291.38289\n\n\nTrain\nEXAM\nConstant\n273.30000\n\n\nTrain\nEXAM\nVaried\n297.91053\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contrasts\n\n\n\nFit_Method\nModel\ncondit\nmean_error\n\n\n\nTest\nALM\nConstant\n297.81814\n\n\nTest\nALM\nVaried\n2016.00582\n\n\nTest\nEXAM\nConstant\n53.90292\n\n\nTest\nEXAM\nVaried\n184.00079\n\n\nTest_Train\nALM\nConstant\n57.39716\n\n\nTest_Train\nALM\nVaried\n132.32472\n\n\nTest_Train\nEXAM\nConstant\n42.92373\n\n\nTest_Train\nEXAM\nVaried\n127.90159\n\n\nTrain\nALM\nConstant\n51.77211\n\n\nTrain\nALM\nVaried\n103.47686\n\n\nTrain\nEXAM\nConstant\n51.43058\n\n\nTrain\nEXAM\nVaried\n107.03430"
  },
  {
    "objectID": "Analysis/test_pdf.html#gt",
    "href": "Analysis/test_pdf.html#gt",
    "title": "HTW Model",
    "section": "gt",
    "text": "gt\n\n\nTable 6: Conditional Means and Contrasts\n\n\n\n\n\n\n\n\n(a) Conditional Means\n\n\n\ncondit\nmean_error\n\n\n\nTest - ALM\n\n\nConstant\n199.93250\n\n\nVaried\n103.36184\n\n\nTest - EXAM\n\n\nConstant\n104.01000\n\n\nVaried\n85.67763\n\n\nTest_Train - ALM\n\n\nConstant\n216.96875\n\n\nVaried\n170.27632\n\n\nTest_Train - EXAM\n\n\nConstant\n127.94250\n\n\nVaried\n144.86316\n\n\nTrain - ALM\n\n\nConstant\n467.73000\n\n\nVaried\n291.38289\n\n\nTrain - EXAM\n\n\nConstant\n273.30000\n\n\nVaried\n297.91053\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contrasts\n\n\n\ncondit\nmean_error\n\n\n\nTest - ALM\n\n\nConstant\n297.81814\n\n\nVaried\n2016.00582\n\n\nTest - EXAM\n\n\nConstant\n53.90292\n\n\nVaried\n184.00079\n\n\nTest_Train - ALM\n\n\nConstant\n57.39716\n\n\nVaried\n132.32472\n\n\nTest_Train - EXAM\n\n\nConstant\n42.92373\n\n\nVaried\n127.90159\n\n\nTrain - ALM\n\n\nConstant\n51.77211\n\n\nVaried\n103.47686\n\n\nTrain - EXAM\n\n\nConstant\n51.43058\n\n\nVaried\n107.03430"
  },
  {
    "objectID": "Analysis/test_pdf.html#markdown-subtable-1",
    "href": "Analysis/test_pdf.html#markdown-subtable-1",
    "title": "HTW Model",
    "section": "Markdown subtable",
    "text": "Markdown subtable\n\n\nTable 7: Main Caption\n\n\n\n\n\n(a) First Table\n\n\n\nCol1\nCol2\nCol3\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\nA\nG\nG\n\n\n\n\n\n\n\n\n\n\n(b) Second Table\n\n\n\nCol1\nCol2\nCol3\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\nA\nG\nG\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Conditional Means and Contrasts\n\n\n\n\n--------------------------------------------\n Fit_Method   Model    condit    mean_error \n------------ ------- ---------- ------------\n    Test       ALM    Constant     297.8    \n\n    Test       ALM     Varied       2016    \n\n    Test      EXAM    Constant      53.9    \n\n    Test      EXAM     Varied       184     \n\n Test_Train    ALM    Constant      57.4    \n\n Test_Train    ALM     Varied      132.3    \n\n Test_Train   EXAM    Constant     42.92    \n\n Test_Train   EXAM     Varied      127.9    \n\n   Train       ALM    Constant     51.77    \n\n   Train       ALM     Varied      103.5    \n\n   Train      EXAM    Constant     51.43    \n\n   Train      EXAM     Varied       107     \n--------------------------------------------\n\n\n\n--------------------------------------------\n Fit_Method   Model    condit    mean_error \n------------ ------- ---------- ------------\n    Test       ALM    Constant     199.9    \n\n    Test       ALM     Varied      103.4    \n\n    Test      EXAM    Constant      104     \n\n    Test      EXAM     Varied      85.68    \n\n Test_Train    ALM    Constant      217     \n\n Test_Train    ALM     Varied      170.3    \n\n Test_Train   EXAM    Constant     127.9    \n\n Test_Train   EXAM     Varied      144.9    \n\n   Train       ALM    Constant     467.7    \n\n   Train       ALM     Varied      291.4    \n\n   Train      EXAM    Constant     273.3    \n\n   Train      EXAM     Varied      297.9    \n--------------------------------------------\n\n\n\n\n\n\n\nTable 9: Conditional Means and Contrasts\n\n\n\n\n\n+------------+-------+----------+------------+\n| Fit_Method | Model |  condit  | mean_error |\n+============+=======+==========+============+\n|    Test    |  ALM  | Constant |   297.8    |\n+------------+-------+----------+------------+\n|    Test    |  ALM  |  Varied  |    2016    |\n+------------+-------+----------+------------+\n|    Test    | EXAM  | Constant |    53.9    |\n+------------+-------+----------+------------+\n|    Test    | EXAM  |  Varied  |    184     |\n+------------+-------+----------+------------+\n| Test_Train |  ALM  | Constant |    57.4    |\n+------------+-------+----------+------------+\n| Test_Train |  ALM  |  Varied  |   132.3    |\n+------------+-------+----------+------------+\n| Test_Train | EXAM  | Constant |   42.92    |\n+------------+-------+----------+------------+\n| Test_Train | EXAM  |  Varied  |   127.9    |\n+------------+-------+----------+------------+\n|   Train    |  ALM  | Constant |   51.77    |\n+------------+-------+----------+------------+\n|   Train    |  ALM  |  Varied  |   103.5    |\n+------------+-------+----------+------------+\n|   Train    | EXAM  | Constant |   51.43    |\n+------------+-------+----------+------------+\n|   Train    | EXAM  |  Varied  |    107     |\n+------------+-------+----------+------------+\n\nTable: test cap1\n\n\n\n\n+------------+-------+----------+------------+\n| Fit_Method | Model |  condit  | mean_error |\n+============+=======+==========+============+\n|    Test    |  ALM  | Constant |   199.9    |\n+------------+-------+----------+------------+\n|    Test    |  ALM  |  Varied  |   103.4    |\n+------------+-------+----------+------------+\n|    Test    | EXAM  | Constant |    104     |\n+------------+-------+----------+------------+\n|    Test    | EXAM  |  Varied  |   85.68    |\n+------------+-------+----------+------------+\n| Test_Train |  ALM  | Constant |    217     |\n+------------+-------+----------+------------+\n| Test_Train |  ALM  |  Varied  |   170.3    |\n+------------+-------+----------+------------+\n| Test_Train | EXAM  | Constant |   127.9    |\n+------------+-------+----------+------------+\n| Test_Train | EXAM  |  Varied  |   144.9    |\n+------------+-------+----------+------------+\n|   Train    |  ALM  | Constant |   467.7    |\n+------------+-------+----------+------------+\n|   Train    |  ALM  |  Varied  |   291.4    |\n+------------+-------+----------+------------+\n|   Train    | EXAM  | Constant |   273.3    |\n+------------+-------+----------+------------+\n|   Train    | EXAM  |  Varied  |   297.9    |\n+------------+-------+----------+------------+\n\n\n\n\n\n\n\nTable 10: Conditional Means and Contrasts\n\n\n\n\n\n| Fit_Method | Model |  condit  | mean_error |\n|:----------:|:-----:|:--------:|:----------:|\n|    Test    |  ALM  | Constant |   297.8    |\n|    Test    |  ALM  |  Varied  |    2016    |\n|    Test    | EXAM  | Constant |    53.9    |\n|    Test    | EXAM  |  Varied  |    184     |\n| Test_Train |  ALM  | Constant |    57.4    |\n| Test_Train |  ALM  |  Varied  |   132.3    |\n| Test_Train | EXAM  | Constant |   42.92    |\n| Test_Train | EXAM  |  Varied  |   127.9    |\n|   Train    |  ALM  | Constant |   51.77    |\n|   Train    |  ALM  |  Varied  |   103.5    |\n|   Train    | EXAM  | Constant |   51.43    |\n|   Train    | EXAM  |  Varied  |    107     |\n\nTable: test cap1\n\n\n\n\n| Fit_Method | Model |  condit  | mean_error |\n|:----------:|:-----:|:--------:|:----------:|\n|    Test    |  ALM  | Constant |   199.9    |\n|    Test    |  ALM  |  Varied  |   103.4    |\n|    Test    | EXAM  | Constant |    104     |\n|    Test    | EXAM  |  Varied  |   85.68    |\n| Test_Train |  ALM  | Constant |    217     |\n| Test_Train |  ALM  |  Varied  |   170.3    |\n| Test_Train | EXAM  | Constant |   127.9    |\n| Test_Train | EXAM  |  Varied  |   144.9    |\n|   Train    |  ALM  | Constant |   467.7    |\n|   Train    |  ALM  |  Varied  |   291.4    |\n|   Train    | EXAM  | Constant |   273.3    |\n|   Train    | EXAM  |  Varied  |   297.9    |\n\nTable: test cap2"
  },
  {
    "objectID": "Misc/HTW_ToDo.html",
    "href": "Misc/HTW_ToDo.html",
    "title": "HTW To-do and Notes",
    "section": "",
    "text": "Model To-do\n\n\n\n\n\n\nFit to Train then Predict Transfer vs. Fitting to all stages\nSeparate ALM and EXAM Fits\nALM + Prior Knowledge (initial anchor at 0)\nEmpirical Learning Model\nIndividual vs. Group fits\nUsing Cognitive Model parameters to predict testing Vx vs. Deviation vs. Discrimination\nModel Recovery?\nApproximate Bayes?\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis To-do\n\n\n\n\nDiscrimination\nMixed Models?\n\n\n\n\n\n\n\n\n\nSite to-do\n\n\n\n\nConfigure react tables",
    "crumbs": [
      "Misc",
      "HTW To-do and Notes"
    ]
  },
  {
    "objectID": "Misc/HTW_ToDo.html#to-do-list",
    "href": "Misc/HTW_ToDo.html#to-do-list",
    "title": "HTW To-do and Notes",
    "section": "",
    "text": "Model To-do\n\n\n\n\n\n\nFit to Train then Predict Transfer vs. Fitting to all stages\nSeparate ALM and EXAM Fits\nALM + Prior Knowledge (initial anchor at 0)\nEmpirical Learning Model\nIndividual vs. Group fits\nUsing Cognitive Model parameters to predict testing Vx vs. Deviation vs. Discrimination\nModel Recovery?\nApproximate Bayes?\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis To-do\n\n\n\n\nDiscrimination\nMixed Models?\n\n\n\n\n\n\n\n\n\nSite to-do\n\n\n\n\nConfigure react tables",
    "crumbs": [
      "Misc",
      "HTW To-do and Notes"
    ]
  },
  {
    "objectID": "Misc/HTW_ToDo.html#notes",
    "href": "Misc/HTW_ToDo.html#notes",
    "title": "HTW To-do and Notes",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nHuman Regression or Rule\n\n\n\n\n\n\n\n\n\n\n\n\nALM & EXAM Implementations\n\n\n\n\n\n\n\n\n\n\n\n\nALM Likelihood\n\n\n\n\n\n\n\n\n\n\n\n\nHTW DP",
    "crumbs": [
      "Misc",
      "HTW To-do and Notes"
    ]
  },
  {
    "objectID": "Misc/Visuals_Interactives/alm_gg.html",
    "href": "Misc/Visuals_Interactives/alm_gg.html",
    "title": "HTW Project",
    "section": "",
    "text": "library(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\nbp &lt;- c(\"#2C3E50\", \"#E74C3C\", \"#3498DB\", \"#F39C12\", \"#8E44AD\", \"#1ABC9C\")  # Color palette\n\n# Define points for nodes\nnodes &lt;- tibble(\n  x = c(0.5, 1.5, 0, 1, 2, 0.5, 1.5),\n  y = c(3, 3, 2, 2, 2, 1, 1),\n  type = c(\"circle\", \"circle\", \"triangle\", \"triangle\", \"triangle\", \"circle\", \"circle\")\n)\n\n# Define lines for associations\nassociations &lt;- tibble(\n  x = c(0.5, 1.5, 0.5, 1.5, 0, 1, 2, 0.5, 1.5),\n  y = c(3, 3, 3, 3, 2, 2, 2, 2, 2),\n  xend = c(0, 1, 1, 2, 0.5, 0.5, 1.5, 1.5, 1.5),\n  yend = c(2, 2, 2, 2, 1, 1, 1, 1, 1)\n)\n\n# Plot nodes and lines\nplot &lt;- ggplot() +\n  geom_segment(data = associations, aes(x = x, y = y, xend = xend, yend = yend), \n               arrow = arrow(type = \"open\", length = unit(0.2, \"inches\")), \n               color = bp[1]) +\n  geom_point(data = filter(nodes, type == \"circle\"), aes(x = x, y = y), shape = 16, size = 5, fill = \"white\", color = bp[1]) +\n  geom_polygon(data = filter(nodes, type == \"triangle\"), aes(x = x, y = y), fill = \"white\", color = bp[1], size = 1) +\n  annotate(\"text\", x = 0.5, y = 3.2, label = \"Category nodes\") +\n  annotate(\"text\", x = 1, y = 2.2, label = \"Learned association weights\", angle = 90, hjust = 1) +\n  annotate(\"text\", x = 1, y = 1.2, label = \"Learned attention strengths\", angle = 90, hjust = 1) +\n  annotate(\"text\", x = 1, y = 2.2, label = \"Exemplar nodes\") +\n  annotate(\"text\", x = 1, y = 0.8, label = \"Stimulus dimension nodes\") +\n  theme_void()\n\nprint(plot)\n\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Data for category, exemplar, and stimulus dimension nodes\ndf_nodes &lt;- tibble(\n  x = c(0, 1, 0.5, 0.5, 0.5),\n  y = c(1, 1, 2, 0, 1.5),\n  label = c(\"Category nodes\", \"Category nodes\", \"Exemplar nodes\", \n            \"Stimulus dimension nodes\", \"Learned association strengths\"),\n  type = c(\"category\", \"category\", \"exemplar\", \"stimulus\", \"association\"),\n  fill = c(NA, NA, \"blue\", NA, NA)  # Gradient fill for exemplar node\n)\n\n# Data for arrows\ndf_arrows &lt;- tibble(\n  x_start = c(0, 1, 0.5, 0.5, 0.5, 0.5),\n  y_start = c(1, 1, 2, 0, 2, 0),\n  x_end = c(0.5, 0.5, 0.5, 0.5, 0, 1),\n  y_end = c(1.5, 1.5, 1.5, 1.5, 1, 1)\n)\n\n# Create the base plot\np &lt;- ggplot() +\n  geom_segment(data = df_arrows, \n               aes(x = x_start, y = y_start, xend = x_end, yend = y_end),\n               arrow = arrow(type = \"closed\", length = unit(0.15, \"inches\"))) +\n  geom_text(data = df_nodes, \n            aes(x = x, y = y, label = label), \n            vjust = \"inward\", size = 4) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n# Modify the exemplar node in df_nodes with a numeric value for activation level\ndf_nodes$activation &lt;- ifelse(df_nodes$type == \"exemplar\", 0.8, NA)  # Example value 0.8\n\n# Updated code segment\np + geom_point(data = df_nodes %&gt;% filter(type == \"exemplar\"), \n               aes(x = x, y = y, fill = activation), \n               shape = 21, size = 5, color = \"black\") +\n    scale_fill_gradient(low = \"white\", high = \"blue\", na.value = NA)\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Define the matrix size\nn_rows &lt;- 8\nn_cols &lt;- 10\n\n# Create an empty matrix\ncategory_matrix &lt;- matrix(NA, n_rows, n_cols)\n\n# Fill the matrix based on the figure\n# 1 for rule training instances and 2 for exceptions\ncategory_matrix[1:4, 1:2] &lt;- 1\ncategory_matrix[5:8, 1:2] &lt;- 2\ncategory_matrix[1:2, 3:10] &lt;- 2\ncategory_matrix[3:4, 3:10] &lt;- 1\ncategory_matrix[5:6, 3:10] &lt;- 1\ncategory_matrix[7:8, 3:10] &lt;- 2\n\n# Convert the matrix to a tidy data frame for plotting\ncategory_df &lt;- as.data.frame(as.table(category_matrix))\n\n# Plot\nggplot(category_df, aes(x=Var2, y=-Var1)) +\n  geom_tile(aes(fill=factor(category_matrix)), color=\"black\") +\n  scale_fill_manual(values=c(\"black\", \"white\")) +\n  theme_minimal() +\n  labs(fill=\"Category\", \n       title=\"Category Structure for Experiment 2\",\n       x=\"Segment Position\", \n       y=\"Rectangle Height\")\n\nWarning in Ops.factor(Var1): '-' not meaningful for factors\n\nWarning in Ops.factor(Var1): '-' not meaningful for factors\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n# Define the architecture\n\n# Rule Module\nrule_module &lt;- function(input, weights) {\n  # Activate rule nodes based on the input and boundary\n  activated_rule &lt;- ifelse(input &gt; some_boundary_value, 1, 0) # some_boundary_value needs to be defined\n  return(activated_rule * weights)\n}\n\n# Exemplar Module (ALCOVE)\nexemplar_module &lt;- function(input, exemplars, attention_weights, exemplar_weights) {\n  # Compute similarity between input and each exemplar\n  similarity &lt;- rowSums((input - exemplars)^2 * attention_weights)\n  activations &lt;- exp(-similarity)\n  \n  # Compute category response\n  response &lt;- sum(activations * exemplar_weights)\n  \n  return(response)\n}\n\n# Gating Mechanism\ngating_mechanism &lt;- function(rule_output, exemplar_output, g) {\n  return(g * rule_output + (1 - g) * exemplar_output)\n}\n\n# Initialize parameters\nrule_weights &lt;- runif(2)  # Example weights for the rule module, actual initialization may differ\nexemplars &lt;- matrix(runif(10), ncol=2)  # Assuming 5 exemplars with 2-dimensional input for demonstration purposes\nattention_weights &lt;- runif(2)  # Example attention weights\nexemplar_weights &lt;- runif(5)  # Example weights for the exemplar module\ng &lt;- 0.5  # Example gating parameter, the actual value should be set based on the model\n\n# Simulate the model with an input\ninput_data &lt;- c(0.5, 0.5)  # Example 2-dimensional input\n\nrule_output &lt;- rule_module(input_data, rule_weights)\nexemplar_output &lt;- exemplar_module(input_data, exemplars, attention_weights, exemplar_weights)\nfinal_output &lt;- gating_mechanism(rule_output, exemplar_output, g)\n\nprint(final_output)"
  },
  {
    "objectID": "Misc/Visuals_Interactives/model_viz.html",
    "href": "Misc/Visuals_Interactives/model_viz.html",
    "title": "Model Visualization",
    "section": "",
    "text": "Code#lapply(c('tidyverse','data.table','igraph','ggraph','kableExtra'),library,character.only=TRUE))\npacman::p_load(tidyverse,data.table,igraph,ggraph,kableExtra,DiagrammeR,png, plantuml)",
    "crumbs": [
      "Misc",
      "Model Visualization"
    ]
  },
  {
    "objectID": "Misc/Visuals_Interactives/model_viz.html#ggplot-model-visualization",
    "href": "Misc/Visuals_Interactives/model_viz.html#ggplot-model-visualization",
    "title": "Model Visualization",
    "section": "ggplot model visualization",
    "text": "ggplot model visualization\n\nCodenInput=6\nnOutput=10\n\ninNodes &lt;- seq(1,nInput,1) %&gt;% as.integer()\noutNodes &lt;- seq(300,1000,length.out=nOutput)%&gt;% as.integer()\nweight.mat &lt;&lt;- matrix(0.001,nrow=nOutput,ncol=nInput) # weights initialized to 0 (as in Delosh 1997)\n\nstim &lt;- 1.5\nc=.1\ninAct &lt;- round(exp(-c*((inNodes-stim)^2)),2)\ninActLab &lt;- paste0(\"x\",inNodes,\"=\",inAct)\noutAct &lt;- weight.mat %*% inAct\noutput.probability &lt;&lt;- outAct/sum(outAct)\noutLab=paste0(\"y\",outNodes,\"=\",round(output.probability,2))\nmean.response &lt;&lt;- round(sum(outNodes * output.probability),0)\n\n\nresp &lt;- mean.response\ninFlow &lt;- tibble(expand.grid(from=stim,to=inActLab)) %&gt;% mutate_all(as.character)\noutFlow &lt;- tibble(expand.grid(from=outLab,to=mean.response)) %&gt;% mutate_all(as.character)\n\ngd &lt;- tibble(expand.grid(from=inActLab,to=outLab)) %&gt;% mutate_all(as.character) %&gt;%\n  rbind(inFlow,.) %&gt;% rbind(.,outFlow)\n\nxInc &lt;- .3\nyInc=.5\n\ng = graph_from_data_frame(gd,directed=TRUE)\ncoords2=layout_as_tree(g)\ncolnames(coords2)=c(\"y\",\"x\")\n\nodf &lt;- as_tibble(coords2) %&gt;% \n  mutate(label=vertex_attr(g,\"name\"),\n         type=c(\"stim\",rep(\"Input\",nInput),rep(\"Output\",nOutput),\"Resp\"),\n         x=x*-1) %&gt;%\n  mutate(y=ifelse(type==\"Resp\",0,y),xmin=x-xInc,xmax=x+xInc,ymin=y-yInc,ymax=y+yInc)\n\nplot_edges = gd %&gt;% mutate(id=row_number()) %&gt;%\n  pivot_longer(cols=c(\"from\",\"to\"),names_to=\"s_e\",values_to=(\"label\")) %&gt;%\n                 mutate(label=as.character(label)) %&gt;% \n  group_by(id) %&gt;%\n  mutate(weight=sqrt(rnorm(1,mean=0,sd=10)^2)/10) %&gt;%\n  left_join(odf,by=\"label\") %&gt;%\n  mutate(xmin=xmin+.02,xmax=xmax-.02)\n\nggplot() + geom_rect(data = odf,\n            mapping = aes(xmin = xmin, ymin = ymin, \n                          xmax = xmax, ymax = ymax, \n                          fill = type, colour = type),alpha = 0.01) +\n  geom_text(data=odf,aes(x=x,y=y,label=label,size=3)) +\n  geom_path(data=plot_edges,mapping=aes(x=x,y=y,group=id,alpha=weight)) +\n  # geom_rect(aes(xmin=-1.05,xmax=-.95,ymin=-10,ymax=5),color=\"red\",alpha=.1)+\n  # geom_rect(aes(xmin=-0.05,xmax=.05,ymin=-10,ymax=5),color=\"blue\",alpha=.1) +\n  theme_void()",
    "crumbs": [
      "Misc",
      "Model Visualization"
    ]
  },
  {
    "objectID": "Misc/Visuals_Interactives/model_viz.html#ggraph-method",
    "href": "Misc/Visuals_Interactives/model_viz.html#ggraph-method",
    "title": "Model Visualization",
    "section": "ggraph method",
    "text": "ggraph method\n\nCodeinNodes &lt;- seq(1,6,1) %&gt;% as.integer()\noutNodes &lt;- seq(300,1000,50)%&gt;% as.integer()\n\nda &lt;- data.frame(expand.grid(inNodes,outNodes))  %&gt;% magrittr::set_colnames(c(\"input\",\"output\"))\nda &lt;- da %&gt;% mutate_all(as.character)\nm = graph_from_data_frame(da, directed = TRUE)\n\ncoords = layout_with_sugiyama(m)\ncolnames(coords$layout) = c(\"y\", \"x\")\ncoords$layout=coords$layout[,c(\"x\",\"y\")]\nplot(m,layout=coords)\n\n\n\n\n\n\nCodeggraph(m,layout=coords$layout)+\ngeom_edge_link0(width=0.2,colour=\"grey\")+\n  geom_node_point(col=\"white\",size=6)+scale_x_reverse()+\n  geom_node_text(aes(label=name)) +\n  # draw rectangle that covers input layer at x=1, min y is min of coords$y and max y is max of coords$y\n  annotate(\"rect\",xmin=0,xmax=.1,ymin=min(coords$layout[,2]),ymax=max(coords$layout[,2]),fill=\"grey\",alpha=0.7)\n\n\n\n\n\n\nCode geom_rect(xmin=0,xmax=1.1,ymin=min(coords$layout[,2]),ymax=max(coords$layout[,2]),fill=\"grey\",alpha=0.7)\n\ngeom_rect: linejoin = mitre, na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n\n\nCodelibrary(tidyverse)\nlibrary(ggforce)\n\ntheme_set(theme_grey() +\n            theme_void() +\n            theme(plot.margin = margin(0, 5.5, 0, 5.5)))\n\n# Parameters\ninput_x &lt;- c(1, 2)\noutput_x &lt;- c(1.5, 2.5, 3.5)\ninput_y &lt;- 3\noutput_y &lt;- 1\nactivation_levels &lt;- c(0.2, 0.7, 0.5)\n\n# Input Layer\ninput_layer &lt;- tibble(x = input_x, y = rep(input_y, length(input_x)))\n\n# Output Layer\noutput_layer &lt;- tibble(x = output_x, y = rep(output_y, length(output_x)), activation = activation_levels)\n\n# Gaussian Activation\ngaussian_activation &lt;- tibble(\n  x = seq(from = 0, to = 3, by = 0.01),\n  y = exp(-2 * (x - 1)^2) + 2.5,\n  x2 = x,\n  y2 = exp(-2 * (x - 2)^2) + 2.5\n)\n\n# Plot\np &lt;- ggplot() +\n  # Input nodes\n  geom_point(data = input_layer, aes(x = x, y = y), size = 4, color = 'green') +\n  annotate(\"text\", x = input_x, y = rep(input_y, length(input_x)) + 0.3, label = c(\"Input #1\", \"Input #2\")) +\n  \n  # Gaussian Activations\n  geom_line(data = gaussian_activation, aes(x = x, y = y), color = 'blue') +\n  geom_line(data = gaussian_activation, aes(x = x2, y = y2), color = 'blue') +\n  \n  # Connections\n  geom_segment(data = expand.grid(input_x, output_x), aes(x = Var1, xend = Var2, y = input_y, yend = output_y), arrow = arrow(type = 'closed', length = unit(0.2, 'inches'))) +\n  \n  # Output nodes\n  geom_point(data = output_layer, aes(x = x, y = y), size = 4, color = 'red') +\n  geom_bar(data = output_layer, aes(x = x, y = activation), stat = 'identity', position = 'dodge', fill = 'red', alpha = 0.3, width = 0.3) +\n  annotate(\"text\", x = output_x, y = rep(output_y, length(output_x)) - 0.3, label = c(\"Output #1\", \"Output #2\", \"Output #3\")) +\n  \n  # Equation annotations\n  annotate(\"text\", x = 0.5, y = input_y + 1, label = \"1\", parse = TRUE) +\n  annotate(\"text\", x = 1.5, y = output_y - 1, label = \"2 \", parse = TRUE) +\n  \n  # Input stimulus and output response\n  annotate(\"text\", x = mean(input_x), y = input_y + 1.3, label = \"Input Stimulus\") +\n  annotate(\"text\", x = mean(output_x), y = output_y - 1.3, label = \"Output Response\") +\n  \n  # Coordinate limits and axis labels\n  coord_cartesian(xlim = c(0, 4), ylim = c(-1, 5)) +\n  labs(x = \"\", y = \"\") +\n  theme_void()\n\n# Show the plot\nprint(p)\n\n\n\n\n\n\n\nAlt ggplot 2\n\nCodelibrary(ggplot2)\nlibrary(ggrepel)\n\nneural_network_plot &lt;- function(layers, layer_labels = NULL, node_labels = c('x', 'y')) {\ndata &lt;- expand.grid(layer = seq_along(layers), node = 1:max(layers))\ndata &lt;- data[data$node &lt;= layers[data$layer], ]\n\nlinks &lt;- data.frame()\nfor (i in 1:(length(layers) - 1)) {\nstart &lt;- subset(data, layer == i)\nend &lt;- subset(data, layer == i + 1)\nlinks &lt;- rbind(links, expand.grid(from = start$node, to = end$node, from_layer = start$layer, to_layer = end$layer))\n}\n\np &lt;- ggplot() +\ngeom_segment(data = links, aes(x = from_layer, xend = to_layer, y = from, yend = to), color = 'grey') +\ngeom_point(data = data, aes(x = layer, y = node), size = 5, color = 'orange') +\ntheme_minimal() +\ntheme(axis.text = element_blank(),\naxis.ticks = element_blank(),\npanel.grid = element_blank(),\naxis.title = element_blank())\n\nif (!is.null(layer_labels)) {\ndata$layer_label &lt;- layer_labels[data$layer]\np &lt;- p + geom_text_repel(data = data, aes(x = layer, y = max(layers) + 1, label = layer_label), size = 5, nudge_y = 1)\n}\n\nif (!is.null(node_labels)) {\ndata$node_label &lt;- ifelse(data$layer == 1, paste0(node_labels[1], '', data$node),\nifelse(data$layer == length(layers), paste0(node_labels[2], '', data$node), ''))\np &lt;- p + geom_text_repel(data = data, aes(x = layer, y = node, label = node_label), size = 3)\n}\n\nreturn(p)\n}\n\n#Example usage\nlayers &lt;- c(8, 5, 3, 5, 8)\nlayer_labels &lt;- c('Input Layer', '', 'Latent\\nRepresentation', '', 'Output Layer')\nneural_network_plot(layers, layer_labels)\n\n\n\n\n\n\n\nAlt ggplot 3\n\nCodelibrary(ggplot2)\n\ndata &lt;- data.frame(\nx = c(1, 2, 2, 3, 4, 4, 1, 2, 3, 4),\ny = c(0, 1, -1, 0, 0, 0, 0, 0, 0, 0),\nlabel = c(\"x_1\", \"w_1\", \"w_2\", \"Σ\", \"f\", \"y\", \"x_2\", \"w_3\", \"b\", \"\"),\nshape = c(\"circle\", \"square\", \"square\", \"circle\", \"square\", \"square\", \"circle\", \"square\", \"square\", \"square\"),\ntext_y = c(0.15, 0.15, -0.15, 0, 0.3, 0.3, -0.15, -0.3, 0.15, 0)\n)\n\ndata$shape &lt;- factor(data$shape, levels = c(\"circle\", \"square\"))\n\nggplot(data, aes(x = x, y = y)) +\ngeom_point(aes(shape = shape), size = 8) +\ngeom_text(aes(label = label, y = y + text_y), size = 5) +\nscale_shape_manual(values = c(\"circle\" = 19, \"square\" = 15)) +\ngeom_segment(aes(x = 2, xend = 3, y = 1, yend = 0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\ngeom_segment(aes(x = 2, xend = 3, y = -1, yend = -0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\ngeom_segment(aes(x = 3, xend = 2, y = 0.2, yend = 1), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\ngeom_segment(aes(x = 3, xend = 2, y = -0.2, yend = -1), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\ngeom_segment(aes(x = 3.5, xend = 4.5, y = 0, yend = 0)) +\ngeom_segment(aes(x = 4.5, xend = 5.5, y = 0, yend = 0)) +\nannotate(\"text\", x = 4, y = 0.5, label = \"Activation\\nFunction\", hjust = 0.5) +\nannotate(\"text\", x = 5, y = 0, label = \"Output\", hjust = 0.5) +\nannotate(\"text\", x = 3, y = 1.1, label = \"Bias\\nb\", hjust = 0.5) +\ngeom_rect(aes(xmin = 0.5, xmax = 1.5, ymin = -1.5, ymax = 1.5), color = \"black\", linetype = \"dashed\", fill = NA) +\nannotate(\"text\", x = 0.3, y = 0, label = \"Inputs\", angle = 90) +\ntheme_void() +\nxlim(0, 6) +\ntheme(legend.position = \"none\")\n\n\n\n\n\n\nCodedata &lt;- data.frame(\n    x = c(2, 3, 3, 4, 5, 5, 2, 3, 4, 5),\n    y = c(0, 1, -1, 0, 0, 0, 0, 0, 0, 0),\n    label = c(\"X\", \"w_{ji}\", \"(ai)\", \"(sum)\", \"O_j(X)\", \"m(X)\", \"X_i\", \"(e^{-gamma * (X-X[i])^2})\", \"(e^{-gamma * (Z-Y[j])^2})\", \"f_j(Z)\"),\n    shape = c(\"circle\", \"square\", \"circle\", \"circle\", \"circle\", \"circle\", \"circle\", \"circle\", \"circle\", \"circle\"),\n    text_y = c(0.15, 0.15, -0.15, 0, 0.3, 0.3, -0.15, -0.3, 0.15, 0)\n)\n\ndata$shape &lt;- factor(data$shape, levels = c(\"circle\", \"square\"))\n\nggplot(data, aes(x = x, y = y)) +\n    geom_point(aes(shape = shape), size = 8) +\n    geom_text(aes(label = label, y = y + text_y), size = 4) +\n    scale_shape_manual(values = c(\"circle\" = 19, \"square\" = 15)) +\n    geom_segment(aes(x = 3, xend = 4, y = 1, yend = 0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 3, xend = 4, y = -1, yend = -0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 4.5, xend = 5.5, y = 0, yend = 0)) +\n    annotate(\"text\", x = 5, y = 0.5, label = \"Mean Output\", hjust = 0.5) +\n    annotate(\"text\", x = 4, y = 1.1, label = \"Normalized Input\\nActivation\", hjust = 0.5) +\n    annotate(\"text\", x = 4, y = -1.1, label = \"Feedback\\nActivation\", hjust = 0.5) +\n    geom_rect(aes(xmin = 1.5, xmax = 2.5, ymin = -1.5, ymax = 1.5), color = \"black\", linetype = \"dashed\", fill = NA) +\n    annotate(\"text\", x = 1.3, y = 0, label = \"Input\\nStimulus\", angle = 90) +\n    annotate(\"text\", x = 3.5, y = -2, label = \"Associative Learning Model - Activation and Learning\", size = 5, hjust = 0.5) +\n    theme_void() +\n    xlim(0, 7) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\nCodedata &lt;- data.frame(\n    x = c(rep(2, 3), rep(4, 2), rep(6, 3)),\n    y = c(-1, 0, 1, 0, 0, -1, 0, 1),\n    label = c(\"e^{-γ(X-X_i)^2}\", \"Σ\", \"e^{-γ(Z-Y_j)^2}\", \"w_{ji}\", \"a_i(X)\", \"w_{ji}(t+1)\", \"O_j(X)\", \"Y_j\"),\n    shape = c(\"circle\", \"circle\", \"circle\", \"square\", \"square\", \"circle\", \"circle\", \"circle\"),\n    text_y = c(0.15, 0.15, 0.15, 0, 0, 0.15, 0.15, 0.15)\n)\n\ndata$shape &lt;- factor(data$shape, levels = c(\"circle\", \"square\"))\n\nggplot(data, aes(x = x, y = y)) +\n    geom_point(aes(shape = shape), size = 8) +\n    geom_text(aes(label = label, y = y + text_y), size = 5) +\n    scale_shape_manual(values = c(\"circle\" = 19, \"square\" = 15)) +\n    geom_segment(aes(x = 2, xend = 4, y = 1, yend = 0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 2, xend = 4, y = -1, yend = -0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 4, xend = 6, y = 0, yend = 1), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 4, xend = 6, y = 0, yend = -1), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    annotate(\"text\", x = 3, y = 1.5, label = \"Input Layer\\nGaussian Activation\\na_i(X)=e^{-γ(X-X_i)^2}\", hjust = 0.5) +\n    annotate(\"text\", x = 5, y = 1.5, label = \"Output Layer\\nO_j(X)=Σ w_{ji} a_i(X)\\nm(X)=Σ Y_j P[Y_j | X]\", hjust = 0.5) +\n    annotate(\"text\", x = 5, y = -1.5, label = \"Learning\\nw_{ji}(t+1)=w_{ji}(t)+α{f_j[Z(t)]-O_j[X(t)]}a_i[X(t)]\\nf_j(Z)=e^{-γ(Z-Y_j)^2}\", hjust = 0.5) +\n    theme_void() +\n    xlim(0, 8) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nCode# Load the required libraries\nlibrary(ggplot2)\nlibrary(ggpp)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Create the Gaussian curve plots for each input node\ninput_node_1 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 1, sd = .4)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\ninput_node_2 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 2, sd = .4)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\ninput_node_3 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = .4)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\n# Create the main plot\nmain_plot &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"plot_npc\", npcx = 0.2, npcy = 0.8, label = input_node_1) +\n  annotate(\"plot_npc\", npcx = 0.5, npcy = 0.8, label = input_node_2) +\n  annotate(\"plot_npc\", npcx = 0.8, npcy = 0.8, label = input_node_3) +\n  annotate(\"text\", x = 0.2, y = 0.7, label = \"Input Node 1\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"Input Node 2\") +\n  annotate(\"text\", x = 0.8, y = 0.7, label = \"Input Node 3\") +\n  annotate(\"text\", x = 0.5, y = 0.6, label = \"Input Layer\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.5, y = 0.3, label = \"Output Layer\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.5, y = 0.1, label = \"ALM + EXAM Response\", fontface = \"bold\")\n\n# Print the main plot\nprint(main_plot)\n\n\n\n\n\n# Create the Gaussian curve plots for each input node\ninput_node_1 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\ninput_node_2 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 2.2, sd = 1)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\ninput_node_3 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 4.3, sd = .1)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\n# Create the main plot\nmain_plot &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"plot_npc\", npcx = 0.2, npcy = 0.8, label = input_node_1, width = 0.1, height = 0.1) +\n  annotate(\"plot_npc\", npcx = 0.5, npcy = 0.8, label = input_node_2, width = 0.1, height = 0.1) +\n  annotate(\"plot_npc\", npcx = 0.8, npcy = 0.8, label = input_node_3, width = 0.1, height = 0.1) +\n  annotate(\"text\", x = 0.2, y = 0.9, label = \"Input Node 1\") +\n  annotate(\"text\", x = 0.5, y = 0.9, label = \"Input Node 2\") +\n  annotate(\"text\", x = 0.8, y = 0.9, label = \"Input Node 3\") +\n  annotate(\"text\", x = 0.5, y = 0.6, label = \"Input Layer\", fontface = \"bold\") +\n  annotate(\"rect\", xmin = 0.1, xmax = 0.3, ymin = 0.4, ymax = 0.5, fill = \"grey80\") +\n  annotate(\"rect\", xmin = 0.4, xmax = 0.6, ymin = 0.4, ymax = 0.5, fill = \"grey80\") +\n  annotate(\"rect\", xmin = 0.7, xmax = 0.9, ymin = 0.4, ymax = 0.5, fill = \"grey80\") +\n  annotate(\"text\", x = 0.5, y = 0.3, label = \"Output Layer\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.5, y = 0.1, label = \"ALM + EXAM Response\", fontface =\"bold\")\nprint(main_plot)\n\n\n# Create the Gaussian curve plots for each input node\ninput_node_1 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = .41)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  ggtitle(\"Input Node 1\")\n\ninput_node_2 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 2, sd = .41)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  ggtitle(\"Input Node 2\")\n\ninput_node_3 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = .41)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  ggtitle(\"Input Node 3\")\n\n# Create the output nodes\noutput_node_1 &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"rect\", xmin = -1, xmax = 1, ymin = -1, ymax = 1, fill = \"blue\") +\n  ggtitle(\"Output Node 1\")\n\noutput_node_2 &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"rect\", xmin = -1, xmax = 1, ymin = -1, ymax = 1, fill = \"blue\") +\n  ggtitle(\"Output Node 2\")\n\n# Combine the input and output nodes using patchwork\ninput_layer &lt;- input_node_1 + input_node_2 + input_node_3\noutput_layer &lt;- output_node_1 + output_node_2\n\n# Create the connection matrix\nconnection_matrix &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"segment\", x = 0.2, xend = 0.8, y = 0.8, yend = 0.2, arrow = arrow()) +\n  annotate(\"segment\", x = 0.5, xend = 0.8, y = 0.8, yend = 0.2, arrow = arrow()) +\n  annotate(\"segment\", x = 0.8, xend = 0.8, y = 0.8, yend = 0.2, arrow = arrow())\n\n# Combine the input layer, connection matrix, and output layer\nmain_plot &lt;- input_layer / connection_matrix / output_layer\n\n# Print the main plot\nprint(main_plot)\n\n\n\n\n\n\n# Function to create an input node\ncreate_input_node &lt;- function(mean, position) {\n  ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n    stat_function(fun = dnorm, args = list(mean = mean, sd = .3)) +\n    theme_void() +\n    theme(plot.margin = margin(0, 0, 0, 0)) +\n    coord_cartesian(xlim = c(0, 3), ylim = c(0, 0.5)) + coord_flip()+\n    labs(title = paste(\"Input Node\", position)) +\n    theme(plot.title = element_text(hjust = 0.5))\n}\n\n# Function to create an output node\ncreate_output_node &lt;- function(position) {\n  ggplot() +\n    theme_void() +\n    theme(plot.margin = margin(0, 0, 0, 0)) +\n    annotate(\"rect\", xmin = -1, xmax = 1, ymin = -1, ymax = 1, fill = \"grey80\") +\n    labs(title = paste(\"Output Node\", position)) +\n    theme(plot.title = element_text(hjust = 0.5))\n}\n\ncreate_input_layer &lt;- function(n) {\n  input_plots &lt;- map(1:n, ~create_gaussian_plot(.x))\n  input_layer &lt;- wrap_plots(input_plots, ncol = 1) + coord_flip()\n  return(input_layer)\n}\n\n\n\n# Create the input nodes\ninput_nodes &lt;- map(1:3, ~create_input_node(.x, .x))\n\n# Create the output nodes\noutput_nodes &lt;- map(1:4, ~create_output_node(.x))\n\n## Create the connection matrix\nconnection_matrix &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"segment\", x = 1, xend = 2, y = rep(1:3, each = 4), yend = rep(1:4, times = 3), arrow = arrow())\n# Combine the plots\ninput_layer &lt;- wrap_plots(input_nodes,byrow=FALSE,ncol=1)\noutput_layer &lt;- wrap_plots(output_nodes, ncol = 1)\nmain_plot &lt;- input_layer + connection_matrix + output_layer + plot_layout(ncol = 3)\n\n# Print the main plot\nprint(main_plot)\n\n\n\n\n\ncombined_plot &lt;- input_layer + connection_matrix + output_layer +\n  plot_layout(ncol = 3)\n\nprint(combined_plot)\n\n\n\nprint(input_layer)\n\n\n\nCode# Load the required libraries\nlibrary(ggplot2)\nlibrary(ggpp)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Function to create the input layer\ncreate_input_layer &lt;- function(n_nodes, stimulus = 0, gamma = 5) {\n  # Create a data frame with the x values and node means\n  df &lt;- expand.grid(x = seq(-3, 3, length.out = 100),\n                    mean = seq(-2, 1.5, length.out = n_nodes)) %&gt;% \n    mutate(noisyMean=map(mean,~dnorm(.x,mean,sd=.5)))\n  \n  df &lt;- df %&gt;%\n    mutate(distance = abs(x - stimulus),\n           noise = rnorm(n(), sd = 0.05),\n           max_activation = exp(-gamma * distance^2) + noise,\n           y = max_activation * exp(-gamma * (x - mean)^2))\n  \n  # Create the plot\n  ggplot(df, aes(x, y, group = mean)) +\n    geom_line() +\n    geom_smooth()+\n    coord_cartesian(xlim = c(-2, 2), ylim = c(0, 1)) +\n    labs(title = \"Input Layer\") +\n    theme(plot.title = element_text(hjust = 0.5))\n}\n\n# Function to create an output node\ncreate_output_node &lt;- function(position) {\n  ggplot() +\n    theme_void() +\n    theme(plot.margin = margin(0, 0, 0, 0)) +\n    annotate(\"rect\", xmin = -1, xmax = 1, ymin = -1, ymax = 1, fill = \"grey80\") +\n    labs(title = paste(\"Output Node\", position)) +\n    theme(plot.title = element_text(hjust = 0.5))\n}\n\n# Create the input layer\ninput_layer &lt;- create_input_layer(3)\n\n# Create the output nodes\noutput_nodes &lt;- map(1:4, ~create_output_node(.x))\n\n# Create the connection matrix\nconnection_matrix &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"segment\", x = 1, xend = 2, y = 1:3, yend = rep(1:4, each = 3), arrow = arrow())\n\n# Combine the plots\noutput_layer &lt;- wrap_plots(output_nodes, ncol = 1)\nmain_plot &lt;- input_layer + connection_matrix + output_layer + plot_layout(ncol = 3)\n\n# Print the main plot\nprint(main_plot)\n\n\n\nCode# Load the visNetwork package\nlibrary(visNetwork)\n\n# Define the nodes\nnodes &lt;- data.frame(\n  id = c(\"InputLayer\", \"I1\", \"I2\", \"I3\", \"OutputLayer\", \"O1\", \"O2\", \"O3\", \"O4\", \"ALM\", \"EXAM\"),\n  label = c(\"Input Layer\", \"Input Node 1\", \"Input Node 2\", \"Input Node 3\", \"Output Layer\", \"Output Node 1\", \"Output Node 2\", \"Output Node 3\", \"Output Node 4\", \"ALM Response\", \"EXAM Response\"),\n  shape = c(\"box\", \"image\", \"image\", \"image\", \"box\", \"circle\", \"circle\", \"circle\", \"circle\", \"box\", \"box\"),\n  image = c(NA, \"gaussian_curve_input_node_1.png\", \"gaussian_curve_input_node_2.png\", \"gaussian_curve_input_node_3.png\", NA, NA, NA, NA, NA, NA, NA)\n)\n\n# Define the edges\nedges &lt;- data.frame(\n  from = c(\"InputLayer\", \"I1\", \"I1\", \"I1\", \"I1\", \"I2\", \"I2\", \"I2\", \"I2\", \"I3\", \"I3\", \"I3\", \"I3\", \"OutputLayer\", \"OutputLayer\", \"ALM\", \"EXAM\"),\n  to = c(\"I1\", \"O1\", \"O2\", \"O3\", \"O4\", \"O1\", \"O2\", \"O3\", \"O4\", \"O1\", \"O2\", \"O3\", \"O4\", \"ALM\", \"EXAM\", \"EXAM\", \"ALM\"),\n  label = c(\"\", \"w1\", \"w2\", \"w3\", \"w4\", \"w5\", \"w6\", \"w7\", \"w8\", \"w9\", \"w10\", \"w11\", \"w12\", \"\", \"\", \"Pure ALM Model\", \"ALM with EXAM Response Component\")\n)\n\n# Create the network diagram\nvisNetwork(nodes, edges, width = \"100%\") %&gt;%\n  visNodes(shapeProperties = list(useBorderWithImage = TRUE)) %&gt;%\n  visLayout(randomSeed = 2)\n\n\n\n\n\n# Define the nodes\nnodes &lt;- data.frame(\n  id = c(\"InputLayer\", \"I1\", \"I2\", \"I3\", \"OutputLayer\", \"O1\", \"O2\", \"O3\", \"O4\", \"ALM\", \"EXAM\"),\n  label = c(\"Input Layer\", \"&lt;img src='gaussian_curve_input_node_1.png' /&gt;\", \"&lt;img src='gaussian_curve_input_node_2.png' /&gt;\", \"&lt;img src='gaussian_curve_input_node_3.png' /&gt;\", \"Output Layer\", \"Output Node 1\", \"Output Node 2\", \"Output Node 3\", \"Output Node 4\", \"ALM Response\", \"EXAM Response\"),\n  shape = c(\"box\", \"circle\", \"circle\", \"circle\", \"box\", \"circle\", \"circle\", \"circle\", \"circle\", \"box\", \"box\")\n)\n\n# Define the edges\nedges &lt;- data.frame(\n  from = c(\"InputLayer\", \"I1\", \"I1\", \"I1\", \"I1\", \"I2\", \"I2\", \"I2\", \"I2\", \"I3\", \"I3\", \"I3\", \"I3\", \"OutputLayer\", \"OutputLayer\", \"ALM\", \"EXAM\"),\n  to = c(\"I1\", \"O1\", \"O2\", \"O3\", \"O4\", \"O1\", \"O2\", \"O3\", \"O4\", \"O1\", \"O2\", \"O3\", \"O4\", \"ALM\", \"EXAM\", \"EXAM\", \"ALM\"),\n  label = c(NA, \"w1\", \"w2\", \"w3\", \"w4\", \"w5\", \"w6\", \"w7\", \"w8\", \"w9\", \"w10\", \"w11\", \"w12\", NA, NA, \"Pure ALM Model\", \"ALM with EXAM Response Component\")\n)\n\n# Create the network diagram\nvisNetwork(nodes, edges, width = \"100%\") %&gt;%\n  visNodes(shapeProperties = list(useBorderWithImage = TRUE)) %&gt;%\n  visLayout(randomSeed = 2)\n\n\n\nCode# Load the DiagrammeR package\nlibrary(DiagrammeR)\n# Define the graph\ngrViz(\"\n  digraph ALM_EXAM {\n\n    # Graph attributes\n    graph [overlap = true, fontsize = 10]\n\n    # Node definitions\n    node [shape = box, fontname = Helvetica]\n    InputLayer [label = 'Input Layer']\n    OutputLayer [label = 'Output Layer']\n    ALM [label = 'ALM Response']\n    EXAM [label = 'EXAM Response']\n\n    node [shape = circle, fixedsize = true, width = 0.9]\n    I1 [label = 'Input Node 1']\n    I2 [label = 'Input Node 2']\n    I3 [label = 'Input Node 3']\n    O1 [label = 'Output Node 1']\n    O2 [label = 'Output Node 2']\n    O3 [label = 'Output Node 3']\n    O4 [label = 'Output Node 4']\n\n    # Edge definitions\n    InputLayer -&gt; I1\n    InputLayer -&gt; I2\n    InputLayer -&gt; I3\n    I1 -&gt; O1 [label = 'w1']\n    I1 -&gt; O2 [label = 'w2']\n    I1 -&gt; O3 [label = 'w3']\n    I1 -&gt; O4 [label = 'w4']\n    I2 -&gt; O1 [label = 'w5']\n    I2 -&gt; O2 [label = 'w6']\n    I2 -&gt; O3 [label = 'w7']\n    I2 -&gt; O4 [label = 'w8']\n    I3 -&gt; O1 [label = 'w9']\n    I3 -&gt; O2 [label = 'w10']\n    I3 -&gt; O3 [label = 'w11']\n    I3 -&gt; O4 [label = 'w12']\n    OutputLayer -&gt; O1\n    OutputLayer -&gt; O2\n    OutputLayer -&gt; O3\n    OutputLayer -&gt; O4\n    OutputLayer -&gt; ALM\n    OutputLayer -&gt; EXAM\n    ALM -&gt; EXAM [label = 'Pure ALM Model']\n    EXAM -&gt; ALM [label = 'ALM with EXAM Response Component']\n  }\n\")\n\n\n\n\n\n\nCode# Load the required packages\nlibrary(ggplot2)\nlibrary(png)\n\n# Define the Gaussian function\ngaussian &lt;- function(x, mean, sd) {\n  1/(sd*sqrt(2*pi)) * exp(-1/2 * ((x - mean)/sd)^2)\n}\n\n# Define the central values for the input nodes\ncentral_values &lt;- c(1, 2, 3)\n\n# Define the input stimulus value\ninput_stimulus &lt;- 2\n\n# Generate a Gaussian curve image for each input node\nfor (i in seq_along(central_values)) {\n  # Define the mean and standard deviation for the Gaussian curve\n  mean &lt;- central_values[i]\n  sd &lt;- 1\n  # Generate the x values\n  x &lt;- seq(mean - 3*sd, mean + 3*sd, length.out = 100)\n  # Generate the y values\n  y &lt;- gaussian(x, mean, sd)\n  # Create the plot\n  p &lt;- ggplot(data.frame(x, y), aes(x, y)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"X\", y = \"Activation\", title = paste(\"Input Node\", i))\n  # Save the plot as a PNG image\n  ggsave(paste0(\"gaussian_curve_input_node_\", i, \".png\"), plot = p, width = 4, height = 3)\n}\n\n\n\nCodegrViz(\"digraph causal {\n                        # Nodes\n                        node [imagescale=true,shape = reactangle, fontname = Arial, style = filled]\n                        iv   [label = 'TRT', fillcolor = '#7FC97F']\n                        me   [label = 'Mediator', shape = ellipse]\n                        dv   [label = 'DLQI', fillcolor = '#7FC97F']\n                        \n                        # Edges\n                        edge [color = black, arrowhead = normal]\n                        rankdir = LR\n                        iv -&gt; me\n                        iv -&gt; dv [label = 'DIRECT', fontcolor = '#7FC97F', color = '#7FC97F']\n                        me -&gt; dv\n                        # Graph\n                        graph [overlap = true, fontsize = 10]\n                      }\")\n\n\ngrViz(\"digraph causal {\n                        # Nodes\n                        node [shape = reactangle, fontname = Arial, style = filled]\n                        iv   [image='gaussian_curve_input_node_1.png',label = '', fillcolor = '#7FC97F']\n                        me   [label = 'Mediator', shape = ellipse]\n                        dv   [label = 'DLQI', fillcolor = '#7FC97F']\n                        \n                        # Edges\n                        edge [color = black, arrowhead = normal]\n                        rankdir = LR\n                        iv -&gt; me\n                        iv -&gt; dv [label = 'DIRECT', fontcolor = '#7FC97F', color = '#7FC97F']\n                        me -&gt; dv\n                        # Graph\n                        graph [overlap = true, fontsize = 10]\n                      }\")\n\n\n\nCode# grViz('digraph structs {\n#     node [shape=plaintext];\n# \n#     struct1 [label=&lt;&lt;TABLE&gt;\n#       &lt;TR&gt;&lt;TD&gt;&lt;IMG SRC=\"gaussian_curve_input_node_1.png\"/&gt;&lt;/TD&gt;&lt;/TR&gt;\n#       &lt;TR&gt;&lt;TD&gt;caption&lt;/TD&gt;&lt;/TR&gt;\n#     &lt;/TABLE&gt;&gt;];\n# }')\n\n\nsvgDataUrl = \"data:image/svg+xml;base64,\"\ngrViz('\ndigraph {\n  node[imagescale=true, shape=circle, width=4, height=4,fontname=\"sans-serif\",penwidth=0]\n  a[label=&lt;&lt;FONT POINT-SIZE=\"10\"&gt;Transaction&lt;/FONT&gt;&lt;BR/&gt;&lt;FONT POINT-SIZE=\"36\"&gt;&lt;B&gt;TRNA&lt;/B&gt;&lt;/FONT&gt;&gt;,image=\"${svgDataUrl}\"]\n  b[label=&lt;&lt;FONT POINT-SIZE=\"10\"&gt;Transaction&lt;/FONT&gt;&lt;BR/&gt;&lt;FONT POINT-SIZE=\"18\"&gt;&lt;B&gt;TRNB&lt;/B&gt;&lt;/FONT&gt;&gt;,image=\"${svgDataUrl}\"]\n  c[label=&lt;&lt;FONT POINT-SIZE=\"10\"&gt;Transaction&lt;/FONT&gt;&lt;BR/&gt;&lt;FONT POINT-SIZE=\"24\"&gt;&lt;B&gt;TRNC&lt;/B&gt;&lt;/FONT&gt;&gt;,image=\"${svgDataUrl}\"]\n  a -&gt; b\n  a -&gt; c\n}')\n\ngrViz('digraph { a[image=\\\"gaussian_curve_input_node_1\\\"]; }\", {\n  images: [{ href: \"gaussian_curve_input_node_1.png\", width: \"400px\", height: \"300px\" }]\n}');\n\ngrViz('\ndigraph g{\n  node[imagescale=true, shape=circle, width=14, height=14,fontname=\"sans-serif\",penwidth=0]\n  I1[image=\"gaussian_curve_input_node_1.png\", label=\"\"];\n}')\n\ngrViz('digraph {\n    ratio=\"fill\";\n    size=\"10,10!\";\n    margin=\"0,0\";\n\n    node [shape=plain];\n    root [label=&lt;&lt;TABLE border=\"0\"&gt;&lt;TR&gt;&lt;TD&gt;&lt;IMG SRC=\"gaussian_curve_input_node_1\"/&gt;&lt;/TD&gt;&lt;/TR&gt;\n                                   &lt;TR&gt;&lt;TD&gt;text under&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;&gt;];\n}')\n\n\n\nCodegrViz(\"\n  digraph ALM_EXAM {\n\n    # Graph attributes\n    graph [overlap = true, fontsize = 10, rankdir = LR]\n\n    # Node definitions\n    node [shape = box, fontname = Helvetica]\n    InputLayer [label = 'Input Layer']\n    OutputLayer [label = 'Output Layer']\n    ALM [label = 'ALM Response']\n    EXAM [label = 'EXAM Response']\n\n    node [shape = none, label = '']\n    I1 [image = 'gaussian_curve_input_node_1.png', label='']\n    I2 [image = 'gaussian_curve_input_node_2.png', label='']\n    I3 [image = 'gaussian_curve_input_node_3.png', label='']\n\n    node [shape = circle, fixedsize = true, width = 0.9]\n    O1 [label = 'Output Node 1']\n    O2 [label = 'Output Node 2']\n    O3 [label = 'Output Node 3']\n    O4 [label = 'Output Node 4']\n\n    # Edge definitions\n    InputLayer -&gt; I1\n    InputLayer -&gt; I2\n    InputLayer -&gt; I3\n    I1 -&gt; O1 [label = 'w1']\n    I1 -&gt; O2 [label = 'w2']\n    I1 -&gt; O3 [label = 'w3']\n    I1 -&gt; O4 [label = 'w4']\n    I2 -&gt; O1 [label = 'w5']\n    I2 -&gt; O2 [label = 'w6']\n    I2 -&gt; O3 [label = 'w7']\n    I2 -&gt; O4 [label = 'w8']\n    I3 -&gt; O1 [label = 'w9']\n    I3 -&gt; O2 [label = 'w10']\n    I3 -&gt; O3 [label = 'w11']\n    I3 -&gt; O4 [label = 'w12']\n    OutputLayer -&gt; O1\n    OutputLayer -&gt; O2\n    OutputLayer -&gt; O3\n    OutputLayer -&gt; O4\n    OutputLayer -&gt; ALM\n    OutputLayer -&gt; EXAM\n    ALM -&gt; EXAM [label = 'Pure ALM Model']\n    EXAM -&gt; ALM [label = 'ALM with EXAM Response Component']\n  }\n\")\n\n\n\nCodeDiagrammeR::grViz(\"digraph {\n\ngraph [layout = dot, rankdir = LR]\n\n# define the global styles of the nodes. We can override these in box if we wish\nnode [shape = rectangle, style = filled, fillcolor = Linen]\n\ndata1 [label = 'Dataset 1', shape = folder, fillcolor = Beige]\ndata2 [label = 'Dataset 2', shape = folder, fillcolor = Beige]\nprocess [label =  'Process \\n Data']\nstatistical [label = 'Statistical \\n Analysis']\nresults [label= 'Results']\n\n# edge definitions with the node IDs\n{data1 data2}  -&gt; process -&gt; statistical -&gt; results\n}\")\n\n\n\n\n\n\n\n\n\n\ngraph LR\n  subgraph InputLayer\n    I1[\"Input Node 1\"]\n    I2[\"Input Node 2\"]\n    I3[\"Input Node 3\"]\n  end\n  subgraph OutputLayer\n    O1[\"Output Node 1\"]\n    O2[\"Output Node 2\"]\n    O3[\"Output Node 3\"]\n    O4[\"Output Node 4\"]\n  end\n  I1 --&gt;|\"w1\"| O1\n  I1 --&gt;|\"w2\"| O2\n  I1 --&gt;|\"w3\"| O3\n  I1 --&gt;|\"w4\"| O4\n  I2 --&gt;|\"w5\"| O1\n  I2 --&gt;|\"w6\"| O2\n  I2 --&gt;|\"w7\"| O3\n  I2 --&gt;|\"w8\"| O4\n  I3 --&gt;|\"w9\"| O1\n  I3 --&gt;|\"w10\"| O2\n  I3 --&gt;|\"w11\"| O3\n  I3 --&gt;|\"w12\"| O4\n  style InputLayer fill:#99cc99,stroke:#333,stroke-width:2px\n  style OutputLayer fill:#cc99cc,stroke:#333,stroke-width:2px\n  IS[\"Input Stimulus (X)\"] --&gt; InputLayer\n  InputLayer --&gt;|\"Gaussian Activation (a_i)\"| OutputLayer\n  OutputLayer --&gt; ALM[\"ALM Response (m(X))\"]\n  OutputLayer --&gt; EXAM[\"EXAM Response (E[Y | X_i])\"]\n  ALM --&gt;|\"Pure ALM Model\"| EXAM\n  EXAM --&gt;|\"ALM with EXAM Response Component\"| ALM\n  linkStyle 0 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 1 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 2 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 3 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 4 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 5 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 6 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 7 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 8 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 9 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 10 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 11 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 12 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 13 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 14 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 15 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 16 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 17 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 0 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 1 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 2 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 3 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 4 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 5 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 6 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 7 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 8 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 9 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 10 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 11 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 12 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 13 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 14 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 15 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 16 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 17 stroke:#2ecd71,stroke-width:2px;",
    "crumbs": [
      "Misc",
      "Model Visualization"
    ]
  },
  {
    "objectID": "Misc/Visuals_Interactives/model_viz.html#p5-sketching",
    "href": "Misc/Visuals_Interactives/model_viz.html#p5-sketching",
    "title": "Model Visualization",
    "section": "P5 sketching",
    "text": "P5 sketching\n\nCodeP5 = require(\"p5\")\nfunction* createSketch(sketch) {\n  const element = DOM.element('div');\n  yield element;\n  const instance = new P5(sketch, element, true);\n  try {\n    while (true) {\n      yield element;\n    }\n  } finally {\n    instance.remove();\n  }\n}\ncreateSketch(s =&gt; {\n  \n    s.setup = function() {\n      s.createCanvas(746, 300);\n      s.textFont('Courgette');\n      s.textStyle(s.BOLD);\n      s.textAlign(s.CENTER, s.CENTER)\n\n      s.button = s.createButton('clear');\n      s.button.mousePressed(s.clearCanvas);\n        s.text('Click and drag to draw', s.width/2, s.height/10);\n\n    };\n    s.draw = function() {\n    if (s.mouseIsPressed) {\n    s.fill(0);\n    s.ellipse(s.mouseX, s.mouseY, 10, 10);\n    } else {\n   //s.fill(255);\n    }\n  // add text input\n\n    };\n\n  // add button to clear canvas\n  s.clearCanvas = function() {\n    s.clear();\n  };\n  // add text\n  // add slider\n  }\n)",
    "crumbs": [
      "Misc",
      "Model Visualization"
    ]
  },
  {
    "objectID": "Misc/Visuals_Interactives/ojs_explore.html",
    "href": "Misc/Visuals_Interactives/ojs_explore.html",
    "title": "OJS data exploration",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,here)\n\nd &lt;- readRDS(here(\"data/dPrune-01-19-23.rds\"))\n\n# Prepare the data for analysis\ndtest &lt;- d %&gt;%\n    filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n    group_by(id, lowBound) %&gt;%\n    mutate(nBand = n(), band = bandInt, id = factor(id)) %&gt;%\n    group_by(id) %&gt;%\n    mutate(nd = n_distinct(lowBound))\ndtest &lt;- dtest %&gt;%\n    group_by(id, lowBound) %&gt;%\n    filter(nBand &gt;= 5 & nd == 6)\ndtest &lt;- dtest %&gt;%\n    group_by(id) %&gt;%\n    filter(!id %in% unique(dtest$id[dtest$nBand &lt; 5]))\n\ndtestAgg &lt;- dtest %&gt;%\n    group_by(id, condit, catOrder, feedbackType, vb, band, lowBound, highBound, input) %&gt;%\n    mutate(vxCapped = ifelse(vx &gt; 1600, 1600, vx)) %&gt;%\n    summarise(\n        vxMean = mean(vx), devMean = mean(dist), vxMed = median(vx), devMed = median(dist),\n        vxMeanCap = mean(vxCapped), .groups = \"keep\"\n    )\nds1 &lt;- d %&gt;%\n    filter(expMode %in% c(\"train\", \"train-Nf\", \"test-Nf\", \"test-train-nf\")) %&gt;%\n    filter(!id %in% unique(dtest$id[dtest$nBand &lt; 5]), vx&lt;1500) %&gt;%\n    select(id, condit, catOrder, feedbackType, expMode, trial, gt.train, vb, band, bandInt, lowBound, highBound, input, vx, dist, vxb)\n\n\nojs_define(dso=ds1)\n\n\n\nCodeimport { aq, op } from \"@uwdata/arquero\"\n\n//data = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n\n\n//ds1=transpose(ds1)\n\nds=transpose(dso)\n\nPlot.plot({\n  facet: {\n    data: ds,\n    x: \"condit\",\n    y: \"bandInt\",\n    marginRight: 80\n  },\n  marks: [\n    Plot.frame(),\n    Plot.rectY(ds, \n      Plot.binX(\n        {y: \"count\"}, \n        {x: \"vx\", thresholds: 50, fill: \"bandInt\"} // thresholds = number of bins\n      )\n    ),\n    Plot.tickX(ds, \n      Plot.groupZ(\n        {x: \"count\"}, \n        {x: \"vx\",\n         z: d =&gt; ds.condit+ ds.bandInt,\n         stroke: \"#333\",\n         strokeWidth: 2\n        }\n      )\n    )\n  ],\n    x: {label: \"Vx\", domain: [0, 1800],grid: true},\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodePlot.plot({\n  marks: [\n    Plot.line(ds, {\n      x: \"gt.train\",      // feature for the x channel\n      y: \"vx\",     // feature for the y channel\n      fill:\"condit\",\n      stroke: \"vb\",     \n    }),\n  ],\n  x: {label: \"Trial Number\"},\n  y: {label: \"Vx\", domain: [0, 1800],grid: true},\n  color: {legend: true, scheme: \"Turbo\",type: \"categorical\"},\n  width: 400,\n  height: 400\n});\n\n\n\n\n\n\n\nCodePlot.plot({\n  grid: true,\n  marks: [\n    Plot.rectY(ds, Plot.binX({y: \"count\"}, {x: \"vx\", fill: \"condit\", fy: \"condit\"})),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\nCodePlot.rectY(ds, Plot.binX({y: \"count\"}, {x: \"condit\", fill: \"condit\"})).plot()\n\n\n\n\n\n\n\nCodePlot.line(ds, {x: \"gt.train\", y: \"vx\",fill:\"condit\"}).plot({y: {grid: true}})\n\n\n\n\n\n\n\nCoded = aq.from(ds)\nd\n  .groupby(\"condit\", \"vb\", \"feedbackType\",\"expMode\",\"catOrder\")\n  .rollup({mean_vx: d =&gt; op.mean(d.vx)}, {mean_dist: d =&gt; op.mean(d.dist)})\n  .view(15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodedt = aq.from(ds)\ndtAgg = dt\n  .filter(dt =&gt; dt.expMode === \"train\")\n  .groupby(\"condit\", \"vb\",\"gt.train\")\n  .rollup({mean_vx: dt =&gt; op.mean(dt.vx)}, {mean_dist: dt =&gt; op.mean(dt.dist)})\n\n\nPlot.line(dtAgg, {x: \"gt.train\", y: \"mean_vx\",fill:\"vb\"}).plot({y: {grid: true}})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodePlot.plot({ \n    grid: true, \n    marginRight: 60,\n    facet: {\n        data: dtAgg,\n        x: \"condit\",\n        y: \"vb\",\n        marginRight: 80\n    },\n    marks: [\n        Plot.frame(),\n        Plot.lineY(dtAgg, {\n            x: \"gt.train\", \n            y: \"mean_vx\",\n            fill:\"vb\"\n            })\n    ]\n})\n\n\n\n\n\n\n\nCodedtAgg\n  .view(25)"
  },
  {
    "objectID": "Misc/benchmarks.html",
    "href": "Misc/benchmarks.html",
    "title": "Benchmarking",
    "section": "",
    "text": "Codesource(here::here(\"Functions\", \"packages.R\"))\ntest &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) |&gt;  filter(expMode2 == \"Test\") |&gt;\n  select(id,condit,bandInt,vb,vx,dist,sdist,bandType)\n\ne1_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e1_testVxBand_RF_5k\")))\nnew_data_grid=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"condit\",\"bandInt\")])))\n\n\nt1 &lt;- system.time({\ntidy_pred &lt;- test |&gt; add_predicted_draws(e1_vxBMM) |&gt; mutate(.residual = vx - .prediction); \npredict_per_row &lt;- tidy_pred |&gt; group_by(.row) |&gt; mean_hdi(.prediction,.residual)\n\n})\n\nm1 &lt;- system.time({\n  predict_vx &lt;- test |&gt; cbind(predict(e1_vxBMM, test)) |&gt; mutate(resid=vx-Estimate)\n}\n)\n\ncat(paste0(\"tidybayes preds = \",round(t1[\"elapsed\"],2),\" \\n \",\n           \"predict.brmsfit = \",round(m1[\"elapsed\"],2)))\n\n\npredict_vx |&gt; group_by(id,condit,bandInt) |&gt; summarise(mean(vx),mean(Estimate),mean(resid))\n\n\ntidybayes preds = 169.94 predict.brmsfit = 10.27\nhttps://cran.r-project.org/web/packages/brms/vignettes/brms_threading.html\nhttps://discourse.mc-stan.org/t/using-the-apple-m1-gpus-question-from-a-noob/23089/39?page=2\n\nCodepacman::p_load(tidyverse,tidybayes,brms,broom,broom.mixed,lme4,here,knitr,gt,gghalves,patchwork,ggdist,microbenchmark)\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\"))\ntest &lt;- e1 |&gt; filter(expMode2 == \"Test\")  \n\noptions(mc.cores = 4, brms.backend = \"cmdstanr\")\n\nn_cores=4\nbayes_seed &lt;- 1234\nn_iter=20000\nn_threads=2\n\n\ngprior&lt;- c( prior(normal(800, 100), class = Intercept),\n    prior(normal(400, 10), class = sigma)\n  )\n\n\n##############\n\ntf3 &lt;- system.time(\nfit_int_norm &lt;- brm(vx ~ 1 + condit, \n  data = test,\n  family = gaussian(),\n  iter=n_iter,\n  prior = gprior,\n  silent=2,\n  cores=n_cores,\n  threads = threading(n_threads),\n  seed=bayes_seed\n)\n)\ntf3\n\ntf4 &lt;- system.time(\nfit_int_norm &lt;- brm(vx ~ 1 + condit +(1|bandInt) + (1 + bandInt|id),\n  data = test |&gt; filter(id %in% 1:15),\n  family = gaussian(),\n  iter=n_iter,\n  prior = gprior,\n  silent=2,\n  cores=n_cores,\n  threads = threading(n_threads),\n  seed=bayes_seed\n) )\ntf4\n\n\n#########\n\n\ntf &lt;- system.time(\nfit_int_norm &lt;- brm(vx ~ 1 + condit, \n  data = test,\n  family = gaussian(),\n  iter=n_iter,\n  prior = gprior,\n  silent=2,\n  cores=n_cores,\n  seed=bayes_seed\n)\n)\ntf\n\ntf2 &lt;- system.time(\nfit_int_norm &lt;- brm(vx ~ 1 + condit +(1|bandInt) + (1 + bandInt|id),\n  data = test |&gt; filter(id %in% 1:15),\n  family = gaussian(),\n  iter=n_iter,\n  prior = gprior,\n  silent=2,\n  cores=n_cores,\n  seed=bayes_seed\n)\n)\ntf2\n\n\n##########\n\n\ncat(paste0(\"int only gaussian = \",round(tf[\"elapsed\"],2),\" \\n \",\n          # \"foreach parallel time=\",round(tfP[\"elapsed\"],2),\" \\n \",\n           \"irt version = \",round(tf2[\"elapsed\"],2), \" \\n \",\n           \"int only threading = \",round(tf3[\"elapsed\"],2),\" \\n \",\n          \"irt  threading = \",round(tf4[\"elapsed\"],2),\" \\n \"\n            \n          ))\n\n\ndefault brms settings (2K iterations)\nint only gaussian = 6.85 irt version = 12.15\nint only gaussian = 7.51 irt version = 12\nint only gaussian = 6.67 irt version = 11.82 int only threading = 7.49 irt threading = 11.33\ndefault brms settings (4K iterations) - 2 threads\nint only gaussian = 8.1 irt version = 14.69 int only threading = 6.99 irt threading = 15.24\ndefault brms settings (4K iterations) - 4 threads\nbrms int only gaussian = 8.11 irt version = 15.05 int only threading = 7.28 irt threading = 16.95\ndefault brms settings (4K iterations) - 4 threads - 2 cores\nint only gaussian = 11.27 irt version = 21.72 int only threading = 8.39 irt threading = 22.67\ndefault brms settings (4K iterations) - 2 threads - 2 cores\nint only gaussian = 11.27 irt version = 21.81 int only threading = 9.38 irt threading = 20.63\ndefault brms settings (4K iterations) - 8 threads - 4 cores\nint only gaussian = 8.27 irt version = 15.4 int only threading = 7.69 irt threading = 21.28\nint only gaussian = 8.37 irt version = 15.18 int only threading = 8.02 irt threading = 21.47\ndefault brms settings (4K iterations) - 8 threads - 8 cores\nint only gaussian = 8.11 irt version = 14.88 int only threading = 7.92 irt threading = 20.72\ndefault brms settings (10K iterations) - 8 threads - 8 cores\nint only gaussian = 11.38 irt version = 23.64 int only threading = 11.58 irt threading = 37.1\nint only gaussian = 11.53 irt version = 23.76 int only threading = 12 irt threading = 37.45\ndefault brms settings (10K iterations) - 8 threads - 4 cores\nint only gaussian = 11.48 irt version = 23.29 int only threading = 10.91 irt threading = 36.54\ndefault brms settings (10K iterations) - 4 threads - 4 cores\nint only gaussian = 11.99 irt version = 24.25 int only threading = 10.13 irt threading = 29.13\ndefault brms settings (10K iterations) - 4 threads - 1 cores\nint only gaussian = 29.35 irt version = 66.61 int only threading = 18.97 irt threading = 71.21\ndefault brms settings (10K iterations) - 2 threads - 2 cores\nint only gaussian = 17.98 irt version = 38.31 int only threading = 13.87 irt threading = 37.78\ndefault brms settings (10K iterations) - 1 threads - 4 cores\nint only gaussian = 11.14 irt version = 22.89 int only threading = 12.25 irt threading = 27.03\nint only gaussian = 11.35 irt version = 22.99 int only threading = 12.28 irt threading = 27.38\ndefault brms settings (10K iterations) - 2 threads - 4 cores\nint only gaussian = 11.22 irt version = 23.07 int only threading = 9 irt threading = 22.34\nint only gaussian = 11.31 irt version = 23.14 int only threading = 9.49 irt threading = 22.21\ndefault brms settings (10K iterations) - 3 threads - 4 cores\nint only gaussian = 11.15 irt version = 22.88 int only threading = 9.4 irt threading = 26.75\nint only gaussian = 11 irt version = 22.88 int only threading = 9.5 irt threading = 25.7\n(20K iterations) - 1 threads - 4 cores\nint only gaussian = 18.24 irt version = 40.37 int only threading = 20.32 irt threading = 48.94\nint only gaussian = 17.8 irt version = 39.26 int only threading = 20.54 irt threading = 48.14\nint only gaussian = 18.12 irt version = 40.31\nint only threading = 20.6 irt threading = 40.02 - when I commented out the threading specification\n(20K iterations) - 2 threads - 4 cores\nint only gaussian = 17.82 irt version = 39.34 int only threading = 14.22 irt threading = 37.73\n(20K iterations) - adding condit factor - 2 threads - 4 cores\nint only gaussian = 7.78 irt version = 45.66 int only threading = 8.57 irt threading = 45.97\n(20K iterations) - adding condit factor - 3 threads - 4 cores\nint only gaussian = 8.58 irt version = 47.07 int only threading = 10.71 irt threading = 57.87\n(20K iterations) - condit factor and bandInt RF slope - 2 threads - 4 cores\nint only gaussian = 7.85 irt version = 80.71 int only threading = 8.69 irt threading = 88.3\n\nCodepacman::p_load(tidyverse,foreach,doParallel,future,furrr,here)\npurrr::walk(c(here(\"Functions/alm_functions.R\",\"Functions/Display_Functions.R\")),source)\n\nn_cores &lt;- parallel::detectCores()\n\nparam_grid &lt;- tibble(crossing(\n  c = seq(.5,5,.25),\n  lr = seq(0.01, 1,.1),\n  noise_sd = c(0,.0001,0.001, 0.01),\n  inNodes = c(5, 7,14,28),\n  outNodes = c(16, 32,64)\n))\nnrow(param_grid)\n\ngen_train &lt;- function(trainVec=c(5,6,7),trainRep=3,noise=0){\n  bandVec=c(0,100,350,600,800,1000,1200)\n  if(class(trainVec)==\"list\"){trainVec=unlist(trainVec)}\n  ts &lt;- rep(seq(1,length(trainVec)),trainRep)\n  noiseVec=rnorm(length(ts),mean=0)*noise\n  if(noise==0) {noiseVec=noiseVec*0}\n  tibble(trial=seq(1,length(ts)),input=trainVec[ts],vx=bandVec[trainVec[ts]]+noiseVec)\n}\nfit_alm &lt;- function(data, c, lr, noise_sd, inNodes, outNodes) {\n  mse_list &lt;- replicate(5, {\n    train_data &lt;- data[, c(\"trial\", \"input\", \"cor\")] %&gt;% rename(\"vx\" = cor)\n    sim_result &lt;- sim_train(\n      dat = train_data,\n      c = c,\n      lr = lr,\n      inNodes = inNodes,\n      outNodes = outNodes,\n      noise_sd = noise_sd\n    )\n    train_data$almTrain &lt;- sim_result$almTrain\n    mse &lt;- mean((data$vx - train_data$almTrain)^2)\n    mse\n  })\n  avg_mse &lt;- mean(mse_list)\n  return(avg_mse)\n}\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=8) %&gt;% mutate(cor=vx,err=(800-0)*exp(-.1*seq(1,n()))+0,vx=cor-err)\n\n\nfurrr::furrr_options(seed = TRUE)\nplan(multisession, workers = n_cores-1)\ntff &lt;- system.time({\n\nparam_grid &lt;- param_grid %&gt;% mutate(performance = future_map_dbl(seq_len(nrow(.)), function(idx) {\n    fit_alm(gt, c = c[idx], lr = lr[idx], noise_sd = noise_sd[idx], inNodes = inNodes[idx], outNodes = outNodes[idx])\n  },.options = furrr_options(seed = T)))\n  best_paramsF &lt;- param_grid %&gt;%\n    arrange((performance)) \n  bestF &lt;- head(best_paramsF,1)\n})\n\n\n# cluster &lt;- parallel::makeCluster(n_cores-1)                 \n# doParallel::registerDoParallel(cluster)\n# tfP &lt;- system.time({\n#   param_grid &lt;- param_grid %&gt;%\n#     mutate(performance = foreach(idx = seq_len(nrow(.)), .combine = c) %dopar% {\n#       fit_alm(gt, c = c[idx], lr = lr[idx], noise_sd = noise_sd[idx], inNodes = inNodes[idx], outNodes = outNodes[idx])\n#     })\n#   best_paramsP &lt;- param_grid %&gt;%\n#     arrange((performance)) \n#   bestP &lt;- head(best_paramsP,1)\n#   })\n# stopImplicitCluster()\n#   tfP\n\n\ntI &lt;- system.time({\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=8) %&gt;% mutate(cor=vx,err=(600-0)*exp(-.1*seq(1,n()))+0,vx=cor-err)\n\nparam_grid &lt;- param_grid %&gt;%\n  mutate(performance = map_dbl(seq_len(nrow(.)), ~ {\n    fit_alm(gt, c = c[.x], lr = lr[.x], noise_sd = noise_sd[.x], inNodes = inNodes[.x], outNodes = outNodes[.x])\n  }))\nbest_paramsI &lt;- param_grid %&gt;%\n  arrange((performance)) \nbestI &lt;- head(best_paramsI,1)\n})\n\ntI\n\n\n\n\ncat(paste0(\"furr time=\",round(tff[\"elapsed\"],2),\" \\n \",\n          # \"foreach parallel time=\",round(tfP[\"elapsed\"],2),\" \\n \",\n           \" Standard Time=\",round(tI[\"elapsed\"],2)))\n\n\n4.2 -6.2 times faster with furr on m1\n\n\nRuns\nMachine\nCores\nMethod\nTime (seconds)\n\n\n\n9120\nM1\n9/10\nFurr\n9.56\n\n\n9120\nM1\n9/10\nStandard\n58.8\n\n\n912\nM1\n8/10\nStandard\n5.7\n\n\n912\nM1\n8/10\nParallel\n1.2\n\n\n912\nM1\n9/10\nFurr\n1.3\n\n\n912\n2015 iMac\n2/4\nStandard\n26.2\n\n\n912\n2015 iMac\n2/4\nParallel\n18.5\n\n\n912\n2015 iMac\n3/4\nFurr\n12.88\n\n\n912\nGTX\n3/4\nStandard\n25.32\n\n\n912\nGTX\n3/4\nFurr\n13.1\n\n\n\n\nFurrr and standard version both work equally fast, tested with up to 120 simulation repetitions\n\nCodelibrary(furrr)\nfurrr::furrr_options(seed = TRUE)\nplan(multisession, workers = parallel::detectCores())\n\nparmVec &lt;- tibble(crossing(c = c(0.1,.5), lr = c(0.4), noise = c(500), trainRep = c(20), lossFun = list(\"RMSE\", \"RMSE.blocked\"), simNum = 1:30))\ntf &lt;- system.time(\nsdpf &lt;- parmVec %&gt;% \n  mutate(d = future_pmap(list(c, lr, noise, trainRep), ~sim_data(c = ..1, lr = ..2, noise = ..3, trainRep = ..4),\n                         .options = furrr_options(seed = T)),\n                           almTrainDat = map(d, \"almTrain\"),\n                          almTestDat = map(d, \"almPred\"),\n                          examTestDat = map(d, \"examPred\"),\n                          td = map(trainRep, ~gen_train(trainRep = .)),\n                          fitO = map2(td, lossFun, ~wrap_optim(.x, .y)),\n                          fitG = map2(td, lossFun, ~wrap_grid(.x, .y)),\n                          cFitO = map_dbl(fitO, \"c\"),\n                          lrFitO = map_dbl(fitO, \"lr\"),\n                          optimValO = map_dbl(fitO, \"Value\"),\n                          cFitG = map_dbl(fitG, \"c\"),\n                          lrFitG = map_dbl(fitG, \"lr\"),\n                          optimValG = map_dbl(fitG, \"Value\"))\n)\ntf\n\nts &lt;- system.time(\nsdp &lt;- parmVec %&gt;% mutate(d = pmap(list(c, lr, noise, trainRep), ~sim_data(c = ..1, lr = ..2, noise = ..3, trainRep = ..4)),\n                          almTrainDat = map(d, \"almTrain\"),\n                          almTestDat = map(d, \"almPred\"),\n                          examTestDat = map(d, \"examPred\"),\n                          td = map(trainRep, ~gen_train(trainRep = .)),\n                          fitO = map2(td, lossFun, ~wrap_optim(.x, .y)),\n                          fitG = map2(td, lossFun, ~wrap_grid(.x, .y)),\n                          cFitO = map_dbl(fitO, \"c\"),\n                          lrFitO = map_dbl(fitO, \"lr\"),\n                          optimValO = map_dbl(fitO, \"Value\"),\n                          cFitG = map_dbl(fitG, \"c\"),\n                          lrFitG = map_dbl(fitG, \"lr\"),\n                          optimValG = map_dbl(fitG, \"Value\"))\n)\nts\n\n\nM1 Max times: tf user system elapsed 221.326 4.517 226.233\nts user system elapsed 221.140 4.288 225.330\nMac Pro times: tf user system elapsed 1125.102 76.859 1474.612\nts user system elapsed 1132.716 76.260 1493.154\nBenchmarking ALM Model Fit Functions\n\nCodepacman::p_load(tidyverse,data.table,microbenchmark())\n\n\nd &lt;- readRDS(here('dPrune-01-19-23.rds'))\n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\n# unique(dtest[dtest$nd==4,]$sbjCode) # 7 in wrong condition\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\n# for any id that has at least 1 nBand &gt;=5, remove all rows with that id. \ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,catOrder,feedbackType,vb,band,lowBound,highBound,input) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")\n\n# select first row for each id in d, then create histogram for nTrain\n#  d  %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ggplot(aes(nTrain)) + geom_histogram() + facet_wrap(~condit)\n  \nds &lt;- d %&gt;% filter(expMode %in% c(\"train\",\"train-Nf\",\"test-Nf\",\"test-train-nf\")) %&gt;% \nfilter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) %&gt;% \nselect(id,condit,catOrder,feedbackType,expMode,trial,gt.train,vb,band,bandInt,lowBound,highBound,input,vx,dist,vxb) \n\ndst &lt;- ds %&gt;% filter(expMode==\"train\",catOrder==\"orig\")\n\nvTrainTrial &lt;- dst %&gt;% filter(condit==\"Varied\",gt.train&lt;=84) %&gt;% group_by(gt.train,vb) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=5,labels=c(1:5)))\n\nbinTrainTrial &lt;- dst %&gt;% filter(gt.train&lt;=83) %&gt;% group_by(gt.train,vb,condit) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=6,labels=c(1:6)))\n\n\ntMax=84\nbandVec &lt;- rep(c(800,1000,1200),each=tMax/3)\nbandVec &lt;- bandVec[sample(1:length(bandVec),tMax,replace=FALSE)]\n\ntrainTrials &lt;- dst %&gt;% filter(gt.train&lt;=tMax) %&gt;% group_by(condit,gt.train,vb,bandInt,input) %&gt;% summarise(vx=mean(vx)) \n\ninput.activation&lt;-function(x.target, c){\n  return(exp(-1*c*(x.target-inputNodes)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, c){\n  return(weights%*%input.activation(x.target, c))\n}\n\nmean.prediction&lt;-function(x.target, weights, association.parameter){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, c,trainVec){\n  #trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, c)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, c)\n  mOver = mean.prediction(xOver, weights, c)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n  \nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  alm.train\n}\n\nwrap_alm &lt;- function(parms,dat, weights,lossFun){\n    c=parms[1]; lr=parms[2]\n   pred=train.alm(dat, c=c, lr=lr, weights=weights)\n   #sqrt(mean((dat$vx -pred)^2))\n   lossFun(dat$vx,pred)\n}\n\nwrap_optim &lt;- function(dat,wm,lossFun){\n  bounds_lower &lt;- c(.0000001, .00001)\n  bounds_upper &lt;- c(5, 5)\n\n optim(c(.1, .2),\n   fn = wrap_alm,\n   dat = dat, weights = wm,lossFun=lossFun,\n   method = \"L-BFGS-B\",\n   lower = bounds_lower,\n   upper = bounds_upper,\n   control = list(maxit = 1e4, pgtol = 0, factr = 0)\n )\n}\n\nRMSE &lt;- function(x,y){\n  sqrt(mean((x-y)^2))\n}\n\n## First average observed and predicted data into blocks, then compute RMSE\nRMSE.tb &lt;- function(x,y,blocks=6){\n  data.frame(x,y) %&gt;% mutate(t=row_number(),fitBins=cut(t,breaks=blocks,labels=c(1:blocks))) %&gt;%\n    group_by(fitBins) %&gt;% \n    summarise(predMean=mean(x),obsMean=mean(y)) %&gt;% \n    summarise(RMSE(predMean,obsMean)) %&gt;% as.numeric()\n}\n\n## Recode RMSE.tb using data.table functions rather than dplyr\nRMSE.tb2 &lt;- function(x,y,blocks=6){\n  data.table(x=x,y=y,t=seq(1,length(x))) %&gt;% \n    .[, `:=`(fitBins = cut(t, breaks = ..blocks, labels = c(1:..blocks)))] %&gt;%\n    .[, .(predMean = mean(x), obsMean = mean(y)), keyby = .(fitBins)] %&gt;%\n    .[, RMSE(predMean,obsMean)] %&gt;% as.numeric()\n}\n\n\ndplyr RMSE vs data.table RMSE\n\nCodedpVsDt=microbenchmark(\n  dplyrMethod={\n  fitVaried &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb);\n  fitConstant &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb)},\n  dtMethod={\n  fitVaried2 &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb2);\n  fitConstant2 &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb2)},\n  times=5\n)\nknitr::kable(summary(dpVsDt),format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\ncld\n\n\n\ndplyrMethod\n11.282291\n12.996857\n13.558623\n13.970367\n14.658515\n14.885086\n5\na\n\n\ndtMethod\n4.933999\n5.306232\n5.681235\n5.515397\n6.131193\n6.519355\n5\nb\n\n\n\nThe data.table version seems to be consistently more than 2x faster\nSeparate fits, vs. nesting method vs. split method\n\nCodenestSplit&lt;-microbenchmark(\nseparate={fitVaried &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb2);\nfitConstant &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb2) },\nnestGroups = tv %&gt;% group_by(condit) %&gt;% nest() %&gt;% mutate(fit=map(data,~wrap_optim(.,wm,RMSE.tb2))),\nsplitGroups = tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE.tb2)),\ntimes=5\n)\nknitr::kable(summary(nestSplit),format=\"markdown\") # 03/02/23 - Mac Pro\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\ncld\n\n\n\nseparate\n6.394459\n6.483235\n6.709882\n6.508353\n7.059420\n7.103941\n5\na\n\n\nnestGroups\n6.253850\n6.457956\n6.789962\n6.756942\n7.044965\n7.436094\n5\na\n\n\nsplitGroups\n6.117184\n6.455866\n6.605814\n6.461572\n6.640798\n7.353648\n5\na\n\n\n\nNot much of an effect for nesting method.\nComputing RMSE over Raw trials vs. RMSE of blocked training performance\n\nCodetrialBlock&lt;-microbenchmark(\n  trialFit = tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE)),\n  blockFit = tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE.tb2)),\n  times=5\n)\nknitr::kable(summary(trialBlock),format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\ncld\n\n\n\ntrialFit\n2.665184\n2.674186\n2.806476\n2.803491\n2.843806\n3.045712\n5\na\n\n\nblockFit\n5.169167\n5.264218\n5.350769\n5.285151\n5.491828\n5.543483\n5\nb\n\n\n\nThe models are fit about 2x faster when computing RMSE over trials. This shouldn’t be surprising, since fitting to blocked data requires several additional computations (e.g. grouping, computing means)",
    "crumbs": [
      "Misc",
      "Benchmarking"
    ]
  },
  {
    "objectID": "Misc/clust_gmm.html",
    "href": "Misc/clust_gmm.html",
    "title": "E1 Discrimination via Clustering",
    "section": "",
    "text": "https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/clustering-analysis.html#gaussian-mixture-models\nhttps://cran.r-project.org/web/packages/mclust/vignettes/mclust.html\nhttps://joshuamrosenberg.com/posts/lpa-in-r-using-mclust/\n\nCodepacman::p_load(tidyverse,here, mclust,furrr,future,broom)\nselect &lt;- dplyr::select\nmutate &lt;- dplyr::mutate\nfilter &lt;- dplyr::filter\nmap &lt;- purrr::map\n\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\n\n\ntest &lt;- e1 |&gt; filter(expMode2 == \"Test\")\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(nHits=sum(dist==0),vxMean=mean(vx),vxMed=median(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n\n\nget_mclust_param &lt;- function(model, param) {\n  if (!is.null(model$parameters[[param]])) {\n    return(model$parameters[[param]])\n  } else {\n    return(NA_real_)\n  }\n}\n\n\nplan(multisession, workers = 8)\n\n\n\nmcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n\n\nmcFree2 &lt;- get_mcFree(mcFree, \"id\", \"condit\", c(\"bandInt\", \"vb\", \"vx\", \"vy\"))\n\n\nmcFit &lt;- function(df, id_var, condit_var, clustvars,G=1:9) {\n  df %&gt;%\n    select({{ id_var }}, {{ condit_var }}, !!!clustvars) %&gt;%\n    ungroup() %&gt;%\n    group_by({{ id_var }}, {{ condit_var }}) %&gt;%\n    nest(data = clustvars) %&gt;%\n    mutate(mc = future_map(data, ~Mclust(.x[c(clustvars)],G=G))) %&gt;%\n    mutate(\n      bic= as.numeric(map(mc,\"bic\")),\n      Gfit = as.numeric(map(mc, \"G\")),\n      means = map(mc, get_mclust_param, param = \"mean\"),\n      proportions = map(mc, get_mclust_param, param = \"pro\")\n    )\n}\n\nmcFree1 &lt;- mcFit(test, id, condit, c(\"vx\"))\nmcFreeVxb &lt;- mcFit(test, id, condit, c(\"vxb\"))\nmcFree2 &lt;- mcFit(test, id, condit, c(\"vx\",\"vy\"))\n\nmcFreeVb &lt;- mcFit(test, id, condit, c(\"vx\",\"bandInt\"))\n\n\ndf &lt;- test |&gt; select(id,condit,bandInt,vx) |&gt; group_by(id,bandInt) |&gt; mutate(m=mean(vx),sd=sd(vx),n=n())\n\n\n\n\n\n# test1 &lt;- dfs |&gt; filter(id==1 || id==2) |&gt; mutate(samps = map2(m,sd, ~rnorm(n=10,mean=.x,sd=.y)))\n# test1 &lt;- test1 %&gt;% unnest(samps)\n\n\ndfs &lt;- test |&gt; select(id,condit,bandInt,vx,vb) |&gt; group_by(id,vb,condit) |&gt; summarise(m=mean(vx),sd=sd(vx),med=median(vx),n=n(),se=sd/sqrt(n))\n\nbdf &lt;- dfs |&gt; mutate(samps = map2(m,se, ~rnorm(n=100,mean=.x,sd=.y))) |&gt; unnest(samps) %&gt;% ungroup()\nmcBoot2 &lt;- mcFit(bdf, id, condit, c(\"samps\"))\n\nmcBoot2 |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nSample Stat bootstrap\n\nCodedfs &lt;- test |&gt; select(id,condit,bandInt,vx,vb) |&gt; group_by(id,vb,condit) |&gt; summarise(m=mean(vx),sd=sd(vx),med=median(vx),n=n(),se=sd/sqrt(n))\n\nbdf &lt;- dfs |&gt; mutate(samps = map2(m,se, ~rnorm(n=100,mean=.x,sd=.y))) |&gt; unnest(samps) %&gt;% ungroup()\nmcBoot2 &lt;- mcFit(bdf, id, condit, c(\"samps\"))\n\nmcBoot2 |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\n\ndfs2 &lt;- test |&gt; select(id,condit,bandInt,vx,vb,vxb) |&gt; group_by(id,vb,condit) |&gt; summarise(m=mean(vxb),sd=sd(vxb),med=median(vxb),n=n(),se=sd/sqrt(n))\n\nbdf2 &lt;- dfs2 |&gt; mutate(samps = map2(m,se, ~rnorm(n=100,mean=.x,sd=.y))) |&gt; unnest(samps) %&gt;% ungroup()\nmcBoot2b &lt;- mcFit(bdf2, id, condit, c(\"samps\"))\n\nmcBoot2b |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nEmpirical Bootstrap\n\nCode# Define the bootstrap function\nbootstrap &lt;- function(x, n) {\n  sample(x, size = n, replace = TRUE)\n}\n\n# Apply the bootstrap function to your data\nbdf2 &lt;- test %&gt;%\n  select(id, condit, bandInt, vx, vb, vxb) %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(vxb_bootstrap = list(bootstrap(vx, 20)), .groups = \"drop\") %&gt;%\n  unnest(vxb_bootstrap)\n\neb1 &lt;- mcFit(bdf2, id, condit, c(\"vxb_bootstrap\"))\n\neb1 |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\neb2&lt;- test %&gt;%\n  select(id, condit, bandInt, vx, vb, vxb) %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(vxb_bootstrap = list(bootstrap(vx, 100)), .groups = \"drop\") %&gt;%\n  unnest(vxb_bootstrap) %&gt;% mcFit(.,id,condit,c(\"vxb_bootstrap\"))\n\neb2 |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nPopulation Clustering\n\nCodebic &lt;- mclustBIC(test$vx)\nplot(bic)\nmod1 &lt;- Mclust(test$vx,x=bic)\nsummary(mod1,parameters=TRUE)\n\nbic &lt;- mclustBIC(c(test$vx,test$vy))\nplot(bic)\n\nbic &lt;- mclustBIC(c(test$sdist))\nplot(bic)\n\nbic &lt;- mclustBIC(c(test$dist))\nplot(bic)\n\nbic &lt;- mclustBIC(c(test$dist,test$vx))\nplot(bic)\n\n\n\nbic &lt;- mclustBIC(c(test$vx,test$bandInt))\nplot(bic)\nsummary(bic)\n\n\n\nCodemv4 &lt;- mclustBIC(subset(test,select = c(vx,bandInt)),\n                 G=1:7,\n                 modelNames=c(\"EII\", \"VVI\", \"EEE\", \"VVV\"))\n y &lt;- mv4%&gt;%\n        as.data.frame.matrix() %&gt;%\n        rownames_to_column(\"n_mixtures\") %&gt;%\n        rename(`Constrained variance, fixed covariance` = EII, \n               `Freed variance, fixed covariance` = VVI,\n               `Constrained variance, constrained covariance` = EEE,\n               `Freed variance, freed covariance` = VVV)\n \nto_plot &lt;- y %&gt;%  gather(`Covariance matrix structure`, val, -n_mixtures) %&gt;% \n    mutate(`Covariance matrix structure` = as.factor(`Covariance matrix structure`),\n           val = abs(val)) # this is to make the BIC values positive (to align with more common formula / interpretation of BIC)\nggplot(to_plot, aes(x = n_mixtures, y = val, color = `Covariance matrix structure`, group = `Covariance matrix structure`)) +\n    geom_line() +\n    geom_point() +\n    ylab(\"BIC (smaller value is better)\") \n\n\n\nCodem = Mclust(test$vx)\ntest$vx\n\n\nsummary(m)\n\nmv &lt;- Mclust(subset(test, condit == \"Varied\", select = vx))\nsummary(mv)\nplot(mv, what = \"classification\")\n\nmc &lt;- Mclust(subset(test, condit == \"Constant\", select = vx))\nsummary(mc)\nplot(mc, what = \"classification\")\n\n\n\nmv2 &lt;- densityMclust(subset(test, condit == \"Varied\", select = c(vxb,vy)))\nsummary(mv2)\nplot(mv2, what = \"density\",type=\"persp\")\nplot(mv2, what = \"BIC\")\n\n\nmv3 &lt;- mclustBIC(subset(test, condit == \"Varied\", select = vx),\n                 G=1:7)\n y &lt;- mv3%&gt;%\n        as.data.frame.matrix() %&gt;%\n        rownames_to_column(\"n_mixtures\") %&gt;%\n        rename(`Constrained variance, constrained covariance` = E,\n               `Freed variance, freed covariance` = V)\n\nplot(mc, what = \"classification\")\n\nmc &lt;- Mclust(subset(test, condit == \"Constant\", select = vx),G=6)\nsummary(mc)\ntidy(mc)\nglance(mc)\n\n\nVx and bandInt\n\nCodemcFreeVb &lt;- mcFit(test, id, condit, c(\"vx\",\"bandInt\"),G=1:6)\n\n\n\nmcFreeVb %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\nmcFreeVb2 &lt;- mcFit(test, id, condit, c(\"vxb\",\"bandInt\"),G=1:6)\n\nmcFreeVb2 %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nmcFreeVb3 &lt;- mcFit(test, id, condit, c(\"vxb\",\"bandInt\",\"vy\"),G=1:6)\n\nmcFreeVb3 %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\n\nCodemcFreeVb1 &lt;- mcFit(test, id, condit, c(\"vx\"),G=1:6)\n\nmcFreeVb1 %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\nmcFreeVb1b &lt;- mcFit(test, id, condit, c(\"vxb\"),G=1:6)\n\nmcFreeVb1b %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nboot1 &lt;- MclustBootstrap(mcFreeVb1b[[4]][[1]], nboot = 1000, type = \"bs\")\nsummary(boot1)\n\n\n\nCodemcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcFree &lt;- mcFree %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  mutate(mc = future_map(data, ~MclustBootstrap(.x$vx))) \n\n\nmcFree &lt;- mcFree %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\n\nFit free to each id\n\nCodemcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcFree &lt;- mcFree %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  mutate(mc = future_map(data, ~Mclust(.x$vx))) \n\n\nmcFree &lt;- mcFree %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nggplot(mcFree,aes(x=condit,y=Gfit))+geom_boxplot()+geom_jitter()\n\nggplot(mcFree,aes(x=Gfit,fill=condit))+geom_bar(position=position_dodge())\n\n# mcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n# mcFree &lt;- mcFree |&gt; group_by(id) |&gt; nest(id) |&gt; mutate(mc=map(vx, ~Mclust(.x$vx))) \n#   \n\nplot(mcCondit6[[2]][[1]], what = \"classification\")\nplot(mcCondit6[[2]][[2]], what = \"classification\")\n\nk=summary(mcCondit6[[2]][[2]],parameters=TRUE)\n\n\nFit xy free to each id\n\nCodemcFree2 &lt;- test |&gt; select(id,condit,bandInt,vb,vx,vy) %&gt;% ungroup()\n\nmcFree2 &lt;- mcFree2 %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx,vy)) %&gt;%\n  mutate(mc = future_map(data, ~Mclust(c(.x$vx,.x$vy)))) \n\n\nmcFree2 &lt;- mcFree2 %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nggplot(mcFree2,aes(x=condit,y=Gfit))+geom_boxplot()+geom_jitter()\n\nggplot(mcFree2,aes(x=Gfit,fill=condit))+geom_bar(position=position_dodge())\n\n# mcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n# mcFree &lt;- mcFree |&gt; group_by(id) |&gt; nest(id) |&gt; mutate(mc=map(vx, ~Mclust(.x$vx))) \n#   \n\nplot(mcCondit6[[2]][[1]], what = \"classification\")\nplot(mcCondit6[[2]][[2]], what = \"classification\")\n\nk=summary(mcCondit6[[2]][[2]],parameters=TRUE)\n\n\nFit xy free to each id\n\nCodemcFree2b &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcFree2b &lt;- mcFree2b %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  mutate(mc = future_map(data, ~Mclust(c(.x$vx,.x$bandInt)))) \n\n\nmcFree2b &lt;- mcFree2b %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nggplot(mcFree2b,aes(x=condit,y=Gfit))+geom_boxplot()+geom_jitter()\nggplot(mcFree2b,aes(x=Gfit,fill=condit))+geom_bar(position=position_dodge())\n\nmcFree2b %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\n\nCodemcFree2b &lt;- test |&gt; select(id,condit,bandInt,vb,vx,vy) %&gt;% ungroup()\n\nmcFree2b &lt;- mcFree2b %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx,vy)) %&gt;%\n  mutate(mc = future_map(data, ~Mclust(c(.x$vx,.x$bandInt,.x$vy)))) \n\n\nmcFree2b &lt;- mcFree2b %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nggplot(mcFree2b,aes(x=condit,y=Gfit))+geom_boxplot()+geom_jitter()\nggplot(mcFree2b,aes(x=Gfit,fill=condit))+geom_bar(position=position_dodge())\n\nmcFree2b %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\n\nCodemcConditFree &lt;- test |&gt; select(condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcConditFree &lt;- mcConditFree %&gt;%\n  group_by(condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  reframe(mc = future_map(data, ~Mclust(.x$vx))) \n\n\nmcConditFree &lt;- mcConditFree %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nmcCondit6 &lt;- test |&gt; select(condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcCondit6 &lt;- mcCondit6 %&gt;%\n  group_by(condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  reframe(mc = future_map(data, ~Mclust(.x$vx,G=6))) \n\n\nmcCondit6 &lt;- mcCondit6 %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    bic=as.numeric(map(mc,\"bic\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\n\ntable(class,mcCondit6[[2]][[2]]$classification)\n\nbic &lt;- mclustBIC(test$vx)\n\n\n\nCodedf &lt;- test %&gt;%\n    select(id, condit, bandInt, vb, vx) %&gt;%\n    group_by(id) %&gt;%\n    nest(data = c(condit, bandInt, vb, vx)) \n\n# Define a function to apply the Mclust operation to one group\nprocess_group &lt;- function(group_data, group_id) {\n    print(paste(\"Processing group\", group_id))\n    if(length(unique(group_data$vx))&gt;=12) {\n        Mclust(group_data$vx, G=6)\n    } else {\n        Mclust(rep(0, 12), G=6)  # dummy Mclust model\n    }\n}\n\n# Apply the function to each group one by one\ndf$mc &lt;- map2(df$data, df$id, process_group)\n\n\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\n\n\nCodecenters &lt;- tibble(\n  cluster = factor(1:3),\n  # number points in each cluster\n  num_points = c(100, 150, 50),\n  # x1 coordinate of cluster center\n  x1 = c(5, 0, -3),\n  # x2 coordinate of cluster center\n  x2 = c(-1, 1, -2)\n)\n\npoints &lt;- centers %&gt;%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %&gt;%\n  select(-num_points, -cluster) %&gt;%\n  unnest(c(x1, x2))\n\n\nm &lt;- Mclust(points)\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    mclust_success = map_lgl(mc, ~ !is.null(.x$parameters))\n  ) %&gt;%\n  filter(mclust_success)\n\n\n\nCodelibrary(mixR)\n\nmr1 &lt;- mixfit(test$vx,ncomp=3)\nplot(mr1)\nmr1\n\nmr2 &lt;- mixfit(test$vx,ncomp=6,family=\"gamma\")\nplot(mr2)\nmr2\n\n\n\nCodelibrary(flexmix)\n#https://rdrr.io/cran/flexmix/man/flexmix.html\n\nfm1=flexmix(vx~1|id,data=test,k=6)\nsummary(fm1)\nplot(fm1)\nparameters(fm1)\n\ntest %&gt;% group_by(vb) %&gt;% summarise(mean(vx))\n\n\n\nCode#http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-known-methods/\n\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\ntest &lt;- e1 |&gt; filter(expMode2 == \"Test\") |&gt; ungroup()\nlibrary(factoextra)\nlibrary(NbClust)\n\n\n\nfviz_nbclust(test |&gt; select(vx), kmeans,method = \"gap_stat\")\n\nNbClust(test |&gt; select(vx),distance=\"euclidean\",min.nc=2,max.nc=15,method=\"average\")\n\n\n\nCodelibrary(brms)\n\ntestS &lt;- test |&gt; filter(id %in% 1:20)\n\noptions(mc.cores = 2, brms.backend = \"cmdstanr\")\n\nmix &lt;- mixture(gaussian, gaussian, gaussian,gaussian,gaussian,gaussian)\nprior &lt;- c(\n  prior(normal(200, 100), Intercept, dpar = mu1),\n  prior(normal(450, 100), Intercept, dpar = mu2),\n  prior(normal(700, 100), Intercept, dpar = mu3),\n  prior(normal(900, 100), Intercept, dpar = mu4),\n  prior(normal(1100, 100), Intercept, dpar = mu5),\n  prior(normal(1300, 100), Intercept, dpar = mu6)\n)\nfit1 &lt;- brm(bf(vxC ~ 1), testS, family = mix,\n            prior = prior, chains = 2) \n\nsummary(fit1)\npp_check(fit1,type=\"stat_grouped\",ndraws=500,group=\"bandInt\",stat=\"mean\")\n\n# saveRDS(object = fit1,\n#         file = here(\"data/model_cache/mix6_vxC_test6.rda\"), \n#         compress = \"xz\")\n\n\n7x speedup for scaled vx\n\nCodelibrary(brms)\n\ntest$scaleVx &lt;- scale(test$vx)\ntest %&gt;% group_by(vb) %&gt;% summarise(mean(scaleVx))\n# vb        `mean(scaleVx)`\n#   &lt;fct&gt;               &lt;dbl&gt;\n# 1 100-300           -0.544 \n# 2 350-550           -0.273 \n# 3 600-800           -0.0194\n# 4 800-1000           0.449 \n# 5 1000-1200          0.774 \n# 6 1200-1400          1.00  \n\ntestS &lt;- test |&gt; filter(id %in% 1:20)\n\noptions(mc.cores = 2, brms.backend = \"cmdstanr\")\n\nmix &lt;- mixture(gaussian, gaussian, gaussian,gaussian,gaussian,gaussian)\nprior &lt;- c(\n  prior(normal(-.5, 10), Intercept, dpar = mu1),\n  prior(normal(-.25, 10), Intercept, dpar = mu2),\n  prior(normal(-.02, 10), Intercept, dpar = mu3),\n  prior(normal(.4, 10), Intercept, dpar = mu4),\n  prior(normal(.7, 10), Intercept, dpar = mu5),\n  prior(normal(1, 10), Intercept, dpar = mu6)\n)\nfitScale &lt;- brm(bf(scaleVx ~ 1), testS, family = mix,\n            prior = prior, chains = 2) \n\nsummary(fitScale)\npp_check(fit1,type=\"stat_grouped\",ndraws=500,group=\"bandInt\",stat=\"mean\")\n\n# saveRDS(object = fit1,\n#         file = here(\"data/model_cache/mix6_vxC_test6.rda\"), \n#         compress = \"xz\")\n\n\n\nCodeoptions(mc.cores = 4, brms.backend = \"cmdstanr\")\n\nmix &lt;- mixture(gaussian, gaussian, gaussian,gaussian,gaussian,gaussian)\nprior &lt;- c(\n  prior(normal(-.5, 10), Intercept, dpar = mu1),\n  prior(normal(-.25, 10), Intercept, dpar = mu2),\n  prior(normal(-.02, 10), Intercept, dpar = mu3),\n  prior(normal(.4, 10), Intercept, dpar = mu4),\n  prior(normal(.7, 10), Intercept, dpar = mu5),\n  prior(normal(1, 10), Intercept, dpar = mu6)\n)\n\nfitScaleInt &lt;- brm(bf(scaleVx ~ 1 + (1|id)), test, family = mix,\n            prior = prior, chains = 2,iter=1000) \n\nsummary(fitScaleInt)\npp_check(fitScaleInt,type=\"stat_grouped\",ndraws=500,stat=\"mean\")\n\n# saveRDS(object = fit1,\n#         file = here(\"data/model_cache/mix6_vxC_test6.rda\"), \n#         compress = \"xz\")"
  },
  {
    "objectID": "Misc/data_organize.html",
    "href": "Misc/data_organize.html",
    "title": "Organization",
    "section": "",
    "text": "pacman::p_load(tidyverse,here,knitr,kableExtra,reactable)\nselect &lt;- dplyr::select; mutate &lt;- dplyr::mutate \noptions(dplyr.summarise.inform=FALSE)\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\")) %&gt;% ungroup()\n\n\nd &lt;- d |&gt; mutate(bandType=case_when(\n  vb %in% c(\"1000-1200\",\"1200-1400\") & condit==\"Constant\" & bandOrder==\"Original\" ~ \"Extrapolation\",\n  vb %in% c(\"100-300\",\"350-550\") & condit==\"Constant\" & bandOrder==\"Reverse\" ~ \"Extrapolation\",\n  (vb %in% c(\"100-300\",\"350-550\",\"600-800\") & bandOrder==\"Original\") |\n  (vb %in% c(\"1200-1400\",\"1000-1200\",\"800-1000\") & bandOrder==\"Reverse\") ~ \"Extrapolation\",\n  \n  (vb %in% c(\"800-1000\",\"1000-1200\",\"1200-1400\") & bandOrder==\"Original\") |\n  (vb %in% c(\"100-300\",\"350-550\",\"600-800\") & bandOrder==\"Reverse\") ~ \"Trained\",\n  TRUE ~ NA_character_\n))\n\nd$bandType &lt;- factor(d$bandType,levels=c(\"Trained\",\"Extrapolation\"))\nd&lt;- d %&gt;% relocate(bandOrder,bandType,.after=bandInt)\n\n\nd &lt;- d |&gt; mutate(Exp=case_when(\n  fb==\"Continuous\" & bandOrder==\"Original\" ~ \"E1\",\n  fb==\"Continuous\" & bandOrder==\"Reverse\" ~ \"E2\",\n  fb==\"Ordinal\" ~ \"E3\",\n  TRUE ~ NA_character_\n))\n\nalt &lt;- d |&gt; filter(expMode2==\"Test\") |&gt; group_by(id,condit,Exp) |&gt; summarise(n=n_distinct(bandInt)) |&gt; \n  filter(n&lt;=4) |&gt; pull(id) |&gt; droplevels()\nd &lt;- d |&gt; filter(!(id %in% alt))\n\nd &lt;- d |&gt; mutate(vxC = ifelse(vx&gt;1800,1800,vx))\nd &lt;- d %&gt;% relocate(vxC,.after=vx)\nd &lt;- d |&gt; select(-goodThrow, -trainVec, -fullCond)\n\n# test &lt;- d |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\"))\n# rtest &lt;- test |&gt; filter(bandOrder==\"Reverse\")\n# rtest |&gt; group_by(vb,bandType,condit,tOrder) |&gt; summarise(n=n())\ne1 &lt;- d |&gt; filter(fb==\"Continuous\" & bandOrder==\"Original\") |&gt; mutate(id=factor(id,levels=unique(id)))\ne2 &lt;- d |&gt; filter(fb==\"Continuous\" & bandOrder==\"Reverse\") |&gt; mutate(id=factor(id,levels=unique(id)))\ne3 &lt;- d |&gt; filter(fb==\"Ordinal\") |&gt; mutate(id=factor(id,levels=unique(id)))\n\ndate.append=\"08-21-23\"\n\n# saveRDS(d,here(paste0(\"data/dAll_\",date.append,\".rds\")))\n# saveRDS(e1,here(paste0(\"data/e1_\",date.append,\".rds\")))\n# saveRDS(e2,here(paste0(\"data/e2_\",date.append,\".rds\")))\n# saveRDS(e3,here(paste0(\"data/e3_\",date.append,\".rds\")))\n\n# save csv versions\n# d %&gt;% write_csv(here(paste0(\"data/dAll_\",date.append,\".csv\")))\n# e1 %&gt;% write_csv(here(paste0(\"data/e1_\",date.append,\".csv\")))\n# e2 %&gt;% write_csv(here(paste0(\"data/e2_\",date.append,\".csv\")))\n# e3 %&gt;% write_csv(here(paste0(\"data/e3_\",date.append,\".csv\")))\nhead(d) colnames(d) d %&gt;% select_if(is.factor) %&gt;% colnames()"
  },
  {
    "objectID": "Misc/data_organize.html#prep-simpler-data-frames-for-model-fitting",
    "href": "Misc/data_organize.html#prep-simpler-data-frames-for-model-fitting",
    "title": "Organization",
    "section": "Prep simpler data frames for model fitting",
    "text": "Prep simpler data frames for model fitting\n\nd &lt;- readRDS(here::here(\"data/e1_08-21-23.rds\"))\nlevels(d$condit)\n\n[1] \"Constant\" \"Varied\"  \n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\nds &lt;- d %&gt;% filter(expMode2 %in% c(\"Train\",\"Test\")) |&gt; \n  filter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) |&gt;   \n  group_by(id,condit,expMode2) |&gt; \n  mutate(input=bandInt,x=bandInt, y=vx, tr= row_number()) |&gt;\n  select(id,condit,expMode2,tr,x,y) \n\n#saveRDS(ds,here::here(\"data/e1_md_11-06-23.rds\"))\n\n\n\nd &lt;- readRDS(here::here(\"data/e2_08-21-23.rds\"))\nlevels(d$condit)\n\n[1] \"Constant\" \"Varied\"  \n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\nds &lt;- d %&gt;% filter(expMode2 %in% c(\"Train\",\"Test\")) |&gt; \n  filter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) |&gt;   \n  group_by(id,condit,expMode2) |&gt; \n  mutate(input=bandInt,x=bandInt, y=vx, tr= row_number()) |&gt;\n  select(id,condit,expMode2,tr,x,y) \n\n#saveRDS(ds,here::here(\"data/e2_md_02-23-24.rds\"))\n\n\nd &lt;- readRDS(here::here(\"data/e3_08-21-23.rds\"))\nlevels(d$condit)\n\n[1] \"Constant\" \"Varied\"  \n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\nds &lt;- d %&gt;% filter(expMode2 %in% c(\"Train\",\"Test\")) |&gt; \n  filter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) |&gt;   \n  group_by(id,condit,expMode2) |&gt; \n  mutate(input=bandInt,x=bandInt, y=vx, tr= row_number()) |&gt;\n  select(id,condit,expMode2,tr,x,y) \n\n\n#saveRDS(ds,here::here(\"data/e3_md_02-23-24.rds\"))\n\n\nd %&gt;% select_if(is.factor) %&gt;% select(-sbjCode,-id) %&gt;% map(levels)\n\n$condit\n[1] \"Constant\" \"Varied\"  \n\n$fb\n[1] \"Continuous\" \"Ordinal\"   \n\n$tOrder\n[1] \"testFirst\"  \"trainFirst\"\n\n$expMode2\n[1] \"Train\"    \"Train-Nf\" \"Test\"     \"Test-Fb\" \n\n$band\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n$vb\n[1] \"100-300\"   \"350-550\"   \"600-800\"   \"800-1000\"  \"1000-1200\" \"1200-1400\"\n\n$bandOrder\n[1] \"Original\" \"Reverse\" \n\n$bandType\n[1] \"Trained\"       \"Extrapolation\"\n\n$vxCat\n [1] \"0\"      \"100\"    \"btw1\"   \"350\"    \"btw2\"   \"600\"    \"800\"    \"1000\"  \n [9] \"1200\"   \"&gt;=1401\"\n\n$prev\n[1] \"100\"  \"350\"  \"600\"  \"800\"  \"1000\" \"1200\"\n\n$bandSeq\n [1] \"100-&gt;100\"   \"350-&gt;100\"   \"600-&gt;100\"   \"100-&gt;350\"   \"350-&gt;350\"  \n [6] \"600-&gt;350\"   \"100-&gt;600\"   \"350-&gt;600\"   \"600-&gt;600\"   \"800-&gt;800\"  \n[11] \"1000-&gt;800\"  \"1200-&gt;800\"  \"800-&gt;1000\"  \"1000-&gt;1000\" \"1200-&gt;1000\"\n[16] \"800-&gt;1200\"  \"1000-&gt;1200\" \"1200-&gt;1200\"\n\n$lowBound\n[1] \"100\"  \"350\"  \"600\"  \"800\"  \"1000\" \"1200\"\n\n$stage\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\"\n\n$expMode\n[1] \"train\"         \"train-Nf\"      \"test-Nf\"       \"test-train-nf\"\n[5] \"test-feedback\"\n\n$trainStage\n[1] \"Beginning\" \"Middle\"    \"End\"       \"Test\"     \n\n$feedback\n[1] \"0\" \"1\"\n\nd %&gt;% select_if(is.factor) %&gt;% select(-sbjCode,-id) %&gt;% droplevels %&gt;% map(levels)\n\n$condit\n[1] \"Constant\" \"Varied\"  \n\n$fb\n[1] \"Ordinal\"\n\n$tOrder\n[1] \"testFirst\"  \"trainFirst\"\n\n$expMode2\n[1] \"Train\"    \"Train-Nf\" \"Test\"     \"Test-Fb\" \n\n$band\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n$vb\n[1] \"100-300\"   \"350-550\"   \"600-800\"   \"800-1000\"  \"1000-1200\" \"1200-1400\"\n\n$bandOrder\n[1] \"Original\" \"Reverse\" \n\n$bandType\n[1] \"Trained\"       \"Extrapolation\"\n\n$vxCat\n [1] \"0\"      \"100\"    \"btw1\"   \"350\"    \"btw2\"   \"600\"    \"800\"    \"1000\"  \n [9] \"1200\"   \"&gt;=1401\"\n\n$prev\n[1] \"100\"  \"350\"  \"600\"  \"800\"  \"1000\" \"1200\"\n\n$bandSeq\n [1] \"100-&gt;100\"   \"350-&gt;100\"   \"600-&gt;100\"   \"100-&gt;350\"   \"350-&gt;350\"  \n [6] \"600-&gt;350\"   \"100-&gt;600\"   \"350-&gt;600\"   \"600-&gt;600\"   \"800-&gt;800\"  \n[11] \"1000-&gt;800\"  \"1200-&gt;800\"  \"800-&gt;1000\"  \"1000-&gt;1000\" \"1200-&gt;1000\"\n[16] \"800-&gt;1200\"  \"1000-&gt;1200\" \"1200-&gt;1200\"\n\n$lowBound\n[1] \"100\"  \"350\"  \"600\"  \"800\"  \"1000\" \"1200\"\n\n$stage\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\"\n\n$expMode\n[1] \"train\"         \"train-Nf\"      \"test-Nf\"       \"test-train-nf\"\n[5] \"test-feedback\"\n\n$trainStage\n[1] \"Beginning\" \"Middle\"    \"End\"       \"Test\"     \n\n$feedback\n[1] \"0\" \"1\"\n\n\n\nd %&gt;% select_if(is.numeric) |&gt; colnames()\n\n [1] \"trial\"        \"bandInt\"      \"vx\"           \"vxC\"          \"dist\"        \n [6] \"sdist\"        \"vxb\"          \"vxi\"          \"vy\"           \"launchX\"     \n[11] \"launchY\"      \"highBound\"    \"nGoodTrial\"   \"nTrain\"       \"nTestNf\"     \n[16] \"nInt\"         \"nTestF\"       \"nTotal\"       \"gt.train\"     \"gt.bandStage\"\n[21] \"gt.stage\"     \"lastTrain\"    \"lastTrial\"   \n\n\n\nd %&gt;%\n  distinct(id, condit, fb, bandOrder, tOrder) %&gt;%\n  group_by(condit, fb, bandOrder, tOrder) %&gt;%\n  summarise(n = n()) %&gt;%\n  kable()\n\n\n\n\ncondit\nfb\nbandOrder\ntOrder\nn\n\n\n\nConstant\nOrdinal\nOriginal\ntestFirst\n51\n\n\nConstant\nOrdinal\nReverse\ntestFirst\n31\n\n\nConstant\nOrdinal\nReverse\ntrainFirst\n28\n\n\nVaried\nOrdinal\nOriginal\ntestFirst\n39\n\n\nVaried\nOrdinal\nReverse\ntestFirst\n28\n\n\nVaried\nOrdinal\nReverse\ntrainFirst\n18\n\n\n\n\n\n\n\n# Average trials per subject by condition  \nd %&gt;%\n  group_by(condit, fb, bandOrder, tOrder, id) %&gt;%\n  summarise(n = n()) %&gt;%\n  group_by(condit, fb, bandOrder, tOrder) %&gt;%\n  summarise(mean_trials = mean(n)) %&gt;%\n  kable()\n\n\n\n\ncondit\nfb\nbandOrder\ntOrder\nmean_trials\n\n\n\nConstant\nOrdinal\nOriginal\ntestFirst\n198.5294\n\n\nConstant\nOrdinal\nReverse\ntestFirst\n198.6452\n\n\nConstant\nOrdinal\nReverse\ntrainFirst\n198.1429\n\n\nVaried\nOrdinal\nOriginal\ntestFirst\n199.9744\n\n\nVaried\nOrdinal\nReverse\ntestFirst\n197.5714\n\n\nVaried\nOrdinal\nReverse\ntrainFirst\n196.2222\n\n\n\n\n\nd %&gt;%\n  group_by(condit, fb, bandOrder, tOrder, id, expMode) %&gt;% \n  summarise(n = n()) %&gt;%\n  pivot_wider(names_from = expMode, values_from = n, names_prefix = \"n_\") %&gt;%\n  group_by(condit, fb, bandOrder, tOrder) %&gt;%\n  summarise(across(starts_with(\"n_\"), ~mean(., na.rm = TRUE))) %&gt;%\n  kable()\n\n\n\n\ncondit\nfb\nbandOrder\ntOrder\nn_train\nn_train-Nf\nn_test-Nf\nn_test-train-nf\nn_test-feedback\n\n\n\nConstant\nOrdinal\nOriginal\ntestFirst\n85.45098\n25.86275\n44.23529\n16.52941\n26.45098\n\n\nConstant\nOrdinal\nReverse\ntestFirst\n85.93548\n25.25806\n43.58065\n17.64516\n26.22581\n\n\nConstant\nOrdinal\nReverse\ntrainFirst\n85.67857\n25.25000\n43.60714\n17.67857\n25.92857\n\n\nVaried\nOrdinal\nOriginal\ntestFirst\n85.28205\n26.43590\n44.10256\n17.56410\n26.58974\n\n\nVaried\nOrdinal\nReverse\ntestFirst\n86.25000\n24.53571\n43.28571\n17.39286\n26.10714\n\n\nVaried\nOrdinal\nReverse\ntrainFirst\n84.88889\n24.77778\n43.05556\n17.55556\n25.94444\n\n\n\n\n\n\n\n# Define column defs function \ncol_defs &lt;- function(data) {\n  \n  cols &lt;- colnames(data)\n  \n  defs &lt;- lapply(cols, function(x) {\n    \n    if(is.factor(data[[x]])) {\n      colDef(sortable = TRUE, filterable = TRUE,minWidth=108) \n    } else {\n      colDef(sortable = TRUE, filterable = FALSE,minWidth=90)\n    }\n    \n  })\n  \n  setNames(defs, cols)\n}\n\nd %&gt;%\n  group_by(id,condit, fb, bandOrder, tOrder, expMode) %&gt;%\n  summarise(n = n()) %&gt;%\n  pivot_wider(names_from = expMode, values_from = n, names_prefix = \"n_\") %&gt;%\n  \n  # Pass original data too \n  reactable(columns = col_defs(.), \n            highlight = TRUE,\n            defaultPageSize = 25)\n\n\n\n\n\n\nd %&gt;% filter(nGoodTrial==1,nTotal&gt;100) %&gt;% ggplot(aes(nTotal)) + geom_histogram() + facet_wrap(~condit)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nd %&gt;% filter(nGoodTrial==1,nTotal&gt;100) %&gt;% ggplot(aes(nTestNf)) + geom_histogram() + facet_wrap(~condit)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nd %&gt;% filter(nGoodTrial==1,nTotal&gt;100) %&gt;% ggplot(aes(nTrain)) + geom_histogram() + facet_wrap(~condit)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Misc/gam_discrim.html",
    "href": "Misc/gam_discrim.html",
    "title": "GAM",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,lme4,emmeans,here,knitr,kableExtra,gt,mgcv,data.table)\nsource(here::here(\"Functions\", \"packages.R\"))\n\n\ntest &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) |&gt; filter(expMode2 == \"Test\")  |&gt;\n  select(id,condit,expMode, trial, bandInt,vb,vx,dist,vy) |&gt;   mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0))  |&gt; data.table()\n\ntrain &lt;-readRDS(here(\"data/e1_08-21-23.rds\")) |&gt; filter(expMode2 == \"Train\" | expMode==\"train-Nf\")  |&gt;\n  select(id,condit,expMode,trial,bandInt,vb,vx,dist,vy) |&gt; mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0))  |&gt; data.table()\n\n\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt) %&gt;%\n  summarise(vx=mean(vx)) %&gt;% data.table()\n\ntrainAvg &lt;- train %&gt;% group_by(id, condit, vb, bandInt) %&gt;%\n  summarise(vx=mean(vx)) %&gt;% data.table()\n\n\n\n\n\nest_gam &lt;- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data &lt;- copy(train_data)\n  test_data &lt;- copy(test_data)\n\n  #--- train a model ---#  \n  #trained_model &lt;- gam(vx ~ condit + s(bandInt, k = k)+ s(id, bandInt,bs='re'), data = train_data)\n  trained_model &lt;- gam(dist~  s(bandInt,k=2) + s(vy,k=k) + s(id,bs=\"re\"), data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data &lt;- \n    rbind(train_data, test_data) %&gt;%\n    .[, num_knots := k]\n\n  return(return_data)\n}\n\n\n\n\nsim_results &lt;- \n  lapply(1:6, function(x) est_gam(x, train, test)) %&gt;%\n  rbindlist()\n\nsim_results &lt;- \n  lapply(c(1,2,3,5,10,20,50), function(x) est_gam(x, train, test)) %&gt;%\n  rbindlist()\n\n# sim_results &lt;- \n#   lapply(1:6, function(x) est_gam(x, trainAvg, testAvg)) %&gt;%\n#   rbindlist()\n\nggplot(sim_results) +\n  geom_point(data = train, aes(y = dist, x = bandInt), color = \"grey\") +\n  geom_line(aes(y = y_hat, x = vy, color = factor(num_knots))) +\n  theme_bw()\n\nggplot(sim_results[num_knots  %in% 1:6, ],aes(fill=vb)) + \n   stat_summary(data=train,geom = \"bar\", position=position_dodge(), fun = mean) +\n   geom_line(aes(y = y_hat, x = bandInt, color = factor(num_knots))) \n\n\nsummary &lt;- \n  sim_results[, .(mse = mean((y_hat - dist)^2)), by = .(num_knots, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_knots, color = type)) +\n  geom_point(aes(y = mse, x = num_knots, color = type)) +\n  xlab(\"Number of knots\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\nCode#https://fromthebottomoftheheap.net/2021/02/02/random-effects-in-gams/\nmod_gam1 = gam(vx ~ 1 + (bandInt, bs = \"cr\",k=4) + s(id, bandInt,bs='re'), data = test)\n\n\ngy = gam(vx ~ s(vy,k=2), data = test)\nsummary(gy)\ncoef(gy)\ngam.check(gy)\n\ngy = gam(vx ~ s(vy,k=6), data = test)\nsummary(gy)\ncoef(gy)\ngam.check(gy)\n\ngy = gam(vx ~ s(bandInt,k=5) + s(vy), data = test)\nsummary(gy)\ncoef(gy)\ngam.check(gy)\n\n\ngy = gam(vx ~ s(bandInt,k=2) + s(vy) + s(id,bs=\"re\"), data = test)\nsummary(gy)\ncoef(gy)\ngam.check(gy)\n\n\n\n\ng1=gam(vx ~ s(bandInt,k=1),data=train)\nsummary(g1)\ng1$coefficients\n\n\ng2=gam(vx ~ s(bandInt,k=2),data=trainAvg)\nsummary(g2)\ng2$coefficients\n\ng3=gam(vx ~ s(bandInt,k=4),data=train)\nsummary(g3)\ng3$coefficients\n\n\nk1=bam(condit_dummy ~ s(vx,k=12),data=train,family=binomial())\nsummary(k1)\n\nk2=bam(bandInt ~ s(vx,k=5),data=train)\nsummary(k2)\n\ng2=gam(vx ~ s(bandInt,k=6),data=trainAvg)\n\n\n\nid1 &lt;- test |&gt; filter(id==1)\n\ng2=gam(vx ~ 0 + s(bandInt,k=6),data=id1)\nsummary(g2)\ng2$coefficients\nkmeans(id1$vx,6)\n\nkn=knnreg(vx ~ bandInt,data=test)\nsummary(kn)\n\ng4 &lt;- gam(vx ~ bandInt + s(id,bs=\"re\"),data=test)\nsummary(g4)\ng4$coefficients\n\ng5 &lt;- gam(vx ~ condit+bandInt + s(id,bs=\"re\",k=4),data=test)\nsummary(g5)\ng5$coefficients\ncoef(g5)\n\ng6 &lt;- gam(vx ~ condit+bandInt + s(id,bs=\"re\")+ s(id,bandInt,bs=\"re\",k=6),data=test,method = 'REML')\nsummary(g6)\ng6$coefficients\n\n\n\n\nhits = test |&gt; filter(dist==0)\n\nhits |&gt; ggplot(aes(x=bandInt,y=vx)) + geom_point()\n\nhg1 &lt;- gam(vx ~ 0 + s(bandInt,k=2),data=hits)\nsummary(hg1)\nhg1$coefficients\nplot(hg1)\n\n\n\nCodetestAvg %&gt;% ggplot(aes(x=bandInt,y=vx))+geom_point(color=\"grey\")+geom_line()+theme_bw()\n\ntm &lt;- gam(vx ~s(bandInt,k=3),sp=0,data=testAvg)\nsummary(tm)\n\n\nresults &lt;- tibble(k = integer(), r_sq = numeric(), edf = numeric(), p_value = numeric(), scale = numeric(), deviance = numeric(), null_deviance = numeric())\n\n# Iterate through different k values\nfor (k in 1:6) {\n  temp_result &lt;- tryCatch({\n    # Fit the GAM model\n    tm &lt;- gam(vx ~ s(bandInt, k = k), sp = 0, data = testAvg)\n    model_sum &lt;- summary(tm)\n    \n    # Extract additional metrics from the model summary and the model itself\n    edf &lt;- model_sum$s.table[1]\n    p_value &lt;- model_sum$s.pv[1]\n    scale &lt;- tm$scale\n    deviance &lt;- tm$deviance\n    null_deviance &lt;- tm$null.deviance\n    \n    # Return data frame with results\n    tibble(k = k, r_sq = model_sum$r.sq, edf = edf, p_value = p_value, scale = scale, deviance = deviance, null_deviance = null_deviance)\n  }, error = function(e) {\n    # Print error message\n    cat(paste(\"Skipping k =\", k, \"due to error:\", conditionMessage(e)), \"\\n\")\n    \n    # Return data frame with NA values\n    tibble(k = k, r_sq = NA_real_, edf = NA_real_, p_value = NA_real_, scale = NA_real_, deviance = NA_real_, null_deviance = NA_real_)\n  })\n  \n  # Add results to data frame\n  results &lt;- bind_rows(results, temp_result)\n}\n\nprint(results)\n\n# Plot R-squared\nresults %&gt;%\n  ggplot(aes(x = k, y = r_sq)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"k\", y = \"R-squared\") +\n  theme_minimal()\n\n# Plot deviance\nresults %&gt;%\n  ggplot(aes(x = k, y = deviance)) +\n  geom_line(color = \"red\") +\n  labs(x = \"k\", y = \"Deviance\") +\n  theme_minimal()\n\n\n\nCode# Iterate through different k values\nfor (k in 1:6) {\n  for (group in unique(testAvg$condit)) {\n    \n    temp_result &lt;- tryCatch({\n      # Filter data by group\n      group_data &lt;- testAvg %&gt;% filter(condit == group)\n      \n      # Fit the GAM model with interaction and more flexible splines\n      tm &lt;- gam(vx ~ s(bandInt, k = k, bs = \"cr\"), data = group_data)\n      model_sum &lt;- summary(tm)\n      \n      # Extract additional metrics from the model summary and the model itself\n      edf &lt;- model_sum$s.table[1]\n      p_value &lt;- model_sum$s.pv[1]\n      scale &lt;- tm$scale\n      deviance &lt;- tm$deviance\n      null_deviance &lt;- tm$null.deviance\n      threshold &lt;- NA_real_\n      \n      # Attempt to calculate discrimination threshold\n      tryCatch({\n        # Value of bandInt where the derivative of the fitted spline is highest\n        bandInt_values &lt;- seq(min(group_data$bandInt), max(group_data$bandInt), length.out = 1000)\n        fitted_spline &lt;- predict(tm, newdata = data.frame(bandInt = bandInt_values, condit_num = ifelse(group == \"Varied\", 1, 0)), type = \"terms\")\n        fitted_derivative &lt;- diff(fitted_spline[,1]) / diff(bandInt_values)\n        threshold &lt;- bandInt_values[which.max(fitted_derivative)]\n      }, error = function(e) {\n        cat(paste(\"Failed to calculate threshold for k =\", k, \"in group\", group, \"due to error:\", conditionMessage(e)), \"\\n\")\n      })\n      \n      # Return data frame with results\n      tibble(group = group, k = k, r_sq = model_sum$r.sq, edf = edf, p_value = p_value, scale = scale, deviance = deviance, null_deviance = null_deviance, threshold = threshold)\n    }, error = function(e) {\n      # Print error message\n      cat(paste(\"Skipping k =\", k, \"for group\", group, \"due to error:\", conditionMessage(e)), \"\\n\")\n      \n      # Return data frame with NA values\n      tibble(group = group, k = k, r_sq = NA_real_, edf = NA_real_, p_value = NA_real_, scale = NA_real_, deviance = NA_real_, null_deviance = NA_real_, threshold = NA_real_)\n    })\n    \n    # Add results to data frame\n    results &lt;- bind_rows(results, temp_result)\n  }\n}\nprint(results)\n\n\n\nCodeest_gam &lt;- function(k, train_data, test_data, group) \n{\n  # Filter by group\n  train_data_group &lt;- train_data %&gt;% filter(condit == group)\n  test_data_group &lt;- test_data %&gt;% filter(condit == group)\n \n  # Train a model  \n  trained_model &lt;- gam(vx ~ s(bandInt, k = k, bs = \"tp\"), data = train_data_group)\n \n  # Print out the number of rows in train_data_group\n  print(paste(\"Number of rows in train_data_group: \", nrow(train_data_group)))\n  \n  # Predict y for the train datasets\n  predicted_values_train &lt;- predict(trained_model, newdata = train_data_group)\n  print(paste(\"Number of predicted values for train data: \", length(predicted_values_train)))\n\n  # Add the predicted values as a new column\n  train_data_group &lt;- train_data_group %&gt;% mutate(vx_hat = predicted_values_train, type = \"Train\")\n \n  # Repeat the same for the test datasets\n  predicted_values_test &lt;- predict(trained_model, newdata = test_data_group)\n  print(paste(\"Number of predicted values for test data: \", length(predicted_values_test)))\n\n  test_data_group &lt;- test_data_group %&gt;% mutate(vx_hat = predicted_values_test, type = \"Test\")\n \n  # Combine before returning\n  return_data &lt;- bind_rows(train_data_group, test_data_group) %&gt;% \n                 mutate(num_knots = k, group = group)\n \n  return(return_data)\n}\n\n# define k-values\nk_values &lt;- 1:5\n\n# Split the data into training and testing datasets\nset.seed(123)\ndata_split &lt;- testAvg %&gt;% \n  resample_partition(c(Train = 0.75, Test = 0.25))\n\n# Extract the data frames from the resample objects\ntrain_data &lt;- as.data.frame(data_split$Train)\ntest_data &lt;- as.data.frame(data_split$Test)\n\n# Use a nested loop to apply the est_gam function over all combinations of groups and k_values\nresults &lt;- map_df(unique(testAvg$condit), ~{\n  group &lt;- .\n  map_df(k_values, ~{\n    est_gam(.x, train_data %&gt;% filter(condit == group), \n            test_data %&gt;% filter(condit == group), group)\n  })\n})\n\n\n\nCode# Create an empty data frame to store results\nresults &lt;- tibble(group = character(), k = integer(), r_sq = numeric(), edf = numeric(), p_value = numeric(), scale = numeric(), deviance = numeric(), null_deviance = numeric())\n\n# Iterate through different k values\nfor (k in 1:6) {\n  # Iterate through different conditions\n  for (condit in unique(testAvg$condit)) {\n    temp_result &lt;- tryCatch({\n      # Subset the data for the current condition\n      data_subset &lt;- testAvg[testAvg$condit == condit, ]\n      \n      # Fit the GAM model\n      tm &lt;- gam(vx ~ s(bandInt, k = k), sp = 0, data = data_subset)\n      model_sum &lt;- summary(tm)\n      \n      # Extract additional metrics from the model summary and the model itself\n      edf &lt;- model_sum$s.table[1]\n      p_value &lt;- model_sum$s.pv[1]\n      scale &lt;- tm$scale\n      deviance &lt;- tm$deviance\n      null_deviance &lt;- tm$null.deviance\n      \n      # Return data frame with results\n      tibble(group = condit, k = k, r_sq = model_sum$r.sq, edf = edf, p_value = p_value, scale = scale, deviance = deviance, null_deviance = null_deviance)\n    }, error = function(e) {\n      # Print error message\n      cat(paste(\"Skipping k =\", k, \"in group\", condit, \"due to error:\", conditionMessage(e)), \"\\n\")\n      \n      # Return data frame with NA values\n      tibble(group = condit, k = k, r_sq = NA_real_, edf = NA_real_, p_value = NA_real_, scale = NA_real_, deviance = NA_real_, null_deviance = NA_real_)\n    })\n    \n    # Add results to data frame\n    results &lt;- bind_rows(results, temp_result)\n  }\n}\n\nprint(results)\n\n# Plot R-squared\nresults %&gt;%\n  ggplot(aes(x = k, y = r_sq, color = group)) +\n  geom_line() +\n  labs(x = \"k\", y = \"R-squared\") +\n  theme_minimal()\n\n# Plot deviance\nresults %&gt;%\n  ggplot(aes(x = k, y = deviance, color = group)) +\n  geom_line() +\n  labs(x = \"k\", y = \"Deviance\") +\n  theme_minimal()\n\n# Add additional plots as necessary\n\n\n\nCode# Create an empty data frame to store results\nresults &lt;- data.frame(k = numeric(), r_sq = numeric(), deviance_explained = numeric())\n\n# Iterate through different k values\nfor (k in 1:6) {\n  temp_result &lt;- tryCatch({\n    # Fit the GAM model\n    tm &lt;- gam(vx ~ s(bandInt, k = k), sp = 0, data = testAvg)\n    model_sum &lt;- summary(tm)\n    \n    # Return data frame with results\n    data.frame(k = k, r_sq = model_sum$r.sq.adj, deviance_explained = model_sum$dev.expl)\n  }, error = function(e) {\n    # Print error message\n    cat(paste(\"Skipping k =\", k, \"due to error:\", conditionMessage(e)), \"\\n\")\n\n    # Return data frame with NA values\n    data.frame(k = k, r_sq = NA, deviance_explained = NA)\n  })\n\n  # Add results to data frame\n  results &lt;- rbind(results, temp_result)\n}\n\n# Check if we have successfully fitted models and results have more than zero rows \nif(nrow(results) &gt; 0){\n  # No need to set column names here, you've already set them when creating the data frames inside the loop\n\n  # Plotting\n  plot(results$k, results$r_sq, type = \"l\", xlab = \"k\", ylab = \"R-squared\", col = \"blue\")\n  lines(results$k, results$deviance_explained, type = \"l\", col = \"red\") \n  legend(\"topright\", legend = c(\"R-squared\", \"Deviance Explained\"), col = c(\"blue\", \"red\"), lty = 1)\n}\n\n\n\nCodegam_fit &lt;- gam(vx ~ s(bandInt, k = 6, bs = \"cr\"), data = testAvg)\ngam_fit$coefficient\n\nbasis_data &lt;- gam_fit %&gt;%  predict(., type = \"lpmatrix\") %&gt;%\n  data.table() %&gt;% \n  .[, bandInt := testAvg[, bandInt]] %&gt;% \n  melt(id.var = \"bandInt\")\n\n\n\n\ndata.table(\n  variable = unique(basis_data$variable)[-1],\n  coef = gam_fit$coefficient[-1]\n) %&gt;% \n.[basis_data, on = \"variable\"] %&gt;% \n.[, .(y_no_int = sum(coef * value)), by = bandInt] %&gt;% \n.[, y_hat := gam_fit$coefficient[1] + vx] %&gt;% \nggplot(data = .) +\n  geom_line(aes(y = vx, x = bandInt, color = \"gam-fitted\")) +\n  geom_line(data = data, aes(y = vx, x = bandInt, color = \"True\")) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"gam-fitted\" = \"red\", \"True\" = \"blue\")\n  ) +\n  ylab(\"y\") +\n  xlab(\"x\") +\n  theme_bw()"
  },
  {
    "objectID": "Misc/indv_diff.html",
    "href": "Misc/indv_diff.html",
    "title": "Individual Differenes",
    "section": "",
    "text": "new_data_grid=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"condit\",\"bandInt\")]))) |&gt; \n  dplyr::arrange(id,bandInt) |&gt; \n  mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0)) \n\nindv_coefs &lt;- coef(e1_vxBMM)$id |&gt; \n  as_tibble(rownames=\"id\") |&gt; \n  select(id, starts_with(\"Esti\")) |&gt;\n  left_join(e1Sbjs, by=join_by(id))\n\nindv_coefs_preds &lt;- new_data_grid |&gt; left_join(indv_coefs,by=join_by(id,condit,condit_dummy)) |&gt;\n  mutate(\n    estimate = (Estimate.Intercept) + \n      (bandInt*(Estimate.bandInt)) + \n      (condit_dummy*Estimate.conditVaried) +\n      (bandInt * condit_dummy) * `Estimate.conditVaried:bandInt`)\n  \n# indv_linpred &lt;- e1_vxBMM |&gt; linpred_draws(new_data_grid) |&gt; \n#   summarize(across(.linpred, lst(mean, sd, median), .names = \"{.fn}\"))\n# \n# indv_epred &lt;- e1_vxBMM |&gt; epred_draws(new_data_grid) |&gt;\n#   summarize(across(.epred, lst(mean, sd, median), .names = \"{.fn}\"))\n# \n# indv_pred &lt;- e1_vxBMM |&gt; predicted_draws(new_data_grid) |&gt;\n#    summarize(across(.prediction, lst(mean, sd, median), .names = \"{.fn}\"))\n\n# fitted(e1_vxBMM,re_formula=NA)\n# fitted(e1_vxBMM,re_formula=NULL) # matches epred \n\n\n# |&gt; \n#   group_by(condit) |&gt; \n#   mutate(rank = rank(desc(Estimate.bandInt)),\n#          intErrorRank=rank((Est.Error.Intercept)),\n#          bandErrorRank=rank((Est.Error.bandInt)),\n#          nCond = n()) |&gt; arrange(id) |&gt; ungroup()\n\n\n\n\nfixed_effects &lt;- e1_vxBMM |&gt; \n  spread_draws(`^b_.*`,regex=TRUE) |&gt; arrange(.chain,.draw,.iteration)\n\n\nrandom_effects &lt;- e1_vxBMM |&gt; \n  gather_draws(`^r_id.*$`, regex = TRUE, ndraws = 1000) |&gt; \n  separate(.variable, into = c(\"effect\", \"id\", \"term\"), sep = \"\\\\[|,|\\\\]\") |&gt; \n  mutate(id = factor(id,levels=levels(test$id))) |&gt; \n  pivot_wider(names_from = term, values_from = .value) |&gt; arrange(id,.chain,.draw,.iteration)\n\n\n indvDraws &lt;- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |&gt; \n  rename(bandInt_RF = bandInt,RF_Intercept=Intercept) |&gt;\n  right_join(new_data_grid, by = join_by(\"id\")) |&gt; \n  mutate(\n    Slope = bandInt_RF+b_bandInt,\n    Intercept= RF_Intercept + b_Intercept,\n    estimate = (b_Intercept + RF_Intercept) + \n      (bandInt*(b_bandInt+bandInt_RF)) + \n      (condit_dummy*b_conditVaried)+\n      (bandInt * condit_dummy) * `b_conditVaried:bandInt`,\n    SlopeInt=bandInt_RF+b_bandInt + ((`b_conditVaried:bandInt` + bandInt_RF)*condit_dummy),\n    SlopeInt2=bandInt_RF+b_bandInt + ((`b_conditVaried:bandInt`)*condit_dummy)\n  ) \n\n  indvSlopes &lt;- indvDraws |&gt; group_by(id) |&gt; median_qi(Slope,SlopeInt,SlopeInt2, Intercept,b_Intercept,b_bandInt) |&gt;\n  left_join(e1Sbjs, by=join_by(id)) |&gt; group_by(condit) |&gt;\n    select(id,condit,condit_dummy,Intercept,b_Intercept,Slope,SlopeInt,SlopeInt2,b_bandInt, n) |&gt;\n  mutate(rankSlope=rank(Slope)) |&gt; ungroup()  #|&gt; arrange(rankSlope)   |&gt; ungroup()\n\n\nfull_tab &lt;- testAvg |&gt; select(id,condit,vx,vb,bandInt) |&gt; \n  left_join(indvDraws |&gt; group_by(id,bandInt) |&gt; median_qi(estimate,Intercept,Slope), by=join_by(id,bandInt)) |&gt;\n  mutate(resid=vx-estimate) |&gt; relocate(vx,resid,.before=estimate) |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x,2))) \n\n\nindvResid &lt;- full_tab |&gt; group_by(id,condit,bandInt) |&gt; summarize(across(where(is.numeric), \\(x) mean(x))) |&gt;\n  group_by(id,condit) |&gt; mutate(meanResid=mean(resid)) |&gt;\n  arrange(meanResid)\n\nfull_tab |&gt; group_by(condit) |&gt; summarise(mean(resid),sd(resid))\n\n# A tibble: 2 × 3\n  condit   `mean(resid)` `sd(resid)`\n  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n1 Constant         12.6         126.\n2 Varied            7.28        110.\n\nfull_tab |&gt; group_by(condit,id) |&gt; summarise(m=mean(resid),s=sd(resid)) |&gt; summarise(m=mean(m),s=sd(s)) \n\n# A tibble: 2 × 3\n  condit       m     s\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Constant 12.6   64.3\n2 Varied    7.28  49.5\n\nfull_tab |&gt; DT::datatable(options = list(pageLength=10))\n\n\n\n\n\n\n  indvSlopes |&gt; group_by(condit) |&gt; reframe(enframe(quantile(Slope, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"Slope\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=Slope,names_prefix=\"Q_\") |&gt;\n  group_by(condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kbl()\n\n\n\n\ncondit\nQ_0%_mean\nQ_25%_mean\nQ_50%_mean\nQ_75%_mean\nQ_100%_mean\n\n\n\nConstant\n-0.1034525\n0.4745830\n0.6895313\n0.933177\n1.397654\n\n\nVaried\n-0.0606000\n0.4078445\n0.7244277\n1.041584\n1.434908\n\n\n\n\n\n  indvSlopes |&gt; group_by(condit)  |&gt; reframe(enframe(quantile(SlopeInt, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"SlopeInt\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=SlopeInt,names_prefix=\"Q_\") |&gt;\n  group_by(condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kbl()\n\n\n\n\ncondit\nQ_0%_mean\nQ_25%_mean\nQ_50%_mean\nQ_75%_mean\nQ_100%_mean\n\n\n\nConstant\n-0.1034525\n0.4745830\n0.6895313\n0.933177\n1.397654\n\n\nVaried\n-0.9698544\n-0.0388319\n0.5973872\n1.235211\n2.022140\n\n\n\n\n\n  indvSlopes |&gt; group_by(condit)  |&gt; reframe(enframe(quantile(SlopeInt2, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"SlopeInt2\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=SlopeInt2,names_prefix=\"Q_\") |&gt;\n  group_by(condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kbl()\n\n\n\n\ncondit\nQ_0%_mean\nQ_25%_mean\nQ_50%_mean\nQ_75%_mean\nQ_100%_mean\n\n\n\nConstant\n-0.1034525\n0.4745830\n0.6895313\n0.9331770\n1.397654\n\n\nVaried\n-0.1984330\n0.2661345\n0.5845202\n0.9019325\n1.295316\n\n\n\n\n\n  ggplot(indvSlopes, aes(x = Slope, color = condit)) + \n  geom_density() \n\n\n\n\n\n\n    ggplot(indvSlopes, aes(x = SlopeInt2, color = condit)) + \n  geom_density() \n\n\n\n\n\n\n\n\ne1_vxBMM %&gt;% as_draws_df(variable = c('^b_', 'sigma'), regex = TRUE) |&gt;\n  select(starts_with(\"b_\"),sigma) |&gt; \n  pivot_longer(\n    everything(),\n    names_to = 'key',\n    values_to = 'value') %&gt;% \n  ggplot(aes(x = value, fill = key)) +\n  stat_halfeye(\n    # plot the mode\n    point_interval = mode_hdi,\n    # plot HDIs be at both the 95% levels\n    .width = c(.95),\n    outline_bars = TRUE,\n    normalize = 'panels',\n    show.legend = FALSE\n  ) +\n  facet_wrap(~key, scales = 'free', ncol = 4) +\n  ggtitle('Posterior distributions')\n\n\n\n\n\n\n indvDraws |&gt; filter(id %in% 1:10) |&gt; group_by(id,condit,.draw) |&gt; summarise(Slope=mean(Slope)) |&gt; \n   ggplot(aes(x=Slope,col=condit))+geom_density() + facet_wrap(~id)\n\n\n\n\n\n\n  indvDraws  |&gt; group_by(id,condit,.draw) |&gt; summarise(Slope=mean(Slope)) |&gt; \n   ggplot(aes(x=Slope,col=condit,fill=condit))+stat_halfeye()\n\n\n\n\n\n\nindvDraws |&gt; group_by(condit,.draw) |&gt; summarise(Slope=mean(Slope)) |&gt; \n   ggplot(aes(x=Slope,col=condit,fill=condit))+stat_halfeye()\n\n\n\n\n\n\n indvDraws |&gt; group_by(id,condit) |&gt; summarise(Slope=mean(Slope)) |&gt; \n   ggplot(aes(x=Slope,col=condit,fill=condit))+geom_density()\n\n\n\n\n\n\n  indvDraws |&gt; group_by(id,condit,.draw) |&gt; summarise(SlopeInt2=mean(SlopeInt2)) |&gt; \n   ggplot(aes(x=SlopeInt2,col=condit,fill=condit))+geom_density()\n\n\n\n\n\n\n  # pp_check(e1_vxBMM,\n  #        type = 'stat_grouped',\n  #        ndraws = 200,\n  #        group = 'id',\n  #        stat = 'mean')\n\n\n  n_sbj = 3\n  \n  \n{indvDraws |&gt; filter(condit==\"Varied\")  |&gt; \n   indv_model_plot(indvSlopes |&gt; filter(condit==\"Varied\"), \n                   testAvg |&gt; filter(condit==\"Varied\"),slopeVar=Slope, rank_variable=Slope,n_sbj=n_sbj,\"max\") + \n     ggtitle(\"Slope = bandInt_RF+b_bandInt\") } /\n \n{indvDraws |&gt; filter(condit==\"Varied\")  |&gt; \n  indv_model_plot(indvSlopes |&gt; filter(condit==\"Varied\"), \n                  testAvg |&gt; filter(condit==\"Varied\"),slopeVar=SlopeInt, rank_variable=Slope,n_sbj=n_sbj,\"max\")  + \n     ggtitle(\"SlopeInt=bandInt_RF+b_bandInt + ((`b_conditVaried:bandInt` + bandInt_RF)*condit_dummy)\") } /\n\nindvDraws |&gt; filter(condit==\"Varied\")  |&gt; \n  indv_model_plot(indvSlopes |&gt; filter(condit==\"Varied\"), \n                  testAvg |&gt; filter(condit==\"Varied\"),slopeVar=SlopeInt2, rank_variable=Slope,n_sbj=n_sbj,\"max\") + \n     ggtitle(\"SlopeInt2=bandInt_RF+b_bandInt + ((`b_conditVaried:bandInt` + b_bandInt)*condit_dummy)\") \n\n\n\n\n\n\n{indvDraws |&gt; filter(condit==\"Varied\")  |&gt; \n   indv_model_plot(indvSlopes |&gt; filter(condit==\"Varied\"), \n                   test |&gt; filter(condit==\"Varied\"),slopeVar=Slope, rank_variable=Slope,n_sbj=n_sbj,\"max\") + \n     ggtitle(\"Slope = bandInt_RF+b_bandInt\") } /\n \n{indvDraws |&gt; filter(condit==\"Varied\")  |&gt; \n  indv_model_plot(indvSlopes |&gt; filter(condit==\"Varied\"), \n                  test |&gt; filter(condit==\"Varied\"),slopeVar=SlopeInt, rank_variable=Slope,n_sbj=n_sbj,\"max\")  + \n     ggtitle(\"SlopeInt=bandInt_RF+b_bandInt + ((`b_conditVaried:bandInt` + bandInt_RF)*condit_dummy)\") } /\n\nindvDraws |&gt; filter(condit==\"Varied\")  |&gt; \n  indv_model_plot(indvSlopes |&gt; filter(condit==\"Varied\"), \n                  test |&gt; filter(condit==\"Varied\"),slopeVar=SlopeInt2, rank_variable=Slope,n_sbj=n_sbj,\"max\") + \n     ggtitle(\"SlopeInt2=bandInt_RF+b_bandInt + ((`b_conditVaried:bandInt` + b_bandInt)*condit_dummy)\") \n\n\n\n\n\n\n\n\n{indvDraws  |&gt; indv_model_plot(indvSlopes, test,slopeVar=Slope, rank_variable=Slope,n_sbj=n_sbj,\"max\") + \n     ggtitle(\"Slope = bandInt_RF+b_bandInt\") } /\n {ggplot(indvSlopes, aes(x = Slope, color = condit)) + geom_density() } /\n gridExtra::tableGrob( indvSlopes |&gt; group_by(condit) |&gt; summarise(Mean=mean(Slope),Sd=sd(Slope)) ) + \n  #gt_temp(indvSlopes |&gt; group_by(condit) |&gt; summarise(Mean=mean(Slope),Sd=sd(Slope)) ) +\n  plot_layout(heights=unit(c(16,6,3),c('cm'))) + plot_annotation(\"vx ~ condit * bandInt + (1 + bandInt | id) \")\n\n\n\n\n\n\n\nBand Only Model\n\ne1_testVx_grRF8 &lt;- readRDS(here::here(\"data/model_cache/e1_testVx_grRF8.rds\"))\n# e1_testVx_grRF8$formula\n# vx ~ bandInt + (1 + bandInt || gr(id, by = condit)) \n\n\nnew_data_grid=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"bandInt\")]))) |&gt; \n  dplyr::arrange(id,bandInt) \n\n\nfixed_effects &lt;- e1_testVx_grRF8 |&gt; \n  spread_draws(`^b_.*`,regex=TRUE) |&gt; arrange(.chain,.draw,.iteration)\n\n\nrandom_effects &lt;- e1_testVx_grRF8 |&gt; \n  gather_draws(`^r_id.*$`, regex = TRUE, ndraws = 1000) |&gt; \n  separate(.variable, into = c(\"effect\", \"id\", \"term\"), sep = \"\\\\[|,|\\\\]\") |&gt; \n  mutate(id = factor(id,levels=levels(test$id))) |&gt; \n  pivot_wider(names_from = term, values_from = .value) |&gt; arrange(id,.chain,.draw,.iteration)\n\n\nindvDraws &lt;- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |&gt; \n  rename(bandInt_RF = bandInt,RF_Intercept=Intercept) %&gt;%\n  right_join(e1Sbjs,., by=join_by(id)) |&gt;\n  right_join(new_data_grid, by = join_by(\"id\")) |&gt; \n  mutate(\n    Slope = bandInt_RF+b_bandInt,\n    Intercept= RF_Intercept + b_Intercept,\n    estimate = (b_Intercept + RF_Intercept) + (bandInt*(b_bandInt+bandInt_RF)),\n  ) \n\nindvSlopes &lt;- indvDraws |&gt; group_by(id) |&gt; median_qi(RF_Intercept,b_Intercept,Intercept,bandInt_RF,b_bandInt,Slope) |&gt;\n  left_join(e1Sbjs, by=join_by(id)) |&gt; group_by(condit) |&gt;\n    select(id,condit,RF_Intercept,b_Intercept,Intercept,bandInt_RF,b_bandInt,Slope, n) |&gt;\n  ungroup() |&gt;\n  mutate(rankSlope=rank(Slope), \n         Slope2 = ((coef(e1_testVx_grRF8)$id |&gt; as_tibble(rownames=\"id\") ) |&gt; pull(Estimate.bandInt)) )\n \n\n{ indvDraws |&gt; indv_model_plot(indvSlopes ,test, slopeVar=Slope, rank_variable=Slope,n_sbj=n_sbj,\"max\") + \n     ggtitle(\"Slope = bandInt_RF+b_bandInt\") } /\n {ggplot(indvSlopes, aes(x = Slope, color = condit)) + geom_density() } /\n gridExtra::tableGrob( indvSlopes |&gt; group_by(condit) |&gt; summarise(Mean=mean(Slope),Sd=sd(Slope)) ) + \n # gt_temp(indvSlopes |&gt; group_by(condit) |&gt; summarise(Mean=mean(Slope),Sd=sd(Slope)) ) +\n  plot_layout(heights=unit(c(16,6,2),c('cm'))) + plot_annotation(\"vx ~ bandInt + (1 + bandInt || gr(id, by = condit))  \")\n\n\n\n\n\n\n\n\nnoInt_tab &lt;- testAvg |&gt; select(id,condit,vx,vb,bandInt) |&gt; \n  left_join(indvDraws |&gt; group_by(id,bandInt) |&gt; median_qi(estimate,Intercept,Slope), by=join_by(id,bandInt)) |&gt;\n  mutate(resid=vx-estimate) |&gt; relocate(vx,resid,.before=estimate) |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x,0))) \n\nnoInt_tab |&gt; group_by(condit) |&gt; summarise(mean(resid),sd(resid))\n\n# A tibble: 2 × 3\n  condit   `mean(resid)` `sd(resid)`\n  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n1 Constant         12.6         127.\n2 Varied            6.58        111.\n\nnoInt_tab |&gt; group_by(condit,id) |&gt; summarise(m=mean(resid),s=sd(resid)) |&gt; summarise(m=mean(m),s=sd(s)) \n\n# A tibble: 2 × 3\n  condit       m     s\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Constant 12.6   64.7\n2 Varied    6.58  49.5\n\nnoInt_tab |&gt; DT::datatable(options = list(pageLength=15))\n\n\n\n\n\n\nrandom_effects |&gt; group_by(id) |&gt; median_hdi()\n\nindv_coefs |&gt; select(id,condit,Estimate.bandInt)\nindv_coefs |&gt; select(id,condit,Estimate.bandInt) |&gt; group_by(condit) |&gt; summarise(mean(Estimate.bandInt))\n\n\nfixef(e1_vxBMM)\nranef(e1_vxBMM)$id[, ,\"bandInt\"] |&gt; as_tibble(rownames=\"id\") |&gt; \n  left_join(e1Sbjs,by=join_by(id)) |&gt;\n  group_by(condit) |&gt; summarise(mean(Estimate))\n\nranef(e1_vxBMM)$id[, ,\"bandInt\"] |&gt; as_tibble(rownames=\"id\") |&gt; \n  left_join(e1Sbjs,by=join_by(id)) |&gt; ggplot(aes(Estimate,color=condit)) + geom_density()\n\n\npopSlope &lt;- tidy(e1_vxBMM, effects = \"fixed\") |&gt; filter(term==\"bandInt\") |&gt; pull(estimate)\npopInteraction &lt;- tidy(e1_vxBMM, effects = \"fixed\") |&gt; filter(term==\"conditVaried:bandInt\") |&gt; pull(estimate)\n\n\ne1Vx &lt;- ranef(e1_vxBMM)$id[, ,\"bandInt\"] |&gt; as_tibble(rownames=\"id\") %&gt;%\n  left_join(e1Sbjs,., by=join_by(id)) |&gt; select(id,condit,condit_dummy,Estimate) |&gt; \n  mutate(pop=popSlope,Slope1=Estimate + pop) |&gt; \n  left_join(indv_coefs |&gt; select(id,Slope2=Estimate.bandInt), join_by(id)) |&gt;\n  mutate(Slope3=Estimate + pop + ((popInteraction + Estimate)*condit_dummy))\n  \ne1Vx |&gt; group_by(condit) |&gt; summarise(Slope_with_int=mean(Slope3), sd_Slope=sd(Slope3))\n\ne1Vx |&gt; group_by(condit) |&gt; reframe(enframe(quantile(Slope3, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"Slope3\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=Slope3,names_prefix=\"Q_\") |&gt;\n  group_by(condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kbl()\n\n{ e1Vx |&gt; ggplot(aes(Slope1,color=condit)) + geom_density() +ggtitle(\"RF_Estimate + Population_Est\") } /\ne1Vx |&gt; ggplot(aes(Slope3,color=condit)) + geom_density() + \n  ggtitle(\"Slope3=Estimate + pop + ((popInteraction + Estimate)*condit_dummy)\")\n\nExamine slopes from different models\n\n# vx ~ bandInt + (1 + bandInt || gr(id, by = condit)) \nbandOnly &lt;- ranef(e1_testVx_grRF8)$id[, ,\"bandInt\"] |&gt; as_tibble(rownames=\"id\") |&gt; \n  left_join(e1Sbjs,by=join_by(id)) |&gt; select(id,condit,condit_dummy,starts_with(\"Estimate\")) |&gt;\n # group_by(condit) |&gt; \n  #summarise(Estimate=mean(Estimate)) |&gt;\n  mutate(pop = tidy(e1_testVx_grRF8, effects = \"fixed\") |&gt; filter(term==\"bandInt\") |&gt; pull(estimate),\n         Slope1=pop+Estimate,\n         Slope2 = (coef(e1_testVx_grRF8)$id |&gt; as_tibble(rownames=\"id\") |&gt; pull(Estimate.bandInt))) \n\n# vx ~ condit + bandInt + (1 + bandInt | id) \nnoInt &lt;- ranef(e1_conditPlusBand_RF)$id[, ,\"bandInt\"] |&gt; as_tibble(rownames=\"id\") |&gt; \n  left_join(e1Sbjs,by=join_by(id)) |&gt; select(id,condit,condit_dummy,starts_with(\"Estimate\")) |&gt;\n # group_by(condit) |&gt; \n  #summarise(Estimate=mean(Estimate)) |&gt;\n  mutate(pop = tidy(e1_conditPlusBand_RF, effects = \"fixed\") |&gt; filter(term==\"bandInt\") |&gt; pull(estimate),\n         Slope1=pop+Estimate,\n         Slope2 = (coef(e1_conditPlusBand_RF)$id |&gt; as_tibble(rownames=\"id\") |&gt; pull(Estimate.bandInt))) \n\n\n# vx ~ condit * bandInt + (1 + bandInt | gr(id, by = condit)) \npopSlope &lt;- tidy(e1_testVxBand_grRF, effects = \"fixed\") |&gt; filter(term==\"bandInt\") |&gt; pull(estimate)\npopInteraction &lt;- tidy(e1_testVxBand_grRF, effects = \"fixed\") |&gt; filter(term==\"conditVaried:bandInt\") |&gt; pull(estimate)\nvx_grRF &lt;- ranef(e1_testVxBand_grRF)$id[, ,\"bandInt\"] |&gt; as_tibble(rownames=\"id\") |&gt; \n  left_join(e1Sbjs,by=join_by(id)) |&gt; select(id,condit,condit_dummy,starts_with(\"Estimate\")) |&gt;\n # group_by(condit) |&gt; \n  #summarise(Estimate=mean(Estimate)) |&gt;\n  mutate(pop = tidy(e1_testVxBand_grRF, effects = \"fixed\") |&gt; filter(term==\"bandInt\") |&gt; pull(estimate),\n         Slope1=pop+Estimate,\n         Slope2 = (coef(e1_testVxBand_grRF)$id |&gt; as_tibble(rownames=\"id\") |&gt; pull(Estimate.bandInt)), \n         Slope3=Estimate + pop + ((popInteraction + Estimate)*condit_dummy)) \n\n\nbandOnly |&gt; group_by(condit) |&gt; summarise(Estimate=mean(Slope1))\nnoInt |&gt; group_by(condit) |&gt; summarise(Estimate=mean(Slope1))\nvx_grRF |&gt; group_by(condit) |&gt; summarise(Estimate=mean(Slope3))\n\n\nmodelsummary(list(\"Band Only\" = e1_testVx_grRF8,\n                  \"No Interaction\" = e1_conditPlusBand_RF,\n                  \"Full Model - Gr\" = e1_testVxBand_grRF,\n                  \"Full Model\" = e1_vxBMM),\n             coef_map = c(\"b_Intercept\"=\"Intercept\",\"b_bandInt\"=\"Band Slope\", \"b_conditVaried\"=\"Condition\",\n                          'b_conditVaried:bandInt'=\"Interaction\",\"sigma\"=\"Sigma\"),\n             output=\"markdown\",gof_map=NA,fmt=2)\n\n{ bandOnly |&gt; ggplot(aes(Slope1,color=condit)) + geom_density() + \n  ggtitle(\"vx ~ bandInt + (1 + bandInt || gr(id, by = condit))\") } /\n  { noInt |&gt; ggplot(aes(Slope1,color=condit)) + geom_density() + \n  ggtitle(\"vx ~ condit + bandInt + (1 + bandInt | id)  \") } / \n{ vx_grRF |&gt; ggplot(aes(Slope3,color=condit)) + geom_density() + \n  ggtitle(\"vx ~ condit * bandInt + (1 + bandInt | gr(id, by = condit)) \") } /\n{ e1Vx |&gt; ggplot(aes(Slope3,color=condit)) + geom_density() + \n  ggtitle(\"vx ~ condit * bandInt + (1 + bandInt | id) \") }\n\n\nbandOnly |&gt; group_by(condit) |&gt; reframe(enframe(quantile(Slope1, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"Slope1\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=Slope1,names_prefix=\"Q_\") |&gt;\n  group_by(condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; \n  pandoc.table(caption=\"vx ~ bandInt + (1 + bandInt || gr(id, by = condit))\")\n\nnoInt |&gt; group_by(condit) |&gt; reframe(enframe(quantile(Slope1, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"Slope1\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=Slope1,names_prefix=\"Q_\") |&gt;\n  group_by(condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; \n  pandoc.table(caption=\"vx ~ condit + bandInt + (1 + bandInt | id)  \")\n\nvx_grRF |&gt; group_by(condit) |&gt; reframe(enframe(quantile(Slope3, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"Slope1\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=Slope1,names_prefix=\"Q_\") |&gt;\n  group_by(condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; \n  pandoc.table(caption=\"vx ~ condit * bandInt + (1 + bandInt | gr(id, by = condit))  \")\n\ne1Vx |&gt; group_by(condit) |&gt; reframe(enframe(quantile(Slope3, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"Slope3\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=Slope3,names_prefix=\"Q_\") |&gt;\n  group_by(condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt;\n  pandoc.table(caption=\"vx ~ condit * bandInt + (1 + bandInt | id)  \")\n\n\n# \n# indv_model_plot &lt;- function(combined_df, indv_coefs, testAvg, rank_variable = \"Estimate.Intercept\", n_sbj = 5, order = \"min\") {\n#   slice_fn &lt;- if (order == \"min\") slice_min else slice_max\n#     rank_variable_name &lt;- deparse(substitute(rank_variable))\n#   selected_ids &lt;- indv_coefs %&gt;%\n#     slice_fn({{ rank_variable }}, n = n_sbj, by = condit) %&gt;%\n#     select(id, rank_var = {{ rank_variable }})\n#   print(head(selected_ids))\n#    cdf &lt;- combined_df %&gt;%\n#     inner_join(selected_ids, by = join_by(id)) %&gt;%\n#     mutate(label = paste0(\"Subject \", id, \"; \", rank_variable_name, \"=\", round(rank_var, 2)))\n#     #print(head(cdf))\n# \n#     cdf |&gt;\n#     group_by(id, bandInt)  |&gt;\n#     sample_n(50) %&gt;%\n#     ggplot(aes(x = bandInt, y = estimate)) +\n#     geom_abline(aes(intercept = Intercept + b_Intercept, slope = bandInt_RF + b_bandInt), color = \"grey50\") +\n#     geom_abline(data = indv_coefs %&gt;%\n#                   slice_fn({{ rank_variable }}, n = n_sbj, by = condit),\n#                 aes(intercept = {{ rank_variable }}, slope = {{ rank_variable }}), color = \"red\") +\n#     stat_halfeye() +\n#     stat_halfeye(data = testAvg %&gt;%\n#                    filter(id %in% selected_ids$id),\n#                  aes(x = bandInt, y = vx), color = \"blue\") +\n#     ggh4x::facet_nested_wrap(vars(condit, label)) +\n#        ggtitle(glue(\"Order {n_sbj} Individuals for {rank_variable_name}\"))\n# \n# }\n\n\ncombined_df |&gt; indv_model_plot(indv_coefs, testAvg, rank_variable=Estimate.Intercept,n_sbj=6,\"min\")\ncombined_df |&gt; indv_model_plot(indv_coefs, testAvg, rank_variable=Estimate.Intercept,n_sbj=6,\"max\")\ncombined_df |&gt; indv_model_plot(indv_coefs, testAvg, rank_variable=Estimate.bandInt,n_sbj=6,\"min\")\ncombined_df |&gt; indv_model_plot(indv_coefs, testAvg, rank_variable=Estimate.bandInt,n_sbj=6,\"max\")\n\ncombined_df |&gt; indv_model_plot(indv_coefs, testAvg, rank_variable=Est.Error.Intercept,n_sbj=6,\"min\")\ncombined_df |&gt; indv_model_plot(indv_coefs, testAvg, rank_variable=Est.Error.Intercept,n_sbj=6,\"max\")\n\n\n\n\n## Sort by intercept \ncombined_df |&gt; \n  filter(id %in% (indv_coefs |&gt; slice_min(Estimate.Intercept,n=5,by=condit) |&gt; pull(id))) |&gt; \n  group_by(id, bandInt) |&gt;\n  sample_n(50) |&gt;\n  ggplot(aes(x=bandInt,y=estimate)) + \n  geom_abline(aes(intercept=Intercept+b_Intercept, slope=bandInt_RF+b_bandInt), color=\"grey50\") +\n  geom_abline(data=indv_coefs |&gt; slice_min(Estimate.Intercept,n=5,by=condit),aes(intercept=Estimate.Intercept,slope=Estimate.bandInt,color=\"red\")) +\n  stat_halfeye() +\n  stat_halfeye(data=testAvg |&gt; filter(id %in% (indv_coefs |&gt; slice_min(Estimate.Intercept,n=5,by=condit) |&gt; pull(id))), aes(x=bandInt,y=vx),color=\"blue\") +\n  ggh4x::facet_nested_wrap(vars(condit,id))\n\n\n\nindv_coefs |&gt; ggplot(aes(y=id, x=Estimate.bandInt)) + geom_pointrange()\n\n\n\n\n\ncd &lt;- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |&gt; \n  rename(bandInt_RF = bandInt) |&gt;\n  mutate(Slope=bandInt_RF+b_bandInt) |&gt; group_by(id) \n\ncdMed &lt;- cd |&gt; group_by(id) |&gt; median_qi(Slope) |&gt; mutate(rankSlope=rank(Slope)) |&gt; arrange(rankSlope) |&gt; left_join(e1Sbjs, by=join_by(id))\n\ncdMed %&gt;% ggplot(aes(y=rankSlope, x=Slope,fill=condit,color=condit)) + \n  geom_pointrange(aes(xmin=.lower , xmax=.upper)) + \n  labs(x=\"Estimated Slope\", y=\"Participant\") # + facet_wrap(~condit) + \n\ncd |&gt;  ggplot(aes(y=id, x=Slope)) + geom_pointrange()\n\n\ntestAvg |&gt; select(id,condit,bandInt,vx) |&gt;\n  group_by(bandInt) |&gt;\n  mutate(across(c(vx), list(c = ~ . - mean(.)))) %&gt;% \n  group_by(id,bandInt) %&gt;% \n  mutate(\n    across(\n      c(vx_c), \n      list(\n        # Between-person center (= person's mean)\n        b = ~ mean(.), \n        # Within-person center (= deviation from person's mean)\n        w = ~ . - mean(.)\n      ),\n      .names = \"{.col}{.fn}\"\n    )\n  ) %&gt;% \n  ungroup()\n\n# A tibble: 936 × 7\n   id    condit bandInt    vx  vx_c vx_cb vx_cw\n   &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1     Varied     100  565. -27.5 -27.5     0\n 2 1     Varied     350  791.  76.1  76.1     0\n 3 1     Varied     600  985. 160.  160.      0\n 4 1     Varied     800 1061.  22.1  22.1     0\n 5 1     Varied    1000 1187.  10.3  10.3     0\n 6 1     Varied    1200 1379.  90.1  90.1     0\n 7 2     Varied     100 1278. 686.  686.      0\n 8 2     Varied     350 1321. 606.  606.      0\n 9 2     Varied     600 1346. 522.  522.      0\n10 2     Varied     800 1614. 575.  575.      0\n# ℹ 926 more rows\n\ntestAvg |&gt; select(id,condit,bandInt,vx) |&gt; group_by(condit,bandInt) |&gt; summarise(mean(vx))\n\n# A tibble: 12 × 3\n# Groups:   condit [2]\n   condit   bandInt `mean(vx)`\n   &lt;fct&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Constant     100       523.\n 2 Constant     350       661.\n 3 Constant     600       772.\n 4 Constant     800      1009.\n 5 Constant    1000      1173.\n 6 Constant    1200      1307.\n 7 Varied       100       665.\n 8 Varied       350       772.\n 9 Varied       600       880.\n10 Varied       800      1070.\n11 Varied      1000      1181.\n12 Varied      1200      1269.\n\n\n\ncoef(e1_vxBMM)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id, starts_with(\"Est\")) |&gt; print(n=15)\n\nn_lines &lt;- 20\nf &lt;- fitted(e1_vxBMM, newdata = new_data_grid, re_formula = NA, summary=F, ndraws=n_lines) |&gt; \n  as_tibble() %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw)\n\n\n\n\n\n\ncombined_df |&gt; \n  filter(id %in% top_indv_coefs$id) |&gt; \n  group_by(id, bandInt) |&gt;\n  sample_n(50) |&gt;\n  ggplot(aes(x=bandInt,y=estimate)) + \n  geom_abline(aes(intercept=Intercept+b_Intercept, slope=bandInt_RF+b_bandInt), color=\"grey50\") +\n  geom_abline(data=indv_coefs |&gt; filter(id %in% top_indv_coefs$id),aes(intercept=Estimate.Intercept,slope=Estimate.bandInt,color=\"red\")) +\n  stat_halfeye() +\n  stat_halfeye(data=testAvg |&gt; filter(id %in% top_indv_coefs$id), aes(x=bandInt,y=vx),color=\"blue\") +\n  facet_wrap(~id)\n\n\n\n\n\n## Sort by intercept \ncombined_df |&gt; \n  filter(id %in% (indv_coefs |&gt; slice_min(Estimate.Intercept,n=5,by=condit) |&gt; pull(id))) |&gt; \n  group_by(id, bandInt) |&gt;\n  sample_n(50) |&gt;\n  ggplot(aes(x=bandInt,y=estimate)) + \n  geom_abline(aes(intercept=Intercept+b_Intercept, slope=bandInt_RF+b_bandInt), color=\"grey50\") +\n  geom_abline(data=indv_coefs |&gt; slice_min(Estimate.Intercept,n=5,by=condit),aes(intercept=Estimate.Intercept,slope=Estimate.bandInt,color=\"red\")) +\n  stat_halfeye() +\n  stat_halfeye(data=testAvg |&gt; filter(id %in% (indv_coefs |&gt; slice_min(Estimate.Intercept,n=5,by=condit) |&gt; pull(id))), aes(x=bandInt,y=vx),color=\"blue\") +\n  ggh4x::facet_nested_wrap(vars(condit,id))\n\n\n## Sort by intercept fit error\ncombined_df |&gt; \n  filter(id %in% (indv_coefs |&gt; slice_min(Est.Error.Intercept,n=5,by=condit) |&gt; pull(id))) |&gt; \n  group_by(id, bandInt) |&gt;\n  sample_n(50) |&gt;\n  ggplot(aes(x=bandInt,y=estimate)) + \n  geom_abline(aes(intercept=Intercept+b_Intercept, slope=bandInt_RF+b_bandInt), color=\"grey50\") +\n  geom_abline(data=indv_coefs |&gt; slice_min(Est.Error.Intercept,n=5,by=condit),aes(intercept=Estimate.Intercept,slope=Estimate.bandInt,color=\"red\")) +\n  stat_halfeye() +\n  stat_halfeye(data=testAvg |&gt; filter(id %in% well_fit$id), aes(x=bandInt,y=vx),color=\"blue\") +\n  ggh4x::facet_nested_wrap(vars(condit,id))\n\n\n\ncombined_df |&gt; \n  filter(id %in% (indv_coefs |&gt; filter(rank&lt;=2) |&gt; pull(id)) )  |&gt; \n  group_by(id, bandInt) |&gt;\n  sample_n(50) |&gt;\n  ggplot(aes(x=bandInt,y=estimate)) + \n  geom_abline(aes(intercept=Intercept+b_Intercept, slope=bandInt_RF+b_bandInt), color=\"grey50\") +\n  geom_abline(data=indv_coefs |&gt; filter(id %in% (indv_coefs |&gt; filter(rank&lt;=2) |&gt; pull(id))),aes(intercept=Estimate.Intercept,slope=Estimate.bandInt,color=\"red\")) +\n  stat_halfeye() +\n  stat_halfeye(data=testAvg |&gt; filter(id %in% (indv_coefs |&gt; filter(rank&lt;=2) |&gt; pull(id))), aes(x=bandInt,y=vx),color=\"blue\") +\n  ggh4x::facet_nested_wrap(vars(condit,id))\n\n\nindv_coefs |&gt; \n  filter(id %in% top_indv_coefs$id) |&gt; ggplot(aes(x=bandInt,y=estimate)) + \n  geom_abline(aes(intercept=Intercept+b_Intercept, slope=bandInt_RF+b_bandInt), color=\"grey50\") +\n  stat_halfeye()\n\n\n\nall_effects &lt;- e1_vxBMM |&gt; \n  gather_draws(b_Intercept, b_conditVaried, b_bandInt, `b_conditVaried:bandInt`, `^r_id.*$`, regex = TRUE, ndraws = 1)\n\nhead(all_effects)\n\n\n nd &lt;- e1_vxBMM |&gt;  \n   spread_draws(b_Intercept, b_bandInt, `b_conditVaried`,`b_conditVaried:bandInt`,  r_id[id,term], ndraws=10)  \n\n\ndf_pred &lt;- \n  posterior_predict(e1_distBMM, ndraws = 500) |&gt;\n  array_branch(margin=1) |&gt; \n   map_dfr( \n    function(yrep_iter) {\n      test %&gt;%\n        mutate(dist_pred = yrep_iter)\n    },\n    .id = 'iter'\n  ) |&gt;\n  mutate(iter = as.numeric(iter))\n\ndf_pred %&gt;% filter(id %in% 1:162 ) |&gt;\n  filter(iter &lt; 100) %&gt;%\n  ggplot(aes(dist_pred, group = iter)) +\n  geom_line(alpha = .05, stat = 'density', color = 'blue') +\n  geom_density(data = test |&gt; filter(id %in% 1:162),\n               aes(dist,col=vb),\n               inherit.aes = FALSE,\n               size = 0.8) + # 1\n  facet_grid(condit ~ vb) +\n  xlab('vx')\n\ndf_pred %&gt;% filter(id %in% 1:10 ) |&gt;\n  filter(iter &lt; 100) %&gt;%\n  ggplot(aes(dist_pred, group = iter)) +\n  geom_line(alpha = .05, stat = 'density', color = 'blue') +\n  geom_density(data = test |&gt; filter(id %in% 1:10),\n               aes(dist),\n               inherit.aes = FALSE,\n               size = 0.5) + # 1\n  facet_wrap(id ~ .) +\n  xlab('vx')\n\n\nvx_pred &lt;- \n  posterior_predict(e1_vxBMM, ndraws = 500) |&gt; \n  array_branch(margin=1) |&gt; \n   map_dfr( \n    function(yrep_iter) {\n      test %&gt;%\n        mutate(vx_pred = yrep_iter)\n    },\n    .id = 'iter'\n  ) %&gt;%\n  mutate(iter = as.numeric(iter))\n\nvx_pred %&gt;% filter(id %in% 1:2 ) |&gt;\n  filter(iter &lt; 100) %&gt;%\n  ggplot(aes(vx_pred, group = iter)) +\n  geom_line(alpha = .05, stat = 'density', color = 'blue') +\n  geom_density(data = test |&gt; filter(id %in% 1:2),\n               aes(vx,col=vb),\n               inherit.aes = FALSE,\n               size = 0.5) + # 1\n  facet_grid(id ~ vb) +\n  xlab('vx')\n\nvx_pred  |&gt;\n  filter(iter &lt; 100) %&gt;%\n  ggplot(aes(vx_pred, group = iter)) +\n  geom_line(alpha = .05, stat = 'density', color = 'blue') +\n  geom_density(data = test,\n               aes(vx,col=vb),\n               inherit.aes = FALSE,\n               size = 0.5) + # 1\n  facet_grid(condit ~ vb) +\n  xlab('vx')\n\nvx_pred %&gt;% filter(id %in% 1:9 ) |&gt;\n  filter(iter &lt; 100) %&gt;%\n  ggplot(aes(vx_pred, group = iter)) +\n  geom_line(alpha = .05, stat = 'density', color = 'blue') +\n  geom_density(data = test |&gt; filter(id %in% 1:9),\n               aes(vx),\n               inherit.aes = FALSE,\n               size = 0.5) + # 1\n  facet_wrap(id ~ .,ncol=3) +\n  xlab('vx')\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindv_coefs |&gt; \n  filter(id %in% top_indv_coefs$id) |&gt; ggplot(aes(x=bandInt,y=estimate)) + \n  geom_abline(aes(intercept=Intercept+b_Intercept, slope=bandInt_RF+b_bandInt), color=\"grey50\") +\n  stat_halfeye()\n\n\n\nall_effects &lt;- e1_vxBMM |&gt; \n  gather_draws(b_Intercept, b_conditVaried, b_bandInt, `b_conditVaried:bandInt`, `^r_id.*$`, regex = TRUE, ndraws = 1)\n\nhead(all_effects)\n\n\n nd &lt;- e1_vxBMM |&gt;  \n   spread_draws(b_Intercept, b_bandInt, `b_conditVaried`,`b_conditVaried:bandInt`,  r_id[id,term], ndraws=10)  \n:::\n\ndf_posterior &lt;- e1_vxBMM |&gt; as_tibble()\n\nggplot(df_posterior) + \n  aes(x = b_Intercept, y = b_bandInt) + \n  stat_density_2d( geom = \"polygon\")\n\n\ndf_posterior |&gt; sample_n(100) |&gt; ggplot() + \n  aes(x = b_Intercept, y = b_bandInt) + \n  geom_raster(interpolate = T)\n\ndf_posterior |&gt; sample_n(100) |&gt; ggplot(aes(x = b_Intercept, y = b_bandInt)) + \n  stat_ellipse(geom = \"polygon\", level = 0.1, alpha = 1/2)\n\ndf_posterior |&gt; \n  sample_n(1000) |&gt;\n  ggplot() + \n  aes(x = b_Intercept, y = b_bandInt) + \n  stat_dist_dotsinterval()\n\n\ntest |&gt; ggplot(aes(x=vx,y=bandInt)) + geom_jitter(width=0,height=0.3)"
  },
  {
    "objectID": "Model/abc_ojs_react.html",
    "href": "Model/abc_ojs_react.html",
    "title": "abc_ojs_react",
    "section": "",
    "text": "viewof rank = Inputs.range(\n  [1, 50], \n  { value: 50, step: 1, label: \"Rank:\" }\n)\n\nviewof Fit_Method = Inputs.checkbox(\n  [\"Test\", \"Test_Train\", \"Train\"], \n  { value: [\"Test\", \"Test_Train\"], label: \"Fit_Method:\" }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReactable.setFilter('tbl', 'rank', rank)\n\n\n\n\n\n\n\nReactable.setFilter('tbl', 'Fit_Method', Fit_Method)\n\n\n\n\n\n\n\n\n\n// Create an Observable value that automatically tracks the table's filtered data\nfilteredData = Generators.observe(change =&gt; {\n  return Reactable.onStateChange('tbl', state =&gt; {\n    change(state.sortedData)\n  })\n})"
  },
  {
    "objectID": "Model/htw_model.html",
    "href": "Model/htw_model.html",
    "title": "HTW Modeling",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable,ggstance, htmltools,\n               ggdist,ggh4x,brms,tidybayes,emmeans,bayestestR, gt)\nwalk(c(\"dplyr\"), conflict_prefer_all, quiet = TRUE)\n#options(brms.backend=\"cmdstanr\",mc.cores=4)\noptions(digits=3, scipen=999, dplyr.summarise.inform=FALSE)\nwalk(c(\"Display_Functions\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\", \"prep_model_data\",\"org_functions\"), ~source(here::here(paste0(\"Functions/\", .x, \".R\"))))\nCodeinvisible(list2env(load_sbj_data(), envir = .GlobalEnv))\ninvisible(list2env(load_e1(), envir = .GlobalEnv))\ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ne2_model &lt;- load_e2()\ne3_model &lt;- load_e3()\nCodealm_plot()\n\n\n\n\n\n\nFigure 1: The Associative Learning Model (ALM). The diagram illustrates the basic structure of the ALM model as used in the present work. Input nodes are activated as a function of their similarity to the lower-boundary of the target band. The generalization parameter, \\(c\\), determines the degree to which nearby input nodes are activated. The output nodes are activated as a function of the weighted sum of the input nodes - weights are updated via the delta rule.",
    "crumbs": [
      "Model",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Model/htw_model.html#alm-exam-description",
    "href": "Model/htw_model.html#alm-exam-description",
    "title": "HTW Modeling",
    "section": "ALM & Exam Description",
    "text": "ALM & Exam Description\nALM is a localist neural network model (Page, 2000), with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on the value of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter. The EXAM model is an extension of ALM, with the same learning rule and representational scheme for input and output units. EXAM differs from ALM only in its response rule, as it includes a linear extrapolation mechanism for generating novel responses. Although this extrapolation rule departs from a strictly similarity-based generalization mechanism, EXAM is distinct from pure rule-based models in that it remains constrained by the weights learned during training. EXAM retrieves the two nearest training inputs, and the ALM responses associated with those inputs, and computes the slope between these two points. The slope is then used to extrapolate the response to the novel test stimulus. Because EXAM requires at least two input-output pairs to generate a response, additional assumptions were required in order for it to generate resposnes for the constant group. We assumed that participants come to the task with prior knowledge of the origin point (0,0), which can serve as a reference point necessary for the model to generate responses for the constant group. This assumption is motivated by previous function learning research (Brown & Lacroix (2017)), which through a series of manipulations of the y intercept of the underlying function, found that participants consistently demonstrated knowledge of, or a bias towards, the origin point (see Kwantes & Neal (2006) for additional evidence of such a bias in function learning tasks).\nSee Table 1 for a full specification of the equations that define ALM and EXAM, and Figure 1 for a visual representation of the ALM model.\n\n\n\nTable 1: ALM & EXAM Equations\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).\n\n\n\nEXAM Extrapolation\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\n\nSlope Computation\n\n\\(S =\\) \\(\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\n\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nALM response \\(m(X_i)\\) adjusted by slope.",
    "crumbs": [
      "Model",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Model/htw_model.html#model-fitting-strategy",
    "href": "Model/htw_model.html#model-fitting-strategy",
    "title": "HTW Modeling",
    "section": "Model Fitting Strategy",
    "text": "Model Fitting Strategy\nTo fit ALM and EXAM to our participant data, we employ a similar method to Mcdaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data in with separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase.Models to were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate parameters, we used approximate bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g. many neural network and evidence accumulation models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individuals velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.a\nModelling Results\nGroup level Patterns\n\nCodepost_tabs &lt;- abc_tables(post_dat,post_dat_l)\ntrain_tab &lt;- abc_train_tables(pd_train,pd_train_l)\n\n\n\ne1_tab &lt;- rbind(post_tabs$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Test\"), train_tab$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n  # |&gt;\n  # flextable::tabulator(rows=c(\"stage\",\"Fit_Method\",\"Model\"), columns=c(\"condit\"),\n  #                      `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\n\n\n\ne1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Mean Error by Fit Method, Task Stage, and Condition\",\n    subtitle = \"Comparison across Models\"\n  ) %&gt;%\n  cols_move_to_start(columns = c(`Task Stage`)) %&gt;%\n  cols_label(\n    `Task Stage` = \"Task Stage\"\n  ) %&gt;%\n  fmt_number(\n    columns = starts_with(\"ALM\") | starts_with(\"EXAM\"),\n    decimals = 2\n  ) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"white\"),\n     locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = \"top\", color = \"black\", weight = px(1)),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_options(\n    column_labels.font.size = 10,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    table.font.size = 10, \n    quarto.disable_processing = TRUE\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Note: Mean errors are calculated as the absolute difference between the predicted and actual values.\",\n    locations = cells_title(groups = \"subtitle\")\n  ) %&gt;%\n  sub_missing(missing_text = \"\")\n#  post_dat  |&gt; group_by(condit,Model,Fit_Method,x) |&gt; \n#     mutate(e2=abs(dist-pred_dist)) |&gt; \n#     summarise(dist=mean(dist), pred=mean(pred_dist), mean_error=mean(e2)) |&gt;\n#     group_by(condit,Model,Fit_Method) |&gt; \n#     summarise(mean_error=mean(mean_error)) |&gt; \n#     round_tibble(1) |&gt; \n#   mutate(Fit_Method=rename_fm(Fit_Method)) |&gt;\n#   flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"),\n#                        `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\n\n\n\n# post_tabs$agg_pred_full_bt |&gt; \n#    pander::pandoc.table(style=\"rmarkdown\", split.table=Inf)\n#   \n# post_tabs$agg_pred_full |&gt; \n#    pander::pandoc.table(style=\"rmarkdown\", split.table=Inf)\n# \n# post_tabs$agg_x_full_bt |&gt; filter((Fit_Method==\"Test_Train\")) |&gt;\n#   mutate(Fit_Method=(Fit_Method)) |&gt;\n#   rename(\"Band\"=x, \"Condition\"=condit, \"Model Error\"=mean_error) |&gt;\n#   pander::pandoc.table(style=\"rmarkdown\", split.table=Inf)\n\n\n#  post_dat |&gt; group_by(id,condit,Model,Fit_Method) |&gt; summarise(y=mean(y), pred=mean(pred), ae=abs(y-pred)) |&gt;\n#     group_by(id,condit,Model,Fit_Method) |&gt; \n#     summarise(mean_error=mean(ae)) |&gt; \n# ggplot(aes(x=mean_error,fill=condit,y=interaction(Model,Fit_Method))) + stat_halfeye()\n# \n#  post_dat |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt; summarise(y=mean(y), pred=mean(pred), ae=abs(y-pred)) |&gt;\n#     group_by(id,condit,Model,Fit_Method,x) |&gt; \n#     summarise(mean_error=mean(ae)) |&gt; \n#   ggplot(aes(x=mean_error,fill=condit,y=interaction(Model,Fit_Method))) + \n#   stat_halfeye() +\n#   facet_wrap(~x)\n\n\nTable 2: Mean model errors predicting empirical data from the testing and training stage, aggregated over all participants and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on\n\n\n\n\n\n\n\nMean Error by Fit Method, Task Stage, and Condition\n    \n\nComparison across Models1\n\n    \n\nTask Stage\n      Fit Method\n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nTest\nFit to Test Data\n199.93\n103.36\n104.01\n85.68\n\n\nTest\nFit to Test & Training Data\n216.97\n170.28\n127.94\n144.86\n\n\nTest\nFit to Training Data\n467.73\n291.38\n273.30\n297.91\n\n\nTrain\nFit to Test Data\n297.82\n2,016.01\n53.90\n184.00\n\n\nTrain\nFit to Test & Training Data\n57.40\n132.32\n42.92\n127.90\n\n\nTrain\nFit to Training Data\n51.77\n103.48\n51.43\n107.03\n\n\n\n\n1 Note: Mean errors are calculated as the absolute difference between the predicted and actual values.\n    \n\n\n\n\n\n\n\n\nCodec_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n# post_dat_avg %&gt;%\n#     group_by(id, condit, Model, Fit_Method, rank&lt;3) %&gt;%\n#     slice_head(n = 1) |&gt;\n#     ggplot(aes(y=interaction(Model,Fit_Method), x = c, col=condit)) + \n#   #stat_pointinterval(position=position_dodge(.2)) +\n#  # geom_freqpoly() +\n#   scale_x_continuous(trans='log10')+\n#   stat_halfeye() +\n#   #geom_density() +\n#    # ggh4x::facet_nested_wrap(~Model) + \n#   labs(title=\"c parameter\") +\n#   theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n# \n#     \n#     p1 &lt;- post_dat_avg |&gt; group_by(id,Model,Fit_Method,condit) |&gt; \n#     summarise(c=median(c),lr=median(lr)) |&gt; \n#       group_by(Model,Fit_Method,condit) |&gt; \n#       mutate(c=scale(c),lr=scale(lr)) |&gt; \n#       #ggplot(aes(scale(c),scale(lr),color=condit))+\n#       ggplot(aes(c,lr,color=condit))+\n#       geom_point() +\n#       scale_x_continuous(trans='log10')\n#     \n#     ggExtra::ggMarginal(p1, groupColor=TRUE,groupFill = TRUE)\n\n\n\n\n\n\nFigure 1: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\n\nCode##| layout: [[45,-5, 45], [100]]\n##| fig-subcap: [\"Model Residuals - training data\", \"Model Residuals - testing data\",\"Full posterior predictive distributions vs. observed data from participants.\"]\ntrain_resid &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method, Block,x) |&gt; \n  summarise(y=mean(y), pred=mean(pred), mean_error=abs(y-pred)) |&gt;\n  group_by(id,condit,Model,Fit_Method,Block) |&gt;\n  summarise(mean_error=mean(mean_error)) |&gt;\n  ggplot(aes(x=interaction(Block,Model), y = mean_error, fill=factor(Block))) + \n  stat_bar + \n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, scales=\"free\",ncol=2) +\n   scale_x_discrete(guide = \"axis_nested\") +\n  scale_fill_manual(values=c(\"gray10\",\"gray50\",\"gray92\"))+\n  labs(title=\"Model Residual Errors - Training Stage\", y=\"RMSE\", x= \"Model\",fill=\"Training Block\") +\n  theme(legend.position=\"top\")\n\ntest_resid &lt;-  post_dat |&gt; \n   group_by(id,condit,x,Model,Fit_Method) |&gt;\n    summarise(y=mean(y), pred=mean(pred), error=abs(y-pred)) |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200))) |&gt;\n  ggplot(aes(x = Model, y = abs(error), fill=vbLab)) + \n  stat_bar + \n  #scale_fill_manual(values=wes_palette(\"AsteroidCity2\"))+\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\") +\n  labs(title=\"Model Residual Errors - Testing Stage\",y=\"RMSE\", x=\"Velocity Band\") \n\n\n(train_resid / test_resid) +\n  #plot_layout(heights=c(1,1.5)) & \n  plot_annotation(tag_levels = list(c('A','B')),tag_suffix = ') ') \n\n\n\n\n\n\nFigure 2: Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y axes are scaled differently between facets.\n\n\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 1, and model predictions are shown alongside the empirical data in Figure 3 and ?@fig-cm-dev-pat (i.e. these plots combine all the posterior samples from all of the subjects). There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Aggregated residuals are displayed in ?@tbl-htw-modelError. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences in training performance between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 2) also show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that y axes are scaled different between plots). Unsurprisingly, the advantage of EXAM is strongest for extrapolation positions (the three smallest bands for both groups - as well as the two highest bands for the Constant group). Although EXAM tends to perform better for both Constant and Varied participants (see also ?@tbl-htw-modelError), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods.\nPanel B of Figure 2 directly compares the aggregated observed data to the posterior predictive distributions for the testing stage. Of interest are a) the extent to which the median estimates of the ALM and EXAM posteriors deviate from the observed medians for each velocity band; b) the ability of ALM and EXAM to discriminate between velocity bands; c) the relative performance of models that are constrained by the training data (i.e. the ‘fit to train only’ and ‘fit to both’ models) compared to the ‘fit to test only’ models;\nConsidering first the models fit to only the testing data, which reflect the best possible performance of ALM and EXAM at capturing the group-aggregated testing patterns. For the varied group, both ALM and EXAM are able to capture the median values of the observed data within the 66% credible intervals, and the spread of model predictions generally matches that of the observed data. For the constant group, only EXAM is able to capture the median range of values across the velocity bands, with ALM generally underestimating human velocoties in the upper bands, and overestimating in the lower bands. In the case of band 100, the median ALM prediction appears to match that of our participants - however this is due to a large subset of participants have ALM predictions near 0 for band 100, a pattern we will explore further in our considertation of individual patterns below. Models fit to both training and testing data show a similar pattern to only the testing data display the same basic pattern as those fit to only the testing data, albeit with slightly larger residuals. However models fit to only the training data display markedly worse performance at accounting for the key testing patterns.\n\n** explain how the constant group ALM predictions for band 100 look deceptively good due to aggregation of a large subset of subjects having ALM predictions of 0 for vb100, and a large subset with ALM predictions close to their position 800 value. This is relected by much greater variance of the ALM esimates in the posterior predictive plot\n** comment on how much constrained by the training data has a worse impact on the EXAM predictions for varied than for constant - perhaps due to the varied training data being much noisier than the constant training data.\n** comment on EXAM doing a better job mimicing the within-condition variance of the observed data\n** comment on the % of Constant subjects being best accounted for by EXAM being higher.\n** does EXAM do better for the Constant group because the constant group performs better? Or does training with a single example encourage an exam sort of strategy?\n\n\nCode# library(brms)\n# options(brms.backend=\"cmdstanr\",mc.cores=4)\n\npdl &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n  filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\n\n# aerror is model error, which is predicted by Model(ALM vs. EXAM) & condit (Constant vs. Varied)\ne1_ee_brm_ae &lt;- brm(data=pdl,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e1_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbct_e1 &lt;- as.data.frame(bayestestR::describe_posterior(e1_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) %&gt;%\n  kable(booktabs = TRUE)\n\n#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\n\np1 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n(p1 + p2+ p3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\nCodebm1 &lt;- get_coef_details(e1_ee_brm_ae, \"conditVaried\")\nbm2 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM\")\nbm3 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nposterior_estimates &lt;- as.data.frame(e1_ee_brm_ae) %&gt;%\n  select(starts_with(\"b_\")) %&gt;%\n  setNames(c(\"Intercept\", \"ModelEXAM\", \"conditVaried\", \"ModelEXAM_conditVaried\"))\n\n\nlibrary(tidybayes)\nlibrary(emmeans)\n\nconstant_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM\nvaried_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM + posterior_estimates$conditVaried + posterior_estimates$ModelEXAM_conditVaried\ncomparison_EXAM &lt;- constant_EXAM - varied_EXAM\nsummary_EXAM &lt;- bayestestR::describe_posterior(comparison_EXAM, centrality = \"Mean\")\n\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NULL)\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NA)\n\n\n  # full set of Model x condit contrasts\n\n\n\n# ALM - EXAM\nbtw_model &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model | condit, re_formula=NULL)  |&gt; \n  pluck(\"contrasts\") |&gt; \n  gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw,condit) |&gt; summarise(value=mean(.value), n=n()) \n\n# btw_model |&gt; ggplot(aes(x=value,y=contrast,fill=condit)) +stat_halfeye()\n\n\n# Constant - Varied\nemm_condit &lt;- e1_ee_brm_ae |&gt; emmeans(~ condit | Model, re_formula = NULL)\nbtw_con &lt;- emm_condit |&gt;  pairs() |&gt; gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw, Model) |&gt; summarise(value=mean(.value), n=n()) \n# btw_con |&gt; ggplot(aes(x=value,y=Model,fill=Model)) +stat_halfeye()                              \n\n\n\ne1_ee_brm_ae |&gt; emmeans(pairwise~ Model*condit, re_formula=NA)  |&gt; \n  pluck(\"contrasts\") |&gt;\n  gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw) |&gt; summarise(value=mean(.value), n=n()) |&gt; \n  filter(!(contrast %in% c(\"ALM Constant - EXAM Constant\",\"ALM Constant - EXAM Varied\",\"ALM Varied - EXAM Varied \", \"EXAM Constant - ALM Varied\" ))) |&gt; \n  ggplot(aes(x=value,y=contrast,fill=contrast)) +stat_halfeye()\n\n\n\n\n\n\nCode# {btw_con |&gt; ggplot(aes(x=value,y=Model,fill=Model)) +stat_halfeye()} +\n# {btw_model |&gt; ggplot(aes(x=value,y=contrast,fill=condit)) +stat_halfeye()}\n\n\n# emm_models_bandint_specific &lt;- e1_ee_brm_ae |&gt;\n#   emmeans(~ Model | condit, at = list(bandInt = c(100,350,600,800,1000,1200)),\n#           re_formula = NA)\n# \n# \n# emm_models_bandint_by_levels &lt;- e1_ee_brm_ae |&gt;\n#   emmeans(~ Model | condit | bandInt, \n#           at = list(bandInt = c(100,350,600,800,1000,1200)),\n#           re_formula = NULL, by = \"bandInt\")\n\n# within_condition_bandint_by_levels_contrasts &lt;- emm_models_bandint_by_levels |&gt;\n#   pairs()\n# \n# emm_models_bandint_by_levels |&gt; as_tibble() \n\n# emm_models_bandint_by_levels |&gt; pairs() |&gt; gather_emmeans_draws() |&gt; \n#   as_tibble() |&gt; group_by(bandInt,condit) |&gt; tidybayes::median_hdi(.value) |&gt;\n#   ggplot(aes(x=.value,y=condit,fill=condit)) + stat_halfeye() +facet_wrap(~bandInt,ncol=1)\n\n  \n#print(summary_EXAM)\n\n# alternative emmeans approach to assess constant vs. varied difference in EXAM\n#emmeans(e1_ee_brm_ae, specs = \"condit\", by = \"Model\") |&gt; summary()\n# is the difference significant for EXAM in particular\n\n\nTo quantitatively assess whether ALM or EXAM did significantly better, and to see whether if the fits of the two models varied between Constant and Varied subjects, we fit a bayesian regressions predicting the error of the model predictions as a function of the Model (ALM vs. EXAM) and training condition (Constant vs. Varied). As is shown in Table 3, the model error was significantly lower for EXAM (\\(\\beta\\) = -37.54, 95% CrI [-60.4, -14.17], pd = 99.85%) than ALM, and for the Varied condition than the Constant condition (\\(\\beta\\) = -88.44, 95% CrI [-104.51, -71.81], pd = 100%). There was also a significant interaction between Model and Condition (\\(\\beta\\) = 60.42, 95% CrI [36.17, 83.85], pd = 100%), indicating that the difference in model error between ALM and EXAM was significantly greater for the constant condition. To assess whether EXAM predicts constant performance significantly better for Constant than for Varied subjects, we calculated the difference in model error between the Constant and Varied conditions specifically for EXAM. The results indicated that the model error for EXAM was significantly lower in the Constant condition compared to the Varied condition, with a mean difference of -22.879 (95% CrI [-46.016, -0.968], pd = 0.981).\n\n\nTable 3: Comparison of model errors - experiment 1 - assessed bayesian regression in the form of Model Error ~ Model * Condition. The intercept reflects the baseline of ALM and Constant. The other estimates indicate deviations from the baseline for the EXAM mode and varied condition.\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n176.3\n156.9\n194.6\n1\n\n\nModelEXAM\n-88.4\n-104.5\n-71.8\n1\n\n\nconditVaried\n-37.5\n-60.4\n-14.2\n1\n\n\nModelEXAM:conditVaried\n60.4\n36.2\n83.8\n1\n\n\n\n\n\n\n\nCode#post_dat_l |&gt; group_by(id,condit,x) |&gt; filter(Resp==\"Observed\") |&gt; slice_head(n=1)\n\npemp1 &lt;- e1 |&gt; filter(expMode2==\"Test\") |&gt; mutate(Resp=\"Observed\") |&gt; \n  ggplot(aes(x=condit,y=vx, fill=vb, col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar+ \n  scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) + \n  theme(legend.position=\"right\", axis.title.x = element_blank(), plot.title = element_text(hjust=.50)) +\n  labs(title=\"Empirical Data - Experiment 1\",y=\"vx\", x=\"Condition\",fill=\"Band\") \n\nlayout &lt;- \"\n#A#\nCCC\n\"\n\npmod1 &lt;- post_dat_l |&gt; filter(!(Resp==\"Observed\")) |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; \n #left_join(testAvgE1, by=join_by(id,condit,x==bandInt)) |&gt;\n ggplot( aes(x=condit,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n    facet_wrap(~Resp+rename_fm(Fit_Method), strip.position = \"top\", scales = \"free_x\") +\n        scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n         strip.background = element_blank(),\n         strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(20,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions - Experiment 1 Data\", y=\"vx\")\n\n(pemp1)  / pmod1 + \n  plot_layout(design = layout, heights = unit(c(5,7),c('cm','cm'))) #heights = unit(c(5,-5, 8), c('cm','null'))\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nTable 4: Model predictions experiment 1 - empirically observed values and predictions of ALM and EXAM for each band (x) and bandType (Trained vs. Extrapolation)\n\n\n\n\n\n\n\n\n\n\n\n\ncondit\nFit_Method\nx\nbandType\nALM\nEXAM\nObserved\n\n\n\nConstant\nTest_Train\n100\nExtrapolation\n462.6\n400\n491.8\n\n\nConstant\nTest_Train\n350\nExtrapolation\n883.9\n581.8\n644.7\n\n\nConstant\nTest_Train\n600\nExtrapolation\n903.7\n774.7\n759.7\n\n\nConstant\nTest_Train\n800\nTrained\n903.7\n924.4\n967.2\n\n\nConstant\nTest_Train\n1000\nExtrapolation\n903.7\n1064\n1164\n\n\nConstant\nTest_Train\n1200\nExtrapolation\n898.9\n1197\n1241\n\n\nVaried\nTest_Train\n100\nExtrapolation\n592.3\n569.5\n544.4\n\n\nVaried\nTest_Train\n350\nExtrapolation\n861.9\n703.5\n696.6\n\n\nVaried\nTest_Train\n600\nExtrapolation\n918.1\n843.8\n830.2\n\n\nVaried\nTest_Train\n800\nTrained\n940.9\n945.3\n1075\n\n\nVaried\nTest_Train\n1000\nTrained\n1014\n1044\n1166\n\n\nVaried\nTest_Train\n1200\nTrained\n1031\n1042\n1295\n\n\n\n\n\n\n\nCodepemp1 &lt;- e1 |&gt; filter(expMode2==\"Test\") |&gt; \n  group_by(id,condit,vb,bandType) |&gt; \n  ggplot(aes(x=condit,y=dist, fill=vb, col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_summary(fun=mean, geom=\"bar\", position=position_dodge()) + \n  stat_summary(fun.data=mean_se, geom=\"errorbar\", color=\"black\", position=position_dodge(), size=.5) + \n  scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) + \n  scale_y_continuous( breaks=seq(0,600,by=200),labels=as.character(seq(0,600,by=200))) +\n  expand_limits(y=600) +\n  theme(legend.position=\"right\", axis.title.x = element_blank(),plot.title = element_text(hjust=.40)) +\n  labs(title=\"Empirical Data\", y=\"Deviation from target band\", fill=\"Band\") \n\n# layout &lt;- \"#A#\n# #B#\n# CCC\"\n\nlayout &lt;- \"#A#\nCCC\"\n\npmod1 &lt;- post_dat_l |&gt; filter(!(Resp==\"Observed\")) |&gt; \n  #left_join(testAvgE1, by=join_by(id,condit,x==bandInt)) |&gt;\n  group_by(id,condit, Fit_Method,Resp,vb,bandType) |&gt; \n summarize(dist=median(dist)) |&gt; \n ggplot( aes(x=condit,y=dist, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n    facet_wrap(~Resp+rename_fm(Fit_Method), strip.position = \"top\", scales = \"free_x\") +\n        scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) +\n    theme(panel.spacing = unit(0, \"lines\"), \n         strip.background = element_blank(),\n         strip.placement = \"outside\",\n         legend.position = \"none\",\n         plot.title = element_text(hjust=.50),\n         plot.margin = unit(c(20,0,0,0), \"pt\"),\n         axis.title.x = element_blank()) + \n         labs(title=\"Model Predictions - Experiment 1 Data\", y=\"Deviation from target band\")\n\n(pemp1) / pmod1 + plot_layout(design = layout) #heights = unit(c(5,-5, 8), c('cm','null'))\n\n\n\nCodee2_model &lt;- load_e2()\npost_tabs2 &lt;- abc_tables(e2_model$post_dat,e2_model$post_dat_l)\ntrain_tab2 &lt;- abc_train_tables(e2_model$pd_train,e2_model$pd_train_l)\n\npdl2 &lt;- e2_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne2_tab &lt;- rbind(post_tabs2$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Test\"), train_tab2$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n  # |&gt;\n  # flextable::tabulator(rows=c(\"Task Stage\",\"Fit_Method\",\"Model\"), columns=c(\"condit\"),\n  #                      `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\n\n\ne2_ee_brm_ae &lt;- brm(data=pdl2,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e2_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1 &lt;- get_coef_details(e2_ee_brm_ae, \"conditVaried\")\nbm2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM\")\nbm3 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM:conditVaried\")\n# bct_e2 &lt;- as.data.frame(bayestestR::describe_posterior(e2_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n#   select(1,2,4,5,6) %&gt;%\n#   setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n#   mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n#   tibble::remove_rownames() %&gt;%\n#   mutate(Term = stringr::str_remove(Term, \"b_\")) %&gt;%\n#   kable(booktabs = TRUE)\n\ne3_model &lt;- load_e3()\npost_tabs3 &lt;- abc_tables(e3_model$post_dat,e3_model$post_dat_l)\ntrain_tab3 &lt;- abc_train_tables(e3_model$pd_train,e3_model$pd_train_l)\n\n\npdl3 &lt;- e3_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\n\ne3_tab &lt;- rbind(post_tabs3$agg_pred_full |&gt; \n  mutate(\"Task Stage\"=\"Test\"), train_tab3$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n  # |&gt;\n  # flextable::tabulator(rows=c(\"Task Stage\",\"Fit_Method\",\"Model\"), columns=c(\"condit\"),\n  #                      `ME` = as_paragraph(mean_error)) |&gt; as_flextable()\n\n\n# e3_ee_brm_ae &lt;- brm(data=pdl3,\n#   aerror ~  Model * condit+ (1+bandInt|id) + (1+condit|bandOrder), \n#   file = paste0(here(\"data/model_cache/e3_ae_modelCondBo_RFint.rds\")),\n#   chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\ne3_ee_brm_ae &lt;- brm(data=pdl3,\n  aerror ~  Model * condit*bandOrder + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e3_ae_modelCondBo_RFint2.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1 &lt;- get_coef_details(e3_ee_brm_ae, \"conditVaried\")\nbm2 &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM\")\nbm3 &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\n# bct_e3  &lt;- as.data.frame(bayestestR::describe_posterior(e3_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n#   select(1,2,4,5,6) %&gt;%\n#   setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n#   mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n#   tibble::remove_rownames() %&gt;%\n#   mutate(Term = stringr::str_remove(Term, \"b_\")) %&gt;%\n#   kable(booktabs = TRUE)\n\n\nmsa &lt;- modelsummary::modelsummary(list(\"Exp 1.\"=e1_ee_brm_ae,\"Exp 2.\"=e2_ee_brm_ae,\"Exp 3. \"=e3_ee_brm_ae), \n    gof_map = NA,\n    fmt=1,\n    estimate = \"{estimate}\",\n    statistic=\"[{conf.low}, {conf.high}]\", \n    coef_omit = \"^sigma|^sd|^cor\",\n    coef_rename=function(x) stringr::str_remove(x, \"b_\"), output=\"gt\")\n\nmsa\n#library(modelsummary)\n# modelsummary::modelsummary(get_estimates(e1_ee_brm_ae, test = c(\"pd\")),gof_map = NA)\n#modelsummary::get_estimates(e1_ee_brm_ae,test=\"pd\")\n\n\nTable 5: Results of Bayesian Regression models predictin model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and the interaction between Model and Condition. The values represent the estimate coefficient for each term, with 95% credible intervals in brackets. The intercept reflects the baseline of ALM and Constant. The other estimates indicate deviations from the baseline for the EXAM mode and varied condition. Lower values indicate better model fit.\n\n\n\n\n\n\n\n\nExp 1.\nExp 2.\nExp 3.\n\n\n\nIntercept\n176.3\n246.0\n164.7\n\n\n\n[156.9, 194.6]\n[226.2, 264.5]\n[140.1, 189.4]\n\n\nModelEXAM\n-88.5\n-137.7\n-65.6\n\n\n\n[-104.5, -71.8]\n[-160.2, -115.5]\n[-86.0, -46.0]\n\n\nconditVaried\n-37.9\n-86.0\n-40.8\n\n\n\n[-60.4, -14.2]\n[-113.5, -59.3]\n[-75.9, -3.0]\n\n\nModelEXAM:conditVaried\n60.6\n56.9\n42.3\n\n\n\n[36.2, 83.9]\n[25.3, 88.0]\n[11.2, 72.5]\n\n\nbandOrderReverse\n\n\n26.0\n\n\n\n\n\n[-9.3, 58.7]\n\n\nModelEXAM:bandOrderReverse\n\n\n-7.6\n\n\n\n\n\n[-34.5, 21.1]\n\n\nconditVaried:bandOrderReverse\n\n\n30.8\n\n\n\n\n\n[-19.6, 83.6]\n\n\nModelEXAM:conditVaried:bandOrderReverse\n\n\n-60.8\n\n\n\n\n\n[-101.8, -18.7]\n\n\n\n\n\n\n\n\n\n\n\nCodee23_tab &lt;- rbind(e2_tab |&gt; mutate(Exp=\"E2\"), e3_tab |&gt; mutate(Exp=\"E3\")) \n\ngt_table &lt;- e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Mean Error by Fit Method, Task Stage, and Experiment\",\n    subtitle = \"Comparison across Conditions and Models\"\n  ) %&gt;%\n  cols_move_to_start(columns = `Task Stage`) %&gt;%\n  cols_label(`Task Stage` = \"Task Stage\") %&gt;%\n  fmt_number(columns = matches(\"E2|E3\"), decimals = 1) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_options(\n    column_labels.font.size = 10,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    table.font.size = 10,\n    quarto.disable_processing = TRUE\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Note: Mean errors are calculated as the absolute difference between the predicted and actual values.\",\n    locations = cells_title(groups = \"subtitle\")\n  )\n\ngt_table\n\n\n\n\n\n\nMean Error by Fit Method, Task Stage, and Experiment\n    \n\nComparison across Conditions and Models1\n\n    \n\n\n      \n        E2\n      \n      \n        E3\n      \n    \n\nTask Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nFit to Test Data\n    \n\nTest\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n    \n\nTest\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n    \n\nTest\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n1 Note: Mean errors are calculated as the absolute difference between the predicted and actual values.\n    \n\n\n\n\n\nCode#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\np1 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\") + labs(title=\"E2. Model Error\")\np2 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n p_e2 &lt;- (p1 + p2+ p3) \n\n#wrap_plots(plot(conditional_effects(e3_ee_brm_ae),points=FALSE,plot=FALSE))\n\np_e3 &lt;- plot(conditional_effects(e3_ee_brm_ae, \n                         effects = \"Model:condit\", \n                         conditions=make_conditions(e3_ee_brm_ae,vars=c(\"bandOrder\"))),\n     points=FALSE,plot=FALSE)$`Model:condit` + \n     labs(x=\"Model\",y=\"Model Error\", title=\"E3. Model Error\") + \n     theme(legend.position=\"right\") + \n     scale_color_manual(values=wes_palette(\"Darjeeling1\")) \n\np1 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n #p2 &lt;- (p1 + p2+ p3)\n (p_e2 / p_e3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\nFigure 4: Conditional effects of Model and Condition on Model Error for Experiment 2 and 3 data.\n\n\n\n\n\nCoderbind(e2_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\"), \n e3_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb,bandOrder) |&gt;\n  summarize(vx=median(val)) |&gt; mutate(Exp=\"E3\")) |&gt;\n  ggplot( aes(x=condit,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) +\n  stat_bar + \n    facet_nested_wrap(~Exp+bandOrder+Resp, strip.position = \"top\", scales = \"free_x\") +\n    scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .7), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n        #  strip.background = element_blank(),\n        #  strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(20,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions Experiment 2 & 3\", y=\"vx\")\n\n\n\nEmpirical data and Model predictions from Experiment 2 and 3 for the testing stage. Observed data is shown on the right.\n\n\n\nAccounting for individual patterns\nTo more accurately assess the relative abilities of ALM and EXAM to capture important empirical patterns - we will now examine the predictions of both models for the subset of individual participants shown in Figure 5. Panel A presents three varied and constant participants who demonstrated a reasonable degree of discrimination between the 6 velocity bands during testing.\n\n** comment on the different ways ALM can completely fail to mimic discrimination patterns (sbj. 35; sbj. 137),and on how it can sometimes partially succeed (sbj. 11; 14,74)\n** comment on how EXAM can somtimes mimic non-monotonic spacing between bands due to associative stregth from training (i.e. subject 47)\n** compare c values to slope parameters from the statistical models earlier in paper\n\n\nCodecId_tr &lt;- c(137, 181, 11)\nvId_tr &lt;- c(14, 193, 47)\ncId_tt &lt;- c(11, 93, 35)\nvId_tt &lt;- c(1,14,74)\ncId_new &lt;- c(175, 185, 134, 155, 68, 93, 74, 134)\n# filter(id %in% (filter(bestTestEXAM,group_rank&lt;=9, Fit_Method==\"Test\")\n\n# e3 sbjs - 245, 219, 264, 280, 249, 274\n\n\ntestIndv &lt;- post_dat_l |&gt; filter(id %in% c(cId_tt,vId_tt,cId_new), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Individual Participant fits from Test & Train Fitting Method\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\ntestIndv \n\n\n\n\n\n\nFigure 5: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data.\n\n\n\n\n\nCodee2_model &lt;- load_e2()\n\n\n\ne3_model &lt;- load_e3()",
    "crumbs": [
      "Model",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Model/htw_model_e2.html",
    "href": "Model/htw_model_e2.html",
    "title": "HTW Model e2",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable, flextable,ggstance, htmltools, ggdist)\n#conflict_prefer_all(\"dplyr\", quiet = TRUE)\noptions(scipen = 999)\nwalk(c(\"Display_Functions\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))",
    "crumbs": [
      "Model",
      "HTW Model e2"
    ]
  },
  {
    "objectID": "Model/htw_model_e2.html#deviation-predictions",
    "href": "Model/htw_model_e2.html#deviation-predictions",
    "title": "HTW Model e2",
    "section": "Deviation Predictions",
    "text": "Deviation Predictions\n\nCode post_dat_l |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvg$vb))) |&gt;\n  ggplot(aes(x=condit,y=dist,fill=vbLab)) + \n  stat_bar + \n  #facet_wrap(~Resp)\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~Resp, axes = \"all\",ncol=3,scale=\"free\")\n\n\n\n\n\n\n\n\nCodec_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n\n\n\nFigure 2: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\nAccounting for individual patterns\n\nCode# could compute best model for each posterior parameter - examine consistency\n# then I'd have an error bar for each subject in the model error diff. figure\n\ntid1 &lt;- post_dat  |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt; \n  mutate(e2=abs(y-pred)) |&gt; \n  summarise(y1=mean(y), pred1=mean(pred),mean_error=abs(y1-pred1)) |&gt;\n  group_by(id,condit,Model,Fit_Method) |&gt; \n  summarise(mean_error=mean(mean_error)) |&gt; \n  arrange(id,condit,Fit_Method) |&gt;\n  round_tibble(1) \n\nbest_id &lt;- tid1 |&gt; \n  group_by(id,condit,Fit_Method) |&gt; mutate(best=ifelse(mean_error==min(mean_error),1,0)) \n\nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\n\nerror_difference&lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(mean_error)) %&gt;%\n  mutate(Error_difference = (ALM - EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\nfull_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n  geom_col() +\n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  labs(fill=\"Best Model\",x=\"Mean Model Error Difference (ALM - EXAM)\",y=\"Participant\")\n\n\n\n# full_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n#   ungroup() |&gt;\n#   mutate(id = reorder(id, Error_difference)) |&gt;\n#   left_join(post_dat_avg |&gt; filter(x==100) |&gt; select(-x) |&gt; ungroup(), by=c(\"id\",\"condit\")) |&gt;\n#   ggplot(aes(y=id,x=c,fill=Best_Model))+\n#   stat_pointinterval(position=position_dodge(.1))\n\n\n\n\n\n\nFigure 3: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\nSubjects with biggest differential favoring ALM\n\nCodevAlm &lt;- c(307,331,197); cAlm &lt;- c(372,173,157)\n\npost_dat_l |&gt; filter(id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Subjects with biggest differential favoring ALM\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\n\n\n\n\n\n\nSubjects with biggest differential favoring EXAM\n\nCodevAlm &lt;- c(312,334,295); cAlm &lt;- c(132,366,415)\n\npost_dat_l |&gt; filter(id %in% c(vAlm,cAlm), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=x)) + \n  stat_bar_sd + ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  labs(title=\"Subjects with biggest differential favoring EXAM\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())",
    "crumbs": [
      "Model",
      "HTW Model e2"
    ]
  },
  {
    "objectID": "Model/hybrid.html",
    "href": "Model/hybrid.html",
    "title": "HTW Hybrid Models",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable, flextable,ggstance, htmltools,kableExtra,ggdist)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\noptions(scipen = 999)\nwalk(c(\"Display_Functions\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))",
    "crumbs": [
      "Model",
      "HTW Hybrid Models"
    ]
  },
  {
    "objectID": "Model/hybrid.html#e1",
    "href": "Model/hybrid.html#e1",
    "title": "HTW Hybrid Models",
    "section": "E1",
    "text": "E1\n\nCodeds &lt;- readRDS(here::here(\"data/e1_md_11-06-23.rds\"))  |&gt; as.data.table()\nnbins &lt;- 3\n\nfd &lt;- readRDS(here(\"data/e1_08-21-23.rds\"))\ntest &lt;- fd |&gt; filter(expMode2 == \"Test\") \ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(nHits=sum(dist==0),vx=mean(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n\ntrainAvg &lt;- fd |&gt; filter(expMode2 == \"Train\") |&gt; group_by(id) |&gt; \n  mutate(tr=trial,x=vb,Block=case_when(expMode2==\"Train\" ~ cut(tr,breaks=seq(1,max(tr), length.out=nbins+1),include.lowest=TRUE,labels=FALSE),\n                                         expMode2==\"Test\" ~ 4)) |&gt; \n  group_by(id,condit,vb,x,Block) |&gt; \n  summarise(dist=mean(dist),y=mean(vx))\n\ninput_layer &lt;&lt;- output_layer &lt;&lt;-  c(100,350,600,800,1000,1200)\nids2 &lt;- c(1,66,36)\n\nfile_name &lt;- \"e1_hybrid_n_iter_250_ntry_200_0637\"\n\nind_fits &lt;- map(list.files(here(paste0('data/abc_reject/'),file_name),full.names=TRUE), readRDS)\nind_fits_df &lt;- ind_fits |&gt; map(~list(dat=.x[[1]], Model = .x[[\"Model\"]], Fit_Method=.x[[\"Fit_Method\"]]))\nind_fits_df &lt;- ind_fits_df |&gt; map(~rbindlist(.x$dat) |&gt; mutate(Model = .x$Model, Fit_Method = .x$Fit_Method)) |&gt; rbindlist() \n\n\nprocess_folder &lt;- function(folder_name) {\n  ind_fits &lt;- map(list.files(here(paste0('data/abc_reject/'), folder_name), \n                             full.names = TRUE), readRDS)\n  ind_fits_df &lt;- ind_fits |&gt; \n    map(~list(dat = .x[[1]], Model = .x[[\"Model\"]], Fit_Method = .x[[\"Fit_Method\"]],tolM=.x$tolM,ar=.x$min_accept_rate,\n              t=.x$ri$elapsed)) |&gt; \n    map(~rbindlist(.x$dat) |&gt; \n           mutate(Model = .x$Model, Fit_Method = .x$Fit_Method,\n                 exp = str_extract(folder_name, \"^e\\\\d\"),\n                 n_int = as.numeric(str_extract(folder_name, \"(?&lt;=_n_iter_)\\\\d+\")),\n                 ntry = as.numeric(str_extract(folder_name, \"(?&lt;=_ntry_)\\\\d+\")),\n                 tolM=.x$tolM,ar=.x$ar,\n                 run_name = folder_name,\n                 min = round(.x$t/60,0))) |&gt; \n    rbindlist()\n  return(ind_fits_df)\n}\n\n# folder_names &lt;- c(\"e1_hybrid_n_iter_250_ntry_200_0637\",\n# \"e1_hybrid_n_iter_60_ntry_150_2335\",\n# \"e1_hybrid_n_iter_200_ntry_300_0741\",\n# \"e1_hybrid_n_iter_150_ntry_150_4749\",\"e1_hybrid_n_iter_100_ntry_100_1435\",\"e1_hybrid_n_iter_400_ntry_300_3744\")\n\nfolder_names &lt;- list.files(here(\"data/abc_reject\"),pattern=\"e1_hyb*\")\n\n\n\nall_fits &lt;- map_df(folder_names, process_folder)\n\nall_fits |&gt; \n  group_by(exp,n_int,ntry,tolM,ar,condit,Fit_Method) |&gt; \n  summarise(min=first(min),me=mean(mean_error),\n            w=median(weight_exam),sd_w=sd(weight_exam),c=median(c),\n            lr=median(lr),n=n_distinct(id)) |&gt;\n   mutate(across(c(me, w, lr, sd_w), ~round(., 2))) |&gt;\n  arrange(condit,Fit_Method,me) |&gt; kable(caption=\"E1 Fit Comparisons\") |&gt; \n  kable_styling(full_width = F) |&gt;\n  column_spec(8,bold=T,border_left=T) \n\n\nE1 Fit Comparisons\n\nexp\nn_int\nntry\ntolM\nar\ncondit\nFit_Method\nmin\nme\nw\nsd_w\nc\nlr\nn\n\n\n\ne1\n150\n2500\n0.65\n0.10\nConstant\nTest\n847\n214.73\n0.80\n0.25\n0.0000403\n2.37\n80\n\n\ne1\n100\n2500\n0.65\n0.10\nConstant\nTest\n209\n214.87\n0.80\n0.25\n0.0000402\n2.37\n80\n\n\ne1\n200\n1500\n0.70\n0.10\nConstant\nTest\n402\n215.79\n0.80\n0.25\n0.0000411\n2.37\n80\n\n\ne1\n50\n900\n0.80\n0.12\nConstant\nTest\n55\n217.65\n0.79\n0.25\n0.0000433\n2.32\n80\n\n\ne1\n90\n900\n0.80\n0.09\nConstant\nTest\n101\n217.77\n0.79\n0.25\n0.0000439\n2.31\n80\n\n\ne1\n150\n500\n0.95\n0.10\nConstant\nTest\n112\n220.00\n0.79\n0.25\n0.0000500\n2.30\n80\n\n\ne1\n70\n500\n0.85\n0.10\nConstant\nTest\n177\n220.15\n0.79\n0.24\n0.0000486\n2.30\n80\n\n\ne1\n400\n300\n0.85\n0.04\nConstant\nTest\n337\n220.33\n0.79\n0.24\n0.0000491\n2.28\n80\n\n\ne1\n50\n500\n0.85\n0.05\nConstant\nTest\n201\n220.67\n0.79\n0.24\n0.0000472\n2.28\n80\n\n\ne1\n200\n300\n0.90\n0.10\nConstant\nTest\n129\n220.96\n0.79\n0.24\n0.0000525\n2.28\n80\n\n\ne1\n49\n800\n0.95\n0.18\nConstant\nTest\n286\n221.49\n0.79\n0.25\n0.0000513\n2.31\n80\n\n\ne1\n61\n500\n0.95\n0.10\nConstant\nTest\n41\n221.57\n0.79\n0.24\n0.0000552\n2.30\n80\n\n\ne1\n50\n900\n0.95\n0.10\nConstant\nTest\n37\n221.77\n0.79\n0.25\n0.0000524\n2.33\n80\n\n\ne1\n250\n200\n0.85\n0.10\nConstant\nTest\n176\n222.09\n0.78\n0.24\n0.0000594\n2.27\n80\n\n\ne1\n52\n600\n0.95\n0.18\nConstant\nTest\n199\n222.28\n0.79\n0.25\n0.0000584\n2.32\n80\n\n\ne1\n65\n300\n0.95\n0.10\nConstant\nTest\n29\n223.14\n0.79\n0.25\n0.0000602\n2.26\n80\n\n\ne1\n100\n100\n0.85\n0.01\nConstant\nTest\n372\n223.15\n0.79\n0.25\n0.0000567\n2.24\n80\n\n\ne1\n150\n150\n0.85\n0.10\nConstant\nTest\n72\n223.85\n0.78\n0.24\n0.0000647\n2.27\n80\n\n\ne1\n51\n300\n0.95\n0.15\nConstant\nTest\n149\n223.96\n0.79\n0.25\n0.0000643\n2.31\n80\n\n\ne1\n60\n150\n0.85\n0.10\nConstant\nTest\n24\n224.74\n0.78\n0.25\n0.0000670\n2.28\n80\n\n\ne1\n105\n100\n0.85\n0.05\nConstant\nTest\n228\n225.38\n0.78\n0.25\n0.0000644\n2.24\n80\n\n\ne1\n100\n100\n0.90\n0.10\nConstant\nTest\n162\n226.26\n0.79\n0.25\n0.0000798\n2.25\n80\n\n\ne1\n100\n2500\n0.65\n0.10\nConstant\nTest_Train\n317\n233.60\n0.79\n0.24\n0.0000381\n0.72\n80\n\n\ne1\n200\n1500\n0.70\n0.10\nConstant\nTest_Train\n466\n234.43\n0.79\n0.24\n0.0000388\n0.72\n80\n\n\ne1\n90\n900\n0.80\n0.09\nConstant\nTest_Train\n120\n235.49\n0.78\n0.25\n0.0000388\n0.73\n80\n\n\ne1\n50\n900\n0.80\n0.12\nConstant\nTest_Train\n71\n235.70\n0.77\n0.25\n0.0000388\n0.71\n80\n\n\ne1\n70\n500\n0.85\n0.10\nConstant\nTest_Train\n173\n237.28\n0.77\n0.25\n0.0000401\n0.76\n80\n\n\ne1\n150\n500\n0.95\n0.10\nConstant\nTest_Train\n138\n237.33\n0.78\n0.25\n0.0000400\n0.75\n80\n\n\ne1\n50\n500\n0.85\n0.05\nConstant\nTest_Train\n217\n237.64\n0.78\n0.25\n0.0000401\n0.77\n80\n\n\ne1\n400\n300\n0.85\n0.04\nConstant\nTest_Train\n475\n237.70\n0.77\n0.25\n0.0000409\n0.79\n80\n\n\ne1\n49\n800\n0.95\n0.18\nConstant\nTest_Train\n263\n238.08\n0.78\n0.25\n0.0000400\n0.79\n80\n\n\ne1\n200\n300\n0.90\n0.10\nConstant\nTest_Train\n153\n238.15\n0.77\n0.25\n0.0000413\n0.82\n80\n\n\ne1\n50\n900\n0.95\n0.10\nConstant\nTest_Train\n38\n238.19\n0.78\n0.25\n0.0000398\n0.79\n80\n\n\ne1\n52\n600\n0.95\n0.18\nConstant\nTest_Train\n210\n238.88\n0.77\n0.25\n0.0000395\n0.82\n80\n\n\ne1\n250\n200\n0.85\n0.10\nConstant\nTest_Train\n183\n239.01\n0.77\n0.25\n0.0000429\n0.83\n80\n\n\ne1\n61\n500\n0.95\n0.10\nConstant\nTest_Train\n46\n239.02\n0.77\n0.25\n0.0000410\n0.81\n80\n\n\ne1\n100\n100\n0.85\n0.01\nConstant\nTest_Train\n465\n239.10\n0.77\n0.25\n0.0000429\n0.83\n80\n\n\ne1\n65\n300\n0.95\n0.10\nConstant\nTest_Train\n28\n239.92\n0.77\n0.25\n0.0000423\n0.82\n80\n\n\ne1\n150\n150\n0.85\n0.10\nConstant\nTest_Train\n89\n240.04\n0.77\n0.25\n0.0000453\n0.84\n80\n\n\ne1\n60\n150\n0.85\n0.10\nConstant\nTest_Train\n17\n240.55\n0.76\n0.25\n0.0000466\n0.83\n80\n\n\ne1\n51\n300\n0.95\n0.15\nConstant\nTest_Train\n137\n240.77\n0.77\n0.25\n0.0000424\n0.87\n80\n\n\ne1\n105\n100\n0.85\n0.05\nConstant\nTest_Train\n212\n241.24\n0.76\n0.26\n0.0000480\n0.91\n80\n\n\ne1\n100\n100\n0.90\n0.10\nConstant\nTest_Train\n194\n242.04\n0.76\n0.25\n0.0000519\n0.94\n80\n\n\ne1\n100\n100\n0.90\n0.10\nConstant\nTrain\n30\n211.21\n0.50\n0.29\n0.0000284\n2.00\n135\n\n\ne1\n200\n300\n0.90\n0.10\nConstant\nTrain\n146\n220.24\n0.50\n0.29\n0.0001014\n2.08\n80\n\n\ne1\n70\n500\n0.85\n0.10\nConstant\nTrain\n84\n220.80\n0.48\n0.29\n0.0001000\n2.09\n80\n\n\ne1\n50\n500\n0.85\n0.05\nConstant\nTrain\n91\n221.54\n0.50\n0.29\n0.0000965\n2.08\n80\n\n\ne1\n60\n150\n0.85\n0.10\nConstant\nTrain\n32\n221.76\n0.50\n0.29\n0.0000750\n1.38\n80\n\n\ne1\n52\n600\n0.95\n0.18\nConstant\nTrain\n84\n224.05\n0.50\n0.29\n0.0001002\n2.01\n80\n\n\ne1\n150\n2500\n0.65\n0.10\nVaried\nTest\n847\n217.30\n0.72\n0.27\n0.0000745\n3.16\n76\n\n\ne1\n100\n2500\n0.65\n0.10\nVaried\nTest\n209\n217.57\n0.72\n0.27\n0.0000807\n3.11\n76\n\n\ne1\n200\n1500\n0.70\n0.10\nVaried\nTest\n402\n219.21\n0.71\n0.27\n0.0000771\n3.09\n76\n\n\ne1\n90\n900\n0.80\n0.09\nVaried\nTest\n101\n222.25\n0.71\n0.27\n0.0000799\n3.03\n76\n\n\ne1\n50\n900\n0.80\n0.12\nVaried\nTest\n55\n222.79\n0.71\n0.27\n0.0000727\n2.98\n76\n\n\ne1\n70\n500\n0.85\n0.10\nVaried\nTest\n177\n225.34\n0.70\n0.27\n0.0000810\n2.87\n76\n\n\ne1\n150\n500\n0.95\n0.10\nVaried\nTest\n112\n225.47\n0.70\n0.27\n0.0000810\n2.89\n76\n\n\ne1\n50\n900\n0.95\n0.10\nVaried\nTest\n37\n226.30\n0.70\n0.27\n0.0000811\n2.89\n76\n\n\ne1\n400\n300\n0.85\n0.04\nVaried\nTest\n337\n226.51\n0.69\n0.27\n0.0000845\n2.84\n76\n\n\ne1\n50\n500\n0.85\n0.05\nVaried\nTest\n201\n226.53\n0.70\n0.27\n0.0000860\n2.86\n76\n\n\ne1\n49\n800\n0.95\n0.18\nVaried\nTest\n286\n226.86\n0.69\n0.27\n0.0000783\n2.95\n76\n\n\ne1\n61\n500\n0.95\n0.10\nVaried\nTest\n41\n227.08\n0.69\n0.27\n0.0000855\n2.81\n76\n\n\ne1\n200\n300\n0.90\n0.10\nVaried\nTest\n129\n227.66\n0.69\n0.27\n0.0000844\n2.82\n76\n\n\ne1\n52\n600\n0.95\n0.18\nVaried\nTest\n199\n227.96\n0.69\n0.27\n0.0000787\n2.84\n76\n\n\ne1\n250\n200\n0.85\n0.10\nVaried\nTest\n176\n229.83\n0.68\n0.27\n0.0000894\n2.71\n76\n\n\ne1\n100\n100\n0.85\n0.01\nVaried\nTest\n372\n230.20\n0.68\n0.27\n0.0000877\n2.71\n76\n\n\ne1\n65\n300\n0.95\n0.10\nVaried\nTest\n29\n231.00\n0.67\n0.26\n0.0000874\n2.71\n76\n\n\ne1\n51\n300\n0.95\n0.15\nVaried\nTest\n149\n231.32\n0.68\n0.27\n0.0000827\n2.78\n76\n\n\ne1\n150\n150\n0.85\n0.10\nVaried\nTest\n72\n231.85\n0.68\n0.27\n0.0000879\n2.57\n76\n\n\ne1\n60\n150\n0.85\n0.10\nVaried\nTest\n24\n234.05\n0.66\n0.27\n0.0000959\n2.54\n76\n\n\ne1\n105\n100\n0.85\n0.05\nVaried\nTest\n228\n234.81\n0.67\n0.27\n0.0000966\n2.51\n76\n\n\ne1\n100\n100\n0.90\n0.10\nVaried\nTest\n162\n235.56\n0.66\n0.27\n0.0000958\n2.48\n76\n\n\ne1\n100\n2500\n0.65\n0.10\nVaried\nTest_Train\n317\n290.40\n0.54\n0.27\n0.0000764\n0.99\n76\n\n\ne1\n200\n1500\n0.70\n0.10\nVaried\nTest_Train\n466\n291.65\n0.54\n0.27\n0.0000776\n0.84\n76\n\n\ne1\n90\n900\n0.80\n0.09\nVaried\nTest_Train\n120\n293.35\n0.55\n0.27\n0.0000747\n0.78\n76\n\n\ne1\n50\n900\n0.80\n0.12\nVaried\nTest_Train\n71\n293.77\n0.56\n0.27\n0.0000750\n0.76\n76\n\n\ne1\n70\n500\n0.85\n0.10\nVaried\nTest_Train\n173\n295.56\n0.55\n0.27\n0.0000752\n0.73\n76\n\n\ne1\n49\n800\n0.95\n0.18\nVaried\nTest_Train\n263\n295.63\n0.55\n0.26\n0.0000721\n0.78\n76\n\n\ne1\n50\n500\n0.85\n0.05\nVaried\nTest_Train\n217\n295.74\n0.56\n0.27\n0.0000708\n0.71\n76\n\n\ne1\n50\n900\n0.95\n0.10\nVaried\nTest_Train\n38\n295.80\n0.55\n0.26\n0.0000748\n0.79\n76\n\n\ne1\n150\n500\n0.95\n0.10\nVaried\nTest_Train\n138\n295.84\n0.55\n0.27\n0.0000732\n0.75\n76\n\n\ne1\n52\n600\n0.95\n0.18\nVaried\nTest_Train\n210\n296.14\n0.56\n0.26\n0.0000705\n0.76\n76\n\n\ne1\n400\n300\n0.85\n0.04\nVaried\nTest_Train\n475\n296.24\n0.56\n0.26\n0.0000720\n0.74\n76\n\n\ne1\n61\n500\n0.95\n0.10\nVaried\nTest_Train\n46\n296.78\n0.56\n0.27\n0.0000710\n0.78\n76\n\n\ne1\n200\n300\n0.90\n0.10\nVaried\nTest_Train\n153\n296.83\n0.56\n0.27\n0.0000713\n0.72\n76\n\n\ne1\n100\n100\n0.85\n0.01\nVaried\nTest_Train\n465\n297.79\n0.55\n0.27\n0.0000734\n0.75\n76\n\n\ne1\n250\n200\n0.85\n0.10\nVaried\nTest_Train\n183\n298.19\n0.55\n0.27\n0.0000712\n0.71\n76\n\n\ne1\n65\n300\n0.95\n0.10\nVaried\nTest_Train\n28\n298.71\n0.56\n0.26\n0.0000735\n0.74\n76\n\n\ne1\n150\n150\n0.85\n0.10\nVaried\nTest_Train\n89\n299.31\n0.56\n0.26\n0.0000700\n0.71\n76\n\n\ne1\n51\n300\n0.95\n0.15\nVaried\nTest_Train\n137\n299.59\n0.55\n0.26\n0.0000738\n0.72\n76\n\n\ne1\n60\n150\n0.85\n0.10\nVaried\nTest_Train\n17\n300.04\n0.56\n0.26\n0.0000687\n0.69\n76\n\n\ne1\n105\n100\n0.85\n0.05\nVaried\nTest_Train\n212\n300.51\n0.56\n0.26\n0.0000690\n0.69\n76\n\n\ne1\n100\n100\n0.90\n0.10\nVaried\nTest_Train\n194\n301.28\n0.56\n0.27\n0.0000667\n0.70\n76\n\n\ne1\n100\n100\n0.90\n0.10\nVaried\nTrain\n30\n265.62\n0.50\n0.29\n0.0000226\n0.92\n131\n\n\ne1\n70\n500\n0.85\n0.10\nVaried\nTrain\n84\n312.06\n0.49\n0.29\n0.0000266\n1.51\n76\n\n\ne1\n50\n500\n0.85\n0.05\nVaried\nTrain\n91\n312.34\n0.51\n0.28\n0.0000261\n1.38\n76\n\n\ne1\n200\n300\n0.90\n0.10\nVaried\nTrain\n146\n313.11\n0.50\n0.29\n0.0000260\n1.40\n76\n\n\ne1\n52\n600\n0.95\n0.18\nVaried\nTrain\n84\n314.91\n0.49\n0.29\n0.0000267\n1.40\n76\n\n\ne1\n60\n150\n0.85\n0.10\nVaried\nTrain\n32\n315.65\n0.49\n0.29\n0.0000259\n1.14\n76\n\n\n\n\nCodeall_fits |&gt; \n  filter(id %in% unique(all_fits$id)[1:2]) |&gt;\n  group_by(id,exp,n_int,ntry,condit,Fit_Method) |&gt; \n  mutate(rank=rank(mean_error)) |&gt; filter(rank&lt;n_int*.90) |&gt;\n  group_by(id,exp,n_int,ntry,condit,Fit_Method) |&gt; \n  summarise(me=mean(mean_error),w=median(weight_exam),sd_w=sd(weight_exam),c=median(c),lr=median(lr)) |&gt;\n  arrange(id,condit,Fit_Method,me) |&gt; \n    kbl() |&gt; kable_styling(full_width = F)\n\n\n\nid\nexp\nn_int\nntry\ncondit\nFit_Method\nme\nw\nsd_w\nc\nlr\n\n\n\n1\ne1\n100\n2500\nVaried\nTest\n223.5378\n0.8182305\n0.2035620\n0.0000019\n5.2816922\n\n\n1\ne1\n150\n2500\nVaried\nTest\n223.5554\n0.8585789\n0.1787275\n0.0000022\n5.1924980\n\n\n1\ne1\n50\n900\nVaried\nTest\n223.6162\n0.7473703\n0.1897786\n0.0000019\n5.2693800\n\n\n1\ne1\n150\n500\nVaried\nTest\n224.6697\n0.8149536\n0.1970840\n0.0000020\n5.1997612\n\n\n1\ne1\n200\n1500\nVaried\nTest\n225.4042\n0.7649556\n0.2123507\n0.0000019\n5.2531997\n\n\n1\ne1\n90\n900\nVaried\nTest\n226.6014\n0.7998805\n0.2525561\n0.0000020\n5.1910494\n\n\n1\ne1\n100\n100\nVaried\nTest\n228.0284\n0.8293703\n0.2262617\n0.0000023\n5.1435685\n\n\n1\ne1\n70\n500\nVaried\nTest\n230.9324\n0.8355013\n0.2819217\n0.0000020\n5.1279383\n\n\n1\ne1\n400\n300\nVaried\nTest\n231.2286\n0.8104574\n0.2420737\n0.0000023\n5.1076418\n\n\n1\ne1\n200\n300\nVaried\nTest\n232.2169\n0.8051852\n0.2395735\n0.0000023\n5.0557794\n\n\n1\ne1\n50\n500\nVaried\nTest\n232.5206\n0.8222143\n0.2435945\n0.0000021\n5.0075482\n\n\n1\ne1\n52\n600\nVaried\nTest\n233.0113\n0.8626639\n0.1970062\n0.0000830\n2.6120010\n\n\n1\ne1\n49\n800\nVaried\nTest\n233.2076\n0.8204706\n0.1984141\n0.0000023\n5.0773731\n\n\n1\ne1\n250\n200\nVaried\nTest\n233.5362\n0.7986295\n0.2476705\n0.0000021\n5.0901121\n\n\n1\ne1\n61\n500\nVaried\nTest\n233.7241\n0.8264059\n0.2336978\n0.0000023\n5.0624427\n\n\n1\ne1\n60\n150\nVaried\nTest\n235.2363\n0.7849864\n0.2606166\n0.0000024\n5.0587135\n\n\n1\ne1\n65\n300\nVaried\nTest\n236.8494\n0.7526598\n0.2464510\n0.0000021\n5.1046996\n\n\n1\ne1\n51\n300\nVaried\nTest\n236.8643\n0.8326864\n0.2732785\n0.0000668\n4.6356408\n\n\n1\ne1\n150\n150\nVaried\nTest\n238.8422\n0.8057119\n0.2588631\n0.0000046\n4.8535159\n\n\n1\ne1\n105\n100\nVaried\nTest\n240.9037\n0.7794698\n0.2547383\n0.0001021\n1.8109660\n\n\n1\ne1\n100\n2500\nVaried\nTest_Train\n291.1208\n0.4982487\n0.2149724\n0.0000627\n0.1050512\n\n\n1\ne1\n90\n900\nVaried\nTest_Train\n291.8641\n0.4633852\n0.1755527\n0.0000643\n0.1239611\n\n\n1\ne1\n200\n1500\nVaried\nTest_Train\n292.0506\n0.4656865\n0.1840448\n0.0000650\n0.1342331\n\n\n1\ne1\n50\n900\nVaried\nTest_Train\n292.7664\n0.4828479\n0.2012370\n0.0000584\n0.0823038\n\n\n1\ne1\n150\n500\nVaried\nTest_Train\n293.0012\n0.4539042\n0.1576420\n0.0000639\n0.1014355\n\n\n1\ne1\n70\n500\nVaried\nTest_Train\n294.7952\n0.4768128\n0.1804256\n0.0000711\n0.1343180\n\n\n1\ne1\n400\n300\nVaried\nTest_Train\n294.8046\n0.4961300\n0.1948949\n0.0000650\n0.1460962\n\n\n1\ne1\n50\n500\nVaried\nTest_Train\n294.8834\n0.4763972\n0.1708669\n0.0000729\n0.1072454\n\n\n1\ne1\n49\n800\nVaried\nTest_Train\n295.0383\n0.4812279\n0.1352303\n0.0000736\n0.1449286\n\n\n1\ne1\n100\n100\nVaried\nTest_Train\n295.4118\n0.4917216\n0.1817776\n0.0000666\n0.1636531\n\n\n1\ne1\n52\n600\nVaried\nTest_Train\n295.4456\n0.4399127\n0.1818114\n0.0000624\n0.0987222\n\n\n1\ne1\n200\n300\nVaried\nTest_Train\n295.9199\n0.4870067\n0.2079155\n0.0000664\n0.1371202\n\n\n1\ne1\n250\n200\nVaried\nTest_Train\n296.5507\n0.4909003\n0.1756808\n0.0000672\n0.1867037\n\n\n1\ne1\n105\n100\nVaried\nTest_Train\n297.3157\n0.4826085\n0.1881216\n0.0000674\n0.2043142\n\n\n1\ne1\n51\n300\nVaried\nTest_Train\n297.3525\n0.4793596\n0.2287758\n0.0000671\n0.2355805\n\n\n1\ne1\n61\n500\nVaried\nTest_Train\n297.3594\n0.4408326\n0.1632677\n0.0000699\n0.1775518\n\n\n1\ne1\n60\n150\nVaried\nTest_Train\n297.5422\n0.5341098\n0.1860066\n0.0000555\n0.1508889\n\n\n1\ne1\n150\n150\nVaried\nTest_Train\n298.1399\n0.5021449\n0.1864520\n0.0000740\n0.1710864\n\n\n1\ne1\n65\n300\nVaried\nTest_Train\n300.3247\n0.4955346\n0.1788877\n0.0000656\n0.2270260\n\n\n1\ne1\n200\n300\nVaried\nTrain\n301.0392\n0.4510923\n0.2769286\n0.0000187\n8.0239746\n\n\n1\ne1\n70\n500\nVaried\nTrain\n301.9224\n0.5754325\n0.2973605\n0.0000418\n7.9297719\n\n\n1\ne1\n50\n500\nVaried\nTrain\n302.6788\n0.5817460\n0.2974083\n0.0000424\n8.1026574\n\n\n1\ne1\n60\n150\nVaried\nTrain\n305.4690\n0.4887447\n0.2976968\n0.0000476\n7.9030316\n\n\n1\ne1\n52\n600\nVaried\nTrain\n306.0869\n0.4761011\n0.3128097\n0.0000496\n7.8344141\n\n\n1\ne1\n100\n100\nVaried\nTrain\n306.6696\n0.5408164\n0.2728925\n0.0000464\n7.8161855\n\n\n2\ne1\n100\n2500\nVaried\nTest\n369.8400\n0.5863439\n0.2981269\n0.0000348\n4.0355545\n\n\n2\ne1\n150\n2500\nVaried\nTest\n369.8738\n0.5306497\n0.3050162\n0.0000338\n5.0224378\n\n\n2\ne1\n150\n500\nVaried\nTest\n370.1960\n0.5485465\n0.2928199\n0.0000335\n5.0353815\n\n\n2\ne1\n50\n900\nVaried\nTest\n370.5906\n0.4917389\n0.2895874\n0.0000317\n5.1695962\n\n\n2\ne1\n200\n1500\nVaried\nTest\n371.4073\n0.4989873\n0.2888450\n0.0000336\n5.0766927\n\n\n2\ne1\n90\n900\nVaried\nTest\n374.1310\n0.5857368\n0.2691902\n0.0000345\n4.9535572\n\n\n2\ne1\n100\n100\nVaried\nTest\n374.7765\n0.5144509\n0.2778470\n0.0000341\n4.7317420\n\n\n2\ne1\n49\n800\nVaried\nTest\n376.1818\n0.4814195\n0.3264464\n0.0000352\n4.4422502\n\n\n2\ne1\n61\n500\nVaried\nTest\n376.4798\n0.6001896\n0.2867825\n0.0000353\n4.0377088\n\n\n2\ne1\n400\n300\nVaried\nTest\n377.7010\n0.4745654\n0.2921842\n0.0000345\n4.0564093\n\n\n2\ne1\n50\n500\nVaried\nTest\n377.7084\n0.4700296\n0.2752379\n0.0000385\n4.0132306\n\n\n2\ne1\n200\n300\nVaried\nTest\n377.9897\n0.4644357\n0.3013704\n0.0000351\n4.1080690\n\n\n2\ne1\n70\n500\nVaried\nTest\n378.6938\n0.5585721\n0.3168538\n0.0000356\n4.0292617\n\n\n2\ne1\n51\n300\nVaried\nTest\n380.1584\n0.4959267\n0.2805370\n0.0000381\n4.0102128\n\n\n2\ne1\n65\n300\nVaried\nTest\n381.2897\n0.4558618\n0.3250864\n0.0000353\n4.0229159\n\n\n2\ne1\n52\n600\nVaried\nTest\n381.8625\n0.6456892\n0.3142880\n0.0000380\n3.9758295\n\n\n2\ne1\n250\n200\nVaried\nTest\n382.6800\n0.5574183\n0.2947822\n0.0000368\n4.0376555\n\n\n2\ne1\n105\n100\nVaried\nTest\n386.5428\n0.4663750\n0.3017272\n0.0000374\n4.1107081\n\n\n2\ne1\n150\n150\nVaried\nTest\n386.6443\n0.5275578\n0.3094980\n0.0000363\n4.0164842\n\n\n2\ne1\n60\n150\nVaried\nTest\n387.5823\n0.4715924\n0.3332044\n0.0000386\n4.0013609\n\n\n2\ne1\n50\n900\nVaried\nTest_Train\n428.1180\n0.4978363\n0.2970980\n0.0000385\n4.9800328\n\n\n2\ne1\n150\n500\nVaried\nTest_Train\n429.5278\n0.4630116\n0.2978593\n0.0000375\n4.9235301\n\n\n2\ne1\n100\n2500\nVaried\nTest_Train\n429.7416\n0.4441688\n0.2806442\n0.0000385\n4.8920111\n\n\n2\ne1\n200\n1500\nVaried\nTest_Train\n432.2028\n0.4870858\n0.2860881\n0.0000373\n4.8960848\n\n\n2\ne1\n49\n800\nVaried\nTest_Train\n436.2951\n0.3925758\n0.2483927\n0.0000358\n4.9014996\n\n\n2\ne1\n100\n100\nVaried\nTest_Train\n438.4416\n0.4760461\n0.3051119\n0.0000351\n4.8056798\n\n\n2\ne1\n90\n900\nVaried\nTest_Train\n439.2872\n0.5689515\n0.3051232\n0.0000354\n4.9116788\n\n\n2\ne1\n52\n600\nVaried\nTest_Train\n441.2831\n0.5517423\n0.3000017\n0.0000357\n4.6911650\n\n\n2\ne1\n61\n500\nVaried\nTest_Train\n441.9809\n0.4955319\n0.2817430\n0.0000361\n4.9272707\n\n\n2\ne1\n70\n500\nVaried\nTest_Train\n442.1006\n0.6065832\n0.3080914\n0.0000364\n4.9111936\n\n\n2\ne1\n65\n300\nVaried\nTest_Train\n443.6291\n0.4970180\n0.2848160\n0.0000341\n4.8086849\n\n\n2\ne1\n400\n300\nVaried\nTest_Train\n444.0730\n0.5335898\n0.2966844\n0.0000351\n4.9310083\n\n\n2\ne1\n50\n500\nVaried\nTest_Train\n444.9344\n0.5975558\n0.2789440\n0.0000351\n4.7531000\n\n\n2\ne1\n200\n300\nVaried\nTest_Train\n445.7221\n0.4626835\n0.2983041\n0.0000358\n4.9009381\n\n\n2\ne1\n51\n300\nVaried\nTest_Train\n448.7055\n0.3941551\n0.2908179\n0.0000349\n4.9279146\n\n\n2\ne1\n250\n200\nVaried\nTest_Train\n451.1523\n0.4805013\n0.2915747\n0.0000357\n4.8348896\n\n\n2\ne1\n60\n150\nVaried\nTest_Train\n455.9307\n0.4307338\n0.2992679\n0.0000349\n4.9789964\n\n\n2\ne1\n150\n150\nVaried\nTest_Train\n456.8058\n0.6124759\n0.3025009\n0.0000359\n4.7546298\n\n\n2\ne1\n105\n100\nVaried\nTest_Train\n457.5066\n0.5686353\n0.2888873\n0.0000361\n4.7358573\n\n\n2\ne1\n200\n300\nVaried\nTrain\n445.4326\n0.4992835\n0.2903908\n0.0000143\n1.2852145\n\n\n2\ne1\n50\n500\nVaried\nTrain\n446.0390\n0.4863962\n0.2996348\n0.0000139\n1.2527575\n\n\n2\ne1\n52\n600\nVaried\nTrain\n446.2512\n0.4010491\n0.2681370\n0.0000138\n1.2558990\n\n\n2\ne1\n70\n500\nVaried\nTrain\n446.4922\n0.4650381\n0.2920932\n0.0000139\n1.2373839\n\n\n2\ne1\n60\n150\nVaried\nTrain\n447.9030\n0.5259795\n0.2935894\n0.0000131\n1.1056549\n\n\n2\ne1\n100\n100\nVaried\nTrain\n448.5521\n0.4988236\n0.2767507\n0.0000149\n1.0656699\n\n\n\n\nCodek = all_fits |&gt; \n    filter(id %in% unique(all_fits$id)[1])  |&gt;\n  group_by(Fit_Method) |&gt; arrange(mean_error)\n\nid_mdif &lt;- all_fits |&gt; \n    group_by(id,condit,Fit_Method,run_name) |&gt; \n  mutate(rank=rank(mean_error)) |&gt;\n  summarize(n=n(),all_me=mean(mean_error),\n            p50=mean(mean_error[rank&lt;n*.50]),\n         top10 = mean(mean_error[rank&lt;10])) \n\nid_wdif &lt;- all_fits |&gt; \n    group_by(id,condit,Fit_Method,run_name) |&gt; \n  mutate(rank=rank(mean_error)) |&gt;\n  summarize(n=n(),all_we=mean(weight_exam),\n            p50=mean(weight_exam[rank&lt;n*.50]),\n         top10 = mean(weight_exam[rank&lt;10])) |&gt;\n  mutate(dif=all_we-top10,adif=abs(dif)) |&gt; arrange(adif)\n\nid_wdifAll &lt;- all_fits |&gt; \n    group_by(id,condit,Fit_Method) |&gt; \n  mutate(rank=rank(mean_error)) |&gt;\n  summarize(n=n(),all_we=median(weight_exam),\n            p50=median(weight_exam[rank&lt;n*.50]),\n         top10 = median(weight_exam[rank&lt;30])) |&gt;\n  mutate(dif=all_we-top10,adif=abs(dif)) |&gt; arrange(-adif)\n\n\n\nCode# ind_fits_df |&gt; group_by(id,condit,Fit_Method) |&gt; summarise(w=median(weight_exam),me=mean(mean_error))\n# \n# ind_fits_df |&gt; group_by(id,condit,Fit_Method) |&gt; summarise(we=median(weight_exam),me=mean(mean_error)) |&gt;\n#   group_by(condit,Fit_Method) |&gt; summarise(w=mean(we),sd_w=sd(we),me=mean(me)) \n# \n# ind_fits_df |&gt; \n#   group_by(condit,Fit_Method) |&gt; \n#   summarise(w=median(weight_exam),sd_w=sd(weight_exam),me=mean(mean_error)) \n\n{all_fits |&gt; \n   filter(Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +geom_density(alpha=.5) } /{\n\nall_fits |&gt; \n   filter(Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +geom_density(alpha=.5) +\n  facet_wrap(~run_name)\n    }\n\n\n\n\n\n\nCodeall_fits |&gt; ggplot(aes(x=condit,y=weight_exam,col=condit)) + stat_pointinterval() +\n  facet_wrap(~Fit_Method)\n\n\n\n\n\n\nCodeall_fits |&gt; ggplot(aes(x=run_name,y=weight_exam,col=condit)) + stat_pointinterval() +\n  facet_wrap(condit~Fit_Method)\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +\n  geom_density() +\n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  ggh4x::facet_wrap2(~id+condit, scales=\"free_y\")\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test_Train\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +\n  geom_density() +\n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  ggh4x::facet_wrap2(~id+condit, scales=\"free_y\")\n\n\n\n\n\n\nCodeall_fits |&gt; \n    group_by(id,run_name,condit,Fit_Method) |&gt; \n  mutate(rank=rank(mean_error)) |&gt; \n  filter(rank&lt;n_int*.50) |&gt;\n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +\n  geom_density() +\n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  ggh4x::facet_wrap2(~id+condit, scales=\"free_y\")\n\n\n\n\n\n\nCodeall_fits |&gt; \n  group_by(id,condit,Fit_Method) |&gt;\n  mutate(we_med=median(weight_exam),\n         Best_Model=case_when(we_med&gt;.5 ~\"EXAM\",we_med&lt;.5 ~\"ALM\")) |&gt;\n  filter(Fit_Method==\"Test\") |&gt;\n  ungroup() |&gt;\n  mutate(id=reorder(id,we_med,decreasing = TRUE)) |&gt;\n  ggplot(aes(x=weight_exam,y=id,col=Best_Model)) + \n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  stat_pointinterval() + \n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  theme_minimal() +\n  theme(legend.position=\"top\")\n\n\n\n\n\n\nCodeall_fits |&gt; \n  filter(Fit_Method==\"Test\") |&gt;\n    group_by(id,run_name,condit,Fit_Method) |&gt; \n  mutate(rank=rank(mean_error)) |&gt; \n  filter(rank&lt;n_int*.50) |&gt;\n  group_by(id,condit,Fit_Method,run_name) |&gt;\n  mutate(we_med=median(weight_exam),\n         Best_Model=case_when(we_med&gt;.5 ~\"EXAM\",we_med&lt;.5 ~\"ALM\")) |&gt;\n  ungroup() |&gt;\n  group_by(run_name) |&gt;\n    mutate(id=reorder(id,we_med,decreasing = FALSE)) |&gt;\n  ggplot(aes(x=weight_exam,y=id,col=Best_Model)) + \n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  stat_pointinterval() + \n  ggh4x::facet_grid2(run_name~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  theme_minimal() +\n  theme(legend.position=\"top\")\n\n\n\n\n\n\nCodeall_fits |&gt; ggplot(aes(x=condit,y=weight_exam,col=run_name)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n  facet_wrap(condit~Fit_Method)\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +geom_density(alpha=.5) \n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test\") |&gt;\n  ggplot(aes(x=run_name,y=weight_exam,col=run_name)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n  ggh4x::facet_wrap2(~id+condit) + \n  theme(axis.text.x = element_blank()) \n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test_Train\") |&gt;\n  ggplot(aes(x=run_name,y=weight_exam,col=run_name)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n  ggh4x::facet_wrap2(~id+condit) + \n  theme(axis.text.x = element_blank())",
    "crumbs": [
      "Model",
      "HTW Hybrid Models"
    ]
  },
  {
    "objectID": "Model/hybrid.html#e2",
    "href": "Model/hybrid.html#e2",
    "title": "HTW Hybrid Models",
    "section": "E2",
    "text": "E2\n\nCode# folder_names &lt;- c(\"e2_hybrid_n_iter_60_ntry_150_3658\",\"e2_hybrid_n_iter_250_ntry_200_0540\",\"e2_hybrid_n_iter_150_ntry_150_2904\",\"e2_hybrid_n_iter_100_ntry_100_3211\")\n\nfolder_names &lt;- list.files(here(\"data/abc_reject\"),pattern=\"e2_hyb*\")\n\n\nall_fits &lt;- map_df(folder_names, process_folder)\n\nall_fits |&gt; \n  group_by(exp,n_int,ntry,tolM,ar,condit,Fit_Method) |&gt; \n  summarise(min=first(min),me=mean(mean_error),\n            w=median(weight_exam),sd_w=sd(weight_exam),c=median(c),\n            lr=median(lr),n=n_distinct(id)) |&gt;\n   mutate(across(c(me, w, lr, sd_w), ~round(., 2))) |&gt;\n  arrange(condit,Fit_Method,me) |&gt; kable(caption=\"E2 Fit Comparisons\") |&gt; \n  kable_styling(full_width = F) |&gt;\n  column_spec(8,bold=T,border_left=T) \n\n\nE2 Fit Comparisons\n\nexp\nn_int\nntry\ntolM\nar\ncondit\nFit_Method\nmin\nme\nw\nsd_w\nc\nlr\nn\n\n\n\ne2\n100\n2500\n0.65\n0.10\nConstant\nTest\n171\n221.67\n0.89\n0.21\n0.0000892\n1.82\n55\n\n\ne2\n200\n1500\n0.70\n0.10\nConstant\nTest\n268\n222.80\n0.88\n0.21\n0.0000989\n1.82\n55\n\n\ne2\n90\n900\n0.80\n0.09\nConstant\nTest\n68\n224.39\n0.87\n0.22\n0.0001410\n1.84\n55\n\n\ne2\n50\n900\n0.80\n0.12\nConstant\nTest\n33\n225.12\n0.87\n0.22\n0.0001354\n1.82\n55\n\n\ne2\n70\n500\n0.85\n0.10\nConstant\nTest\n87\n227.01\n0.86\n0.22\n0.0001926\n1.86\n55\n\n\ne2\n150\n500\n0.95\n0.10\nConstant\nTest\n81\n227.14\n0.86\n0.22\n0.0001649\n1.82\n55\n\n\ne2\n400\n300\n0.85\n0.04\nConstant\nTest\n249\n227.22\n0.86\n0.22\n0.0001926\n1.85\n55\n\n\ne2\n50\n500\n0.85\n0.05\nConstant\nTest\n108\n227.28\n0.86\n0.22\n0.0001984\n1.84\n55\n\n\ne2\n50\n900\n0.95\n0.10\nConstant\nTest\n32\n228.36\n0.86\n0.23\n0.0001890\n1.84\n55\n\n\ne2\n52\n600\n0.95\n0.18\nConstant\nTest\n118\n228.78\n0.86\n0.22\n0.0002007\n1.87\n55\n\n\ne2\n61\n500\n0.95\n0.10\nConstant\nTest\n24\n228.91\n0.85\n0.22\n0.0002036\n1.89\n55\n\n\ne2\n49\n800\n0.95\n0.18\nConstant\nTest\n139\n228.94\n0.86\n0.22\n0.0001620\n1.84\n55\n\n\ne2\n250\n200\n0.85\n0.10\nConstant\nTest\n74\n229.10\n0.86\n0.22\n0.0002492\n1.88\n55\n\n\ne2\n100\n100\n0.85\n0.01\nConstant\nTest\n225\n229.42\n0.86\n0.22\n0.0002304\n1.85\n55\n\n\ne2\n65\n300\n0.95\n0.10\nConstant\nTest\n15\n230.22\n0.86\n0.22\n0.0002432\n1.88\n55\n\n\ne2\n150\n150\n0.85\n0.10\nConstant\nTest\n57\n230.44\n0.85\n0.23\n0.0002646\n1.87\n55\n\n\ne2\n51\n300\n0.95\n0.15\nConstant\nTest\n81\n231.20\n0.84\n0.23\n0.0002761\n1.87\n55\n\n\ne2\n60\n150\n0.85\n0.10\nConstant\nTest\n12\n231.32\n0.84\n0.23\n0.0003111\n1.90\n55\n\n\ne2\n105\n100\n0.85\n0.05\nConstant\nTest\n126\n232.04\n0.85\n0.22\n0.0003055\n1.87\n55\n\n\ne2\n100\n100\n0.85\n0.10\nConstant\nTest\n92\n232.82\n0.84\n0.22\n0.0003746\n1.94\n55\n\n\ne2\n100\n2500\n0.65\n0.10\nConstant\nTest_Train\n172\n221.92\n0.84\n0.24\n0.0000962\n0.38\n55\n\n\ne2\n200\n1500\n0.70\n0.10\nConstant\nTest_Train\n301\n222.68\n0.84\n0.24\n0.0001096\n0.42\n55\n\n\ne2\n90\n900\n0.80\n0.09\nConstant\nTest_Train\n62\n223.95\n0.84\n0.24\n0.0001247\n0.48\n55\n\n\ne2\n50\n900\n0.80\n0.12\nConstant\nTest_Train\n39\n224.60\n0.84\n0.23\n0.0001343\n0.53\n55\n\n\ne2\n400\n300\n0.85\n0.04\nConstant\nTest_Train\n191\n225.77\n0.83\n0.24\n0.0001481\n0.69\n55\n\n\ne2\n70\n500\n0.85\n0.10\nConstant\nTest_Train\n95\n225.87\n0.82\n0.24\n0.0001523\n0.67\n55\n\n\ne2\n50\n500\n0.85\n0.05\nConstant\nTest_Train\n118\n225.89\n0.83\n0.24\n0.0001450\n0.64\n55\n\n\ne2\n150\n500\n0.95\n0.10\nConstant\nTest_Train\n74\n225.96\n0.82\n0.24\n0.0001472\n0.66\n55\n\n\ne2\n100\n100\n0.85\n0.01\nConstant\nTest_Train\n236\n227.13\n0.82\n0.24\n0.0001532\n0.74\n55\n\n\ne2\n250\n200\n0.85\n0.10\nConstant\nTest_Train\n83\n227.25\n0.82\n0.24\n0.0001663\n0.88\n55\n\n\ne2\n49\n800\n0.95\n0.18\nConstant\nTest_Train\n152\n227.38\n0.83\n0.24\n0.0001500\n0.76\n55\n\n\ne2\n50\n900\n0.95\n0.10\nConstant\nTest_Train\n31\n227.49\n0.81\n0.25\n0.0001571\n0.69\n55\n\n\ne2\n52\n600\n0.95\n0.18\nConstant\nTest_Train\n130\n227.65\n0.82\n0.24\n0.0001712\n0.80\n55\n\n\ne2\n61\n500\n0.95\n0.10\nConstant\nTest_Train\n23\n227.79\n0.81\n0.24\n0.0001700\n0.89\n55\n\n\ne2\n150\n150\n0.85\n0.10\nConstant\nTest_Train\n52\n227.94\n0.82\n0.24\n0.0001829\n1.03\n55\n\n\ne2\n105\n100\n0.85\n0.05\nConstant\nTest_Train\n124\n228.84\n0.82\n0.24\n0.0002131\n1.22\n55\n\n\ne2\n51\n300\n0.95\n0.15\nConstant\nTest_Train\n90\n228.86\n0.82\n0.24\n0.0001958\n1.20\n55\n\n\ne2\n60\n150\n0.85\n0.10\nConstant\nTest_Train\n15\n228.86\n0.81\n0.24\n0.0002271\n1.66\n55\n\n\ne2\n65\n300\n0.95\n0.10\nConstant\nTest_Train\n18\n229.04\n0.81\n0.24\n0.0001718\n0.96\n55\n\n\ne2\n100\n100\n0.85\n0.10\nConstant\nTest_Train\n91\n229.41\n0.82\n0.24\n0.0002158\n1.61\n55\n\n\ne2\n50\n500\n0.85\n0.05\nConstant\nTrain\n36\n193.84\n0.50\n0.29\n0.0000033\n2.43\n55\n\n\ne2\n70\n500\n0.85\n0.10\nConstant\nTrain\n28\n193.88\n0.50\n0.29\n0.0000030\n2.40\n55\n\n\ne2\n100\n100\n0.85\n0.10\nConstant\nTrain\n31\n194.16\n0.49\n0.29\n0.0000028\n2.33\n55\n\n\ne2\n60\n150\n0.85\n0.10\nConstant\nTrain\n5\n194.66\n0.51\n0.29\n0.0000032\n2.35\n55\n\n\ne2\n52\n600\n0.95\n0.18\nConstant\nTrain\n31\n196.66\n0.50\n0.29\n0.0000037\n2.45\n55\n\n\ne2\n100\n2500\n0.65\n0.10\nVaried\nTest\n171\n208.73\n0.87\n0.26\n0.0001168\n1.95\n55\n\n\ne2\n200\n1500\n0.70\n0.10\nVaried\nTest\n268\n209.95\n0.86\n0.26\n0.0001098\n1.95\n55\n\n\ne2\n90\n900\n0.80\n0.09\nVaried\nTest\n68\n211.50\n0.85\n0.27\n0.0001103\n1.89\n55\n\n\ne2\n50\n900\n0.80\n0.12\nVaried\nTest\n33\n212.52\n0.85\n0.26\n0.0000967\n1.87\n55\n\n\ne2\n150\n500\n0.95\n0.10\nVaried\nTest\n81\n214.26\n0.84\n0.26\n0.0001105\n1.88\n55\n\n\ne2\n50\n500\n0.85\n0.05\nVaried\nTest\n108\n214.52\n0.85\n0.27\n0.0000934\n1.83\n55\n\n\ne2\n400\n300\n0.85\n0.04\nVaried\nTest\n249\n214.62\n0.84\n0.26\n0.0001073\n1.87\n55\n\n\ne2\n70\n500\n0.85\n0.10\nVaried\nTest\n87\n214.71\n0.84\n0.27\n0.0001021\n1.88\n55\n\n\ne2\n50\n900\n0.95\n0.10\nVaried\nTest\n32\n215.29\n0.84\n0.27\n0.0000970\n1.90\n55\n\n\ne2\n52\n600\n0.95\n0.18\nVaried\nTest\n118\n215.97\n0.84\n0.26\n0.0001130\n1.93\n55\n\n\ne2\n49\n800\n0.95\n0.18\nVaried\nTest\n139\n216.30\n0.84\n0.26\n0.0001029\n1.90\n55\n\n\ne2\n61\n500\n0.95\n0.10\nVaried\nTest\n24\n216.51\n0.84\n0.27\n0.0001072\n1.91\n55\n\n\ne2\n250\n200\n0.85\n0.10\nVaried\nTest\n74\n216.66\n0.83\n0.26\n0.0001020\n1.87\n55\n\n\ne2\n100\n100\n0.85\n0.01\nVaried\nTest\n225\n217.90\n0.82\n0.27\n0.0000965\n1.85\n55\n\n\ne2\n150\n150\n0.85\n0.10\nVaried\nTest\n57\n217.98\n0.83\n0.27\n0.0001039\n1.85\n55\n\n\ne2\n65\n300\n0.95\n0.10\nVaried\nTest\n15\n218.08\n0.83\n0.27\n0.0000977\n1.90\n55\n\n\ne2\n51\n300\n0.95\n0.15\nVaried\nTest\n81\n219.08\n0.83\n0.27\n0.0001295\n1.85\n55\n\n\ne2\n60\n150\n0.85\n0.10\nVaried\nTest\n12\n219.11\n0.83\n0.26\n0.0001021\n1.83\n55\n\n\ne2\n105\n100\n0.85\n0.05\nVaried\nTest\n126\n219.59\n0.82\n0.26\n0.0001012\n1.82\n55\n\n\ne2\n100\n100\n0.85\n0.10\nVaried\nTest\n92\n220.43\n0.81\n0.26\n0.0001067\n1.81\n55\n\n\ne2\n100\n2500\n0.65\n0.10\nVaried\nTest_Train\n172\n219.33\n0.89\n0.30\n0.0000330\n0.60\n55\n\n\ne2\n200\n1500\n0.70\n0.10\nVaried\nTest_Train\n301\n220.15\n0.88\n0.29\n0.0000330\n0.62\n55\n\n\ne2\n50\n900\n0.80\n0.12\nVaried\nTest_Train\n39\n221.27\n0.87\n0.29\n0.0000316\n0.62\n55\n\n\ne2\n90\n900\n0.80\n0.09\nVaried\nTest_Train\n62\n221.29\n0.86\n0.29\n0.0000305\n0.64\n55\n\n\ne2\n70\n500\n0.85\n0.10\nVaried\nTest_Train\n95\n222.78\n0.85\n0.29\n0.0000291\n0.67\n55\n\n\ne2\n150\n500\n0.95\n0.10\nVaried\nTest_Train\n74\n223.03\n0.85\n0.28\n0.0000292\n0.67\n55\n\n\ne2\n50\n500\n0.85\n0.05\nVaried\nTest_Train\n118\n223.40\n0.85\n0.28\n0.0000289\n0.67\n55\n\n\ne2\n400\n300\n0.85\n0.04\nVaried\nTest_Train\n191\n223.41\n0.84\n0.28\n0.0000281\n0.68\n55\n\n\ne2\n50\n900\n0.95\n0.10\nVaried\nTest_Train\n31\n223.58\n0.86\n0.29\n0.0000299\n0.71\n55\n\n\ne2\n61\n500\n0.95\n0.10\nVaried\nTest_Train\n23\n223.86\n0.84\n0.28\n0.0000292\n0.70\n55\n\n\ne2\n49\n800\n0.95\n0.18\nVaried\nTest_Train\n152\n224.02\n0.85\n0.28\n0.0000329\n0.69\n55\n\n\ne2\n52\n600\n0.95\n0.18\nVaried\nTest_Train\n130\n224.57\n0.85\n0.28\n0.0000307\n0.68\n55\n\n\ne2\n250\n200\n0.85\n0.10\nVaried\nTest_Train\n83\n224.70\n0.82\n0.28\n0.0000269\n0.71\n55\n\n\ne2\n100\n100\n0.85\n0.01\nVaried\nTest_Train\n236\n224.85\n0.83\n0.28\n0.0000274\n0.70\n55\n\n\ne2\n65\n300\n0.95\n0.10\nVaried\nTest_Train\n18\n225.46\n0.83\n0.28\n0.0000273\n0.72\n55\n\n\ne2\n150\n150\n0.85\n0.10\nVaried\nTest_Train\n52\n225.70\n0.81\n0.28\n0.0000255\n0.73\n55\n\n\ne2\n51\n300\n0.95\n0.15\nVaried\nTest_Train\n90\n226.28\n0.83\n0.28\n0.0000276\n0.74\n55\n\n\ne2\n105\n100\n0.85\n0.05\nVaried\nTest_Train\n124\n226.63\n0.80\n0.28\n0.0000251\n0.72\n55\n\n\ne2\n60\n150\n0.85\n0.10\nVaried\nTest_Train\n15\n226.77\n0.80\n0.28\n0.0000256\n0.76\n55\n\n\ne2\n100\n100\n0.85\n0.10\nVaried\nTest_Train\n91\n227.37\n0.79\n0.27\n0.0000253\n0.77\n55\n\n\ne2\n70\n500\n0.85\n0.10\nVaried\nTrain\n28\n192.05\n0.50\n0.29\n0.0000191\n0.68\n55\n\n\ne2\n50\n500\n0.85\n0.05\nVaried\nTrain\n36\n193.06\n0.50\n0.29\n0.0000194\n0.69\n55\n\n\ne2\n100\n100\n0.85\n0.10\nVaried\nTrain\n31\n193.62\n0.51\n0.29\n0.0000190\n0.70\n55\n\n\ne2\n60\n150\n0.85\n0.10\nVaried\nTrain\n5\n193.70\n0.50\n0.29\n0.0000190\n0.70\n55\n\n\ne2\n52\n600\n0.95\n0.18\nVaried\nTrain\n31\n195.83\n0.51\n0.29\n0.0000198\n0.71\n55\n\n\n\n\nCode{all_fits |&gt; \n   filter(Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +geom_density(alpha=.5) } /{\n\nall_fits |&gt; \n   filter(Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +geom_density(alpha=.5) +\n  facet_wrap(~run_name)\n    }\n\n\n\n\n\n\nCodeall_fits |&gt; ggplot(aes(x=condit,y=weight_exam,col=condit)) + stat_pointinterval() +\n  facet_wrap(~Fit_Method)\n\n\n\n\n\n\nCodeall_fits |&gt; ggplot(aes(x=run_name,y=weight_exam,col=condit)) + stat_pointinterval() +\n  facet_wrap(condit~Fit_Method)\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +\n  geom_density() +\n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  ggh4x::facet_wrap2(~id+condit, scales=\"free_y\")\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test_Train\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +\n  geom_density() +\n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  ggh4x::facet_wrap2(~id+condit, scales=\"free_y\")\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test\") |&gt;\n  ggplot(aes(x=run_name,y=weight_exam,col=run_name)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n  ggh4x::facet_wrap2(~id+condit) + \n  theme(axis.text.x = element_blank()) \n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test_Train\") |&gt;\n  ggplot(aes(x=run_name,y=weight_exam,col=run_name)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n  ggh4x::facet_wrap2(~id+condit) + \n  theme(axis.text.x = element_blank()) \n\n\n\n\n\n\nCodeall_fits |&gt; \n  group_by(id,condit,Fit_Method) |&gt;\n  mutate(we_med=median(weight_exam),\n         Best_Model=case_when(we_med&gt;.5 ~\"EXAM\",we_med&lt;.5 ~\"ALM\")) |&gt;\n  filter(Fit_Method==\"Test\") |&gt;\n  ungroup() |&gt;\n  mutate(id=reorder(id,we_med,decreasing = TRUE)) |&gt;\n  ggplot(aes(x=weight_exam,y=id,col=Best_Model)) + \n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  stat_pointinterval() + \n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  theme_minimal() +\n  theme(legend.position=\"top\")",
    "crumbs": [
      "Model",
      "HTW Hybrid Models"
    ]
  },
  {
    "objectID": "Model/hybrid.html#e3",
    "href": "Model/hybrid.html#e3",
    "title": "HTW Hybrid Models",
    "section": "E3",
    "text": "E3\n\nCode# folder_names &lt;- c(\"e3_hybrid_n_iter_150_ntry_150_1757\",\"e3_hybrid_n_iter_250_ntry_200_4300\",\"e3_hybrid_n_iter_60_ntry_150_0946\",\"e3_hybrid_n_iter_100_ntry_100_0545\")\ntestE3 &lt;- readRDS(here(\"data/e3_08-04-23.rds\")) |&gt; filter(expMode2 == \"Test\") \ne3Sbjs &lt;- testE3 |&gt; group_by(id,condit,bandOrder) |&gt; summarise(n=n())\n\nfolder_names &lt;- list.files(here(\"data/abc_reject\"),pattern=\"e3_hyb*\")\n\nall_fits &lt;- map_df(folder_names, process_folder) |&gt; \n  left_join(e3Sbjs,by=c(\"id\",\"condit\")) \n\n\nall_fits |&gt; \n  group_by(exp,n_int,ntry,tolM,ar,condit,Fit_Method) |&gt; \n  summarise(min=first(min),me=mean(mean_error),\n            w=median(weight_exam),sd_w=sd(weight_exam),c=median(c),\n            lr=median(lr),n=n_distinct(id)) |&gt;\n   mutate(across(c(me, w, lr, sd_w), ~round(., 2))) |&gt;\n  arrange(condit,Fit_Method,me) |&gt; kable(caption=\"E3 Fit Comparisons\") |&gt; \n  kable_styling(full_width = F) |&gt;\n  column_spec(8,bold=T,border_left=T) \n\n\nE3 Fit Comparisons\n\nexp\nn_int\nntry\ntolM\nar\ncondit\nFit_Method\nmin\nme\nw\nsd_w\nc\nlr\nn\n\n\n\ne3\n100\n2500\n0.65\n0.10\nConstant\nTest\n246\n213.60\n0.75\n0.27\n0.0000413\n2.08\n110\n\n\ne3\n200\n1500\n0.70\n0.10\nConstant\nTest\n348\n214.91\n0.74\n0.27\n0.0000417\n2.08\n110\n\n\ne3\n90\n900\n0.80\n0.09\nConstant\nTest\n92\n217.05\n0.73\n0.27\n0.0000413\n2.04\n110\n\n\ne3\n50\n900\n0.80\n0.12\nConstant\nTest\n53\n217.53\n0.73\n0.27\n0.0000406\n2.02\n110\n\n\ne3\n70\n500\n0.85\n0.10\nConstant\nTest\n186\n219.42\n0.72\n0.27\n0.0000397\n2.06\n110\n\n\ne3\n150\n500\n0.95\n0.10\nConstant\nTest\n96\n219.56\n0.72\n0.27\n0.0000397\n2.04\n110\n\n\ne3\n50\n500\n0.85\n0.05\nConstant\nTest\n312\n220.20\n0.72\n0.27\n0.0000394\n2.06\n110\n\n\ne3\n400\n300\n0.85\n0.04\nConstant\nTest\n353\n220.22\n0.72\n0.27\n0.0000403\n2.04\n110\n\n\ne3\n50\n900\n0.95\n0.10\nConstant\nTest\n37\n220.50\n0.72\n0.27\n0.0000387\n2.10\n110\n\n\ne3\n49\n800\n0.95\n0.18\nConstant\nTest\n303\n221.29\n0.72\n0.27\n0.0000390\n2.10\n110\n\n\ne3\n52\n600\n0.95\n0.18\nConstant\nTest\n228\n221.74\n0.72\n0.28\n0.0000387\n2.04\n110\n\n\ne3\n61\n500\n0.95\n0.10\nConstant\nTest\n36\n221.78\n0.71\n0.27\n0.0000381\n2.02\n110\n\n\ne3\n250\n200\n0.85\n0.10\nConstant\nTest\n184\n222.77\n0.72\n0.28\n0.0000395\n2.06\n110\n\n\ne3\n100\n100\n0.85\n0.01\nConstant\nTest\n804\n223.08\n0.71\n0.27\n0.0000407\n2.08\n110\n\n\ne3\n65\n300\n0.95\n0.10\nConstant\nTest\n31\n224.28\n0.71\n0.27\n0.0000391\n2.03\n110\n\n\ne3\n150\n150\n0.85\n0.10\nConstant\nTest\n92\n224.50\n0.72\n0.28\n0.0000396\n2.03\n110\n\n\ne3\n60\n150\n0.85\n0.10\nConstant\nTest\n38\n225.40\n0.72\n0.28\n0.0000373\n2.07\n110\n\n\ne3\n51\n300\n0.95\n0.15\nConstant\nTest\n162\n225.64\n0.72\n0.27\n0.0000380\n2.01\n110\n\n\ne3\n105\n100\n0.85\n0.05\nConstant\nTest\n351\n225.99\n0.71\n0.28\n0.0000388\n2.04\n110\n\n\ne3\n100\n100\n0.85\n0.10\nConstant\nTest\n202\n227.29\n0.72\n0.28\n0.0000383\n2.07\n110\n\n\ne3\n100\n2500\n0.65\n0.10\nConstant\nTest_Train\n244\n230.55\n0.73\n0.27\n0.0000384\n0.60\n110\n\n\ne3\n200\n1500\n0.70\n0.10\nConstant\nTest_Train\n480\n231.40\n0.71\n0.27\n0.0000372\n0.60\n110\n\n\ne3\n90\n900\n0.80\n0.09\nConstant\nTest_Train\n94\n232.37\n0.70\n0.27\n0.0000369\n0.62\n110\n\n\ne3\n50\n900\n0.80\n0.12\nConstant\nTest_Train\n58\n232.77\n0.70\n0.28\n0.0000365\n0.63\n110\n\n\ne3\n70\n500\n0.85\n0.10\nConstant\nTest_Train\n186\n233.90\n0.68\n0.27\n0.0000364\n0.67\n110\n\n\ne3\n50\n500\n0.85\n0.05\nConstant\nTest_Train\n224\n234.15\n0.68\n0.28\n0.0000352\n0.69\n110\n\n\ne3\n400\n300\n0.85\n0.04\nConstant\nTest_Train\n613\n234.32\n0.69\n0.27\n0.0000358\n0.71\n110\n\n\ne3\n150\n500\n0.95\n0.10\nConstant\nTest_Train\n106\n234.35\n0.69\n0.27\n0.0000358\n0.70\n110\n\n\ne3\n50\n900\n0.95\n0.10\nConstant\nTest_Train\n40\n235.16\n0.69\n0.28\n0.0000359\n0.73\n110\n\n\ne3\n49\n800\n0.95\n0.18\nConstant\nTest_Train\n263\n235.38\n0.69\n0.28\n0.0000358\n0.70\n110\n\n\ne3\n100\n100\n0.85\n0.01\nConstant\nTest_Train\n645\n235.54\n0.69\n0.28\n0.0000357\n0.76\n110\n\n\ne3\n250\n200\n0.85\n0.10\nConstant\nTest_Train\n289\n235.70\n0.68\n0.28\n0.0000351\n0.77\n110\n\n\ne3\n52\n600\n0.95\n0.18\nConstant\nTest_Train\n227\n235.74\n0.68\n0.28\n0.0000358\n0.77\n110\n\n\ne3\n61\n500\n0.95\n0.10\nConstant\nTest_Train\n48\n236.01\n0.68\n0.28\n0.0000355\n0.74\n110\n\n\ne3\n65\n300\n0.95\n0.10\nConstant\nTest_Train\n30\n236.55\n0.68\n0.27\n0.0000347\n0.77\n110\n\n\ne3\n150\n150\n0.85\n0.10\nConstant\nTest_Train\n92\n236.60\n0.67\n0.28\n0.0000345\n0.81\n110\n\n\ne3\n51\n300\n0.95\n0.15\nConstant\nTest_Train\n152\n236.95\n0.68\n0.28\n0.0000352\n0.79\n110\n\n\ne3\n60\n150\n0.85\n0.10\nConstant\nTest_Train\n23\n237.40\n0.67\n0.28\n0.0000340\n0.85\n110\n\n\ne3\n105\n100\n0.85\n0.05\nConstant\nTest_Train\n284\n237.51\n0.67\n0.28\n0.0000348\n0.86\n110\n\n\ne3\n100\n100\n0.85\n0.10\nConstant\nTest_Train\n167\n237.92\n0.67\n0.28\n0.0000341\n0.86\n110\n\n\ne3\n70\n500\n0.85\n0.10\nConstant\nTrain\n70\n213.90\n0.50\n0.29\n0.0000236\n1.56\n110\n\n\ne3\n100\n100\n0.85\n0.10\nConstant\nTrain\n77\n214.41\n0.50\n0.29\n0.0000195\n1.48\n110\n\n\ne3\n50\n500\n0.85\n0.05\nConstant\nTrain\n78\n214.84\n0.50\n0.29\n0.0000225\n1.57\n110\n\n\ne3\n60\n150\n0.85\n0.10\nConstant\nTrain\n16\n215.59\n0.51\n0.29\n0.0000208\n1.48\n110\n\n\ne3\n52\n600\n0.95\n0.18\nConstant\nTrain\n74\n216.80\n0.50\n0.29\n0.0000286\n1.65\n110\n\n\ne3\n100\n2500\n0.65\n0.10\nVaried\nTest\n246\n194.62\n0.81\n0.27\n0.0000966\n2.19\n85\n\n\ne3\n200\n1500\n0.70\n0.10\nVaried\nTest\n348\n196.04\n0.80\n0.26\n0.0000974\n2.19\n85\n\n\ne3\n90\n900\n0.80\n0.09\nVaried\nTest\n92\n197.98\n0.79\n0.26\n0.0000914\n2.18\n85\n\n\ne3\n50\n900\n0.80\n0.12\nVaried\nTest\n53\n198.23\n0.78\n0.27\n0.0000913\n2.23\n85\n\n\ne3\n70\n500\n0.85\n0.10\nVaried\nTest\n186\n200.61\n0.78\n0.27\n0.0000958\n2.15\n85\n\n\ne3\n150\n500\n0.95\n0.10\nVaried\nTest\n96\n200.72\n0.78\n0.26\n0.0000980\n2.18\n85\n\n\ne3\n50\n500\n0.85\n0.05\nVaried\nTest\n312\n200.88\n0.78\n0.27\n0.0000973\n2.20\n85\n\n\ne3\n50\n900\n0.95\n0.10\nVaried\nTest\n37\n201.12\n0.78\n0.27\n0.0001014\n2.15\n85\n\n\ne3\n400\n300\n0.85\n0.04\nVaried\nTest\n353\n201.41\n0.78\n0.26\n0.0000998\n2.17\n85\n\n\ne3\n49\n800\n0.95\n0.18\nVaried\nTest\n303\n201.72\n0.78\n0.27\n0.0000950\n2.25\n85\n\n\ne3\n52\n600\n0.95\n0.18\nVaried\nTest\n228\n202.47\n0.77\n0.27\n0.0001007\n2.18\n85\n\n\ne3\n61\n500\n0.95\n0.10\nVaried\nTest\n36\n203.09\n0.77\n0.26\n0.0001052\n2.20\n85\n\n\ne3\n250\n200\n0.85\n0.10\nVaried\nTest\n184\n203.72\n0.77\n0.27\n0.0000987\n2.15\n85\n\n\ne3\n100\n100\n0.85\n0.01\nVaried\nTest\n804\n204.11\n0.77\n0.26\n0.0000951\n2.16\n85\n\n\ne3\n65\n300\n0.95\n0.10\nVaried\nTest\n31\n204.23\n0.77\n0.26\n0.0000958\n2.17\n85\n\n\ne3\n150\n150\n0.85\n0.10\nVaried\nTest\n92\n205.29\n0.76\n0.26\n0.0000935\n2.15\n85\n\n\ne3\n51\n300\n0.95\n0.15\nVaried\nTest\n162\n205.69\n0.76\n0.26\n0.0000900\n2.15\n85\n\n\ne3\n60\n150\n0.85\n0.10\nVaried\nTest\n38\n206.85\n0.77\n0.27\n0.0001061\n2.13\n85\n\n\ne3\n105\n100\n0.85\n0.05\nVaried\nTest\n351\n207.63\n0.76\n0.26\n0.0001001\n2.13\n85\n\n\ne3\n100\n100\n0.85\n0.10\nVaried\nTest\n202\n207.79\n0.76\n0.26\n0.0001005\n2.15\n85\n\n\ne3\n100\n2500\n0.65\n0.10\nVaried\nTest_Train\n244\n236.31\n0.83\n0.28\n0.0000443\n0.72\n85\n\n\ne3\n200\n1500\n0.70\n0.10\nVaried\nTest_Train\n480\n236.94\n0.82\n0.28\n0.0000421\n0.74\n85\n\n\ne3\n90\n900\n0.80\n0.09\nVaried\nTest_Train\n94\n238.09\n0.81\n0.28\n0.0000396\n0.74\n85\n\n\ne3\n50\n900\n0.80\n0.12\nVaried\nTest_Train\n58\n238.16\n0.81\n0.28\n0.0000393\n0.72\n85\n\n\ne3\n50\n900\n0.95\n0.10\nVaried\nTest_Train\n40\n239.45\n0.80\n0.28\n0.0000381\n0.78\n85\n\n\ne3\n70\n500\n0.85\n0.10\nVaried\nTest_Train\n186\n239.45\n0.79\n0.28\n0.0000375\n0.76\n85\n\n\ne3\n50\n500\n0.85\n0.05\nVaried\nTest_Train\n224\n239.63\n0.79\n0.28\n0.0000378\n0.76\n85\n\n\ne3\n150\n500\n0.95\n0.10\nVaried\nTest_Train\n106\n239.73\n0.79\n0.28\n0.0000383\n0.80\n85\n\n\ne3\n400\n300\n0.85\n0.04\nVaried\nTest_Train\n613\n240.07\n0.78\n0.28\n0.0000370\n0.79\n85\n\n\ne3\n49\n800\n0.95\n0.18\nVaried\nTest_Train\n263\n240.16\n0.79\n0.28\n0.0000398\n0.78\n85\n\n\ne3\n52\n600\n0.95\n0.18\nVaried\nTest_Train\n227\n240.47\n0.79\n0.28\n0.0000393\n0.78\n85\n\n\ne3\n61\n500\n0.95\n0.10\nVaried\nTest_Train\n48\n240.80\n0.78\n0.27\n0.0000377\n0.81\n85\n\n\ne3\n250\n200\n0.85\n0.10\nVaried\nTest_Train\n289\n241.42\n0.77\n0.28\n0.0000352\n0.81\n85\n\n\ne3\n100\n100\n0.85\n0.01\nVaried\nTest_Train\n645\n241.44\n0.77\n0.28\n0.0000360\n0.80\n85\n\n\ne3\n51\n300\n0.95\n0.15\nVaried\nTest_Train\n152\n242.07\n0.77\n0.28\n0.0000362\n0.83\n85\n\n\ne3\n65\n300\n0.95\n0.10\nVaried\nTest_Train\n30\n242.42\n0.78\n0.27\n0.0000372\n0.83\n85\n\n\ne3\n150\n150\n0.85\n0.10\nVaried\nTest_Train\n92\n242.42\n0.76\n0.27\n0.0000347\n0.81\n85\n\n\ne3\n60\n150\n0.85\n0.10\nVaried\nTest_Train\n23\n242.93\n0.77\n0.27\n0.0000355\n0.83\n85\n\n\ne3\n105\n100\n0.85\n0.05\nVaried\nTest_Train\n284\n243.66\n0.76\n0.27\n0.0000342\n0.85\n85\n\n\ne3\n100\n100\n0.85\n0.10\nVaried\nTest_Train\n167\n244.18\n0.74\n0.28\n0.0000342\n0.85\n85\n\n\ne3\n70\n500\n0.85\n0.10\nVaried\nTrain\n70\n231.82\n0.50\n0.29\n0.0000210\n0.82\n85\n\n\ne3\n50\n500\n0.85\n0.05\nVaried\nTrain\n78\n232.06\n0.51\n0.29\n0.0000213\n0.82\n85\n\n\ne3\n60\n150\n0.85\n0.10\nVaried\nTrain\n16\n233.78\n0.50\n0.29\n0.0000218\n0.80\n85\n\n\ne3\n52\n600\n0.95\n0.18\nVaried\nTrain\n74\n234.14\n0.50\n0.29\n0.0000219\n0.81\n85\n\n\ne3\n100\n100\n0.85\n0.10\nVaried\nTrain\n77\n234.53\n0.50\n0.29\n0.0000211\n0.81\n85\n\n\n\n\nCodeall_fits |&gt; \n  group_by(exp,n_int,ntry,tolM,ar,condit,Fit_Method) |&gt; \n  summarise(min=first(min),me=mean(mean_error),\n            w=median(weight_exam),sd_w=sd(weight_exam),c=median(c),\n            lr=median(lr),n=n_distinct(id)) |&gt;\n   mutate(across(c(me, w, lr, sd_w), ~round(., 2))) |&gt;\n  arrange(condit,Fit_Method,me) |&gt; kable(caption=\"E2 Fit Comparisons\") |&gt; \n  kable_styling(full_width = F) |&gt;\n  column_spec(8,bold=T,border_left=T) \n\n\nE2 Fit Comparisons\n\nexp\nn_int\nntry\ntolM\nar\ncondit\nFit_Method\nmin\nme\nw\nsd_w\nc\nlr\nn\n\n\n\ne3\n100\n2500\n0.65\n0.10\nConstant\nTest\n246\n213.60\n0.75\n0.27\n0.0000413\n2.08\n110\n\n\ne3\n200\n1500\n0.70\n0.10\nConstant\nTest\n348\n214.91\n0.74\n0.27\n0.0000417\n2.08\n110\n\n\ne3\n90\n900\n0.80\n0.09\nConstant\nTest\n92\n217.05\n0.73\n0.27\n0.0000413\n2.04\n110\n\n\ne3\n50\n900\n0.80\n0.12\nConstant\nTest\n53\n217.53\n0.73\n0.27\n0.0000406\n2.02\n110\n\n\ne3\n70\n500\n0.85\n0.10\nConstant\nTest\n186\n219.42\n0.72\n0.27\n0.0000397\n2.06\n110\n\n\ne3\n150\n500\n0.95\n0.10\nConstant\nTest\n96\n219.56\n0.72\n0.27\n0.0000397\n2.04\n110\n\n\ne3\n50\n500\n0.85\n0.05\nConstant\nTest\n312\n220.20\n0.72\n0.27\n0.0000394\n2.06\n110\n\n\ne3\n400\n300\n0.85\n0.04\nConstant\nTest\n353\n220.22\n0.72\n0.27\n0.0000403\n2.04\n110\n\n\ne3\n50\n900\n0.95\n0.10\nConstant\nTest\n37\n220.50\n0.72\n0.27\n0.0000387\n2.10\n110\n\n\ne3\n49\n800\n0.95\n0.18\nConstant\nTest\n303\n221.29\n0.72\n0.27\n0.0000390\n2.10\n110\n\n\ne3\n52\n600\n0.95\n0.18\nConstant\nTest\n228\n221.74\n0.72\n0.28\n0.0000387\n2.04\n110\n\n\ne3\n61\n500\n0.95\n0.10\nConstant\nTest\n36\n221.78\n0.71\n0.27\n0.0000381\n2.02\n110\n\n\ne3\n250\n200\n0.85\n0.10\nConstant\nTest\n184\n222.77\n0.72\n0.28\n0.0000395\n2.06\n110\n\n\ne3\n100\n100\n0.85\n0.01\nConstant\nTest\n804\n223.08\n0.71\n0.27\n0.0000407\n2.08\n110\n\n\ne3\n65\n300\n0.95\n0.10\nConstant\nTest\n31\n224.28\n0.71\n0.27\n0.0000391\n2.03\n110\n\n\ne3\n150\n150\n0.85\n0.10\nConstant\nTest\n92\n224.50\n0.72\n0.28\n0.0000396\n2.03\n110\n\n\ne3\n60\n150\n0.85\n0.10\nConstant\nTest\n38\n225.40\n0.72\n0.28\n0.0000373\n2.07\n110\n\n\ne3\n51\n300\n0.95\n0.15\nConstant\nTest\n162\n225.64\n0.72\n0.27\n0.0000380\n2.01\n110\n\n\ne3\n105\n100\n0.85\n0.05\nConstant\nTest\n351\n225.99\n0.71\n0.28\n0.0000388\n2.04\n110\n\n\ne3\n100\n100\n0.85\n0.10\nConstant\nTest\n202\n227.29\n0.72\n0.28\n0.0000383\n2.07\n110\n\n\ne3\n100\n2500\n0.65\n0.10\nConstant\nTest_Train\n244\n230.55\n0.73\n0.27\n0.0000384\n0.60\n110\n\n\ne3\n200\n1500\n0.70\n0.10\nConstant\nTest_Train\n480\n231.40\n0.71\n0.27\n0.0000372\n0.60\n110\n\n\ne3\n90\n900\n0.80\n0.09\nConstant\nTest_Train\n94\n232.37\n0.70\n0.27\n0.0000369\n0.62\n110\n\n\ne3\n50\n900\n0.80\n0.12\nConstant\nTest_Train\n58\n232.77\n0.70\n0.28\n0.0000365\n0.63\n110\n\n\ne3\n70\n500\n0.85\n0.10\nConstant\nTest_Train\n186\n233.90\n0.68\n0.27\n0.0000364\n0.67\n110\n\n\ne3\n50\n500\n0.85\n0.05\nConstant\nTest_Train\n224\n234.15\n0.68\n0.28\n0.0000352\n0.69\n110\n\n\ne3\n400\n300\n0.85\n0.04\nConstant\nTest_Train\n613\n234.32\n0.69\n0.27\n0.0000358\n0.71\n110\n\n\ne3\n150\n500\n0.95\n0.10\nConstant\nTest_Train\n106\n234.35\n0.69\n0.27\n0.0000358\n0.70\n110\n\n\ne3\n50\n900\n0.95\n0.10\nConstant\nTest_Train\n40\n235.16\n0.69\n0.28\n0.0000359\n0.73\n110\n\n\ne3\n49\n800\n0.95\n0.18\nConstant\nTest_Train\n263\n235.38\n0.69\n0.28\n0.0000358\n0.70\n110\n\n\ne3\n100\n100\n0.85\n0.01\nConstant\nTest_Train\n645\n235.54\n0.69\n0.28\n0.0000357\n0.76\n110\n\n\ne3\n250\n200\n0.85\n0.10\nConstant\nTest_Train\n289\n235.70\n0.68\n0.28\n0.0000351\n0.77\n110\n\n\ne3\n52\n600\n0.95\n0.18\nConstant\nTest_Train\n227\n235.74\n0.68\n0.28\n0.0000358\n0.77\n110\n\n\ne3\n61\n500\n0.95\n0.10\nConstant\nTest_Train\n48\n236.01\n0.68\n0.28\n0.0000355\n0.74\n110\n\n\ne3\n65\n300\n0.95\n0.10\nConstant\nTest_Train\n30\n236.55\n0.68\n0.27\n0.0000347\n0.77\n110\n\n\ne3\n150\n150\n0.85\n0.10\nConstant\nTest_Train\n92\n236.60\n0.67\n0.28\n0.0000345\n0.81\n110\n\n\ne3\n51\n300\n0.95\n0.15\nConstant\nTest_Train\n152\n236.95\n0.68\n0.28\n0.0000352\n0.79\n110\n\n\ne3\n60\n150\n0.85\n0.10\nConstant\nTest_Train\n23\n237.40\n0.67\n0.28\n0.0000340\n0.85\n110\n\n\ne3\n105\n100\n0.85\n0.05\nConstant\nTest_Train\n284\n237.51\n0.67\n0.28\n0.0000348\n0.86\n110\n\n\ne3\n100\n100\n0.85\n0.10\nConstant\nTest_Train\n167\n237.92\n0.67\n0.28\n0.0000341\n0.86\n110\n\n\ne3\n70\n500\n0.85\n0.10\nConstant\nTrain\n70\n213.90\n0.50\n0.29\n0.0000236\n1.56\n110\n\n\ne3\n100\n100\n0.85\n0.10\nConstant\nTrain\n77\n214.41\n0.50\n0.29\n0.0000195\n1.48\n110\n\n\ne3\n50\n500\n0.85\n0.05\nConstant\nTrain\n78\n214.84\n0.50\n0.29\n0.0000225\n1.57\n110\n\n\ne3\n60\n150\n0.85\n0.10\nConstant\nTrain\n16\n215.59\n0.51\n0.29\n0.0000208\n1.48\n110\n\n\ne3\n52\n600\n0.95\n0.18\nConstant\nTrain\n74\n216.80\n0.50\n0.29\n0.0000286\n1.65\n110\n\n\ne3\n100\n2500\n0.65\n0.10\nVaried\nTest\n246\n194.62\n0.81\n0.27\n0.0000966\n2.19\n85\n\n\ne3\n200\n1500\n0.70\n0.10\nVaried\nTest\n348\n196.04\n0.80\n0.26\n0.0000974\n2.19\n85\n\n\ne3\n90\n900\n0.80\n0.09\nVaried\nTest\n92\n197.98\n0.79\n0.26\n0.0000914\n2.18\n85\n\n\ne3\n50\n900\n0.80\n0.12\nVaried\nTest\n53\n198.23\n0.78\n0.27\n0.0000913\n2.23\n85\n\n\ne3\n70\n500\n0.85\n0.10\nVaried\nTest\n186\n200.61\n0.78\n0.27\n0.0000958\n2.15\n85\n\n\ne3\n150\n500\n0.95\n0.10\nVaried\nTest\n96\n200.72\n0.78\n0.26\n0.0000980\n2.18\n85\n\n\ne3\n50\n500\n0.85\n0.05\nVaried\nTest\n312\n200.88\n0.78\n0.27\n0.0000973\n2.20\n85\n\n\ne3\n50\n900\n0.95\n0.10\nVaried\nTest\n37\n201.12\n0.78\n0.27\n0.0001014\n2.15\n85\n\n\ne3\n400\n300\n0.85\n0.04\nVaried\nTest\n353\n201.41\n0.78\n0.26\n0.0000998\n2.17\n85\n\n\ne3\n49\n800\n0.95\n0.18\nVaried\nTest\n303\n201.72\n0.78\n0.27\n0.0000950\n2.25\n85\n\n\ne3\n52\n600\n0.95\n0.18\nVaried\nTest\n228\n202.47\n0.77\n0.27\n0.0001007\n2.18\n85\n\n\ne3\n61\n500\n0.95\n0.10\nVaried\nTest\n36\n203.09\n0.77\n0.26\n0.0001052\n2.20\n85\n\n\ne3\n250\n200\n0.85\n0.10\nVaried\nTest\n184\n203.72\n0.77\n0.27\n0.0000987\n2.15\n85\n\n\ne3\n100\n100\n0.85\n0.01\nVaried\nTest\n804\n204.11\n0.77\n0.26\n0.0000951\n2.16\n85\n\n\ne3\n65\n300\n0.95\n0.10\nVaried\nTest\n31\n204.23\n0.77\n0.26\n0.0000958\n2.17\n85\n\n\ne3\n150\n150\n0.85\n0.10\nVaried\nTest\n92\n205.29\n0.76\n0.26\n0.0000935\n2.15\n85\n\n\ne3\n51\n300\n0.95\n0.15\nVaried\nTest\n162\n205.69\n0.76\n0.26\n0.0000900\n2.15\n85\n\n\ne3\n60\n150\n0.85\n0.10\nVaried\nTest\n38\n206.85\n0.77\n0.27\n0.0001061\n2.13\n85\n\n\ne3\n105\n100\n0.85\n0.05\nVaried\nTest\n351\n207.63\n0.76\n0.26\n0.0001001\n2.13\n85\n\n\ne3\n100\n100\n0.85\n0.10\nVaried\nTest\n202\n207.79\n0.76\n0.26\n0.0001005\n2.15\n85\n\n\ne3\n100\n2500\n0.65\n0.10\nVaried\nTest_Train\n244\n236.31\n0.83\n0.28\n0.0000443\n0.72\n85\n\n\ne3\n200\n1500\n0.70\n0.10\nVaried\nTest_Train\n480\n236.94\n0.82\n0.28\n0.0000421\n0.74\n85\n\n\ne3\n90\n900\n0.80\n0.09\nVaried\nTest_Train\n94\n238.09\n0.81\n0.28\n0.0000396\n0.74\n85\n\n\ne3\n50\n900\n0.80\n0.12\nVaried\nTest_Train\n58\n238.16\n0.81\n0.28\n0.0000393\n0.72\n85\n\n\ne3\n50\n900\n0.95\n0.10\nVaried\nTest_Train\n40\n239.45\n0.80\n0.28\n0.0000381\n0.78\n85\n\n\ne3\n70\n500\n0.85\n0.10\nVaried\nTest_Train\n186\n239.45\n0.79\n0.28\n0.0000375\n0.76\n85\n\n\ne3\n50\n500\n0.85\n0.05\nVaried\nTest_Train\n224\n239.63\n0.79\n0.28\n0.0000378\n0.76\n85\n\n\ne3\n150\n500\n0.95\n0.10\nVaried\nTest_Train\n106\n239.73\n0.79\n0.28\n0.0000383\n0.80\n85\n\n\ne3\n400\n300\n0.85\n0.04\nVaried\nTest_Train\n613\n240.07\n0.78\n0.28\n0.0000370\n0.79\n85\n\n\ne3\n49\n800\n0.95\n0.18\nVaried\nTest_Train\n263\n240.16\n0.79\n0.28\n0.0000398\n0.78\n85\n\n\ne3\n52\n600\n0.95\n0.18\nVaried\nTest_Train\n227\n240.47\n0.79\n0.28\n0.0000393\n0.78\n85\n\n\ne3\n61\n500\n0.95\n0.10\nVaried\nTest_Train\n48\n240.80\n0.78\n0.27\n0.0000377\n0.81\n85\n\n\ne3\n250\n200\n0.85\n0.10\nVaried\nTest_Train\n289\n241.42\n0.77\n0.28\n0.0000352\n0.81\n85\n\n\ne3\n100\n100\n0.85\n0.01\nVaried\nTest_Train\n645\n241.44\n0.77\n0.28\n0.0000360\n0.80\n85\n\n\ne3\n51\n300\n0.95\n0.15\nVaried\nTest_Train\n152\n242.07\n0.77\n0.28\n0.0000362\n0.83\n85\n\n\ne3\n65\n300\n0.95\n0.10\nVaried\nTest_Train\n30\n242.42\n0.78\n0.27\n0.0000372\n0.83\n85\n\n\ne3\n150\n150\n0.85\n0.10\nVaried\nTest_Train\n92\n242.42\n0.76\n0.27\n0.0000347\n0.81\n85\n\n\ne3\n60\n150\n0.85\n0.10\nVaried\nTest_Train\n23\n242.93\n0.77\n0.27\n0.0000355\n0.83\n85\n\n\ne3\n105\n100\n0.85\n0.05\nVaried\nTest_Train\n284\n243.66\n0.76\n0.27\n0.0000342\n0.85\n85\n\n\ne3\n100\n100\n0.85\n0.10\nVaried\nTest_Train\n167\n244.18\n0.74\n0.28\n0.0000342\n0.85\n85\n\n\ne3\n70\n500\n0.85\n0.10\nVaried\nTrain\n70\n231.82\n0.50\n0.29\n0.0000210\n0.82\n85\n\n\ne3\n50\n500\n0.85\n0.05\nVaried\nTrain\n78\n232.06\n0.51\n0.29\n0.0000213\n0.82\n85\n\n\ne3\n60\n150\n0.85\n0.10\nVaried\nTrain\n16\n233.78\n0.50\n0.29\n0.0000218\n0.80\n85\n\n\ne3\n52\n600\n0.95\n0.18\nVaried\nTrain\n74\n234.14\n0.50\n0.29\n0.0000219\n0.81\n85\n\n\ne3\n100\n100\n0.85\n0.10\nVaried\nTrain\n77\n234.53\n0.50\n0.29\n0.0000211\n0.81\n85\n\n\n\n\nCode{all_fits |&gt; \n   filter(Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +geom_density(alpha=.5) } /{\n\nall_fits |&gt; \n   filter(Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +geom_density(alpha=.5) +\n  facet_wrap(~run_name)\n    }\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +geom_density(alpha=.5) + facet_wrap(~bandOrder) \n\n\n\n\n\n\nCodeall_fits |&gt; ggplot(aes(x=condit,y=weight_exam,col=condit)) + stat_pointinterval() +\n  facet_wrap(bandOrder~Fit_Method)\n\n\n\n\n\n\nCodeall_fits |&gt; ggplot(aes(x=run_name,y=weight_exam,col=condit)) + stat_pointinterval() +\n  facet_wrap(condit~Fit_Method)\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +\n  geom_density() +\n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  ggh4x::facet_wrap2(~id+condit, scales=\"free_y\")\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test_Train\") |&gt;\n    ggplot(aes(x=weight_exam,fill=condit)) +\n  geom_density() +\n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  ggh4x::facet_wrap2(~id+condit, scales=\"free_y\")\n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test\") |&gt;\n  ggplot(aes(x=run_name,y=weight_exam,col=run_name)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n  ggh4x::facet_wrap2(~id+condit) + \n  theme(axis.text.x = element_blank()) \n\n\n\n\n\n\nCodeall_fits |&gt; \n   filter(id %in% unique(all_fits$id)[1:50], Fit_Method==\"Test_Train\") |&gt;\n  ggplot(aes(x=run_name,y=weight_exam,col=run_name)) + \n  stat_pointinterval(position=position_dodge(.5)) +\n  ggh4x::facet_wrap2(~id+condit) + \n  theme(axis.text.x = element_blank()) \n\n\n\n\n\n\nCode{all_fits |&gt; \n  group_by(id,condit,Fit_Method) |&gt;\n  mutate(we_med=median(weight_exam),\n         Best_Model=case_when(we_med&gt;.5 ~\"EXAM\",we_med&lt;.5 ~\"ALM\")) |&gt;\n  filter(Fit_Method==\"Test\") |&gt;\n  ungroup() |&gt;\n  mutate(id=reorder(id,we_med,decreasing = TRUE)) |&gt;\n  ggplot(aes(x=weight_exam,y=id,col=Best_Model)) + \n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  stat_pointinterval() + \n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  theme_minimal() +\n  theme(legend.position=\"top\")} / \n  {all_fits |&gt; \n  group_by(id,condit,Fit_Method,bandOrder) |&gt;\n  mutate(we_med=median(weight_exam),\n         Best_Model=case_when(we_med&gt;.5 ~\"EXAM\",we_med&lt;.5 ~\"ALM\")) |&gt;\n  filter(Fit_Method==\"Test\") |&gt;\n  ungroup() |&gt;\n  mutate(id=reorder(id,we_med,decreasing = TRUE)) |&gt;\n  ggplot(aes(x=weight_exam,y=id,col=Best_Model)) + \n  geom_vline(xintercept = .5,linetype=\"dashed\") +\n  stat_pointinterval() + \n  ggh4x::facet_grid2(~bandOrder+condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  theme_minimal() +\n  theme(legend.position=\"top\")}",
    "crumbs": [
      "Model",
      "HTW Hybrid Models"
    ]
  },
  {
    "objectID": "Sections/Discussion.html",
    "href": "Sections/Discussion.html",
    "title": "General Discussion",
    "section": "",
    "text": "General Discussion\nExperimental Result Summary\nAcross three experiments, we investigated the impact of training variability on learning and extrapolation in a visuomotor function learning task. In Experiment 1, participants in the varied training condition, who experienced a wider range of velocity bands during training, showed lower accuracy at the end of training compared to those in the constant training condition.\nCrucially, during the testing phase, the varied group exhibited significantly larger deviations from the target velocity bands, particularly for the extrapolation bands that were not encountered during training. The varied group also showed less discrimination between velocity bands, as evidenced by shallower slopes when predicting response velocity from target velocity band.\nExperiment 2 extended these findings by reversing the order of the training and testing bands. Similar to Experiment 1, the varied group demonstrated poorer performance during both training and testing phases. However, unlike Experiment 1, the varied group did not show a significant difference in discrimination between bands compared to the constant group.\nIn Experiment 3, we provided only ordinal feedback during training, in contrast to the continuous feedback provided in the previous experiments. Participants were assigned to both an order condition (original or reverse) and a training condition (constant or varied). The varied condition showed larger deviations at the end of training, consistent with the previous experiments. Interestingly, there was a significant interaction between training condition and band order, with the varied condition showing greater accuracy in the reverse order condition. During testing, the varied group once again exhibited larger deviations, particularly for the extrapolation bands. The reverse order conditions showed smaller deviations compared to the original order conditions. Discrimination between velocity bands was poorer for the varied group in the original order condition, but not in the reverse order condition.\nAll three of our experiments yielded evidence that varied training conditions produced less learning by the end of training, a pattern consistent with much of the previous research on the influence of training variability (Catalano & Kleiner, 1984; Soderstrom & Bjork, 2015; Wrisberg et al., 1987). The sole exception to this pattern was the reverse order condition in Experiment 3, where the varied group was not significantly worse than the constant group. Neither the varied condition trained with the same reverse-order items in Experiment 2, nor the original-order varied condition trained with ordinal feedback in Experiment 3 were able to match the performance of their complementary constant groups by the end of training, suggesting that the relative success of the ordinal-reverse ordered varied group cannot be attributed to item or feedback effects alone.\nOur findings also diverge from the two previous studies to cleanly manipulate the variability of training items in a function learning task (DeLosh et al., 1997; van Dam & Ernst, 2015), although the varied training condition of van Dam & Ernst (2015) also exhibited less learning, neither of these previous studies observed any difference between training conditions in extrapolation to novel items. Like DeLosh et al. (1997) , our participants exhibited above chance extrapolation/discrimination of novel items, however they observed no difference between any of their three training conditions. A noteworthy difference difference between our studies is that DeLosh et al. (1997) trained participants with either 8, 20, or 50 unique items (all receiving the same total number of training trials). These larger sets of unique items, combined with the fact that participants achieved near ceiling level performance by the end of training - may have made it more difficult to observe any between-group differences of training variation in their study. van Dam & Ernst (2015) ’s variability manipulation was more similar to our own, as they trained participants with either 2 or 5 unique items. However, although the mapping between their input stimuli and motor responses was technically linear, the input dimension was more complex than our own, as it was defined by the degree of “spikiness” of the input shape. This entirely arbitrary mapping also would have preculded any sense of a “0” point, which may partially explain why neither of their training conditions were able to extrapolate linearly in the manner observed in the current study or in DeLosh et al. (1997).\nModeling Summary EXAM is the best model for both groups, but EXAM does relatively good at accounting for the constant group. May have seemed counterintuitive, if one assumed that multiple, varied, examples were necessary to extract a rule. But, EXAM is not a conventional rule model - it doesn’t require explictly abstract of a rule, but rather the rule-based response occurs during retrieval. The constant groups formation of a single, accurate, input-output association, in combination with the usefulness of the zero point, may have been sufficient for EXAM, and the constant group, to perform well. One concern may have been that the assumption of participants making use of the zero point turned the extrapolation problem into an interpolation problem - however this concern is ameliorated by the consistency of the results across both the original and reverse order conditions.\nLimitations\nWhile the present study provides valuable insights into the influence of training variability on visuomotor function learning and extrapolation, there are several limitations that should be flagged. First, although the constant training group never had experience from a velocity band closer to the extrapolation bands than the varied group, they always had a three times more trials with the nearest velocity band. Such a difference may be an unavoidable consequence of varied vs. constant design which match the total number of training trials between the two groups. However in order to more carefully tease apart the influence of variability from the influence of frequency/repetition effects, future research could explore alternative designs that maintain the variability manipulation while equating the amount of training on the nearest examples across conditions, such as by increasing the total number of trials for the varied group. Another limitation is that the testing stage did not include any interpolation items, i.e. the participants tested only from the training bands they experienced during training, or from extrapolation bands. The absence of interpolation testing makes it more difficult to distinguish between the effects of training variability on extrapolation specifically, as opposed to generalization more broadly. Of course, the nature of the constant training condition makes interpolation teseting impossible to implement, however future studies might compare a training regimes that each include at least 2 distinct items, but still differ in total amount of variability experienced, which would then allow groups to be compared in terms of both interpolation and extrapolation testing. Finally, the task employed in the present study consisted of only a linear, positive function. Previous work in human function learning has repeatedly shown that such functions are among the easiest to learn, but that humans are nonetheless capable of learning negative, non-linear, or discontinuous functions (Busemeyer et al., 1997; DeLosh et al., 1997; Kalish, 2013; Mcdaniel et al., 2009). It thus remains an open question as to whether the influence of training variability might interact with various components of the to-be-learned function.\n\n\n\n\n\nReferences\n\nBusemeyer, J. R., Byun, E., DeLosh, E. L., & McDaniel, M. A. (1997). Learning Functional Relations Based on Experience with Input-output Pairs by Humans and Artificial Neural Networks. In Knowledge Concepts and Categories (pp. 405–437). Psychology Press.\n\n\nCatalano, J. F., & Kleiner, B. M. (1984). Distant Transfer in Coincident Timing as a Function of Variability of Practice. Perceptual and Motor Skills, 58(3), 851–856. https://doi.org/10.2466/pms.1984.58.3.851\n\n\nDeLosh, E. L., McDaniel, M. A., & Busemeyer, J. R. (1997). Extrapolation: The Sine Qua Non for Abstraction in Function Learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 23(4), 19. https://doi.org/10.1037/0278-7393.23.4.968\n\n\nKalish, M. L. (2013). Learning and extrapolating a periodic function. Memory & Cognition, 41(6), 886–896. https://doi.org/10.3758/s13421-013-0306-9\n\n\nMcdaniel, M. A., Dimperio, E., Griego, J. A., & Busemeyer, J. R. (2009). Predicting transfer performance: A comparison of competing function learning models. Journal of Experimental Psychology. Learning, Memory, and Cognition, 35, 173–195. https://doi.org/10.1037/a0013982\n\n\nSoderstrom, N. C., & Bjork, R. A. (2015). Learning versus performance: An integrative review. Perspectives on Psychological Science, 10(2), 176–199. https://doi.org/10.1177/1745691615569000\n\n\nvan Dam, L. C. J., & Ernst, M. O. (2015). Mapping Shape to Visuomotor Mapping: Learning and Generalisation of Sensorimotor Behaviour Based on Contextual Information. PLOS Computational Biology, 11(3), e1004172. https://doi.org/10.1371/journal.pcbi.1004172\n\n\nWrisberg, C. A., Winter, T. P., & Kuhlman, J. S. (1987). The Variability of Practice Hypothesis: Further Tests and Methodological Discussion. Research Quarterly for Exercise and Sport, 58(4), 369–374. https://doi.org/10.1080/02701367.1987.10608114",
    "crumbs": [
      "Sections",
      "General Discussion"
    ]
  },
  {
    "objectID": "Sections/Discussion.html#comparison-to-project-1",
    "href": "Sections/Discussion.html#comparison-to-project-1",
    "title": "General Discussion",
    "section": "",
    "text": "There are a number of differences between Project 1’s Hit The Target (HTT), and Project 2’s Hit The Wall (HTW) tasks.\n\nTask Space Complexity: In HTW, the task space is also almost perfectly smooth, at least for the continuous feedback subjects, if they throw 100 units too hard, they’ll be told that they were 100 units too hard. Whereas in HTT,  it was possible to produce xy velocity combinations that were technically closer to the empirical solution space than other throws, but which resulted in worse feedback due to striking the barrier.\nPerceptual Distinctiveness: HTT offers perceptually distinct varied conditions that directly relate to the task’s demands, which may increase the sallience between training positions encounted by the varied group. In contrast, HTW’s varied conditions differ only in the numerical values displayed, lacking the same level of perceptual differentiation. Conversely in HTW, the only difference between conditions for the varied group are the numbers displayed at the top of the screen which indicate the current target band(e.g. 800-1000, or 1000-1200)\nIn HTW, our primary testing stage of interest has no feedback, whereas in HTT testing always included feedback (the intermittent testing in HTT expt 1 being the only exception). Of course, we do collect testing with feedback data at the end of HTW, but we haven’t focused on that data at all in our modelling work thus far. It’s also interesting to recall that the gap between varied and constant in HTW does seem to close substantially in the testing-with-feedback stage. The difference between no-feedback and feedback testing might be relevant if the benefits of variation have anything to do with improving subsequent learning (as opposed to subsequent immediate performance), OR if the benefits of constant training rely on having the most useful anchor, having the most useful anchor might be a lot less helpful if you’re getting feedback from novel positions and can thus immediately begin to form position-specific anchors for the novelties, rather than relying on a training anchor. \nHTW and HTT both have a similar amount of training trials (~200), and thus the constant groups acquire a similar amount of experience with their single position/velocity in both experiments. However, the varied conditions in both HTT experiments train on 2 positions, whereas the varied group in HTW trains on 3 velocity bands. This means that in HTT the varied group gets half as much experience on any one position as the constant group, and in HTW they only get 1/3 as much experience in any one position. There are likely myriad ways in which this might impact the success of the varied group regardless of how you think the benefits of variation might be occurring, e.g. maybe they also need to develop a coherent anchor, maybe they need more experience in order to extract a function, or more experience in order to properly learn to tune their c parameter.",
    "crumbs": [
      "Sections",
      "General Discussion"
    ]
  },
  {
    "objectID": "Sections/Methods.html",
    "href": "Sections/Methods.html",
    "title": "Methods",
    "section": "",
    "text": "Codepacman::p_load(dplyr, here)\n#source(here::here(\"Functions\", \"packages.R\"))\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) \ne2 &lt;- readRDS(here(\"data/e2_08-04-23.rds\")) \ne3 &lt;- readRDS(here(\"data/e3_08-04-23.rds\"))",
    "crumbs": [
      "Sections",
      "Methods"
    ]
  },
  {
    "objectID": "Sections/Methods.html#methods",
    "href": "Sections/Methods.html#methods",
    "title": "Methods",
    "section": "Methods",
    "text": "Methods\nParticipants A total of 156 participants were recruited from the Indiana University Introductory Psychology Course. Participants were randomly assigned to one of two training conditions: varied training or constant training.\nTask. The “Hit The Wall” (HTW) visuomotor extrapolation task task was programmed in Javascript, making heavy use of the phaser.io game library. The HTW task involved launching a projectile such that it would strike the “wall” at target speed indicated at the top of the screen (see Figure 1). The target velocities were given as a range, or band, of acceptable velocity values (e.g. band 800-1000). During the training stage, participants received feedback indicating whether they had hit the wall within the target velocity band, or how many units their throw was above or below from the target band. Participants were instructed that only the x velocity component of the ball was relevant to the task. The y velocity, or the location at which the ball struck the wall, had no influence on the task feedback.\n\n\n\n\n\n\n\n\nFigure 1: The Hit the wall task. Participants launch the blue ball to hit the red wall at the target velocity band indicated at the top of the screen. The ball must be released from within the orange square - but the location of release, and the location at which the ball strikes the wall are both irrelevant to the task feedback.\n\n\n\n\nProcedure\nProcedure. All participants completed the task online. Participants were provided with a description of the experiment and indicated informed consent. Figure 2 illustrates the general procedure. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). Participants in the constant training condition trained on only one velocity band (800-1000) - the closest band to what would be the novel extrapolation bands in the testing stage.\nFollowing the training stage, participants proceeded immediately to the testing stage. Participants were tested from all six velocity bands, in two separate stages. In the novel extrapolation testing stage, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials. Participants were also tested from the three velocity bands that were trained by the varied condition (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training. The order in which participants completed the novel-extrapolation and testing-from-3-varied bands was counterbalanced across participants. A final training stage presented participants with “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800).\n\nCode/////| column: screen-inset-right\ndigraph {\n  graph [layout = dot, rankdir = LR]\n\n  // define the global styles of the nodes\n  node [shape = rectangle, style = filled]\n\n  data1 [label = \" Varied Training \\n800-1000\\n1000-1200\\n1200-1400\", fillcolor = \"#FF0000\"]\n  data2 [label = \" Constant Training \\n800-1000\", fillcolor = \"#00A08A\"]\n  Test3 [label = \"    Final Test \\n  Novel With Feedback  \\n100-300\\n350-550\\n600-800\", fillcolor = \"#ECCBAE\"]\n\n  // edge definitions with the node IDs\n  data1 -&gt; Test1\n  data2 -&gt; Test1\n\n  subgraph cluster {\n    label = \"Test Phase \\n(Counterbalanced Order)\"\n    Test1 [label = \"Test  \\nNovel Bands \\n100-300\\n350-550\\n600-800\", fillcolor = \"#ECCBAE\"]\n    Test2 [label = \"  Test \\n  Varied Training Bands  \\n800-1000\\n1000-1200\\n1200-1400\", fillcolor = \"#ECCBAE\"]\n    Test1 -&gt; Test2\n  }\n\n  Test2 -&gt; Test3\n}\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 800-10001000-12001200-1400Test1\nTest  Novel Bands 100-300350-550600-800data1-&gt;Test1\ndata2\n Constant Training 800-1000data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  100-300350-550600-800Test2\n  Test   Varied Training Bands  800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 2: Experiment 1 Design. Constant and Varied participants complete different training conditions.",
    "crumbs": [
      "Sections",
      "Methods"
    ]
  },
  {
    "objectID": "Sections/Methods.html#htw-task",
    "href": "Sections/Methods.html#htw-task",
    "title": "Methods",
    "section": "HTW Task",
    "text": "HTW Task\n\nneed to create a demo version without consent form. And maybe separate windows for the different versions.\n\nExperimental Task for the HTW Project. Programmed in Javascript, and making use of phaser.js.",
    "crumbs": [
      "Sections",
      "Methods"
    ]
  },
  {
    "objectID": "Sections/Task.html",
    "href": "Sections/Task.html",
    "title": "HTW Task",
    "section": "",
    "text": "need to create a demo version without consent form. And maybe separate windows for the different versions.\n\nExperimental Task for the HTW Project. Programmed in Javascript, and making use of phaser.js."
  },
  {
    "objectID": "Sections/Task.html#htw-task",
    "href": "Sections/Task.html#htw-task",
    "title": "HTW Task",
    "section": "",
    "text": "need to create a demo version without consent form. And maybe separate windows for the different versions.\n\nExperimental Task for the HTW Project. Programmed in Javascript, and making use of phaser.js."
  },
  {
    "objectID": "Sections/Task.html#live-task-demo",
    "href": "Sections/Task.html#live-task-demo",
    "title": "HTW Task",
    "section": "Live Task Demo",
    "text": "Live Task Demo\nCheck the box at the bottom of the consent form, and then click Start Experiment.\n\n\nHTW_Task\n\n   –&gt;"
  },
  {
    "objectID": "Sections/e1_model_appendix.html",
    "href": "Sections/e1_model_appendix.html",
    "title": "ABC-rejection models",
    "section": "",
    "text": "Codepacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable, flextable,ggstance, htmltools)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\noptions(scipen = 999)\nwalk(c(\"Display_Functions\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\nds &lt;- readRDS(here::here(\"data/e1_md_11-06-23.rds\"))  |&gt; as.data.table()\n\nfd &lt;- readRDS(here(\"data/e1_08-21-23.rds\"))\ntest &lt;- fd |&gt; filter(expMode2 == \"Test\") \ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(nHits=sum(dist==0),vx=mean(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\nnbins=3\ntrainAvg &lt;- fd |&gt; filter(expMode2 == \"Train\") |&gt; group_by(id) |&gt; \n  mutate(tr=trial,x=vb,Block=case_when(expMode2==\"Train\" ~ cut(tr,breaks=seq(1,max(tr), length.out=nbins+1),include.lowest=TRUE,labels=FALSE),\n                                         expMode2==\"Test\" ~ 4)) |&gt; \n  group_by(id,condit,vb,x,Block) |&gt; \n  summarise(dist=mean(dist),y=mean(vx))\n\n# df with trial count columns for each level of expMode, for each id\nid_n &lt;- ds |&gt; group_by(id,condit) |&gt; summarise(nTest=n_distinct(tr[expMode2==\"Test\"]),\n                                        nTrain=n_distinct(tr[expMode2==\"Train\"])) |&gt;\n  mutate(nTotal=nTest+nTrain)\n\n\n\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(nHits=sum(dist==0),vx=mean(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n\ninput_layer &lt;&lt;- output_layer &lt;&lt;-  c(100,350,600,800,1000,1200)\n\nids &lt;- c(1,2,4,5,6,7,8, 10,11,12,13)\nids2 &lt;- c(1,66,36)\nids3 &lt;- c(20,71,101,4,76,192)\nidsBad &lt;- c(76,192, 101)\n\n#file_name &lt;- \"n_iter_300_ntry_3000_0800\"\n#file_name &lt;- \"n_iter_100_ntry_200_4509\"\nfile_name &lt;- \"n_iter_100_ntry_400_3247\"\nfile_name &lt;- \"n_iter_200_ntry_300_5354\"\n\n# list.files(here('data/abc_reject'))\n# (grep(\"Train\",list.files(here(paste0('data/abc_reject/',file_name)),\n#                                            pattern=\"EXAM_Test\",full.names = TRUE),\n#                                 invert=TRUE, value=TRUE))\n\n\nind_fits &lt;- map(list.files(here(paste0('data/abc_reject/'),file_name),full.names=TRUE), readRDS)\n# ind_fits &lt;- map(list.files(here('data/abc_reject/n_iter_2000_ntry_10_2918'),full.names=TRUE), readRDS)\nind_fits_df &lt;- ind_fits |&gt; map(~list(dat=.x[[1]], Model = .x[[\"Model\"]], Fit_Method=.x[[\"Fit_Method\"]]))\nind_fits_df &lt;- ind_fits_df |&gt; map(~rbindlist(.x$dat) |&gt; mutate(Model = .x$Model, Fit_Method = .x$Fit_Method)) |&gt; rbindlist() \n\n\navg_samples &lt;- ind_fits_df |&gt; group_by(id,condit,Model,Fit_Method) |&gt; \n  summarise(inc=max(inc_count),iter_sum=sum(iter_count))\nmean(avg_samples$iter_sum)\n\n[1] 205708.7\n\nCodemean(avg_samples$iter_sum)\n\n[1] 205708.7\n\nCodemin(avg_samples$iter_sum)\n\n[1] 9965\n\nCodemax(avg_samples$iter_sum)\n\n[1] 1644491\n\nCodesum(avg_samples$iter_sum)\n\n[1] 192543336\n\nCodesum(avg_samples$iter_sum)\n\n[1] 192543336\n\nCodeavg_samples |&gt; group_by(Model,Fit_Method) |&gt; summarise(mean(inc),mean(iter_sum))\n\n# A tibble: 6 × 4\n# Groups:   Model [2]\n  Model Fit_Method `mean(inc)` `mean(iter_sum)`\n  &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 ALM   Test              763.          295556.\n2 ALM   Test_Train        501.          197177.\n3 ALM   Train             358.          163335.\n4 EXAM  Test              581.          220765.\n5 EXAM  Test_Train        445.          185647.\n6 EXAM  Train             351.          171773.\n\nCode#run_params &lt;- tibble::lst(file_name,cMean=ind_fits[[1]]$cMean, cSig=ind_fits[[1]]$cSig, \n                          \nrun_params &lt;- keep(ind_fits[[1]], ~any(class(.x) %in% c(\"character\", \"numeric\")))\nrun_params &lt;- tibble(!!!run_params) |&gt; select(-Model,-Fit_Method)\n\nextract_info &lt;- function(raw_names, run_params) {\n  fname &lt;- tools::file_path_sans_ext(basename(raw_names))\n  n_samp &lt;- str_extract(raw_names, \"(?&lt;=_)\\\\d+(?=_)\")\n  ntry_ &lt;- str_extract(raw_names, \"(?&lt;=ntry_)\\\\d+\")\n  type &lt;- 'ss'\n  data.frame(run_params, n_samp, ntry_, type, fname)\n}\n\nrun_info &lt;- extract_info(file_name,run_params)\nCodegenerate_data &lt;- function(Model, post_samples, data, num_samples = 1, return_dat = \"train_data, test_data\") {\n  # Filter data for the specific id without invalidating selfref\n  sbj_data &lt;- copy(data[id == post_samples$id[1]])\n  simulation_function &lt;- ifelse(Model == \"EXAM\", full_sim_exam, full_sim_alm)\n\n  target_data &lt;- switch(return_dat,\n                        \"test_data\" = copy(sbj_data[expMode2 == \"Test\"]),\n                        \"train_data\" = copy(sbj_data[expMode2 == \"Train\"]),\n                        \"train_data, test_data\" = copy(sbj_data[expMode2 %in% c(\"Test\", \"Train\")]))\n  \n  post_samples &lt;- post_samples[order(mean_error)][1:num_samples, .(c, lr, mean_error, rank = .I)]\n\n  simulated_data_list &lt;- lapply(1:nrow(post_samples), function(i) {\n    params &lt;- post_samples[i]\n    sim_data &lt;- simulation_function(sbj_data, params$c, params$lr, input_layer = input_layer, \n                                    output_layer = output_layer, return_dat = return_dat)\n    sim_data_dt &lt;- data.table(id = sbj_data$id[1], condit = sbj_data$condit[1], \n                              expMode2 = target_data$expMode2, Model = Model,tr=target_data$tr,\n                              y = target_data$y, x = target_data$x, c = params$c, \n                              lr = params$lr, mean_error = params$mean_error, rank = i,\n                              pred = sim_data)\n    return(sim_data_dt)\n  })\n  \n  result_dt &lt;- rbindlist(simulated_data_list)\n  setcolorder(result_dt, c(\"id\", \"condit\", \"expMode2\",\"tr\", \"c\", \"lr\", \"x\", \"y\", \"pred\"))\n  return(result_dt)\n}\n\nfuture::plan(multisession)\n\nnestSbjModelFit &lt;- ind_fits_df %&gt;% nest(.by=c(id,Model,Fit_Method))\n\n# organize test data predictions\npost_dat &lt;- nestSbjModelFit |&gt; mutate(pp=furrr::future_pmap(list(id,Model,Fit_Method,data), ~{\n   generate_data(..2, ..4 |&gt; mutate(id=..1), ds, num_samples = 20, return_dat=\"test_data\")\n   })) |&gt; \n  select(Fit_Method,pp,-data) |&gt;  \n  unnest(pp) |&gt;  filter(expMode2==\"Test\") |&gt; as.data.table()\n\npost_dat_avg &lt;- post_dat |&gt; group_by(id, condit, Model, Fit_Method, x, c, lr, rank) |&gt; \n  summarise(y = mean(y), pred = mean(pred), error = y - pred) |&gt; as.data.table() \n\npost_dat_avg2 &lt;- post_dat_avg |&gt;\n  group_by(id, condit, Model, Fit_Method, c, lr, rank) |&gt; \n  summarise(y = mean(y), pred = mean(pred), error = y - pred) |&gt; as.data.table() |&gt; arrange(rank)\n\nmap_id &lt;- post_dat |&gt; group_by(id, condit, Model, Fit_Method) |&gt; \n  filter(rank==1) |&gt; slice_head(n=1) |&gt; select(id,condit,Fit_Method,Model,c,lr,mean_error,rank) |&gt;\n  left_join(id_n,by=(c(\"id\",\"condit\")))\n\n# compute BIC for each row of map_id; p=2 for c and lr\nmap_id &lt;- map_id |&gt; mutate(n = fifelse(Fit_Method == \"Test\", nTest, fifelse(Fit_Method == \"Train\", nTrain, nTotal)),BIC = -2 * log(mean_error) + log(n)*2)\n\nbic_id &lt;- map_id %&gt;%\n  group_by(id, Fit_Method) %&gt;%\n  arrange(Model) %&gt;%\n  summarize(BIC_diff = diff(BIC),\n            .groups = 'drop') %&gt;%\n  left_join(map_id, by = c(\"id\", \"Fit_Method\")) %&gt;%\n  select(id, Fit_Method, Model, BIC, BIC_diff)\n\n\n\nsetorder(post_dat_avg, id, x, rank)\npost_dat_l &lt;- melt(post_dat_avg, id.vars = c(\"id\", \"condit\", \"Model\", \"Fit_Method\", \"x\", \"c\", \"lr\", \"rank\",\"error\"),\n                   measure.vars = c(\"pred\", \"y\"), variable.name = \"Resp\", value.name = \"val\")\npost_dat_l[, Resp := fifelse(Resp == \"y\", \"Observed\",\n                             fifelse(Model == \"ALM\", \"ALM\", \"EXAM\"))]\nsetorder(post_dat_l, id, Resp)\n#rm(post_dat_avg)\n\n\n# organize training data predictions\npd_train &lt;- nestSbjModelFit |&gt; mutate(pp=furrr::future_pmap(list(id,Model,Fit_Method,data), ~{\n   generate_data(..2, ..4 |&gt; mutate(id=..1), ds, num_samples = 20, return_dat=\"train_data\")\n   })) |&gt;\n  select(Fit_Method,pp,-data) |&gt;\n  unnest(pp) |&gt; as.data.table() |&gt; filter(expMode2==\"Train\")\n\nnbins &lt;- 3\npd_train &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method) |&gt;\n  mutate(Block=cut(tr,breaks=seq(1,max(tr), length.out=nbins+1),include.lowest=TRUE,labels=FALSE))\nsetorder(pd_train, id, x,Block, rank)\n\npd_train_l &lt;- melt(pd_train, id.vars = c(\"id\", \"condit\", \"Model\",\"Block\", \"Fit_Method\", \"x\", \"c\", \"lr\", \"rank\"),\n                   measure.vars = c(\"pred\", \"y\"), variable.name = \"Resp\", value.name = \"val\") |&gt; as.data.table()\npd_train_l[, Resp := fifelse(Resp == \"y\", \"Observed\",\n                             fifelse(Model == \"ALM\", \"ALM\", \"EXAM\"))] \nsetorder(pd_train_l, id,Block, Resp) \n\npd_train_l &lt;- pd_train_l |&gt;\n  mutate(dist = case_when(\n    val &gt;= x & val &lt;= x + 200 ~ 0,                 \n    val &lt; x ~ abs(x - val),                       \n    val &gt; x + 200 ~ abs(val - (x + 200)),           \n    TRUE ~ NA_real_                                 \n  ))\nplan(sequential)\n\nplan(sequential)\n\n\n# theta_map &lt;- post_dat_avg |&gt; group_by(condit,Model,Fit_Method) |&gt; summarize(c=median(c),lr=median(lr))\n# \n# map &lt;- simulation_function(sbj_data &lt;- ds |&gt; theta_map$c[1], , params$lr, input_layer = input_layer, \n#                                     output_layer = output_layer, return_dat = return_dat)\n\n# AIC and BIC\n# mc2 &lt;- post_dat |&gt; filter(rank==1) |&gt;\n#   group_by(id,condit,Model,Fit_Method) |&gt;\n#   mutate(e2=abs(y-pred),n=n(),k=2) |&gt;\n#   group_by(id,condit,Model,Fit_Method,x) |&gt;\n#   summarise(y=mean(y), pred=mean(pred), e2=mean(e2), mean_error=first(mean_error),n=first(n),k=first(k)) |&gt;\n#   group_by(id,condit,Model,Fit_Method) |&gt;\n#   summarise(e2=mean(e2), mean_error=first(mean_error),n=first(n),k=first(k)) |&gt;\n#   mutate(AIC=2*k+n*mean_error, BIC=log(n)*k+n*mean_error)\n\n# Single run extraction:\n# exam_test &lt;- ind_fits_df |&gt; filter(Model == \"EXAM\", Fit_Method == \"Test\")\n# post_dat_trial &lt;- exam_test %&gt;% split(f =c(.$id), drop=TRUE) |&gt; \n#   map(~generate_data(.x$Model, .x, ds, num_samples = 15, return_dat=\"train_data, test_data\")) |&gt; \n#   rbindlist() |&gt; \n#   filter(expMode2==\"Test\")\n# \n# post_dat_avg &lt;- post_dat_trial |&gt; group_by(id,condit,x,c,lr,rank) |&gt; \n#   summarise(y = mean(y),pred = mean(pred)) |&gt; arrange(id,x,rank)\n\n#post_dat_l |&gt; filter(Model == \"EXAM\", Fit_Method == \"Test\",id==1) |&gt; arrange(rank)\n# head(post_dat_l)\n\n\nbestTestEXAM &lt;- post_dat_l |&gt; \n  filter(Fit_Method==\"Test\", rank==1, Resp==\"EXAM\") |&gt; \n  group_by(id,condit,Model,Fit_Method) |&gt;\n  summarise(mae=mean(abs(error))) |&gt; arrange(mae) |&gt; ungroup() |&gt;\n  mutate(group_rank = row_number(),.by=c(condit))\n\nbestTestALM&lt;- post_dat_l |&gt; \n  filter(Fit_Method==\"Test\", rank==1, Resp==\"ALM\") |&gt; \n  group_by(id,condit,Model,Fit_Method) |&gt;\n  summarise(mae=mean(abs(error))) |&gt; arrange(mae) |&gt; ungroup() |&gt;\n  mutate(group_rank = row_number(),.by=c(condit))\n\nbestTrainALM&lt;- post_dat_l |&gt; \n  filter(Fit_Method==\"Train\", rank==1, Resp==\"ALM\") |&gt; \n  group_by(id,condit,Model,Fit_Method) |&gt;\n  summarise(mae=mean(abs(error))) |&gt; arrange(mae) |&gt; ungroup() |&gt;\n  mutate(group_rank = row_number(),.by=c(condit))\n\nbestTrainEXAM&lt;- post_dat_l |&gt; \n  filter(Fit_Method==\"Train\", rank==1, Resp==\"EXAM\") |&gt; \n  group_by(id,condit,Model,Fit_Method) |&gt;\n  summarise(mae=mean(abs(error))) |&gt; arrange(mae) |&gt; ungroup() |&gt;\n  mutate(group_rank = row_number(),.by=c(condit))\n\nbestTtEXAM &lt;- post_dat_l |&gt; \n  filter(Fit_Method==\"Test_Train\", rank==1, Resp==\"EXAM\") |&gt; \n  group_by(id,condit,Model,Fit_Method) |&gt;\n  summarise(mae=mean(abs(error))) |&gt; arrange(mae) |&gt; ungroup() |&gt;\n  mutate(group_rank = row_number(),.by=c(condit))",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#posterior-average-table",
    "href": "Sections/e1_model_appendix.html#posterior-average-table",
    "title": "ABC-rejection models",
    "section": "Posterior Average Table",
    "text": "Posterior Average Table\n\nCode# post_tabs &lt;- abc_tables(post_dat_l)\n# post_tabs$et_sum |&gt; gt::gt()\n\npost_tabs &lt;- abc_tables(post_dat,post_dat_l)\n\n# tables with fit error\n# post_tabs$agg_full |&gt; flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"), \n#                        `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n#   set_caption(\"Mean from full_posterior\") \n\n# post_tabs$agg_best |&gt; flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"), \n#                        `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n#   set_caption(\"Mean from best parameters\") \n\npost_tabs$agg_pred_full |&gt; flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"), \n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n  set_caption(\"Mean from full_posterior\") \n\n\n\n\n\nFull Posterior {#tbl-anonymous-2089966-1}\n\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nTest\nALM\n\n275.4\n\n226.9\n\n\nEXAM\n\n214.6\n\n212.4\n\n\nTest_Train\nALM\n\n286.4\n\n265.5\n\n\nEXAM\n\n227.5\n\n246.2\n\n\nTrain\nALM\n\n530.6\n\n365.5\n\n\nEXAM\n\n339.1\n\n370.7\n\n\n\n\n\nCodepost_tabs$agg_pred_best |&gt; flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"), \n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n  set_caption(\"Mean from full_posterior\") \n\n\n\n\n\nBest Posterior {#tbl-anonymous-2089966-2}\n\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nTest\nALM\n\n273.8\n\n219.3\n\n\nEXAM\n\n213.3\n\n208.4\n\n\nTest_Train\nALM\n\n285.1\n\n260.8\n\n\nEXAM\n\n226.9\n\n240.8\n\n\nTrain\nALM\n\n517.0\n\n363.7\n\n\nEXAM\n\n338.5\n\n356.7\n\n\n\n\n\nPosterior Distribution",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#interactive-test-predictions-table",
    "href": "Sections/e1_model_appendix.html#interactive-test-predictions-table",
    "title": "ABC-rejection models",
    "section": "Interactive Test Predictions Table",
    "text": "Interactive Test Predictions Table\n\nCodetags$select(\n  tags$option(value = \"\", \"All\"),\n  purrr::map(unique(post_tabs$agg_x_full$condit), tags$option),\n    purrr::map(unique(post_tabs$agg_x_full$Model), tags$option),\n    purrr::map(unique(post_tabs$agg_x_full$Fit_Method), tags$option),\n   purrr::map(unique(post_tabs$agg_x_full$x), tags$option)\n)\n\nAll\nConstant\nVaried\nALM\nEXAM\nTest\nTest_Train\nTrain\n100\n350\n600\n800\n1000\n1200\n\nCodeselectFilter &lt;- function(tableId, style = \"width: 100%; height: 100%;\") {\n  function(values, name) {\n    tags$select(\n      # Set to undefined to clear the filter\n      onchange = sprintf(\"\n        const value = event.target.value\n        Reactable.setFilter('%s', '%s', value === '__ALL__' ? undefined : value)\n      \", tableId, name),\n      # \"All\" has a special value to clear the filter, and is the default option\n      tags$option(value = \"__ALL__\", \"All\"),\n      lapply(unique(values), tags$option),\n      \"aria-label\" = sprintf(\"Filter %s\", name),\n      style = style\n    )\n  }\n}\n\n\npost_tabs$agg_x_full |&gt; \n  left_join(post_tabs$agg_x_best |&gt; rename(\"best_error\"=mean_error,\"best_pred\"=pred), \n            by=c(\"condit\",\"Model\",\"Fit_Method\",\"x\",\"y\")) |&gt; \n  arrange(desc(condit),x,Fit_Method) |&gt;\n  mutate(x=as.character(x)) |&gt;\n  reactable::reactable(\n    columns = list(condit=colDef(name=\"Condit\",filterInput=selectFilter(\"my-tbl\")),\n                   Model=colDef(name=\"Model\",filterInput=selectFilter(\"my-tbl\")),\n                   Fit_Method=colDef(name=\"Fit_Method\",filterInput=selectFilter(\"my-tbl\"),\n                                     filterMethod = JS(\"(rows, columnId, filterValue) =&gt; {\n        return rows.filter(row =&gt; row.values[columnId] === filterValue)\n      }\")),\n      x=colDef(name=\"x\",filterInput=selectFilter(\"my-tbl\"),\n               filterMethod = JS(\"(rows, columnId, filterValue) =&gt; {\n        return rows.filter(row =&gt; row.values[columnId] === filterValue)\n      }\"))),\n    elementId = \"my-tbl\",\n    defaultPageSize=15,\n    filterable=TRUE)",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#train-predictions",
    "href": "Sections/e1_model_appendix.html#train-predictions",
    "title": "ABC-rejection models",
    "section": "Train Predictions",
    "text": "Train Predictions\n\nCodept_train &lt;- abc_train_tables(pd_train,pd_train_l)\npt_train$agg_pred_full |&gt; flextable::tabulator(rows=c(\"Fit_Method\",\"Model\"), columns=c(\"condit\"), \n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n  set_caption(\"Mean from full_posterior\")\n\n\n\n\n\nFit_Method\nModel\n\nConstant\n\nVaried\n\n\n\nTest\nALM\n\n851.2\n\n3,049.9\n\n\nEXAM\n\n711.9\n\n1,047.5\n\n\nTest_Train\nALM\n\n229.7\n\n324.7\n\n\nEXAM\n\n233.4\n\n331.3\n\n\nTrain\nALM\n\n216.1\n\n301.8\n\n\nEXAM\n\n216.0\n\n302.3\n\n\n\n\n\nCodept_train$block_pred_full |&gt; \n  flextable::tabulator(rows=c(\"Fit_Method\",\"Model\",\"Block\"), columns=c(\"condit\"), \n                       `Observed` = as_paragraph(y), \n                       `Predicted` = as_paragraph(pred),\n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n  set_caption(\"Mean from full_posterior\")\n\n\n\n\n\n\nFit_Method\nModel\nBlock\n\nConstant\n\nVaried\n\n\n\nObserved\nPredicted\nME\n\nObserved\nPredicted\nME\n\n\n\n\nTest\nALM\n1\n\n927.4\n730.3\n876.2\n\n1,069.0\n746.6\n1,838.5\n\n\n2\n\n922.8\n484.7\n1,001.5\n\n1,091.9\n-4,869.1\n7,073.9\n\n\n3\n\n916.5\n1,202.4\n714.1\n\n1,100.8\n951.3\n1,022.6\n\n\nEXAM\n1\n\n927.4\n796.9\n980.6\n\n1,069.0\n889.1\n1,248.6\n\n\n2\n\n922.8\n895.1\n720.6\n\n1,091.9\n1,024.3\n1,139.0\n\n\n3\n\n916.5\n918.0\n442.5\n\n1,100.8\n1,005.0\n772.5\n\n\nTest_Train\nALM\n1\n\n927.4\n820.4\n294.8\n\n1,069.0\n872.4\n393.2\n\n\n2\n\n922.8\n885.6\n202.9\n\n1,091.9\n980.9\n307.6\n\n\n3\n\n916.5\n886.9\n191.9\n\n1,100.8\n1,008.5\n277.9\n\n\nEXAM\n1\n\n927.4\n832.4\n296.8\n\n1,069.0\n864.5\n411.7\n\n\n2\n\n922.8\n902.8\n205.8\n\n1,091.9\n989.2\n309.7\n\n\n3\n\n916.5\n902.2\n198.3\n\n1,100.8\n1,020.4\n277.8\n\n\nTrain\nALM\n1\n\n927.4\n830.6\n272.3\n\n1,069.0\n921.7\n355.6\n\n\n2\n\n922.8\n889.9\n193.6\n\n1,091.9\n1,005.0\n289.1\n\n\n3\n\n916.5\n890.3\n183.0\n\n1,100.8\n1,022.5\n265.3\n\n\nEXAM\n1\n\n927.4\n831.4\n272.1\n\n1,069.0\n916.3\n355.6\n\n\n2\n\n922.8\n890.3\n193.4\n\n1,091.9\n1,001.8\n289.5\n\n\n3\n\n916.5\n890.0\n183.1\n\n1,100.8\n1,020.8\n266.1\n\n\n\n\n\nCodept_train$block_pred_full |&gt; filter(condit==\"Varied\") |&gt;\n  flextable::tabulator(rows=c(\"Fit_Method\",\"Block\"), columns=c(\"Model\"), \n                      # `Observed` = as_paragraph(y), \n                       `Predicted` = as_paragraph(pred),\n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n  set_caption(\"Mean from full_posterior\")\n\n\n\n\n\n\nFit_Method\nBlock\n\nALM\n\nEXAM\n\n\n\nPredicted\nME\n\nPredicted\nME\n\n\n\n\nTest\n1\n\n746.6\n1,838.5\n\n889.1\n1,248.6\n\n\n2\n\n-4,869.1\n7,073.9\n\n1,024.3\n1,139.0\n\n\n3\n\n951.3\n1,022.6\n\n1,005.0\n772.5\n\n\nTest_Train\n1\n\n872.4\n393.2\n\n864.5\n411.7\n\n\n2\n\n980.9\n307.6\n\n989.2\n309.7\n\n\n3\n\n1,008.5\n277.9\n\n1,020.4\n277.8\n\n\nTrain\n1\n\n921.7\n355.6\n\n916.3\n355.6\n\n\n2\n\n1,005.0\n289.1\n\n1,001.8\n289.5\n\n\n3\n\n1,022.5\n265.3\n\n1,020.8\n266.1\n\n\n\n\n\nCodept_train$block_pred_full |&gt;\n  flextable::tabulator(rows=c(\"Fit_Method\",\"Block\"), columns=c(\"condit\",\"Model\"), \n                      # `Observed` = as_paragraph(y), \n                       `Predicted` = as_paragraph(pred),\n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n  set_caption(\"Mean from full_posterior\")\n\n\n\n\n\n\nFit_Method\nBlock\n\nConstant\n\nVaried\n\n\n\nALM\n\nEXAM\n\nALM\n\nEXAM\n\n\n\nPredicted\nME\n\nPredicted\nME\n\nPredicted\nME\n\nPredicted\nME\n\n\n\n\nTest\n1\n\n730.3\n876.2\n\n796.9\n980.6\n\n746.6\n1,838.5\n\n889.1\n1,248.6\n\n\n2\n\n484.7\n1,001.5\n\n895.1\n720.6\n\n-4,869.1\n7,073.9\n\n1,024.3\n1,139.0\n\n\n3\n\n1,202.4\n714.1\n\n918.0\n442.5\n\n951.3\n1,022.6\n\n1,005.0\n772.5\n\n\nTest_Train\n1\n\n820.4\n294.8\n\n832.4\n296.8\n\n872.4\n393.2\n\n864.5\n411.7\n\n\n2\n\n885.6\n202.9\n\n902.8\n205.8\n\n980.9\n307.6\n\n989.2\n309.7\n\n\n3\n\n886.9\n191.9\n\n902.2\n198.3\n\n1,008.5\n277.9\n\n1,020.4\n277.8\n\n\nTrain\n1\n\n830.6\n272.3\n\n831.4\n272.1\n\n921.7\n355.6\n\n916.3\n355.6\n\n\n2\n\n889.9\n193.6\n\n890.3\n193.4\n\n1,005.0\n289.1\n\n1,001.8\n289.5\n\n\n3\n\n890.3\n183.0\n\n890.0\n183.1\n\n1,022.5\n265.3\n\n1,020.8\n266.1\n\n\n\n\n\nCodept_train$block_pred_full |&gt;\n  flextable::tabulator(rows=c(\"Fit_Method\",\"Block\"), columns=c(\"condit\",\"Model\"), \n                      # `Observed` = as_paragraph(y), \n                       `ME` = as_paragraph(mean_error)) |&gt; as_flextable() |&gt;\n  set_caption(\"Mean from full_posterior\")\n\n\n\n\n\n\nFit_Method\nBlock\n\nConstant\n\nVaried\n\n\n\nALM\n\nEXAM\n\nALM\n\nEXAM\n\n\n\n\nTest\n1\n\n876.2\n\n980.6\n\n1,838.5\n\n1,248.6\n\n\n2\n\n1,001.5\n\n720.6\n\n7,073.9\n\n1,139.0\n\n\n3\n\n714.1\n\n442.5\n\n1,022.6\n\n772.5\n\n\nTest_Train\n1\n\n294.8\n\n296.8\n\n393.2\n\n411.7\n\n\n2\n\n202.9\n\n205.8\n\n307.6\n\n309.7\n\n\n3\n\n191.9\n\n198.3\n\n277.9\n\n277.8\n\n\nTrain\n1\n\n272.3\n\n272.1\n\n355.6\n\n355.6\n\n\n2\n\n193.6\n\n193.4\n\n289.1\n\n289.5\n\n\n3\n\n183.0\n\n183.1\n\n265.3\n\n266.1\n\n\n\n\n\nCodebest_indices &lt;- pt_train$block_pred_full_x %&gt;%\n   filter(!(Fit_Method==\"Test\")) |&gt;\n  nest(data = -condit) %&gt;%\n  mutate(indices = map(data, ~{\n    .x %&gt;%\n      select(-y, -pred, -mean_error) %&gt;%\n      pivot_wider(names_from = Model, values_from = best) %&gt;%\n      ungroup() |&gt;\n      select(ALM, EXAM) %&gt;%\n      pmap(.f = function(ALM, EXAM) {\n        which(c(ALM, EXAM) == 1)\n      })\n  })) %&gt;%\n  select(condit, indices)\n   \n\n\npt_train$block_pred_full_x |&gt; filter(condit==\"Varied\",!(Fit_Method==\"Test\")) |&gt;\n  flextable::tabulator(rows=c(\"Fit_Method\",\"Block\",\"x\"), columns=c(\"Model\"), \n                      # `Observed` = as_paragraph(y), \n                       `Predicted` = as_paragraph(pred),\n                       `ME` = as_paragraph(mean_error)) |&gt; \n  as_flextable() %&gt;% \n  apply_best_formatting(.,pull(filter(best_indices,condit==\"Varied\"),indices) %&gt;% .[[1]]) |&gt; \n  set_caption(\"Vared Train Predictions\")\n\n\n\n\n\n\nFit_Method\nBlock\nx\n\nALM\n\nEXAM\n\n\n\nPredicted\nME\n\nPredicted\nME\n\n\n\n\nTest_Train\n1\n800\n\n845.3\n356.8\n\n849.0\n375.5\n\n\n1,000\n\n875.0\n387.1\n\n868.9\n404.0\n\n\n1,200\n\n898.1\n423.9\n\n879.2\n440.9\n\n\n2\n800\n\n940.2\n291.3\n\n960.6\n292.8\n\n\n1,000\n\n984.0\n291.7\n\n992.2\n293.7\n\n\n1,200\n\n1,019.2\n339.7\n\n1,015.3\n341.9\n\n\n3\n800\n\n960.2\n261.2\n\n973.8\n267.9\n\n\n1,000\n\n1,009.0\n259.7\n\n1,026.3\n260.9\n\n\n1,200\n\n1,054.8\n308.0\n\n1,060.3\n300.4\n\n\nTrain\n1\n800\n\n892.3\n321.8\n\n887.0\n319.7\n\n\n1,000\n\n923.4\n355.1\n\n918.1\n355.7\n\n\n1,200\n\n947.3\n378.0\n\n941.6\n379.8\n\n\n2\n800\n\n979.9\n269.3\n\n975.1\n269.5\n\n\n1,000\n\n1,001.0\n281.0\n\n997.7\n282.1\n\n\n1,200\n\n1,035.6\n316.4\n\n1,033.5\n316.5\n\n\n3\n800\n\n992.9\n253.4\n\n989.2\n255.0\n\n\n1,000\n\n1,017.2\n247.3\n\n1,017.3\n247.6\n\n\n1,200\n\n1,055.9\n291.5\n\n1,053.9\n291.9\n\n\n\n\n\nCodept_train$block_pred_full_x |&gt; filter(condit==\"Constant\",!(Fit_Method==\"Test\")) |&gt;\n  flextable::tabulator(rows=c(\"Fit_Method\",\"Block\",\"x\"), columns=c(\"Model\"), \n                      # `Observed` = as_paragraph(y), \n                       `Predicted` = as_paragraph(pred),\n                       `ME` = as_paragraph(mean_error)) |&gt; \n  as_flextable() %&gt;% \n  apply_best_formatting(.,pull(filter(best_indices,condit==\"Constant\"),indices) %&gt;% .[[1]]) |&gt; \n  set_caption(\"Constant Train Predictions\")\n\n\n\n\n\n\nFit_Method\nBlock\nx\n\nALM\n\nEXAM\n\n\n\nPredicted\nME\n\nPredicted\nME\n\n\n\n\nTest_Train\n1\n800\n\n820.8\n294.3\n\n832.5\n296.1\n\n\n2\n\n886.0\n202.5\n\n902.6\n205.4\n\n\n3\n\n887.7\n191.1\n\n902.8\n197.5\n\n\nTrain\n1\n\n830.7\n271.6\n\n831.6\n271.4\n\n\n2\n\n889.9\n193.4\n\n890.3\n193.2\n\n\n3\n\n890.6\n182.3\n\n890.4\n182.3\n\n\n\n\n\nCode# pull(filter(best_indices,condit==\"Varied\"),indices) %&gt;% .[[1]]",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#posterior-predictive",
    "href": "Sections/e1_model_appendix.html#posterior-predictive",
    "title": "ABC-rejection models",
    "section": "Posterior Predictive:",
    "text": "Posterior Predictive:\n\nCodegroup_predictive_plots(post_dat_l) \n\n\n\n\n\n\nCodetestAvg |&gt; ggplot(aes(x=vx)) + geom_density() + facet_wrap(~vb) +\n\n\n\npost_dat_avg |&gt; filter(rank==1) |&gt; ggplot(aes(x=pred)) + geom_density() + facet_wrap(~x)\n\n\n\n\n\n\nCodepost_dat_avg |&gt; filter(rank==1) |&gt; ggplot(aes(x=pred)) + \n  geom_density() + geom_density(data=post_dat_avg, aes(x=y), color=\"red\") +\n  facet_wrap(condit~x,ncol=6)\n\n\n\n\n\n\nCodepost_dat_avg |&gt; filter(Model==\"EXAM\",Fit_Method==\"Test\") |&gt; ggplot(aes(x=pred)) + \n  geom_density() + geom_density(data=post_dat_avg, aes(x=y), color=\"red\") +\n  facet_wrap(condit~x,ncol=6)\n\n\n\n\n\n\nCodepost_dat_avg |&gt; filter(Model==\"ALM\",Fit_Method==\"Test\") |&gt; ggplot(aes(x=pred)) + \n  geom_density() + geom_density(data=post_dat_avg, aes(x=y), color=\"red\") +\n  facet_wrap(condit~x,ncol=6)\n\n\n\n\n\n\nCodepost_dat_avg |&gt; filter(Fit_Method==\"Test\") |&gt; ggplot(aes(x=pred)) + \n  geom_density(aes(fill=Model),alpha=.5) + geom_density(data=post_dat_avg, aes(x=y), color=\"black\",alpha=2) +\n  facet_wrap(condit~x,ncol=6)\n\n\n\n\n\n\nCodepost_dat_avg |&gt; filter(Fit_Method==\"Test_Train\") |&gt; ggplot(aes(x=pred)) + \n  geom_density(aes(fill=Model),alpha=.5) + geom_density(data=post_dat_avg, aes(x=y), color=\"black\",alpha=2) +\n  facet_wrap(condit~x,ncol=6)\n\n\n\n\n\n\nCodepost_dat_avg |&gt; filter(Fit_Method==\"Train\",pred&gt;0,pred&lt;2000) |&gt; ggplot(aes(x=pred)) + \n  geom_density(aes(fill=Model),alpha=.5) + geom_density(data=post_dat_avg, aes(x=y), color=\"black\",alpha=2) +\n  facet_wrap(condit~x,ncol=6)\n\n\n\n\n\n\n\nAlt grouping of Posterior Predictive\n\nCode group_predictive_plots2(post_dat_l) \n\n\n\n\n\n\n\n\nCodepost_dat_avg |&gt; group_by(id,condit,Fit_Method,x) |&gt;\n  ggplot(aes(x = x, y = error, fill=Model)) + \n  stat_bar + \n  facet_wrap(condit~Fit_Method, scales=\"free\") + \n  labs(title = \"Model Resituals - full posterior\")\n\n\n\n\n\n\nCodepost_dat_avg |&gt; group_by(id,condit,Fit_Method,x) |&gt;\n  filter(rank==1) |&gt; \n  ggplot(aes(x = x, y = error, fill=Model)) + \n  stat_bar + \n  facet_wrap(condit~Fit_Method, scales=\"free\") + \n  labs(title = \"Model Residuals - best parameters\")\n\n\n\n\n\n\n\n\nCodeplot_indv_posterior(ind_fits_df |&gt; mutate(Group=condit))\n\n\n\n\n\n\nCodeplot_indv_posterior(post_dat |&gt; filter(rank==1) |&gt; mutate(Group=condit))\n\n\n\n\n\n\nCode# plot join density of c and lr\n# post_dat |&gt; filter(rank==1, c&lt;.001) |&gt; \n#   ggplot(aes(x=c, y=lr, color=condit)) + geom_point() + facet_wrap(~Model+Fit_Method)\n\npost_dat |&gt;  filter(c&lt;.001) |&gt; ggplot(aes(x=c,y=condit,fill=condit)) + geom_boxploth(width=.5) + facet_wrap(~Model+Fit_Method,scales=\"free\")\n\n\n\n\n\n\nCodepost_dist &lt;- post_dat_avg %&gt;%\n  group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n  slice_head(n = 1) %&gt;%\n  ungroup() %&gt;% # Make sure to remove the grouping by rank before the next group_by\n  group_by(id, condit, Model, Fit_Method) %&gt;%\n  summarize(across(c:lr, list(mean = mean, sd = sd, se = ~sd(.)/sqrt(n()))), .groups = 'keep')\n\npost_dist |&gt; \n  group_by(condit, Model, Fit_Method) %&gt;%\n  mutate(across(c_mean, list(gmean = mean, gsd = sd, gse = ~sd(.)/sqrt(n()))), .groups = 'keep') |&gt;\n  filter(c_mean&lt;(c_mean_gmean+(.5*c_mean_gse))) |&gt;\n  ggplot(aes(y=id, x = c_mean)) + \n  stat_pointinterval() + \n  ggh4x::facet_nested_wrap(Fit_Method~condit~Model,scales=\"free\")\n\n\n\n\n\n\nCodepost_dist |&gt; \n  group_by(condit, Model, Fit_Method) %&gt;%\n  mutate(across(c_mean, list(gmean = median, gsd = sd, gse = ~sd(.)/sqrt(n()))), .groups = 'keep') |&gt;\n  filter(c_mean&lt;(c_mean_gmean+(.5*c_mean_gse))) |&gt;\n  ggplot(aes( x = c_mean,fill=condit)) + geom_density() +\n  ggh4x::facet_nested_wrap(Fit_Method~Model,scales=\"free\")\n\n\n\n\n\n\nCodepost_dist |&gt; \n  group_by(condit, Model, Fit_Method) %&gt;%\n  mutate(across(c_mean, list(gmean = median, gsd = sd, gse = ~sd(.)/sqrt(n()))), .groups = 'keep') |&gt;\n  filter(c_mean&lt;(c_mean_gmean+(.5*c_mean_gse))) |&gt;\n  ggplot(aes( x = log(c_mean),fill=condit)) + geom_density() +\n  ggh4x::facet_nested_wrap(Fit_Method~Model,scales=\"free\")\n\n\n\n\n\n\nCode post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes( x = log(c),fill=condit)) + geom_density() +\n    ggh4x::facet_nested_wrap(Fit_Method~Model,scales=\"free\")\n\n\n\n\n\n\nCode post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=Fit_Method, x = log(c),col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model,scales=\"free\")\n\n\n\n\n\n\nCode post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=Fit_Method, x = lr,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model,scales=\"free\")\n\n\n\n\n\n\nCode# post_dat_avg %&gt;%\n#     group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n#     slice_head(n = 1) |&gt;\n#     ggplot(aes(y=Fit_Method, x = (lr),fill=condit)) + \n#     stat_halfeye(position=position_dodge(.2)) +\n#     ggh4x::facet_nested_wrap(~Model,scales=\"free\")   \n#",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#model-comparison",
    "href": "Sections/e1_model_appendix.html#model-comparison",
    "title": "ABC-rejection models",
    "section": "Model Comparison:",
    "text": "Model Comparison:\n\nCodeindv_best_plots(post_dat_l)\n\n\n\n\n\n\nCodepost_dat_l |&gt; group_by(condit,Model,Fit_Method,rank) |&gt; \n   summarise(mean_error=mean(abs(error)), n=n()) |&gt; \n   ggplot(aes(x=rank,y=mean_error,fill=Model))+geom_col()+facet_wrap(~Fit_Method,scales=\"free\")\n\n\n\n\n\n\nCodepost_dat_l |&gt; group_by(condit,Model,Fit_Method,rank,x) |&gt; \n   summarise(mean_error=mean(abs(error)), n=n()) |&gt; \n   ggplot(aes(x=rank,y=mean_error,fill=Model))+geom_col()+facet_wrap(Fit_Method~x,ncol=6,scales=\"free\")\n\n\n\n\n\n\nCodegroup_best_plots(post_dat_l)",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#individual-predictive-plots",
    "href": "Sections/e1_model_appendix.html#individual-predictive-plots",
    "title": "ABC-rejection models",
    "section": "Individual Predictive Plots",
    "text": "Individual Predictive Plots\n\nCodeindv_predictive_plots(post_dat_l, ids2)\n\n\n\n\n\n\nCodeindv_predictive_plots(post_dat_l, idsBad)\n\n\n\n\n\n\n\nSubject 1\n\nCodeindv_predictive_dist((post_dat_l |&gt; filter(rank&lt;=200)),ind_fits_df, sbj=list(1))\n\n\n\n\n\n\nCodeabc_tables(post_dat |&gt; filter(id==1))$agg_full |&gt; flextable() |&gt; set_caption(\"Mean from full posterior\")\n\n\n\n\n\ncondit\nModel\nFit_Method\nmean_error\n\n\n\nVaried\nALM\nTest\n239.1\n\n\nVaried\nALM\nTest_Train\n291.9\n\n\nVaried\nALM\nTrain\n297.5\n\n\nVaried\nEXAM\nTest\n219.8\n\n\nVaried\nEXAM\nTest_Train\n276.8\n\n\nVaried\nEXAM\nTrain\n297.7\n\n\n\n\n\nCodeabc_tables(post_dat |&gt; filter(id==1))$agg_best |&gt; flextable() |&gt; set_caption(\"Mean from best parameters\")\n\n\n\n\n\ncondit\nModel\nFit_Method\nmean_error\n\n\n\nVaried\nALM\nTest\n237.5\n\n\nVaried\nALM\nTest_Train\n276.0\n\n\nVaried\nALM\nTrain\n296.0\n\n\nVaried\nEXAM\nTest\n219.5\n\n\nVaried\nEXAM\nTest_Train\n269.1\n\n\nVaried\nEXAM\nTrain\n296.0\n\n\n\n\n\nCodepost_dat_l |&gt; filter(id==1) |&gt; group_by(condit,Model,Fit_Method,rank) |&gt; \n   summarise(mean_error=mean(abs(error)), n=n()) |&gt; \n   ggplot(aes(x=rank,y=mean_error,fill=Model))+geom_col()+facet_wrap(~Fit_Method)\n\n\n\n\n\n\nCode# plot_indv_posterior(ind_fits_df |&gt; filter(id==1))\n# ind_fits_df |&gt; filter(id==1, Fit_Method==\"Test Only\", Model==\"EXAM\") |&gt; pull(c) |&gt; unique()\n\n\nSubject 36\n\nCodeindv_predictive_dist(post_dat_l,ind_fits_df, sbj=list(36))\n\n\n\n\n\n\nCode#abc_tables(post_dat_l |&gt; filter(id==36))$et_sum |&gt; gt::gt()\n\nabc_tables(post_dat |&gt; filter(id==36))$agg_full |&gt; flextable() |&gt; set_caption(\"Mean from full posterior\")\n\n\n\n\n\ncondit\nModel\nFit_Method\nmean_error\n\n\n\nVaried\nALM\nTest\n221.2\n\n\nVaried\nALM\nTest_Train\n316.3\n\n\nVaried\nALM\nTrain\n268.5\n\n\nVaried\nEXAM\nTest\n214.2\n\n\nVaried\nEXAM\nTest_Train\n305.6\n\n\nVaried\nEXAM\nTrain\n269.3\n\n\n\n\n\nCodeabc_tables(post_dat |&gt; filter(id==36))$agg_best |&gt; flextable() |&gt; set_caption(\"Mean from best parameters\")\n\n\n\n\n\ncondit\nModel\nFit_Method\nmean_error\n\n\n\nVaried\nALM\nTest\n216.3\n\n\nVaried\nALM\nTest_Train\n315.9\n\n\nVaried\nALM\nTrain\n261.4\n\n\nVaried\nEXAM\nTest\n210.7\n\n\nVaried\nEXAM\nTest_Train\n296.2\n\n\nVaried\nEXAM\nTrain\n268.4\n\n\n\n\n\nCode# plot_indv_posterior(ind_fits_df |&gt; filter(id==1))\n# ind_fits_df |&gt; filter(id==1, Fit_Method==\"Test Only\", Model==\"EXAM\") |&gt; pull(c) |&gt; unique()",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#more-comparisons",
    "href": "Sections/e1_model_appendix.html#more-comparisons",
    "title": "ABC-rejection models",
    "section": "More Comparisons",
    "text": "More Comparisons\n\nCodebest_id_x &lt;- post_dat |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt;\n  mutate(e2=(y-pred)) |&gt;\n  summarise(y=mean(y), pred=mean(pred), mean_error=mean(e2),abs_me=abs(mean_error)) |&gt;\n  group_by(id,condit,Fit_Method,x) |&gt; mutate(best=ifelse(mean_error==min(mean_error),1,0)) |&gt;\n  group_by(id,condit,Fit_Method,Model) |&gt; mutate(n_best=sum(best)) \n\nbest_id &lt;- best_id_x |&gt; group_by(id,condit,Fit_Method,Model) |&gt;\n  summarise(mean_error=mean(mean_error), n_best=first(n_best),abs_me=mean(abs(mean_error)))\n  \nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            n_best = n_best[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\nerror_difference &lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error,abs_me) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(abs_me,mean_error)) %&gt;%\n  mutate(Error_difference = (mean_error_ALM - mean_error_EXAM), abs_error_dif = (abs_me_ALM - abs_me_EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\n\n\nbest_id |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  arrange(mean_error) |&gt;\n  ungroup() |&gt;\n  mutate(id = reorder(id, mean_error)) %&gt;%\n  ggplot(aes(y=id,x=mean_error,fill=Model))+\n  geom_col()+\n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")\n\n\n\n\n\n\nCodebest_id |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, abs_me)) %&gt;%\n  ggplot(aes(y=id,x=abs_me,fill=Model))+\n  geom_col()+\n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")\n\n\n\n\n\n\nCodefull_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, abs_error_dif)) %&gt;%\n  ggplot(aes(y=id,x=abs_error_dif))+\n  geom_col()+\n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")\n\n\n\n\n\n\nCodefull_comparison |&gt; filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference))+\n  geom_col()+\n  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")\n\n\n\n\n\n\nCoded &lt;- testAvg |&gt; left_join(full_comparison, by=c(\"id\",\"condit\")) |&gt; filter(Fit_Method==\"Test_Train\")\n\nd |&gt; ggplot(aes(x=vb,y=dist,fill=condit)) + stat_bar + facet_wrap(Fit_Method~Best_Model,ncol=2)\n\n\n\n\n\n\nCoded |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; \n  mutate(nGrp2=n()) |&gt;\n  filter(abs(Error_difference)&gt;15) |&gt; \n  ggplot(aes(x=vb,y=dist,fill=condit)) + \n  stat_bar + facet_wrap(Fit_Method~Best_Model,ncol=2)\n\n\n\n\n\n\nCoded |&gt; group_by(condit,Fit_Method,Best_Model) %&gt;% tally() |&gt; mutate(n=n/6)\n\n# A tibble: 4 × 4\n# Groups:   condit, Fit_Method [2]\n  condit   Fit_Method Best_Model     n\n  &lt;fct&gt;    &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n1 Constant Test_Train ALM           31\n2 Constant Test_Train EXAM          49\n3 Varied   Test_Train ALM           46\n4 Varied   Test_Train EXAM          30\n\nCoded |&gt; group_by(condit,Fit_Method,Best_Model) |&gt; filter(abs(Error_difference)&gt;15) |&gt; tally() |&gt; mutate(n=n/6)\n\n# A tibble: 4 × 4\n# Groups:   condit, Fit_Method [2]\n  condit   Fit_Method Best_Model     n\n  &lt;fct&gt;    &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n1 Constant Test_Train ALM           11\n2 Constant Test_Train EXAM          35\n3 Varied   Test_Train ALM           31\n4 Varied   Test_Train EXAM          16\n\nCoded |&gt; filter(bandInt==100) |&gt; group_by(condit,Fit_Method) |&gt; summarise(m=mean(Error_difference), \n                                              sd=sd(Error_difference), \n                                              n=n(),se=sd/sqrt(n))\n\n# A tibble: 2 × 6\n# Groups:   condit [2]\n  condit   Fit_Method     m    sd     n    se\n  &lt;fct&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 Constant Test_Train 27.6   59.3    80  6.63\n2 Varied   Test_Train -8.98  54.6    76  6.27\n\nCode# head(map_id,2)\n# # A tibble: 2 × 13\n# # Groups:   id, condit, Model, Fit_Method [2]\n#   id    condit Fit_Method Model          c    lr mean_error  rank nTest nTrain nTotal     n   BIC\n#   &lt;fct&gt; &lt;fct&gt;  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n# 1 1     Varied Test       ALM   0.00000165  5.32       238.     1    63     86    149    63 -6.80\n# 2 1     Varied Test_Train ALM   0.0000634   5.60       276.     1    63     86    149   149 -6.24\n\n\n\nCode#full_comparison |&gt; round_tibble(1) |&gt; reactable(filterable=T,defaultPageSize=15)\n\nggplot(full_comparison, aes(x = Best_Model, fill = condit)) +\n  geom_bar(position = \"dodge\") +\n  facet_wrap(~Fit_Method)+\n  labs(title = \"Distribution of Subjects Best Fit by Model\",\n       x = \"Model with Lowest Error\",\n       y = \"Count of Subjects\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\nCode# Scatter plot showing the differential in mean error\nggplot(full_comparison, aes(x = Error_difference, y = condit, color = Best_Model)) +\n  geom_point(alpha=.5) +\n  stat_pointinterval()+\n  facet_wrap(~Fit_Method)+\n  labs(title = \"Differential in Mean Error Between Models\",\n       x = \"Error Difference (ALM - EXAM)\",\n       y = \"Condition\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\nCode# ggplot(full_comparison, aes(x = Error_difference)) +\n#   geom_histogram(aes(fill=Best_Model),bins = 40, alpha = 0.7) +\n#   geom_vline(aes(xintercept = mean(Error_difference)), color = \"red\", linetype = \"dashed\", size = 1) +\n#   facet_wrap(~Fit_Method)+\n#   labs(\n#     title = \"Distribution of Error Differential (ALM - EXAM)\",\n#     x = \"Error Differential\",\n#     y = \"Frequency\"\n#   ) +\n#   theme_minimal()\n\n\nbest_exam &lt;- full_comparison |&gt; filter(Best_Model==\"EXAM\") \nbest_alm &lt;- full_comparison |&gt; filter(Best_Model==\"ALM\") \n\n\nd &lt;- testAvg |&gt; left_join(full_comparison, by=c(\"id\",\"condit\"))\n\nd |&gt; ggplot(aes(x=vb,y=dist,fill=condit)) + stat_bar + facet_wrap(Fit_Method~Best_Model,ncol=2)\n\n\n\n\n\n\nCoded |&gt; filter(abs(Error_difference)&gt;20) |&gt; \n  ggplot(aes(x=vb,y=dist,fill=condit)) + \n  stat_bar + facet_wrap(Fit_Method~Best_Model,ncol=2)\n\n\n\n\n\n\nCoded |&gt; ggplot(aes(x=vb,y=vx,fill=condit)) + stat_bar + facet_wrap(Fit_Method~Best_Model,ncol=2)\n\n\n\n\n\n\nCoded |&gt; filter(Fit_Method==\"Test\") |&gt;\n  ggplot(aes(x=Error_difference,y=dist,col=condit)) + geom_point() + facet_wrap(~vb,scales=\"free\",ncol=2)\n\n\n\n\n\n\nCoded |&gt; group_by(id,condit,Fit_Method) |&gt; summarise(Error_difference=mean(Error_difference),dist=mean(dist)) |&gt; \n  ggplot(aes(x=Error_difference,y=dist,col=condit)) + geom_point() + facet_wrap(~Fit_Method,scales=\"free\")\n\n\n\n\n\n\nCode# ds |&gt; filter(expMode2==\"Test\", id %in% best_exam$id) |&gt; \n#   group_by(id,condit,x) |&gt; summarise(y=mean(y), dist=mean(abs(x-y))) |&gt;\n#   mutate(x=as.factor(x)) |&gt;\n#   ggplot(aes(x=x,y=dist,fill=condit)) + stat_bar",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#best-test---exam-fits",
    "href": "Sections/e1_model_appendix.html#best-test---exam-fits",
    "title": "ABC-rejection models",
    "section": "Best Test - EXAM Fits",
    "text": "Best Test - EXAM Fits\n\nCodestrong_fits &lt;- full_comparison |&gt; filter(model_rank&lt;=1)\nstrong_fits |&gt; filter(Fit_Method==\"Test\",Best_Model==\"EXAM\") |&gt; pull(id) %&gt;% indv_predictive_plots(post_dat_l, .)",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#best-test---alm-fits",
    "href": "Sections/e1_model_appendix.html#best-test---alm-fits",
    "title": "ABC-rejection models",
    "section": "Best Test - ALM Fits",
    "text": "Best Test - ALM Fits\n\nCodestrong_fits &lt;- full_comparison |&gt; filter(model_rank&lt;=1)\nstrong_fits |&gt; filter(Fit_Method==\"Test\",Best_Model==\"ALM\") |&gt; pull(id) %&gt;% indv_predictive_plots(post_dat_l, .)",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#best-test_train---exam-fits",
    "href": "Sections/e1_model_appendix.html#best-test_train---exam-fits",
    "title": "ABC-rejection models",
    "section": "Best Test_Train - EXAM Fits",
    "text": "Best Test_Train - EXAM Fits\n\nCodestrong_fits &lt;- full_comparison |&gt; filter(model_rank&lt;1)\nstrong_fits |&gt; filter(Fit_Method==\"Test_Train\",Best_Model==\"EXAM\") |&gt; pull(id) %&gt;% indv_predictive_plots(post_dat_l, .)",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#best-test_train---alm-fits",
    "href": "Sections/e1_model_appendix.html#best-test_train---alm-fits",
    "title": "ABC-rejection models",
    "section": "Best Test_Train - ALM Fits",
    "text": "Best Test_Train - ALM Fits\n\nCodestrong_fits &lt;- full_comparison |&gt; filter(model_rank&lt;=1)\nstrong_fits |&gt; filter(Fit_Method==\"Test_Train\",Best_Model==\"ALM\") |&gt; pull(id) %&gt;% indv_predictive_plots(post_dat_l, .)",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#best-train---exam-fits",
    "href": "Sections/e1_model_appendix.html#best-train---exam-fits",
    "title": "ABC-rejection models",
    "section": "Best Train - EXAM Fits",
    "text": "Best Train - EXAM Fits\n\nCodestrong_fits &lt;- full_comparison |&gt; filter(model_rank&lt;=1)\nstrong_fits |&gt; filter(Fit_Method==\"Train\",Best_Model==\"EXAM\") |&gt; pull(id) %&gt;% indv_predictive_plots(post_dat_l, .)",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#best-train---alm-fits",
    "href": "Sections/e1_model_appendix.html#best-train---alm-fits",
    "title": "ABC-rejection models",
    "section": "Best Train - ALM Fits",
    "text": "Best Train - ALM Fits\n\nCodestrong_fits &lt;- full_comparison |&gt; filter(model_rank&lt;=1)\nstrong_fits |&gt; filter(Fit_Method==\"Train\",Best_Model==\"ALM\") |&gt; pull(id) %&gt;% indv_predictive_plots(post_dat_l, .)",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Sections/e1_model_appendix.html#learning-curves",
    "href": "Sections/e1_model_appendix.html#learning-curves",
    "title": "ABC-rejection models",
    "section": "Learning Curves",
    "text": "Learning Curves\n\nCodepd_train_l |&gt; ggplot(aes(x=Block,y=dist,col=Resp))+\n  geom_point( stat = \"summary\", fun = mean) + \n  stat_summary( geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.1,alpha=.4) +\n  #facet_wrap(condit~x,scales=\"free\")\n  ggh4x::facet_nested_wrap(Fit_Method~condit~x,scales=\"free\") +\n  #coord_cartesian(ylim = c(600, 1300)) +\n  labs(title=\"Full Posterior\")\n\n\n\nLearning Curves\n\n\nCodepd_train_l |&gt;filter(rank==1) |&gt; ggplot(aes(x=Block,y=val,col=Resp))+\n  geom_point( stat = \"summary\", fun = mean) + \n  stat_summary( geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.1,alpha=.4) +\n  #facet_wrap(condit~x,scales=\"free\")\n  ggh4x::facet_nested_wrap(Fit_Method~condit~x,scales=\"free\") +\n  coord_cartesian(ylim = c(600, 1300)) +labs(title=\"Best Parameters\")\n\n\n\nLearning Curves\n\n\nCodepd_train_l |&gt; filter(Fit_Method==\"Train\") |&gt; \n  mutate(x=as.factor(x), Resp=as.factor(Resp), Block=as.factor(Block)) |&gt;\n  ggplot(aes(x=x,y=val,fill=Block))+\n  stat_bar +\n  #facet_wrap(condit~x,scales=\"free\")\n  facet_wrap(condit~Resp,scales=\"free\",ncol=3) +\n  coord_cartesian(ylim = c(600, 1300))\n\n\n\nLearning Curves\n\n\nCode# pd_train_l |&gt; filter(id %in% c(1,33,66, 36,76), Fit_Method==\"Train\") |&gt; \n#   ggplot(aes(x=Block,y=val,col=Resp))+\n#   geom_point( stat = \"summary\", fun = mean) + \n#   stat_summary( geom = \"line\", fun = mean) +\n#   stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.1,alpha=.4) +\n#   #facet_wrap(condit~x,scales=\"free\")\n#   facet_wrap(id~x,scales=\"free\",ncol=3) +\n#   coord_cartesian(ylim = c(600, 1300))\n\n\npd_train_l |&gt; \n  #filter(id %in% c(1,33,66, 36,76), Fit_Method==\"Train\") |&gt; \n  mutate(x=as.factor(x), Resp=as.factor(Resp), Block=as.factor(Block)) |&gt;\n  ggplot(aes(x=x,y=dist,fill=Block))+\n  stat_bar +\n  #facet_wrap(condit~x,scales=\"free\")\n   #coord_cartesian(ylim = c(600, 1300)) +\n  facet_wrap(id~Resp,scales=\"free\",ncol=3) \n\n\n\nLearning Curves\n\n\n\n\nCodepost_dat |&gt; ggplot(aes(x=Model,y=mean_error,fill=Model))+geom_col()+facet_wrap(~Fit_Method)\npost_dat |&gt; ggplot(aes(x=Model,y=mean_error,fill=Model))+geom_col()+facet_wrap(condit~Fit_Method)\n\n\n\nCode# post_tabs &lt;- map(post_tabs, ~cbind(.x,run_info) |&gt; mutate(fn=file_name))\n# post_tabs$agg_pred_full_train &lt;- pt_train$agg_pred_full\n# post_tabs$block_pred_full &lt;- pt_train$block_pred_full\n# post_tabs$block_pred_full_x &lt;- pt_train$block_pred_full_x\n\n# # path1 = \"../../../data/abc_tabs\"\n# saveRDS(post_tabs, paste0(path1, \"/\", tools::file_path_sans_ext(basename(file_name)),\"_post_tab\", \".rds\"))\n# saveRDS(post_tabs, paste0(tools::file_path_sans_ext(basename(file_name)),\"_post_tab\", \".rds\"))",
    "crumbs": [
      "Appendix",
      "ABC-rejection models"
    ]
  },
  {
    "objectID": "Simulations/SimReplications.html",
    "href": "Simulations/SimReplications.html",
    "title": "General Simulations",
    "section": "",
    "text": "Functions\n\nCodepacman::p_load(tidyverse,reshape2)\n\ninput.activation&lt;-function(x.target, association.parameter){\n  return(exp(-1*association.parameter*(x.target-x.plotting)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, association.parameter){\n  return(weights%*%input.activation(x.target, association.parameter))\n}\n\nmean.prediction&lt;-function(x.target, weights, association.parameter){\n  probability&lt;-output.activation(x.target, weights, association.parameter)/sum(output.activation(x.target, weights, association.parameter))\n  return(y.plotting%*%probability) # integer prediction\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, association.parameter){\n  trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, association.parameter)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, association.parameter)\n  mOver = mean.prediction(xOver, weights, association.parameter)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n  \nupdate.weights&lt;-function(x.new, y.new, weights, association.parameter, update.parameter){\n  y.feedback.activation&lt;-exp(-1*association.parameter*(y.new-y.plotting)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, association.parameter)\n  return(weights+update.parameter*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, association.parameter)))\n}\n\nlearn.alm&lt;-function(y.learning, association.parameter=0.05, update.parameter=0.5){\n  weights&lt;-matrix(rep(0.00, length(y.plotting)*length(x.plotting)), nrow=length(y.plotting), ncol=length(x.plotting))\n  for (i in 1:length(y.learning)){\n    weights&lt;-update.weights(x.learning[i], y.learning[i], weights, association.parameter, update.parameter)\n    resp=mean.prediction(x.learning[i],weights,association.parameter)\n    weights[weights&lt;0]=0\n  }\n  alm.predictions&lt;-sapply(x.plotting, mean.prediction, weights=weights, association.parameter=association.parameter)\n  exam.predictions &lt;- sapply(x.plotting, exam.prediction, weights=weights, association.parameter=association.parameter)\n  return(list(alm.predictions=alm.predictions, exam.predictions=exam.predictions))\n}\n\n\nNo noise, 1 training rep\nRed dots are training points - gray lines are individual simulations, black line is average of simulations\n\nCode# | eval: false\n\ntrainRep=1\n\nx.plotting&lt;&lt;-seq(0,100, .5)\ny.plotting&lt;&lt;-seq(0, 210, by=2)\nf.plotting&lt;-as.numeric(x.plotting*2.2+30)\nx.learning&lt;-rep(x.plotting[20*c(4:7)+1])\nf.learning&lt;-rep(f.plotting[20*c(4:7)+1])\n\nparmVec &lt;- expand.grid(assoc=c(.1,0.5),update=c(0.2,1),noise=c(0),trainRep=c(1))\n#parmVec &lt;- expand.grid(assoc=c(.01),update=c(0.5),noise=c(30),trainRep=c(1,2,3,4))\n\nparmVec$sim &lt;- 1:nrow(parmVec)\nnSim=nrow(parmVec)\n\nnRep=5\noutput &lt;- list()\nfor (i in 1:nrow(parmVec)){\n  x.learning&lt;-rep(x.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  f.learning&lt;-rep(f.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  #noise.learning &lt;- rnorm(n_distinct(f.learning),sd=parmVec$noise[i])\n  output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning+rep(rnorm(n_distinct(f.learning),sd=parmVec$noise[i]),times=parmVec$trainRep[i]), \n                                         association.parameter=parmVec$assoc[i], update.parameter=parmVec$update[i])))\n}\n\n\n# convert list of dataframes to a list of lists, each list is a simulation, each element is a dataframe\noutput1 &lt;- lapply(output, function(x) lapply(x, as.data.frame)) # 10 dfs x 9 lists\noutput2 &lt;- lapply(output1, function(x) Reduce(rbind,x))# 1 df x 9 lists\noutput3 &lt;- lapply(output2, function(x) mutate(x, x=rep(x.plotting,nRep),y=rep(f.plotting,nRep),\n                                              repN=rep(seq(1,nRep),each=length(x.plotting))))\no4 &lt;- Reduce(rbind,output3) %&gt;% \n  mutate(sim=rep(seq(1,nrow(parmVec)),each=nRep*length(x.plotting))) %&gt;%\n  left_join(.,parmVec,by=\"sim\") %&gt;%\n  mutate(pvec=paste0(\"c=\",assoc,\"_lr=\",update,\"_noise=\",noise,\"_nrep=\",trainRep),pv=factor(pvec),rn=factor(repN)) \n\noMeans &lt;- o4 %&gt;% group_by(pv,x,y) %&gt;% \n  summarise(alm.predictions=mean(alm.predictions),exam.predictions=mean(exam.predictions),.groups=\"keep\")\n\no4 %&gt;% ggplot(aes(x=x,y=alm.predictions,color=rn))+geom_line(alpha=.7)+\n   scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5, linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=alm.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"ALM predictions\")\n\n\n\n\n\n\nCodeo4 %&gt;% ggplot(aes(x=x,y=exam.predictions,color=rn))+ geom_line()+ #geom_line(color=\"grey\",alpha=.4)+\n  scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5,linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=exam.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"EXAM predictions\")\n\n\n\n\n\n\n\nHigh noise, 1 training rep\nRed dots are training points - gray lines are individual simulations, black line is average of simulations\n\nCodetrainRep=1\n\nx.plotting&lt;&lt;-seq(0,100, .5)\ny.plotting&lt;&lt;-seq(0, 210, by=2)\nf.plotting&lt;-as.numeric(x.plotting*2.2+30)\nx.learning&lt;-rep(x.plotting[20*c(4:7)+1])\nf.learning&lt;-rep(f.plotting[20*c(4:7)+1])\n\n\nparmVec &lt;- expand.grid(assoc=c(.1,0.5),update=c(0.2,1),noise=c(30),trainRep=c(1))\n#parmVec &lt;- expand.grid(assoc=c(.01),update=c(0.5),noise=c(30),trainRep=c(1,2,3,4))\n\nparmVec$sim &lt;- 1:nrow(parmVec)\nnSim=nrow(parmVec)\n\nnRep=10\noutput &lt;- list()\nfor (i in 1:nrow(parmVec)){\n  x.learning&lt;-rep(x.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  f.learning&lt;-rep(f.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  #noise.learning &lt;- rnorm(n_distinct(f.learning),sd=parmVec$noise[i])\n  output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning+rep(rnorm(n_distinct(f.learning),sd=parmVec$noise[i]),times=parmVec$trainRep[i]), \n                                         association.parameter=parmVec$assoc[i], update.parameter=parmVec$update[i])))\n}\n\n# \n# nRep=3\n# output &lt;- list()\n# for (i in 1:nrow(parmVec)){\n#   output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning + rnorm(length(f.learning), sd=10), \n#                                          association.parameter=parmVec$assoc[i], update.parameter=parmVec$update[i])))\n# }\n\n\n\n#output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning + rnorm(length(f.learning), sd=10)\n\n# convert list of dataframes to a list of lists, each list is a simulation, each element is a dataframe\noutput1 &lt;- lapply(output, function(x) lapply(x, as.data.frame)) # 10 dfs x 9 lists\noutput2 &lt;- lapply(output1, function(x) Reduce(rbind,x))# 1 df x 9 lists\noutput3 &lt;- lapply(output2, function(x) mutate(x, x=rep(x.plotting,nRep),y=rep(f.plotting,nRep),\n                                              repN=rep(seq(1,nRep),each=length(x.plotting))))\no4 &lt;- Reduce(rbind,output3) %&gt;% \n  mutate(sim=rep(seq(1,nrow(parmVec)),each=nRep*length(x.plotting))) %&gt;%\n  left_join(.,parmVec,by=\"sim\") %&gt;%\n  mutate(pvec=paste0(\"c=\",assoc,\"_lr=\",update,\"_noise=\",noise,\"_nrep=\",trainRep),pv=factor(pvec),rn=factor(repN)) \n\noMeans &lt;- o4 %&gt;% group_by(pv,x,y) %&gt;% \n  summarise(alm.predictions=mean(alm.predictions),exam.predictions=mean(exam.predictions),.groups=\"keep\")\n\no4 %&gt;% ggplot(aes(x=x,y=alm.predictions,color=rn))+geom_line(alpha=.7)+\n   scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5, linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=alm.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"ALM predictions\")+ylim(0,300)\n\no4 %&gt;% ggplot(aes(x=x,y=exam.predictions,color=rn))+ geom_line()+ #geom_line(color=\"grey\",alpha=.4)+\n  scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5,linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=exam.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"EXAM predictions\")+ylim(0,300)\n\n\nHigh noise, 60 training rep\nRed dots are training points - gray lines are individual simulations, black line is average of simulations\n\nCodetrainRep=1\n\nx.plotting&lt;&lt;-seq(0,100, .5)\ny.plotting&lt;&lt;-seq(0, 210, by=2)\nf.plotting&lt;-as.numeric(x.plotting*2.2+30)\nx.learning&lt;-rep(x.plotting[20*c(4:7)+1])\nf.learning&lt;-rep(f.plotting[20*c(4:7)+1])\n\n\nparmVec &lt;- expand.grid(assoc=c(.1,0.5),update=c(0.2,1),noise=c(30),trainRep=c(60))\n#parmVec &lt;- expand.grid(assoc=c(.01),update=c(0.5),noise=c(30),trainRep=c(1,2,3,4))\n\nparmVec$sim &lt;- 1:nrow(parmVec)\nnSim=nrow(parmVec)\n\nnRep=10\noutput &lt;- list()\nfor (i in 1:nrow(parmVec)){\n  x.learning&lt;-rep(x.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  f.learning&lt;-rep(f.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  #noise.learning &lt;- rnorm(n_distinct(f.learning),sd=parmVec$noise[i])\n  output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning+rep(rnorm(n_distinct(f.learning),sd=parmVec$noise[i]),times=parmVec$trainRep[i]), \n                                         association.parameter=parmVec$assoc[i], update.parameter=parmVec$update[i])))\n}\n\n\n# convert list of dataframes to a list of lists, each list is a simulation, each element is a dataframe\noutput1 &lt;- lapply(output, function(x) lapply(x, as.data.frame)) # 10 dfs x 9 lists\noutput2 &lt;- lapply(output1, function(x) Reduce(rbind,x))# 1 df x 9 lists\noutput3 &lt;- lapply(output2, function(x) mutate(x, x=rep(x.plotting,nRep),y=rep(f.plotting,nRep),\n                                              repN=rep(seq(1,nRep),each=length(x.plotting))))\no4 &lt;- Reduce(rbind,output3) %&gt;% \n  mutate(sim=rep(seq(1,nrow(parmVec)),each=nRep*length(x.plotting))) %&gt;%\n  left_join(.,parmVec,by=\"sim\") %&gt;%\n  mutate(pvec=paste0(\"c=\",assoc,\"_lr=\",update,\"_noise=\",noise,\"_nrep=\",trainRep),pv=factor(pvec),rn=factor(repN)) \n\noMeans &lt;- o4 %&gt;% group_by(pv,x,y) %&gt;% \n  summarise(alm.predictions=mean(alm.predictions),exam.predictions=mean(exam.predictions),.groups=\"keep\")\n\no4 %&gt;% ggplot(aes(x=x,y=alm.predictions,color=rn))+geom_line(alpha=.7)+\n   scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5, linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=alm.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"ALM predictions\")+ylim(0,300)\n\no4 %&gt;% ggplot(aes(x=x,y=exam.predictions,color=rn))+ geom_line()+ #geom_line(color=\"grey\",alpha=.4)+\n  scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5,linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=exam.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"EXAM predictions\")+ylim(0,300)\n\n\nShiny App\n\nCodex.plotting&lt;&lt;-seq(0,90, .5)\ny.plotting&lt;&lt;-seq(0, 210, by=2)\nf.plotting&lt;-as.numeric(x.plotting * 2.2 + 30)\nx.learning&lt;-x.plotting[10*c(3:9)+1]\nf.learning&lt;-f.plotting[10*c(3:9)+1]\n\n# Single Simulation\n# get alm and exam predictions for full range of x.plotting\noutput&lt;-learn.alm(f.learning)\nalm.predictions&lt;-output$alm.predictions\nexam.predictions&lt;-output$exam.predictions\n\n# plot the results\nplot(x.plotting, f.plotting, type=\"l\", col=\"blue\", lwd=.5, xlab=\"x\", ylab=\"f(x)\")\npoints(x.learning, f.learning, col=\"red\", pch=19)\nlines(x.plotting, alm.predictions, col=\"green\", lwd=2)\nlines(x.plotting, exam.predictions, col=\"purple\", lwd=2)\nlegend(\"topright\", legend=c(\"f(x)\", \"training data\", \"ALM\", \"Exam\"), col=c(\"blue\", \"red\", \"green\", \"purple\"), lty=1.5, cex=0.8)\n\n#function to plot in greyscale\nplot.grey&lt;-function(predictions){\n  lines(x.plotting, predictions, col=\"grey\")\n}\n\n\n\nCode# Average of 100 simulations:\n# get alm and exam predictions for full range of x.plotting, averaged over 100 simulations\nnSim&lt;-10\noutput &lt;- replicate(nSim, list(learn.alm(f.learning + rnorm(length(f.learning), sd=0.1), \n                                         association.parameter=0.05, update.parameter=0.5)))\n\n#alm.predictions&lt;-do.call(rbind, lapply(output, function(x) x$alm.predictions))\nalm.predictions &lt;- Reduce(rbind,output %&gt;% map(\"alm.predictions\"))\nexam.predictions &lt;- Reduce(rbind,output %&gt;% map(\"exam.predictions\"))\n\nalm.predictions.avg&lt;-apply(alm.predictions, 2, mean)\nexam.predictions.avg&lt;-apply(exam.predictions, 2, mean)\ndfAvg&lt;-data.frame(x=x.plotting, f=f.plotting, alm=alm.predictions.avg, exam=exam.predictions.avg)\ndfAvg&lt;-reshape2::melt(dfAvg, id.vars=\"x\")\ndfAvg$model&lt;-factor(dfAvg$variable, levels=c(\"f\", \"alm\", \"exam\"))\nggplot(dfAvg, aes(x=x, y=value, color=model)) + geom_line() + geom_point(data=data.frame(x=x.learning, f=f.learning), aes(x=x, y=f), color=\"red\", size=2) + theme_bw() + theme(legend.position=\"topright\")\n\n\nalm.predictions&lt;-as.data.frame(alm.predictions) %&gt;% mutate(sim=seq(1:nSim))\nalm.predictions&lt;-pivot_longer(alm.predictions, cols=1:ncol(alm.predictions)-1, \n                              names_to=c(\"sim\"), values_to=\"alm\",names_repair = \"unique\") \ncolnames(alm.predictions)=c(\"sim\",\"x\",\"pred\")\nalm.predictions &lt;- alm.predictions %&gt;% mutate(stim = as.numeric(gsub(\"V\", \"\", x)),model=\"alm\",x=x.plotting[stim])\n\n\nexam.predictions&lt;-as.data.frame(exam.predictions) %&gt;% mutate(sim=seq(1:nSim))\nexam.predictions&lt;-pivot_longer(exam.predictions, cols=1:ncol(exam.predictions)-1, \n                               names_to=c(\"sim\"), values_to=\"exam\",names_repair = \"unique\")\ncolnames(exam.predictions)=c(\"sim\",\"x\",\"pred\")\nexam.predictions &lt;- exam.predictions %&gt;% mutate(stim = as.numeric(gsub(\"V\", \"\", x)),model=\"exam\",x=x.plotting[stim])\n\n\ndf&lt;- rbind(alm.predictions,exam.predictions)\n\nggplot(df, aes(x=x, y=pred, color=sim)) + geom_line(alpha=.2) + facet_wrap(~model) + theme_bw() + \n  geom_point(data=data.frame(x=x.learning, f=f.learning), aes(x=x, y=f), color=\"red\", size=2)+\n  geom_line(data=data.frame(x=x.plotting, f=f.plotting),aes(x=x,y=f),color=\"black\")+\n  geom_line(data=dfAvg %&gt;% filter(model!=\"f\"),aes(x=x,y=value),color=\"green\")",
    "crumbs": [
      "Simulations",
      "General Simulations"
    ]
  },
  {
    "objectID": "Simulations/exp1_sim.html",
    "href": "Simulations/exp1_sim.html",
    "title": "HTW Project",
    "section": "",
    "text": "Design The experiment employed a 2 (Training Condition: varied vs. constant).\nProcedure Upon arrival at the laboratory, participants were provided with a description of the experiment and signed informed consent forms. They were then seated in front of a computer equipped with a mouse and were given instructions on how to perform the “Hit The Wall” (HTW) visuomotor extrapolation task.\nThe HTW task involved launching projectiles to hit a target displayed on the computer screen. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). In contrast, participants in the constant training condition encountered only one velocity band (800-1000).\nDuring the training stage, participants in both conditions also completed “no feedback” trials, where they received no information about their performance. These trials were randomly interleaved with the regular training trials.\nFollowing the training stage, participants proceeded to the testing stage, which consisted of three phases. In the first phase, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials.\nIn the second phase of testing, participants completed “no-feedback” testing from the three velocity bands used during the training stage (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training.\nThe third and final phase of testing involved “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 10 trials. Participants received feedback on their performance during this phase.\nThroughout the experiment, participants’ performance was measured by calculating the distance between the produced x-velocity of the projectiles and the closest edge of the current velocity band. Lower distances indicated better performance.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# simulate design of experiment\nset.seed(123)\nn_participants &lt;- 20\nnum_constant_subjects &lt;- 10\nnum_varied_subjects &lt;- 10\nn_trials &lt;- 90\nn_bands_varied &lt;- 3\nn_bands_constant &lt;- 1\n# Set the training positions\ntraining_pos_constant &lt;- 800\ntraining_pos_varied &lt;- c(800, 1000, 1200)\n\ntest_positions &lt;- c(200, 400, 600, 800, 1000, 1200)\n# Set the noise level (standard deviation of the noise)\nnoise_level &lt;- 0.1\n# Set the initial performance for each subject\ninitial_performance &lt;- 0.5\n# Set the learning rate (how much performance improves with each trial)\nlearning_rate &lt;- 0.01\n\n# Set the number of trials for each phase\nn_training_trials &lt;- 90\nn_testing_trials &lt;- 15\nn_feedback_trials &lt;- 10\n\n# Set the number of phases\nn_phases &lt;- 3\n\n\n# Initialize vectors to store the performance of each subject\nperformance_constant &lt;- rep(initial_performance, num_constant_subjects)\nperformance_varied &lt;- rep(initial_performance, num_varied_subjects)\n\n\n\n\n\n# Simulate the training phase for the constant condition\nfor (i in 1:num_constant_subjects) {\n  for (j in 1:n_training_trials) {\n    # Simulate the participant's response\n    response &lt;- rnorm(1, training_pos_constant, noise_level)\n    # Calculate the distance between the response and the target\n    distance &lt;- abs(response - training_pos_constant)\n    # Update the performance\n    performance_constant[i] &lt;- performance_constant[i] + learning_rate * (1 - distance)\n  }\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HTW Project",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nMar 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "index.html#analyses",
    "href": "index.html#analyses",
    "title": "HTW Project",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nMar 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "index.html#modeling-empircal-data",
    "href": "index.html#modeling-empircal-data",
    "title": "HTW Project",
    "section": "Modeling Empircal Data",
    "text": "Modeling Empircal Data\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nHTW Hybrid Models\n\n\n\nModeling\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\nThomas Gorman\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Model\n\n\n\nModeling\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\nThomas Gorman\n\n\nMar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Model e1\n\n\n\nModeling\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\nThomas Gorman\n\n\nMar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Model e2\n\n\n\nModeling\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\nThomas Gorman\n\n\nMar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Model e3\n\n\n\nModeling\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\nThomas Gorman\n\n\nMar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Modeling\n\n\n\nModeling\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\nThomas Gorman\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "index.html#simulations",
    "href": "index.html#simulations",
    "title": "HTW Project",
    "section": "Simulations",
    "text": "Simulations\n\n\n\n\n\n\n\n\nALM Learning\n\n\n\nSimulation\n\n\nALM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking\n\n\n\nSimulation\n\n\nALM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEXAM Group Fits\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nMar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Simulations\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Recovery Simulations\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating DeLosh 1997\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nMay 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "index.html#misc",
    "href": "index.html#misc",
    "title": "HTW Project",
    "section": "Misc",
    "text": "Misc\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n\n\n\n\n\n\n\n\nALM Shiny App Code\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nShiny\n\n\nInteractive\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Task\n\n\n\nTask\n\n\njs\n\n\n\n\n\n\n\nThomas Gorman\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMermaid with R + consort\n\n\n\nVisualization\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\nModel Visualization\n\n\n\nVisualization\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOJS ALM\n\n\n\nSimulation\n\n\nALM\n\n\nOJS\n\n\nInteractive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOJS data exploration\n\n\n\nAnalysis\n\n\nLearning-Curve\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\n\n\n\n\ntikz\n\n\n\nVisualization\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJan 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Model/pdf_test.html",
    "href": "Model/pdf_test.html",
    "title": "HTW Model",
    "section": "",
    "text": "Task Stage\nFit_Method\nModel\n\nE2\n\nE3\n\n\n\nConstant\n\nVaried\n\nConstant\n\nVaried\n\n\n\n\nTest\nFit to Test Data\nALM\n\n239.7\n\n129.8\n\n170.1\n\n106.1\n\n\nEXAM\n\n99.7\n\n88.2\n\n92.3\n\n72.8\n\n\nFit to Test & Training Data\nALM\n\n266.0\n\n208.2\n\n197.7\n\n189.5\n\n\nEXAM\n\n125.1\n\n126.4\n\n130.0\n\n128.5\n\n\nFit to Training Data\nALM\n\n357.4\n\n295.9\n\n415.0\n\n298.8\n\n\nEXAM\n\n305.1\n\n234.5\n\n295.5\n\n243.7\n\n\nTrain\nFit to Test Data\nALM\n\n53.1\n\n527.1\n\n70.9\n\n543.5\n\n\nEXAM\n\n108.1\n\n169.3\n\n157.8\n\n212.7\n\n\nFit to Test & Training Data\nALM\n\n40.0\n\n35.4\n\n49.1\n\n85.6\n\n\nEXAM\n\n30.4\n\n23.6\n\n49.2\n\n78.4\n\n\nFit to Training Data\nALM\n\n42.5\n\n23.0\n\n51.4\n\n63.8\n\n\nEXAM\n\n43.2\n\n22.6\n\n51.8\n\n65.3\n\n\n\n\n\n\n\n\nTable 1: Average metrics across the 10\n\n\n&lt;div id=\"ymxpfqmnzk\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\"&gt;\n  &lt;style&gt;#ymxpfqmnzk table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#ymxpfqmnzk thead, #ymxpfqmnzk tbody, #ymxpfqmnzk tfoot, #ymxpfqmnzk tr, #ymxpfqmnzk td, #ymxpfqmnzk th {\n  border-style: none;\n}\n\n#ymxpfqmnzk p {\n  margin: 0;\n  padding: 0;\n}\n\n#ymxpfqmnzk .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#ymxpfqmnzk .gt_title {\n  color: #333333;\n  font-size: large;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#ymxpfqmnzk .gt_subtitle {\n  color: #333333;\n  font-size: medium;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#ymxpfqmnzk .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: small;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#ymxpfqmnzk .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: small;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#ymxpfqmnzk .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#ymxpfqmnzk .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#ymxpfqmnzk .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#ymxpfqmnzk .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#ymxpfqmnzk .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#ymxpfqmnzk .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ymxpfqmnzk .gt_from_md &gt; :first-child {\n  margin-top: 0;\n}\n\n#ymxpfqmnzk .gt_from_md &gt; :last-child {\n  margin-bottom: 0;\n}\n\n#ymxpfqmnzk .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#ymxpfqmnzk .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ymxpfqmnzk .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#ymxpfqmnzk .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#ymxpfqmnzk .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#ymxpfqmnzk .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ymxpfqmnzk .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#ymxpfqmnzk .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ymxpfqmnzk .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#ymxpfqmnzk .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ymxpfqmnzk .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ymxpfqmnzk .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ymxpfqmnzk .gt_left {\n  text-align: left;\n}\n\n#ymxpfqmnzk .gt_center {\n  text-align: center;\n}\n\n#ymxpfqmnzk .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#ymxpfqmnzk .gt_font_normal {\n  font-weight: normal;\n}\n\n#ymxpfqmnzk .gt_font_bold {\n  font-weight: bold;\n}\n\n#ymxpfqmnzk .gt_font_italic {\n  font-style: italic;\n}\n\n#ymxpfqmnzk .gt_super {\n  font-size: 65%;\n}\n\n#ymxpfqmnzk .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#ymxpfqmnzk .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#ymxpfqmnzk .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#ymxpfqmnzk .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#ymxpfqmnzk .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#ymxpfqmnzk .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#ymxpfqmnzk .gt_indent_5 {\n  text-indent: 25px;\n}\n&lt;/style&gt;\n  &lt;table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\"&gt;\n  &lt;thead&gt;\n    &lt;tr class=\"gt_heading\"&gt;\n      &lt;td colspan=\"5\" class=\"gt_heading gt_title gt_font_normal\" style&gt;Mean Error by Model and Condition&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_heading\"&gt;\n      &lt;td colspan=\"5\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\" style&gt;Comparison of ALM and EXAM Models across Different Fit Methods&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings gt_spanner_row\"&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black; text-align: left;\" scope=\"col\" id=\"Task Stage\"&gt;Task Stage&lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"E2\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;E2&lt;/span&gt;\n      &lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"E3\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;E3&lt;/span&gt;\n      &lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings\"&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black; text-align: left;\" scope=\"col\" id=\"Constant\"&gt;Constant&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black; text-align: left;\" scope=\"col\" id=\"Varied\"&gt;Varied&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black; text-align: left;\" scope=\"col\" id=\"Constant\"&gt;Constant&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black; text-align: left;\" scope=\"col\" id=\"Varied\"&gt;Varied&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody class=\"gt_table_body\"&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"5\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Test Data - ALM\"&gt;Fit to Test Data - ALM&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Test Data - ALM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - ALM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;239.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - ALM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;129.8&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - ALM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;170.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - ALM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;106.1&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test Data - ALM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - ALM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;53.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - ALM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;527.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - ALM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;70.9&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - ALM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;543.5&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"5\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Test Data - EXAM\"&gt;Fit to Test Data - EXAM&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Test Data - EXAM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - EXAM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;99.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - EXAM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;88.2&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - EXAM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;92.3&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - EXAM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;72.8&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test Data - EXAM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - EXAM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;108.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - EXAM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;169.3&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - EXAM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;157.8&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data - EXAM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;212.7&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"5\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Test &amp;amp; Training Data - ALM\"&gt;Fit to Test &amp; Training Data - ALM&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Test & Training Data - ALM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - ALM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;266.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - ALM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;208.2&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - ALM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;197.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - ALM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;189.5&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test & Training Data - ALM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - ALM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;40.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - ALM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;35.4&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - ALM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;49.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - ALM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;85.6&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"5\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Test &amp;amp; Training Data - EXAM\"&gt;Fit to Test &amp; Training Data - EXAM&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Test & Training Data - EXAM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - EXAM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;125.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - EXAM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;126.4&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - EXAM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;130.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - EXAM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;128.5&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test & Training Data - EXAM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - EXAM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;30.4&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - EXAM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;23.6&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - EXAM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;49.2&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data - EXAM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;78.4&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"5\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Training Data - ALM\"&gt;Fit to Training Data - ALM&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Training Data - ALM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - ALM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;357.4&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - ALM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;295.9&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - ALM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;415.0&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - ALM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;298.8&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Training Data - ALM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - ALM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;42.5&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - ALM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;23.0&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - ALM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;51.4&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - ALM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;63.8&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"5\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Training Data - EXAM\"&gt;Fit to Training Data - EXAM&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Training Data - EXAM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - EXAM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;305.1&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - EXAM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;234.5&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - EXAM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;295.5&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - EXAM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;243.7&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Training Data - EXAM  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - EXAM  E2_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;43.2&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - EXAM  E2_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;22.6&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - EXAM  E3_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;51.8&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data - EXAM  E3_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;65.3&lt;/td&gt;&lt;/tr&gt;\n  &lt;/tbody&gt;\n  \n  \n&lt;/table&gt;\n&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1.\n\n\nAn example of a gt table with e23_tab data\n\n\nTask Stage\nExp\nConstant (ALM)\nVaried (ALM)\nConstant (EXAM)\nVaried (EXAM)\n\n\n\n\nFit to Test Data\n\n\nTest\nE2\n239.7\n129.8\n99.7\n88.2\n\n\nTrain\nE2\n53.1\n527.1\n108.1\n169.3\n\n\nTest\nE3\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\nE3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n\n\nTest\nE2\n266.0\n208.2\n125.1\n126.4\n\n\nTrain\nE2\n40.0\n35.4\n30.4\n23.6\n\n\nTest\nE3\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\nE3\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n\n\nTest\nE2\n357.4\n295.9\n305.1\n234.5\n\n\nTrain\nE2\n42.5\n23.0\n43.2\n22.6\n\n\nTest\nE3\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\nE3\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1.\n\n\nAn example of a gt table with e23_tab data\n\n\nTask Stage\nExp\nVaried (ALM)\nVaried (EXAM)\nConstant (ALM)\nConstant (EXAM)\n\n\n\n\nFit to Test Data\n\n\nTest\nE2\n129.8\n88.2\n239.7\n99.7\n\n\nTrain\nE2\n527.1\n169.3\n53.1\n108.1\n\n\nTest\nE3\n106.1\n72.8\n170.1\n92.3\n\n\nTrain\nE3\n543.5\n212.7\n70.9\n157.8\n\n\nFit to Test & Training Data\n\n\nTest\nE2\n208.2\n126.4\n266.0\n125.1\n\n\nTrain\nE2\n35.4\n23.6\n40.0\n30.4\n\n\nTest\nE3\n189.5\n128.5\n197.7\n130.0\n\n\nTrain\nE3\n85.6\n78.4\n49.1\n49.2\n\n\nFit to Training Data\n\n\nTest\nE2\n295.9\n234.5\n357.4\n305.1\n\n\nTrain\nE2\n23.0\n22.6\n42.5\n43.2\n\n\nTest\nE3\n298.8\n243.7\n415.0\n295.5\n\n\nTrain\nE3\n63.8\n65.3\n51.4\n51.8\n\n\n\n\n\n\n\n\n\n&lt;div id=\"kvovcbitvd\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\"&gt;\n  &lt;style&gt;#kvovcbitvd table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#kvovcbitvd thead, #kvovcbitvd tbody, #kvovcbitvd tfoot, #kvovcbitvd tr, #kvovcbitvd td, #kvovcbitvd th {\n  border-style: none;\n}\n\n#kvovcbitvd p {\n  margin: 0;\n  padding: 0;\n}\n\n#kvovcbitvd .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 100%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#kvovcbitvd .gt_title {\n  color: #333333;\n  font-size: 14px;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#kvovcbitvd .gt_subtitle {\n  color: #333333;\n  font-size: 12px;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#kvovcbitvd .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 10px;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#kvovcbitvd .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 10px;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#kvovcbitvd .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#kvovcbitvd .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#kvovcbitvd .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#kvovcbitvd .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#kvovcbitvd .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#kvovcbitvd .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#kvovcbitvd .gt_from_md &gt; :first-child {\n  margin-top: 0;\n}\n\n#kvovcbitvd .gt_from_md &gt; :last-child {\n  margin-bottom: 0;\n}\n\n#kvovcbitvd .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#kvovcbitvd .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvovcbitvd .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#kvovcbitvd .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#kvovcbitvd .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#kvovcbitvd .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvovcbitvd .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#kvovcbitvd .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvovcbitvd .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#kvovcbitvd .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvovcbitvd .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#kvovcbitvd .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kvovcbitvd .gt_left {\n  text-align: left;\n}\n\n#kvovcbitvd .gt_center {\n  text-align: center;\n}\n\n#kvovcbitvd .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#kvovcbitvd .gt_font_normal {\n  font-weight: normal;\n}\n\n#kvovcbitvd .gt_font_bold {\n  font-weight: bold;\n}\n\n#kvovcbitvd .gt_font_italic {\n  font-style: italic;\n}\n\n#kvovcbitvd .gt_super {\n  font-size: 65%;\n}\n\n#kvovcbitvd .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#kvovcbitvd .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#kvovcbitvd .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#kvovcbitvd .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#kvovcbitvd .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#kvovcbitvd .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#kvovcbitvd .gt_indent_5 {\n  text-indent: 25px;\n}\n&lt;/style&gt;\n  &lt;table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\"&gt;\n  &lt;thead&gt;\n    &lt;tr class=\"gt_heading\"&gt;\n      &lt;td colspan=\"9\" class=\"gt_heading gt_title gt_font_normal\" style&gt;Mean Error by Fit Method, Task Stage, and Experiment&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_heading\"&gt;\n      &lt;td colspan=\"9\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\" style&gt;Comparison across Conditions and Models&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings gt_spanner_row\"&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id&gt;&lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"E2\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;E2&lt;/span&gt;\n      &lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"E3\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;E3&lt;/span&gt;\n      &lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings gt_spanner_row\"&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"&lt;strong&gt;Task Stage&lt;/strong&gt;&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;&lt;strong&gt;Task Stage&lt;/strong&gt;&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"ALM\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;ALM&lt;/span&gt;\n      &lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"EXAM\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;EXAM&lt;/span&gt;\n      &lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"ALM\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;ALM&lt;/span&gt;\n      &lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"EXAM\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;EXAM&lt;/span&gt;\n      &lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings\"&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Constant&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;Constant&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Varied&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;Varied&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Constant&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;Constant&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Varied&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;Varied&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Constant&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;Constant&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Varied&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;Varied&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Constant&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;Constant&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Varied&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;\"&gt;Varied&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody class=\"gt_table_body\"&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"9\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Test Data\"&gt;Fit to Test Data&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Test Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E2_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;239.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E2_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;129.8&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E2_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;99.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E2_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;88.2&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E3_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;170.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E3_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;106.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E3_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;92.3&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E3_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;72.8&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E2_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;53.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E2_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;527.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E2_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;108.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E2_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;169.3&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E3_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;70.9&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E3_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;543.5&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E3_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;157.8&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  E3_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;212.7&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"9\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Test &amp;amp; Training Data\"&gt;Fit to Test &amp; Training Data&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Test & Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E2_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;266.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E2_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;208.2&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E2_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;125.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E2_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;126.4&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E3_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;197.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E3_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;189.5&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E3_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;130.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E3_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;128.5&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test & Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E2_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;40.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E2_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;35.4&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E2_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;30.4&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E2_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;23.6&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E3_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;49.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E3_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;85.6&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E3_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;49.2&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  E3_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;78.4&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"9\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Training Data\"&gt;Fit to Training Data&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E2_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;357.4&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E2_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;295.9&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E2_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;305.1&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E2_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;234.5&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E3_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;415.0&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E3_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;298.8&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E3_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;295.5&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E3_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;243.7&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E2_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;42.5&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E2_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;23.0&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E2_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;43.2&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E2_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;22.6&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E3_ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;51.4&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E3_ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;63.8&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E3_EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;51.8&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  E3_EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;65.3&lt;/td&gt;&lt;/tr&gt;\n  &lt;/tbody&gt;\n  \n  &lt;tfoot class=\"gt_footnotes\"&gt;\n    &lt;tr&gt;\n      &lt;td class=\"gt_footnote\" colspan=\"9\"&gt;&lt;span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt; Note: All values represent mean errors.&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tfoot&gt;\n&lt;/table&gt;\n&lt;/div&gt;\n\n\n\n\nTable 2: this one has potential\n\n\n&lt;div id=\"lgrsdflbif\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\"&gt;\n  &lt;style&gt;#lgrsdflbif table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#lgrsdflbif thead, #lgrsdflbif tbody, #lgrsdflbif tfoot, #lgrsdflbif tr, #lgrsdflbif td, #lgrsdflbif th {\n  border-style: none;\n}\n\n#lgrsdflbif p {\n  margin: 0;\n  padding: 0;\n}\n\n#lgrsdflbif .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 100%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#lgrsdflbif .gt_title {\n  color: #333333;\n  font-size: large;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#lgrsdflbif .gt_subtitle {\n  color: #333333;\n  font-size: medium;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#lgrsdflbif .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: small;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#lgrsdflbif .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: small;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#lgrsdflbif .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#lgrsdflbif .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#lgrsdflbif .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#lgrsdflbif .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#lgrsdflbif .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#lgrsdflbif .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#lgrsdflbif .gt_from_md &gt; :first-child {\n  margin-top: 0;\n}\n\n#lgrsdflbif .gt_from_md &gt; :last-child {\n  margin-bottom: 0;\n}\n\n#lgrsdflbif .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#lgrsdflbif .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#lgrsdflbif .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#lgrsdflbif .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#lgrsdflbif .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#lgrsdflbif .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#lgrsdflbif .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#lgrsdflbif .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#lgrsdflbif .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#lgrsdflbif .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#lgrsdflbif .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#lgrsdflbif .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#lgrsdflbif .gt_left {\n  text-align: left;\n}\n\n#lgrsdflbif .gt_center {\n  text-align: center;\n}\n\n#lgrsdflbif .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#lgrsdflbif .gt_font_normal {\n  font-weight: normal;\n}\n\n#lgrsdflbif .gt_font_bold {\n  font-weight: bold;\n}\n\n#lgrsdflbif .gt_font_italic {\n  font-style: italic;\n}\n\n#lgrsdflbif .gt_super {\n  font-size: 65%;\n}\n\n#lgrsdflbif .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#lgrsdflbif .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#lgrsdflbif .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#lgrsdflbif .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#lgrsdflbif .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#lgrsdflbif .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#lgrsdflbif .gt_indent_5 {\n  text-indent: 25px;\n}\n&lt;/style&gt;\n  &lt;table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\"&gt;\n  &lt;thead&gt;\n    &lt;tr class=\"gt_heading\"&gt;\n      &lt;td colspan=\"6\" class=\"gt_heading gt_title gt_font_normal\" style&gt;Mean Error by Fit Method, Task Stage, and Experiment&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_heading\"&gt;\n      &lt;td colspan=\"6\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\" style&gt;Comparison across Conditions and Models&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings gt_spanner_row\"&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Task Stage\"&gt;Task Stage&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Exp\"&gt;Exp&lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"ALM\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;ALM&lt;/span&gt;\n      &lt;/th&gt;\n      &lt;th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"EXAM\"&gt;\n        &lt;span class=\"gt_column_spanner\"&gt;EXAM&lt;/span&gt;\n      &lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_col_headings\"&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Constant\"&gt;Constant&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Varied\"&gt;Varied&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Constant\"&gt;Constant&lt;/th&gt;\n      &lt;th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" style=\"border-top-width: 2px; border-top-style: solid; border-top-color: black;\" scope=\"col\" id=\"Varied\"&gt;Varied&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody class=\"gt_table_body\"&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"6\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Test Data\"&gt;Fit to Test Data&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Test Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E2&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;239.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;129.8&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;99.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;88.2&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E3&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;170.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;106.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;92.3&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;72.8&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E2&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;53.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;527.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;108.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;169.3&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E3&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;70.9&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;543.5&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;157.8&lt;/td&gt;\n&lt;td headers=\"Fit to Test Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;212.7&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"6\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Test &amp;amp; Training Data\"&gt;Fit to Test &amp; Training Data&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Test & Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E2&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;266.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;208.2&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;125.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;126.4&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test & Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E3&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;197.7&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;189.5&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;130.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;128.5&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test & Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E2&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;40.0&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;35.4&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;30.4&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;23.6&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Test & Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E3&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;49.1&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;85.6&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;49.2&lt;/td&gt;\n&lt;td headers=\"Fit to Test & Training Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;78.4&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr class=\"gt_group_heading_row\"&gt;\n      &lt;th colspan=\"6\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"Fit to Training Data\"&gt;Fit to Training Data&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"gt_row_group_first\"&gt;&lt;td headers=\"Fit to Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E2&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;357.4&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;295.9&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;305.1&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;234.5&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Test&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E3&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;415.0&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;298.8&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;295.5&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;243.7&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E2&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;42.5&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;23.0&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;43.2&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;22.6&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td headers=\"Fit to Training Data  Task Stage\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;Train&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  Exp\" class=\"gt_row gt_left\" style=\"background-color: #F2F2F2;\"&gt;E3&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  ALM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;51.4&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  ALM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;63.8&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  EXAM_Constant\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;51.8&lt;/td&gt;\n&lt;td headers=\"Fit to Training Data  EXAM_Varied\" class=\"gt_row gt_right\" style=\"background-color: #F2F2F2;\"&gt;65.3&lt;/td&gt;&lt;/tr&gt;\n  &lt;/tbody&gt;\n  \n  \n&lt;/table&gt;\n&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1.\n\n\nAn example of a gt table with e23_tab data\n\n\nTask Stage\nConstant (E2)\nVaried (E2)\nConstant (E3)\nVaried (E3)\n\n\n\n\nFit to Test Data - ALM\n\n\nTest\n239.7\n129.8\n170.1\n106.1\n\n\nTrain\n53.1\n527.1\n70.9\n543.5\n\n\nFit to Test Data - EXAM\n\n\nTest\n99.7\n88.2\n92.3\n72.8\n\n\nTrain\n108.1\n169.3\n157.8\n212.7\n\n\nFit to Test & Training Data - ALM\n\n\nTest\n266.0\n208.2\n197.7\n189.5\n\n\nTrain\n40.0\n35.4\n49.1\n85.6\n\n\nFit to Test & Training Data - EXAM\n\n\nTest\n125.1\n126.4\n130.0\n128.5\n\n\nTrain\n30.4\n23.6\n49.2\n78.4\n\n\nFit to Training Data - ALM\n\n\nTest\n357.4\n295.9\n415.0\n298.8\n\n\nTrain\n42.5\n23.0\n51.4\n63.8\n\n\nFit to Training Data - EXAM\n\n\nTest\n305.1\n234.5\n295.5\n243.7\n\n\nTrain\n43.2\n22.6\n51.8\n65.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean Error by Model and Condition\n\n\nComparison of ALM and EXAM Models across Different Fit Methods\n\n\ncondit\nMean Error\nDetails\n\n\nTask Stage\nExperiment\n\n\n\n\nFit to Test Data - ALM\n\n\nConstant\n239.7\nTest\nE2\n\n\nVaried\n129.8\nTest\nE2\n\n\nConstant\n53.1\nTrain\nE2\n\n\nVaried\n527.1\nTrain\nE2\n\n\nConstant\n170.1\nTest\nE3\n\n\nVaried\n106.1\nTest\nE3\n\n\nConstant\n70.9\nTrain\nE3\n\n\nVaried\n543.5\nTrain\nE3\n\n\nFit to Test Data - EXAM\n\n\nConstant\n99.7\nTest\nE2\n\n\nVaried\n88.2\nTest\nE2\n\n\nConstant\n108.1\nTrain\nE2\n\n\nVaried\n169.3\nTrain\nE2\n\n\nConstant\n92.3\nTest\nE3\n\n\nVaried\n72.8\nTest\nE3\n\n\nConstant\n157.8\nTrain\nE3\n\n\nVaried\n212.7\nTrain\nE3\n\n\nFit to Test & Training Data - ALM\n\n\nConstant\n266.0\nTest\nE2\n\n\nVaried\n208.2\nTest\nE2\n\n\nConstant\n40.0\nTrain\nE2\n\n\nVaried\n35.4\nTrain\nE2\n\n\nConstant\n197.7\nTest\nE3\n\n\nVaried\n189.5\nTest\nE3\n\n\nConstant\n49.1\nTrain\nE3\n\n\nVaried\n85.6\nTrain\nE3\n\n\nFit to Test & Training Data - EXAM\n\n\nConstant\n125.1\nTest\nE2\n\n\nVaried\n126.4\nTest\nE2\n\n\nConstant\n30.4\nTrain\nE2\n\n\nVaried\n23.6\nTrain\nE2\n\n\nConstant\n130.0\nTest\nE3\n\n\nVaried\n128.5\nTest\nE3\n\n\nConstant\n49.2\nTrain\nE3\n\n\nVaried\n78.4\nTrain\nE3\n\n\nFit to Training Data - ALM\n\n\nConstant\n357.4\nTest\nE2\n\n\nVaried\n295.9\nTest\nE2\n\n\nConstant\n42.5\nTrain\nE2\n\n\nVaried\n23.0\nTrain\nE2\n\n\nConstant\n415.0\nTest\nE3\n\n\nVaried\n298.8\nTest\nE3\n\n\nConstant\n51.4\nTrain\nE3\n\n\nVaried\n63.8\nTrain\nE3\n\n\nFit to Training Data - EXAM\n\n\nConstant\n305.1\nTest\nE2\n\n\nVaried\n234.5\nTest\nE2\n\n\nConstant\n43.2\nTrain\nE2\n\n\nVaried\n22.6\nTrain\nE2\n\n\nConstant\n295.5\nTest\nE3\n\n\nVaried\n243.7\nTest\nE3\n\n\nConstant\n51.8\nTrain\nE3\n\n\nVaried\n65.3\nTrain\nE3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean Error for Different Fitting Methods and Models\n    \n\nResults from Experiments E2 and E3\n    \n\nCondition\n      Mean Error\n      Task Stage\n      Experiment\n    \n\n\n\nFit to Test Data - ALM\n    \n\nConstant1\n\n239.7\nTest\nE2\n\n\nVaried1\n\n129.8\nTest\nE2\n\n\nConstant1\n\n53.1\nTrain\nE2\n\n\nVaried1\n\n527.1\nTrain\nE2\n\n\nConstant1\n\n170.1\nTest\nE3\n\n\nVaried1\n\n106.1\nTest\nE3\n\n\nConstant1\n\n70.9\nTrain\nE3\n\n\nVaried1\n\n543.5\nTrain\nE3\n\n\nFit to Test Data - EXAM\n    \n\nConstant1\n\n99.7\nTest\nE2\n\n\nVaried1\n\n88.2\nTest\nE2\n\n\nConstant1\n\n108.1\nTrain\nE2\n\n\nVaried1\n\n169.3\nTrain\nE2\n\n\nConstant1\n\n92.3\nTest\nE3\n\n\nVaried1\n\n72.8\nTest\nE3\n\n\nConstant1\n\n157.8\nTrain\nE3\n\n\nVaried1\n\n212.7\nTrain\nE3\n\n\nFit to Test & Training Data - ALM\n    \n\nConstant1\n\n266.0\nTest\nE2\n\n\nVaried1\n\n208.2\nTest\nE2\n\n\nConstant1\n\n40.0\nTrain\nE2\n\n\nVaried1\n\n35.4\nTrain\nE2\n\n\nConstant1\n\n197.7\nTest\nE3\n\n\nVaried1\n\n189.5\nTest\nE3\n\n\nConstant1\n\n49.1\nTrain\nE3\n\n\nVaried1\n\n85.6\nTrain\nE3\n\n\nFit to Test & Training Data - EXAM\n    \n\nConstant1\n\n125.1\nTest\nE2\n\n\nVaried1\n\n126.4\nTest\nE2\n\n\nConstant1\n\n30.4\nTrain\nE2\n\n\nVaried1\n\n23.6\nTrain\nE2\n\n\nConstant1\n\n130.0\nTest\nE3\n\n\nVaried1\n\n128.5\nTest\nE3\n\n\nConstant1\n\n49.2\nTrain\nE3\n\n\nVaried1\n\n78.4\nTrain\nE3\n\n\nFit to Training Data - ALM\n    \n\nConstant1\n\n357.4\nTest\nE2\n\n\nVaried1\n\n295.9\nTest\nE2\n\n\nConstant1\n\n42.5\nTrain\nE2\n\n\nVaried1\n\n23.0\nTrain\nE2\n\n\nConstant1\n\n415.0\nTest\nE3\n\n\nVaried1\n\n298.8\nTest\nE3\n\n\nConstant1\n\n51.4\nTrain\nE3\n\n\nVaried1\n\n63.8\nTrain\nE3\n\n\nFit to Training Data - EXAM\n    \n\nConstant1\n\n305.1\nTest\nE2\n\n\nVaried1\n\n234.5\nTest\nE2\n\n\nConstant1\n\n43.2\nTrain\nE2\n\n\nVaried1\n\n22.6\nTrain\nE2\n\n\nConstant1\n\n295.5\nTest\nE3\n\n\nVaried1\n\n243.7\nTest\nE3\n\n\nConstant1\n\n51.8\nTrain\nE3\n\n\nVaried1\n\n65.3\nTrain\nE3\n\n\n\nData source: e23_tab\n    \n\n\n1 Constant and Varied refer to the conditions of the experiment."
  },
  {
    "objectID": "Model/pdf_test2.html",
    "href": "Model/pdf_test2.html",
    "title": "HTW Model2",
    "section": "",
    "text": "Table 1: Mean Error by Fit Method, Task Stage, and Experiment\n\n\n\n\n\n\n\nMean Error by Fit Method, Task Stage, and Experiment\n    \n\nComparison across Conditions and Models1\n\n    \n\n\n      \n        E2\n      \n      \n        E3\n      \n    \n\nTask Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nFit to Test Data\n    \n\nTest\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n    \n\nTest\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n    \n\nTest\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n1 Note: Mean errors are calculated as the absolute difference between the predicted and actual values."
  },
  {
    "objectID": "Model/pdf_test2.html#first-gt",
    "href": "Model/pdf_test2.html#first-gt",
    "title": "HTW Model2",
    "section": "",
    "text": "Table 1: Mean Error by Fit Method, Task Stage, and Experiment\n\n\n\n\n\n\n\nMean Error by Fit Method, Task Stage, and Experiment\n    \n\nComparison across Conditions and Models1\n\n    \n\n\n      \n        E2\n      \n      \n        E3\n      \n    \n\nTask Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nFit to Test Data\n    \n\nTest\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n    \n\nTest\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n    \n\nTest\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n1 Note: Mean errors are calculated as the absolute difference between the predicted and actual values."
  },
  {
    "objectID": "Model/pdf_test2.html#nd-gt",
    "href": "Model/pdf_test2.html#nd-gt",
    "title": "HTW Model2",
    "section": "2nd GT",
    "text": "2nd GT\n\n\n\nTable 2: Mean Error by Fit Method, Task Stage, and Experiment\n\n\n\n\n\n\n\nMean Error by Fit Method, Task Stage, and Experiment\n    \n\nComparison across Conditions and Models1\n\n    \n\n\n      \n        E2\n      \n      \n        E3\n      \n    \n\nTask Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nFit to Test Data\n    \n\nTest\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n    \n\nTest\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n    \n\nTest\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n1 Note: Mean errors are calculated as the absolute difference between the predicted and actual values."
  },
  {
    "objectID": "Model/htw_model.html#model-fitting",
    "href": "Model/htw_model.html#model-fitting",
    "title": "HTW Modeling",
    "section": "Model Fitting",
    "text": "Model Fitting\nTo fit ALM and EXAM to our participant data, we employ a similar method to Mcdaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data in with separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase.Models to were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate the parameters of ALM and EXAM, we used approximate bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g. many neural network models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individuals velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.\nModelling Results\nGroup level Patterns\n\nCodepost_tabs &lt;- abc_tables(post_dat,post_dat_l)\ntrain_tab &lt;- abc_train_tables(pd_train,pd_train_l)\n\ne1_tab &lt;- rbind(post_tabs$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Test\"), train_tab$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; mutate(Fit_Method=rename_fm(Fit_Method)) \n\ne1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = c(`Task Stage`)) %&gt;%\n  cols_label(\n    `Task Stage` = \"Task Stage\"\n  ) %&gt;%\n  fmt_number(\n    columns = starts_with(\"ALM\") | starts_with(\"EXAM\"),\n    decimals = 2\n  ) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"white\"),\n     locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = \"top\", color = \"black\", weight = px(1)),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_options(\n    column_labels.font.size = 10,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    table.font.size = 10, \n    quarto.disable_processing = TRUE\n  ) \n\n\nTable 2: Models errors predicting empirical data - aggregated over all participants, posterior parameter values, and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n\nTask Stage\n      Fit Method\n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nTest\nFit to Test Data\n199.93\n103.36\n104.01\n85.68\n\n\nTest\nFit to Test & Training Data\n216.97\n170.28\n127.94\n144.86\n\n\nTest\nFit to Training Data\n467.73\n291.38\n273.30\n297.91\n\n\nTrain\nFit to Test Data\n297.82\n2,016.01\n53.90\n184.00\n\n\nTrain\nFit to Test & Training Data\n57.40\n132.32\n42.92\n127.90\n\n\nTrain\nFit to Training Data\n51.77\n103.48\n51.43\n107.03\n\n\n\n\n\n\n\n\n\n\nCodec_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n\n\n\nFigure 2: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\n\nCodetrain_resid &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method, Block,x) |&gt; \n  summarise(y=mean(y), pred=mean(pred), mean_error=abs(y-pred)) |&gt;\n  group_by(id,condit,Model,Fit_Method,Block) |&gt;\n  summarise(mean_error=mean(mean_error)) |&gt;\n  ggplot(aes(x=interaction(Block,Model), y = mean_error, fill=factor(Block))) + \n  stat_bar + \n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, scales=\"free\",ncol=2) +\n   scale_x_discrete(guide = \"axis_nested\") +\n  scale_fill_manual(values=c(\"gray10\",\"gray50\",\"gray92\"))+\n  labs(title=\"Model Residual Errors - Training Stage\", y=\"RMSE\", x= \"Model\",fill=\"Training Block\") +\n  theme(legend.position=\"top\")\n\ntest_resid &lt;-  post_dat |&gt; \n   group_by(id,condit,x,Model,Fit_Method,bandType) |&gt;\n    summarise(y=mean(y), pred=mean(pred), error=abs(y-pred)) |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200))) |&gt;\n  ggplot(aes(x = Model, y = abs(error), fill=vbLab,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n  #scale_fill_manual(values=wes_palette(\"AsteroidCity2\"))+\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\") +\n  labs(title=\"Model Residual Errors - Testing Stage\",y=\"RMSE\", x=\"Velocity Band\") \n\n(train_resid / test_resid) +\n  #plot_layout(heights=c(1,1.5)) & \n  plot_annotation(tag_levels = list(c('A','B')),tag_suffix = ') ') \n\n\n\n\n\n\nFigure 3: Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y axes are scaled differently between facets. A) Residuals predicting each block of the training data. B) Residuals predicting each band during the testing stage. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 2, and model predictions are shown alongside the empirical data in Figure 4. There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Aggregated residuals are displayed in Figure 3. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences in training performance between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 3) also show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that y axes are scaled different between plots). Although EXAM tends to perform better for both Constant and Varied participants (see also Figure 5), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods. The primary predictive difference between ALM and EXAM is made clear in Figure 4, which directly compares the observed data against the posterior predictive distributions for both models. Regardless of how the models are fit, only EXAM can capture the pattern where participants are able to discriminate all 6 target bands.\n\nCodepost_dat_l |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; \n #left_join(testAvgE1, by=join_by(id,condit,x==bandInt)) |&gt;\n ggplot(aes(x=Resp,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n    facet_wrap(~rename_fm(Fit_Method)+condit, ncol=2,strip.position = \"top\", scales = \"free_x\") +\n        scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n         strip.background = element_blank(),\n         strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(10,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions - Experiment 1 Data\", y=\"Vx\")\n\n\n\n\n\n\nFigure 4: Empirical data and Model predictions for mean velocity across target bands. Fitting methods (Test Only, Test & Train, Train Only) - are separated across rows, and Training Condition (Constant vs. Varied) are separated by columns. Each facet contains the predictions of ALM and EXAM, alongside the observed data.\n\n\n\n\n\nCodepdl &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n  filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\n# aerror is model error, which is predicted by Model(ALM vs. EXAM) & condit (Constant vs. Varied)\ne1_ee_brm_ae &lt;- brm(data=pdl,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e1_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbct_e1 &lt;- as.data.frame(bayestestR::describe_posterior(e1_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\n#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\n\np1 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \np_ce_1 &lt;- (p1 + p2+ p3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\nbm1 &lt;- get_coef_details(e1_ee_brm_ae, \"conditVaried\")\nbm2 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM\")\nbm3 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nposterior_estimates &lt;- as.data.frame(e1_ee_brm_ae) %&gt;%\n  select(starts_with(\"b_\")) %&gt;%\n  setNames(c(\"Intercept\", \"ModelEXAM\", \"conditVaried\", \"ModelEXAM_conditVaried\"))\n\nconstant_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM\nvaried_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM + posterior_estimates$conditVaried + posterior_estimates$ModelEXAM_conditVaried\ncomparison_EXAM &lt;- constant_EXAM - varied_EXAM\nsummary_EXAM &lt;- bayestestR::describe_posterior(comparison_EXAM, centrality = \"Mean\")\n\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NULL)\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NA)\n\n# full set of Model x condit contrasts\n# ALM - EXAM\nbtw_model &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model | condit, re_formula=NULL)  |&gt; \n  pluck(\"contrasts\") |&gt; \n  gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw,condit) |&gt; summarise(value=mean(.value), n=n()) \n\n# btw_model |&gt; ggplot(aes(x=value,y=contrast,fill=condit)) +stat_halfeye()\n\n# Constant - Varied\nemm_condit &lt;- e1_ee_brm_ae |&gt; emmeans(~ condit | Model, re_formula = NULL)\nbtw_con &lt;- emm_condit |&gt;  pairs() |&gt; gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw, Model) |&gt; summarise(value=mean(.value), n=n()) \n# btw_con |&gt; ggplot(aes(x=value,y=Model,fill=Model)) +stat_halfeye()                              \n\np_em_1&lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model*condit, re_formula=NA)  |&gt; \n  pluck(\"contrasts\") |&gt;\n  gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw) |&gt; summarise(value=mean(.value), n=n()) |&gt; \n  filter(!(contrast %in% c(\"ALM Constant - EXAM Constant\",\"ALM Constant - EXAM Varied\",\"ALM Varied - EXAM Varied \", \"EXAM Constant - ALM Varied\" ))) |&gt; \n  ggplot(aes(x=value,y=contrast,fill=contrast)) +stat_halfeye() + labs(x=\"Model Error Difference\",y=\"Contrast\") + theme(legend.position=\"none\") \n\np_ce_1 / p_em_1\n\n\n\n\n\n\nFigure 5\n\n\n\n\nTo quantitatively assess whether the differences in performance between models, we fit a bayesian regressions predicting the errors of the posterior predictions of each models as a function of the Model (ALM vs. EXAM) and training condition (Constant vs. Varied).\nModel errors were significantly lower for EXAM (\\(\\beta\\) = -37.54, 95% CrI [-60.4, -14.17], pd = 99.85%) than ALM. There was also a significant interaction between Model and Condition (\\(\\beta\\) = 60.42, 95% CrI [36.17, 83.85], pd = 100%), indicating that the advantage of EXAM over ALM was significantly greater for the constant group. To assess whether EXAM predicts constant performance significantly better for Constant than for Varied subjects, we calculated the difference in model error between the Constant and Varied conditions specifically for EXAM. The results indicated that the model error for EXAM was significantly lower in the Constant condition compared to the Varied condition, with a mean difference of -22.879 (95% CrI [-46.016, -0.968], pd = 0.981).\n\nCodepost_tabs2 &lt;- abc_tables(e2_model$post_dat,e2_model$post_dat_l)\ntrain_tab2 &lt;- abc_train_tables(e2_model$pd_train,e2_model$pd_train_l)\n\npdl2 &lt;- e2_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne2_tab &lt;- rbind(post_tabs2$agg_pred_full |&gt;\n mutate(\"Task Stage\"=\"Test\"), train_tab2$agg_pred_full |&gt; \n mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\npost_tabs3 &lt;- abc_tables(e3_model$post_dat,e3_model$post_dat_l)\ntrain_tab3 &lt;- abc_train_tables(e3_model$pd_train,e3_model$pd_train_l)\n\npdl3 &lt;- e3_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne3_tab &lt;- rbind(post_tabs3$agg_pred_full |&gt; \n  mutate(\"Task Stage\"=\"Test\"), train_tab3$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\ne23_tab &lt;- rbind(e2_tab |&gt; mutate(Exp=\"E2\"), e3_tab |&gt; mutate(Exp=\"E3\")) \ngt_table &lt;- e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = `Task Stage`) %&gt;%\n  cols_label(`Task Stage` = \"Task Stage\") %&gt;%\n  fmt_number(columns = matches(\"E2|E3\"), decimals = 1) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_options(\n    column_labels.font.size = 10,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    table.font.size = 10,\n    quarto.disable_processing = TRUE\n  ) \ngt_table\n\n\nTable 3: Models errors predicting empirical data - aggregated over all participants, posterior parameter values, and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n\n\n      \n        E2\n      \n      \n        E3\n      \n    \n\nTask Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nFit to Test Data\n    \n\nTest\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n    \n\nTest\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n    \n\nTest\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n\n\n\n\n\n\nCoderbind(e2_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\"), \n e3_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb,bandOrder) |&gt;\n  summarize(vx=median(val)) |&gt; mutate(Exp=\"E3\")) |&gt;\n  ggplot( aes(x=condit,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) +\n  stat_bar + \n    facet_nested_wrap(~Exp+bandOrder+Resp, strip.position = \"top\", scales = \"free_x\") +\n    scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .7), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n        #  strip.background = element_blank(),\n        #  strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(20,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions Experiment 2 & 3\", y=\"vx\")\n\n\n\n\n\n\nFigure 6: Empirical data and Model predictions from Experiment 2 and 3 for the testing stage. Observed data is shown on the right. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\n\nCodee2_ee_brm_ae &lt;- brm(data=pdl2,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e2_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"conditVaried\")\nbm2_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM\")\nbm3_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nbct_e2 &lt;- as.data.frame(bayestestR::describe_posterior(e2_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) # %&gt;% kable(booktabs = TRUE)\n\ne3_ee_brm_ae &lt;- brm(data=pdl3,\n  aerror ~  Model * condit*bandOrder + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e3_ae_modelCondBo_RFint2.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e3 &lt;- get_coef_details(e3_ee_brm_ae, \"conditVaried\")\nbm2_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM\")\nbm3_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried\")\nbm4_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried:bandOrderReverse\")\n\n\nbct_e3  &lt;- as.data.frame(bayestestR::describe_posterior(e3_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\nbct &lt;- rbind(bct_e1 |&gt; mutate(exp=\"Exp 1\"),bct_e2 |&gt; \n               mutate(exp= \"Exp 2\"),bct_e3 |&gt; mutate(exp=\"Exp 3\")) |&gt; \n  relocate(exp, .before=Term)\n\nbct_table &lt;- bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;%\n  gt() %&gt;%\n  # tab_header(\n  #   title = \"Bayesian Model Results\",\n  #   subtitle = \"Estimates and Credible Intervals for Each Term Across Experiments\"\n  # ) %&gt;%\n  cols_label(\n    exp = \"Experiment\",\n    Term = \"Term\",\n    Estimate = \"Estimate\",\n    `95% CrI Lower` = \"95% CrI Lower\",\n    `95% CrI Upper` = \"95% CrI Upper\",\n    pd = \"pd\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Estimate, `95% CrI Lower`, `95% CrI Upper`),\n    decimals = 2\n  ) %&gt;%\n  fmt_number(\n    columns = pd,\n    decimals = 2\n  ) %&gt;%\n  tab_spanner(\n    label = \"Credible Interval\",\n    columns = c(`95% CrI Lower`, `95% CrI Upper`)\n  ) %&gt;%\n  tab_style(\n    style = list(\n      #cell_fill(color = \"lightgray\"),\n      cell_text(weight = \"bold\"), \n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(\n      columns = c(Estimate, pd),\n      rows = Term==\"ModelEXAM:conditVaried\"\n    )\n  ) %&gt;%\n   tab_row_group(\n    label = \"Experiment 3\",\n    rows = exp == \"Exp 3\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 2\",\n    rows = exp == \"Exp 2\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 1\",\n    rows = exp == \"Exp 1\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 10,\n    heading.title.font.size = 16,\n    heading.subtitle.font.size = 14,\n    quarto.disable_processing = TRUE\n    #row_group.background.color = \"gray95\"\n  )\nbct_table\n\n\nTable 4: Results of Bayesian Regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and the interaction between Model and Condition. The values represent the estimate coefficient for each term, with 95% credible intervals in brackets. The intercept reflects the baseline of ALM and Constant. The other estimates indicate deviations from the baseline for the EXAM mode and varied condition. Lower values indicate better model fit.\n\n\n\n\n\n\n\nExperiment\n      Term\n      Estimate\n      \n        Credible Interval\n      \n      pd\n    \n\n95% CrI Lower\n      95% CrI Upper\n    \n\n\n\nExperiment 1\n    \n\nExp 1\nIntercept\n176.30\n156.86\n194.59\n1.00\n\n\nExp 1\nModelEXAM\n−88.44\n−104.51\n−71.81\n1.00\n\n\nExp 1\nconditVaried\n−37.54\n−60.40\n−14.17\n1.00\n\n\nExp 1\nModelEXAM:conditVaried\n60.42\n36.17\n83.85\n1.00\n\n\nExperiment 2\n    \n\nExp 2\nIntercept\n245.87\n226.18\n264.52\n1.00\n\n\nExp 2\nModelEXAM\n−137.73\n−160.20\n−115.48\n1.00\n\n\nExp 2\nconditVaried\n−86.39\n−113.52\n−59.31\n1.00\n\n\nExp 2\nModelEXAM:conditVaried\n56.87\n25.26\n88.04\n1.00\n\n\nExperiment 3\n    \n\nExp 3\nIntercept\n164.83\n140.05\n189.44\n1.00\n\n\nExp 3\nModelEXAM\n−65.66\n−85.97\n−46.02\n1.00\n\n\nExp 3\nconditVaried\n−40.61\n−75.90\n−3.02\n0.98\n\n\nExp 3\nbandOrderReverse\n25.47\n−9.34\n58.68\n0.93\n\n\nExp 3\nModelEXAM:conditVaried\n41.90\n11.20\n72.54\n0.99\n\n\nExp 3\nModelEXAM:bandOrderReverse\n−7.32\n−34.53\n21.05\n0.70\n\n\nExp 3\nconditVaried:bandOrderReverse\n30.82\n−19.57\n83.56\n0.88\n\n\nExp 3\nModelEXAM:conditVaried:bandOrderReverse\n−60.60\n−101.80\n−18.66\n1.00\n\n\n\n\n\n\n\n\n\nModel Fits to Experiment 2 and 3. Data from Experiments 2 and 3 were fit to ALM and EXAM in the same manner as Experiment1 . For brevity, we only plot and discuss the results of the “fit to training and testing data” models - results from the other fitting methods can be found in the appendix. The model fitting results for Experiments 2 and 3 closely mirrored those observed in Experiment 1. The Bayesian regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and their interaction (see Table 4) revealed a consistent main effect of Model across all three experiments. The negative coefficients for the ModelEXAM term (Exp 2: \\(\\beta\\) = -86.39, 95% CrI -113.52, -59.31, pd = 100%; Exp 3: \\(\\beta\\) = -40.61, 95% CrI -75.9, -3.02, pd = 98.17%) indicate that EXAM outperformed ALM in both experiments. Furthermore, the interaction between Model and Condition was significant in both Experiment 2 (\\(\\beta\\) = 56.87, 95% CrI 25.26, 88.04, pd = 99.98%) and Experiment 3 (\\(\\beta\\) = 41.9, 95% CrI 11.2, 72.54, pd = 99.35%), suggesting that the superiority of EXAM over ALM was more pronounced for the Constant group compared to the Varied group, as was the case in Experiment 1. Recall that Experiment 3 included participants in both the original and reverse order conditions - and that this manipulation interacted with the effect of training condition. We thus also controleld for band order in our Bayesian Regression assessing the relative performance of EXAM and ALM in Experiment 3. There was a significant three way interaction between Model, Training Condition, and Band Order (\\(\\beta\\) = -60.6, 95% CrI -101.8, -18.66, pd = 99.83%), indicating that the relative advantage of EXAM over ALM was only more pronounced in the original order condition, and not the reverse order condition (see Figure 7).\n\nCode#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\np1 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\") + labs(title=\"E2. Model Error\")\np2 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n p_e2 &lt;- (p1 + p2+ p3) \n#wrap_plots(plot(conditional_effects(e3_ee_brm_ae),points=FALSE,plot=FALSE))\n\np_e3 &lt;- plot(conditional_effects(e3_ee_brm_ae, \n                         effects = \"Model:condit\", \n                         conditions=make_conditions(e3_ee_brm_ae,vars=c(\"bandOrder\"))),\n     points=FALSE,plot=FALSE)$`Model:condit` + \n     labs(x=\"Model\",y=\"Model Error\", title=\"E3. Model Error\", fill=NULL, col=NULL) + \n     theme(legend.position=\"right\") + \n     scale_color_manual(values=wes_palette(\"Darjeeling1\")) \n\n# p1 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n#   ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\n# p2 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n#   labs(x=\"Model\",y=NULL)\n# p3 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n#   scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n#   labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n #p2 &lt;- (p1 + p2+ p3)\n (p_e2 / p_e3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\nFigure 7: Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied) on Model Error for Experiment 2 and 3 data. Experiment 3 also includes a control for the order of training vs. testing bands (original order vs. reverse order).\n\n\n\n\nComputational Model Summary. Across the model fits to all three experiments, we found greater support for EXAM over ALM (negative coefficients on the ModelEXAM term in Table 4), and moreover that the constant participants were disproportionately well described by EXAM in comparison to ALM (positive coefficients on ModelEXAM:conditVaried terms in Table 4). This pattern is also clearly depicted in Figure 8, which plots the difference in model errors between ALM and EXAM for each individual participant. Both varied and constant conditions have a greater proportion of subjects better fit by EXAM (positive error differences), with the magnitude of EXAM’s advantage visibly greater for the constant group. It also bears mention that numerous participants were better fit by ALM, or did not show a clear preference for either model. A subset of these participants are shown in Figure 9.\n\nCodetid1 &lt;- post_dat  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-pred_dist, -dist) |&gt;\n  rbind(e2_model$post_dat |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat |&gt; mutate(Exp=\"E3\")) |&gt;\n  filter(Fit_Method==\"Test_Train\") |&gt;\n  group_by(id,condit,Model,Fit_Method,x, Exp) |&gt; \n    mutate(e2=abs(y-pred)) |&gt; \n    summarise(y1=median(y), pred1=median(pred),mean_error=abs(y1-pred1)) |&gt;\n    group_by(id,condit,Model,Fit_Method,Exp) |&gt; \n    summarise(mean_error=mean(mean_error)) |&gt; \n    arrange(id,condit,Fit_Method) |&gt;\n    round_tibble(1) \n\nbest_id &lt;- tid1 |&gt; \n  group_by(id,condit,Fit_Method) |&gt; \n  mutate(best=ifelse(mean_error==min(mean_error),1,0)) \n\nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method, Exp) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\nerror_difference&lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(mean_error)) %&gt;%\n  mutate(Error_difference = (ALM - EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; \n  left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; \n  mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\nfull_comparison |&gt; \n  filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n  geom_col() +\n  #ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  ggh4x::facet_nested_wrap(~condit+Exp,scales=\"free\") + \n  theme(axis.text.y = element_text(size=8)) +\n  labs(fill=\"Best Model\",\n  x=\"Mean Model Error Difference (ALM - EXAM)\",\n  y=\"Participant\")\n\n\n\n\n\n\nFigure 8: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\n\nCodecId_tr &lt;- c(137, 181, 11)\nvId_tr &lt;- c(14, 193, 47)\ncId_tt &lt;- c(11, 93, 35)\nvId_tt &lt;- c(1,14,74)\ncId_new &lt;- c(175, 68, 93, 74)\n# filter(id %in% (filter(bestTestEXAM,group_rank&lt;=9, Fit_Method==\"Test\")\n\ne1_sbjs &lt;- c(49,68,155, 175,74)\ne3_sbjs &lt;-  c(245, 280, 249)\ne2_sbjs &lt;- c(197, 157, 312, 334)\ncFinal &lt;- c(49, 128,202 )\nvFinal &lt;- c(68,70,245)\n\n\nindv_post_l &lt;- post_dat_l  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-signed_dist) |&gt;\n  rbind(e2_model$post_dat_l |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat_l |&gt; mutate(Exp=\"E3\") |&gt; select(-fb)) |&gt;\n  filter(Fit_Method==\"Test_Train\", id %in% c(cFinal,vFinal))\n\ntestIndv &lt;- indv_post_l |&gt; \n#filter(id %in% c(cId_tt,vId_tt,cId_new), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=vb, col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar_sd + \n  ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) + \n  labs(title=\"Individual Participant fits from Test & Train Fitting Method\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\ntestIndv \n\n\n\n\n\n\nFigure 9: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.",
    "crumbs": [
      "Model",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Model/htw_model_e1.html#model-predictions-of-deviation",
    "href": "Model/htw_model_e1.html#model-predictions-of-deviation",
    "title": "HTW Model e1",
    "section": "Model predictions of deviation",
    "text": "Model predictions of deviation\n\nCodepemp1 &lt;- e1 |&gt; filter(expMode2==\"Test\") |&gt; \n  group_by(id,condit,vb,bandType) |&gt; \n  ggplot(aes(x=condit,y=dist, fill=vb, col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_summary(fun=mean, geom=\"bar\", position=position_dodge()) + \n  stat_summary(fun.data=mean_se, geom=\"errorbar\", color=\"black\", position=position_dodge(), size=.5) + \n  scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) + \n  scale_y_continuous( breaks=seq(0,600,by=200),labels=as.character(seq(0,600,by=200))) +\n  expand_limits(y=600) +\n  theme(legend.position=\"right\", axis.title.x = element_blank(),plot.title = element_text(hjust=.40)) +\n  labs(title=\"Empirical Data\", y=\"Deviation from target band\", fill=\"Band\") \n\n# layout &lt;- \"#A#\n# #B#\n# CCC\"\n\nlayout &lt;- \"#A#\nCCC\"\n\npmod1 &lt;- post_dat_l |&gt; filter(!(Resp==\"Observed\")) |&gt; \n  #left_join(testAvgE1, by=join_by(id,condit,x==bandInt)) |&gt;\n  group_by(id,condit, Fit_Method,Resp,vb,bandType) |&gt; \n summarize(dist=median(dist)) |&gt; \n ggplot( aes(x=condit,y=dist, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n    facet_wrap(~Resp+rename_fm(Fit_Method), strip.position = \"top\", scales = \"free_x\") +\n        scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) +\n    theme(panel.spacing = unit(0, \"lines\"), \n         strip.background = element_blank(),\n         strip.placement = \"outside\",\n         legend.position = \"none\",\n         plot.title = element_text(hjust=.50),\n         plot.margin = unit(c(20,0,0,0), \"pt\"),\n         axis.title.x = element_blank()) + \n         labs(title=\"Model Predictions - Experiment 1 Data\", y=\"Deviation from target band\")\n\n(pemp1 / pmod1) + plot_layout(design = layout) #heights = unit(c(5,-5, 8), c('cm','null'))\n\n\n\nCodepost_tabs$et_sum\n\n\net_sum &lt;- post_dat_l |&gt;\n  group_by(id,condit, Fit_Method, Resp) |&gt;\n  summarise(val = mean(val), .groups = 'drop') |&gt;\n  pivot_wider(\n    names_from = Resp,\n    values_from = val,\n    values_fill = list(val = NA)\n  ) |&gt;\n  mutate(\n    ALM_error = round(abs(ALM - Observed),1),\n    EXAM_error = round(abs(EXAM - Observed),1),\n    Best_Model = case_when(\n      ALM_error &lt; EXAM_error ~ \"ALM\",\n      EXAM_error &lt; ALM_error ~ \"EXAM\",\n      TRUE ~ NA_character_  # In case of a tie or missing data\n    )\n  ) |&gt;\n  group_by(condit, Fit_Method) %&gt;%\n  summarise(\n    Avg_ALM_error = round(mean(ALM_error, na.rm = TRUE),1),\n    Avg_EXAM_error = round(mean(EXAM_error, na.rm = TRUE),1),\n    N_Best_ALM = sum(Best_Model == \"ALM\", na.rm = TRUE),\n    N_Best_EXAM = sum(Best_Model == \"EXAM\", na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    Best_Model = case_when(\n      Avg_ALM_error &lt; Avg_EXAM_error ~ \"ALM\",\n      Avg_EXAM_error &lt; Avg_ALM_error ~ \"EXAM\",\n      TRUE ~ \"Tie\"  # In case of a tie or missing data\n    ))\n\n\n\n\net_sum_x &lt;- post_dat_l |&gt;\n  group_by(id,condit, Fit_Method, Resp,x) |&gt;\n  summarise(val = mean(val), .groups = 'drop') |&gt;\n  pivot_wider(\n    names_from = Resp,\n    values_from = val,\n    values_fill = list(val = NA)\n  ) |&gt;\n  mutate(\n    ALM_error = round(abs(ALM - Observed),1),\n    EXAM_error = round(abs(EXAM - Observed),1),\n    Best_Model = case_when(\n      ALM_error &lt; EXAM_error ~ \"ALM\",\n      EXAM_error &lt; ALM_error ~ \"EXAM\",\n      TRUE ~ NA_character_  # In case of a tie or missing data\n    )\n  ) |&gt;\n  group_by(condit, Fit_Method) %&gt;%\n  summarise(\n    Avg_ALM_error = round(mean(ALM_error, na.rm = TRUE),1),\n    Avg_EXAM_error = round(mean(EXAM_error, na.rm = TRUE),1),\n    N_Best_ALM = sum(Best_Model == \"ALM\", na.rm = TRUE),\n    N_Best_EXAM = sum(Best_Model == \"EXAM\", na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    Best_Model = case_when(\n      Avg_ALM_error &lt; Avg_EXAM_error ~ \"ALM\",\n      Avg_EXAM_error &lt; Avg_ALM_error ~ \"EXAM\",\n      TRUE ~ \"Tie\"  # In case of a tie or missing data\n    ))\n\n\n\n\net_sum_x_id &lt;- post_dat_l |&gt;\n  group_by(id,condit, Fit_Method, Resp,x) |&gt;\n  summarise(val = mean(val), .groups = 'drop') |&gt;\n  pivot_wider(\n    names_from = Resp,\n    values_from = val,\n    values_fill = list(val = NA)\n  ) |&gt;\n  mutate(\n    ALM_error = round(abs(ALM - Observed),1),\n    EXAM_error = round(abs(EXAM - Observed),1),\n    Best_Model = case_when(\n      ALM_error &lt; EXAM_error ~ \"ALM\",\n      EXAM_error &lt; ALM_error ~ \"EXAM\",\n      TRUE ~ NA_character_  # In case of a tie or missing data\n    )\n  ) |&gt;\n  group_by(condit, Fit_Method) %&gt;%\n  summarise(\n    Avg_ALM_error = round(mean(ALM_error, na.rm = TRUE),1),\n    Avg_EXAM_error = round(mean(EXAM_error, na.rm = TRUE),1),\n    N_Best_ALM = sum(Best_Model == \"ALM\", na.rm = TRUE),\n    N_Best_EXAM = sum(Best_Model == \"EXAM\", na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    Best_Model = case_when(\n      Avg_ALM_error &lt; Avg_EXAM_error ~ \"ALM\",\n      Avg_EXAM_error &lt; Avg_ALM_error ~ \"EXAM\",\n      TRUE ~ \"Tie\"  # In case of a tie or missing data\n    ))\n\n\n agg_pred_full &lt;-  post_dat  |&gt; group_by(id,condit,Model,Fit_Method,x) |&gt; \n    mutate(e2=abs(y-pred)) |&gt; \n    summarise(y=mean(y), pred=mean(pred), mean_error=mean(e2),ei2=abs(y-pred)) |&gt;\n    group_by(id,condit,Model,Fit_Method) |&gt; \n    summarise(mean_error=mean(mean_error),imean_error=mean(ei2)) |&gt; \n    round_tibble(1) |&gt; \n    group_by(Fit_Method,Model,condit) |&gt; \n   # summarise(mean_error=mean(mean_error),imean_error=mean(imean_error))\n   summarise(mean_error=mean(imean_error))\n\n \n \n   agg_pred_full &lt;-  pd_train  |&gt; group_by(condit,Model,Fit_Method,Block,x) |&gt; \n    mutate(e2=abs(y-pred)) |&gt; \n    summarise(y=mean(y), pred=mean(pred), mean_error=mean(e2),ei2=abs(y-pred)) |&gt;\n    group_by(condit,Model,Fit_Method,Block) |&gt; \n    summarise(mean_error=mean(mean_error), mean_ei2=mean(ei2)) |&gt; \n    group_by(Fit_Method,Model,condit) |&gt;\n    summarise(mean_error=mean(mean_error),mean_ei2=mean(mean_ei2)) \n   \n \n pd_train  |&gt; group_by(id, condit,Model,Fit_Method,Block,x) |&gt; \n    mutate(e2=abs(y-pred)) |&gt; \n    summarise(y=mean(y), pred=mean(pred), mean_error=mean(e2),ei2=abs(y-pred)) |&gt;\n    group_by(condit,Model,Fit_Method,Block) |&gt; \n    summarise(mean_error=mean(mean_error), mean_ei2=mean(ei2)) |&gt; \n    group_by(Fit_Method,Model,condit) |&gt;\n    summarise(mean_error=mean(mean_error),mean_ei2=mean(mean_ei2))   \n\n\n\nCodepost_dat_l |&gt; group_by(condit, Fit_Method,Resp,x) |&gt;mutate(x=as.factor(x)) |&gt;\n summarize(vx=mean(val)) |&gt; ggplot(aes(x=condit,y=vx, fill=x)) + stat_bar + facet_wrap(~Resp+Fit_Method)\n\n\n\n\n\n\nCodepost_dat_l |&gt; group_by(condit, Fit_Method,Resp,x) |&gt;mutate(x=as.factor(x)) |&gt;\n summarize(dist=mean(dist)) |&gt; ggplot(aes(x=condit,y=dist, fill=x)) + stat_bar + facet_wrap(~Resp+Fit_Method)\n\n\n\n\n\n\nCodepost_dat_l |&gt; group_by(id,condit, Fit_Method,Resp,x) |&gt;mutate(x=as.factor(x)) |&gt;\n summarize(dist=mean(dist)) |&gt; ggplot(aes(x=Resp,y=dist, fill=condit)) + stat_bar + facet_wrap(~Fit_Method+x)\n\n\n\n\n\n\nCodepost_dat_l |&gt; group_by(id,condit, Fit_Method,Resp) |&gt;mutate(x=as.factor(x)) |&gt;\n summarize(dist=mean(dist)) |&gt; ggplot(aes(x=Resp,y=dist, fill=condit)) + stat_bar + facet_wrap(~Fit_Method)\n\n\n\n\n\n\nCodepost_dat_l |&gt; group_by(id,condit, Fit_Method,Resp) |&gt;mutate(x=as.factor(x)) |&gt;\n summarize(dist=mean(dist)) |&gt; ggplot(aes(x=Resp,y=dist, fill=condit)) + stat_bar + facet_wrap(~Fit_Method)\n\n\n\n\n\n\n\n\nCode# pdl &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) \n# pd &lt;- post_dat |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\"))\n\n# pdl |&gt; group_by(id,condit, bandType,Resp) |&gt;\n#  summarize(dist=mean(dist)) |&gt; ggplot(aes(x=Resp,y=dist, fill=condit)) + stat_bar + facet_wrap(~bandType)\n\nlibrary(emmeans)\nlibrary(brms)\nlibrary(tidybayes)\noptions(brms.backend=\"cmdstanr\",mc.cores=1)\n\n\ninvisible(list2env(load_sbj_data(), envir = .GlobalEnv))\ninvisible(list2env(load_e1(), envir = .GlobalEnv))\n\ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\n\npdl &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; #left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n    filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\npdl_all &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; #left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n    filter(rank&lt;=1, !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\n\npd &lt;- post_dat_avg |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n    filter(rank&lt;=1,Fit_Method==\"Test_Train\")  |&gt; mutate(aerror = abs(error))\n\n\nd1 &lt;- pdl |&gt; group_by(id,condit,bandInt) |&gt; slice_head(n=1)\n\nd1 |&gt; group_by(condit) |&gt; summarize(c=median(c),lr=median(lr))\n\n\n\n\npdl |&gt; group_by(condit,Model) |&gt; summarize(me=mean(error),ae=mean(aerror))\npdls_sum &lt;- pdl |&gt; group_by(id, condit,Model,bandInt) |&gt; summarize(me=mean(error),ae=mean(aerror)) |&gt; \n  group_by(id,condit, Model) |&gt; summarize(me=mean(me),ae=mean(ae))\npdls_sum_b &lt;- pdl |&gt; group_by(id, condit,Model,bandInt) |&gt; summarize(me=mean(error),ae=mean(aerror))\n# id    condit   Model    me    ae\n#    &lt;fct&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n#  1 1     Varied   ALM    72.7 108. \n#  2 1     Varied   EXAM   34.5  63.3\n#  3 2     Varied   ALM   135.  138. \n#  4 2     Varied   EXAM   75.5  84.0\n\n# for each id and condit - add a column indicating which model had the lower error\npdls_sum |&gt; group_by(id,condit) |&gt; \n  mutate(ModelEXAM = ifelse(me[Model==\"EXAM\"] &lt; me[Model==\"ALM\"],\"EXAM\",\"ALM\")) |&gt; \n  group_by(condit,ModelEXAM) |&gt; \n  summarize(n=n(),me=mean(me),ae=mean(ae)) |&gt; \n  ggplot(aes(x=ModelEXAM,y=me,fill=condit)) + geom_col() + facet_wrap(~condit)\n\n\n\npdls_sum_b |&gt; group_by(id,condit,bandInt) |&gt; \n  mutate(ModelEXAM = ifelse(me[Model==\"EXAM\"] &lt; me[Model==\"ALM\"],\"EXAM\",\"ALM\")) |&gt; \n  group_by(condit,ModelEXAM,bandInt) |&gt; \n  summarize(n=n(),me=mean(me),ae=mean(ae)) |&gt; \n  ggplot(aes(x=ModelEXAM,y=me,fill=condit)) + geom_col() + facet_wrap(~bandInt)\n\n\nformula &lt;- bf(aerror ~ condit + (1 + condit | id) + (1 + condit | Model))\nmodel &lt;- brm(formula, data=pdl, chains=1,iter=500)\n\nformula &lt;- bf(error ~ condit * Model + c + lr + (1 + condit * Model + c + lr | id))\nmodelp &lt;- brm(formula, data=pdl, chains=1,iter=900)\nsummary(modelp)\nwrap_plots(plot(conditional_effects(modelp),points=FALSE,plot=FALSE))\nplot(emmeans(modelp, ~ c | Model), type = \"response\")\nplot(emmeans(modelp, ~ lr | Model), type = \"response\")\n\nformula &lt;- bf(error ~ condit * Model + c + lr + (1 + condit * Model + c + lr | id))\nmodelp &lt;- brm(formula, data=pdl, chains=1,iter=900)\n\n\nformula &lt;- bf(dist ~ condit*vb + c + lr + (1 + condit*vb| id)) \nd1 &lt;- pdl |&gt; group_by(id,condit,bandInt, Model) |&gt; slice_head(n=1)\n\nm_dist &lt;- brm(formula=formula,chains=1,\n      data = d1 ,\n      iter=900) \n\nsummary(m_dist)\nwrap_plots(plot(conditional_effects(m_dist),points=FALSE,plot=FALSE))\nemmeans(m_dist, ~ condit+lr)\nemmeans(m_dist, ~ condit+c)\nindividual_preds &lt;- fitted(m_dist, re_formula = NULL) %&gt;% data.frame() %&gt;% bind_cols(d1)\n\nformula_dist &lt;- bf(dist ~ condit * Model* c + lr + (1 + c * lr || id))\nmodel_dist &lt;- brm(formula_dist,data=d1, iter=2100, chains=2) \nsummary(model_dist)\n bayestestR::describe_posterior(model_dist)\nwrap_plots(plot(conditional_effects(model_dist),points=FALSE,plot=FALSE))\n\n\ntestAvgE1 |&gt; group_by(vb) |&gt; \nggplot(aes(x=vb,y=distAvg)) + \n  #stat_pointinterval(position=position_dodge(.5), alpha=.9) \n  geom_col()\n\n\n\n\nmodel_dist_std &lt;- d1 %&gt;%\n  mutate(c_std = scale(c),\n         lr_std = scale(lr)) %&gt;%\n  brm(\n    bf(dist ~ bandType * condit * c_std * lr_std + (1 + bandType * condit * c_std * lr_std | id)),\n    family = gaussian(),\n    prior = c(\n      prior(normal(0, 10), class = Intercept),\n      prior(normal(0, 10), class = b),\n      prior(cauchy(0, 5), class = sd),\n      prior(lkj(2), class = cor)\n    ),\n    iter = 4000,\n    warmup = 2000,\n    chains = 4,\n    cores = 4\n  )\n\n\n\n\n# fit brms model to determine which model is the better fit for each id and condit\nb4 &lt;- brm(data=pdl |&gt; filter(rank==1),aerror ~ Model + (1+Model|id), chains=1, iter=500, control=list(adapt_delta=0.92, max_treedepth=11))\n\n# visualize the strength of the evidence for each model, for each id\ncoef(b4)$id[,, 'ModelEXAM']  %&gt;% tibble::as_tibble(rownames=\"id\") |&gt; ggplot(aes(x=Estimate)) + geom_histogram()\n\n\n\n# mixed effects model of post_dat, predicting mean_error from Model\n# and Fit_Method, with random intercepts for id and condit\n\nlme4::lmer(data=post_dat |&gt; filter(rank==1),mean_error ~ Model*Fit_Method + (1|condit), REML=FALSE) |&gt; summary()\n\nlme4::lmer(data=post_dat |&gt; filter(rank==1),mean_error ~ Model*condit + (1|id), REML=FALSE) |&gt; summary()\n\nlme4::lmer(data=post_dat |&gt; filter(rank==1),mean_error ~ Model*condit*x + (1|id), REML=FALSE) |&gt; summary()\n\nlme4::lmer(data=pd |&gt; filter(rank==1),mean_error ~ Model*bandType + (1|id), REML=FALSE) |&gt; summary()\nlme4::lmer(data=pd |&gt; filter(rank==1),mean_error ~ Model*bandType*condit + (1|id), REML=FALSE) |&gt; summary()\n\nlme4::lmer(data=pd |&gt; filter(rank&lt;=50, condit==\"Varied\",Fit_Method==\"Test_Train\"),mean_error ~ Model*bandType + (1|id), REML=FALSE) |&gt; summary()\nlme4::lmer(data=pd |&gt; filter(rank&lt;=50, condit==\"Constant\",Fit_Method==\"Test_Train\"),mean_error ~ Model*bandType + (1|id), REML=FALSE) |&gt; summary()\n\nlibrary(brms)\noptions(brms.backend=\"cmdstanr\",mc.cores=4)\nb1 &lt;- brm(data=pd |&gt; \n    filter(rank==1),\n    mean_error ~ Model*bandType*condit, \n    chains=2, iter=1000, control=list(adapt_delta=0.92, max_treedepth=11))\nsummary(b1)\n\n\nb2 &lt;- brm(data=pd |&gt; filter(rank==1),mean_error ~ Model*bandInt*condit, chains=2, iter=1000, control=list(adapt_delta=0.92, max_treedepth=11))\nsummary(b2)\n\n\n\nb_id &lt;- brm(data=pdl |&gt; filter(rank==1),\n  aerror ~ Model + (1+Model|id), \n  file = paste0(here(\"data/model_cache/e1_brms_e_rf_id1.rds\")),\n  chains=1, iter=500, control=list(adapt_delta=0.92, max_treedepth=11))\nsummary(b5); bayestestR::describe_posterior(b5)\nwrap_plots(plot(conditional_effects(b_id),points=FALSE,plot=FALSE))\ncoef(b_id)$id[,, 'ModelEXAM']  %&gt;% tibble::as_tibble(rownames=\"id\") |&gt; ggplot(aes(x=Estimate)) + geom_histogram()\n\n\n\nb_vb &lt;- brm(data=pdl, aerror ~ Model*condit*vb + (1+vb*Model|id), chains=1, iter=900)\nwrap_plots(plot(conditional_effects(b_vb),points=FALSE,plot=FALSE))\nindividual_preds &lt;- fitted(b_vb, re_formula = NULL) %&gt;% data.frame() %&gt;% bind_cols(pdl)\nemmeans(b_vb, ~ condit * Model)\nplot(emmeans(b_vb, ~ condit * Model))\nb_vb %&gt;% emmeans(specs = ~ Model * condit) %&gt;% pairs()\n\n\n\n\n\n\nb5 &lt;- brm(data=pdl |&gt; filter(rank==1),\n  error ~ Model + (1+Model|condit:id), \n  file = paste0(here(\"data/model_cache/e1_brms_e_rfX_id1.rds\")),\n  chains=1, iter=500, control=list(adapt_delta=0.92, max_treedepth=11))\nsummary(b5); bayestestR::describe_posterior(b5)\n\n\nid_ee1 &lt;- coef(b5)$id[,, 'ModelEXAM']  %&gt;% tibble::as_tibble(rownames=\"id\") |&gt; \n  left_join(ranef(b4)$id[,, 'ModelEXAM']  %&gt;% tibble::as_tibble(rownames=\"id\") |&gt; transmute(id=id,ranef=Estimate),by=\"id\") \n\n\nleft_join(e1Sbjs,by=\"id\")\nranef(b4)$id[,, 'ModelEXAM']  %&gt;% tibble::as_tibble(rownames=\"term\")\n\n\nhypothesis(b4, \"ModelALM:conditVaried &lt; ModelEXAM:conditVaried\")\n\nid_ee1 &lt;- coef(b5)$`condit:id`[,, 'ModelEXAM']  %&gt;% tibble::as_tibble(rownames=\"id\")\n# id           Estimate Est.Error  Q2.5 Q97.5\n#    &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#  1 Constant_10     -21.0      18.2 -64.4  9.43\n#  2 Constant_101    -11.6      14.6 -34.0 21.2 \n#  3 Constant_102    -16.5      16.5 -51.7 16.3 \n\n# split id into condit (Constant or Varied) and id (number between 1-250 - occurs after the _)\nid_ee1 |&gt; mutate(condit=word(id,1,sep=\"_\"),id=word(id,2,sep=\"_\")) |&gt; \n    left_join(e1Sbjs,by=c(\"id\",\"condit\")) |&gt; \n    ggplot(aes(x=id,y=Estimate,fill=condit)) + \n    geom_col() + facet_wrap(~condit, scales=\"free_y\")\n\n\nindividual_preds &lt;- fitted(b4, re_formula = NA) %&gt;% data.frame() %&gt;% bind_cols(pdl)\n\n#coef(b4)$id[,, 'ModelEXAM']  %&gt;% tibble::as_tibble(rownames=\"term\")\n# A tibble: 156 × 5\n   term  Estimate Est.Error   Q2.5   Q97.5\n   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 1        -37.7      20.9  -74.1   8.80 \n 2 2        -46.2      21.0  -87.8   1.09 \n 3 3        -95.5      21.1 -145.  -61.2  \n 4 4        -42.8      20.8  -80.6  -0.263\n 5 5        -88.3      20.0 -128.  -54.9  \n\n# ranef(b4)$id[,, 'ModelEXAM']\n#     Estimate Est.Error      Q2.5    Q97.5\n# 1    21.8534      20.8  -14.9709  65.8017\n# 2    13.3688      20.2  -24.7168  60.5872\n# 3   -35.9294      20.4  -85.8305  -1.0799\n\n\nfixef(b4)\n#    fixef(b4)\n#           Estimate Est.Error  Q2.5 Q97.5\n# Intercept    193.1      7.18 177.6   205\n# ModelEXAM    -59.5      6.83 -71.8   -45\n\nfe &lt;- fixef(b4)[,1]\nid_ee1 |&gt; mutate(id=reorder(id,Estimate)) |&gt; \n left_join(e1Sbjs,by=\"id\") |&gt;\n  ggplot(aes(x=id,y=Estimate,fill=condit)) + \n  geom_col() +  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\") + coord_flip()\n\n\n data.frame(ranef(b4, pars = \"ModelEXAM\")$id[, ,'ModelEXAM']) %&gt;% \n  tibble::rownames_to_column(\"id\") %&gt;% \n  left_join(e1Sbjs, by = \"id\") |&gt; \n  mutate(intercept = fixef(b4)[,1][\"Intercept\"], \n          #cond= fe[\"conditVaried\"]*(condit==\"Varied\"),\n           int= fe[\"ModelEXAM:conditVaried\"]*(condit==\"Varied\" & Model==\"EXAM\"), \n         slope = Estimate + adjust)\n\n\n\n data.frame(coef(b4, pars = \"ModelEXAM\")$id[, ,'ModelEXAM']) %&gt;% \n  tibble::rownames_to_column(\"id\") %&gt;% \n  left_join(e1Sbjs, by = \"id\") \n\n\n\nb3 &lt;- brm(data=pdl |&gt; filter(rank==1),\n  aerror ~ Model*id, \n  file = paste0(here(\"data/model_cache/e1_brms_ee_ModelxID.rds\")),\n  chains=1, iter=500, control=list(adapt_delta=0.92, max_treedepth=11))\nsummary(b3)\n\n\n\nwrap_plots(plot(conditional_effects(b5),points=FALSE,plot=FALSE))\n\nplot(conditional_effects(b5, \n                         effects = \"Model:condit\", \n                         conditions=make_conditions(b5,\"bandType\" )),\n     points=FALSE,plot=TRUE)\n\n\n\nb3_coef &lt;- fixef(b3) |&gt; as_tibble() %&gt;% mutate(term=rownames(fixef(b3))) \nexam_coef &lt;- b3_coef |&gt; filter(term==\"ModelEXAM\") |&gt; pull(Estimate)\nhead(b3_coef)\n# A tibble: 6 × 5\n#   Estimate Est.Error  Q2.5   Q97.5 term     \n#      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    \n# 1   270.        3.42 264.  276.    Intercept\n# 2    -8.68      5.60 -18.5   0.269 ModelEXAM\n# 3   133.        4.69 124.  142.    id2      \n# 4    53.9       4.59  45.8  63.2   id3      \n# 5   -42.6       4.57 -51.7 -33.7   id4 \n\n# extract id's from term column - Intercept is id 1, term=ModelEXAM doesn't have an id, so don't include\nk &lt;- b3_coef |&gt; filter(term!=\"Intercept\" & term!=\"ModelEXAM\") |&gt; mutate(id=as.factor(str_extract(term,\"\\\\d+\"))) |&gt; left_join(testAvgE1 |&gt; filter(bandInt==100) |&gt; ungroup() |&gt; select(-bandType,-vb,-bandInt),by=\"id\") |&gt; \n  mutate(exam_coef=exam_coef) \n\n# add new variable parType, which is set to \"Model\" if term contains \"ModelEXAM\" and \"id\" otherwise\nk &lt;- k |&gt; mutate(parType=case_when(\n  str_detect(term,\"ModelEXAM\") ~ \"Model\",\n  TRUE ~ \"id\"\n))\n\nk |&gt; ggplot(aes(x=parType,y=Estimate,fill=condit)) + geom_boxplot()\n# plot Model coefficients for each individual\n\nk |&gt; filter(parType==\"Model\") |&gt; mutate(id=reorder(id,Estimate), Estimate=Estimate+exam_coef) |&gt; \n  ggplot(aes(x=id,y=Estimate,fill=condit)) + \n  geom_col() +  ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\") + coord_flip()\n\n\nmcmc_plot(b3,variable=c(\"b_ModelExam\"),regex=T)\n\n\n\n# t test for Model coefficients between conditions\nt.test(Estimate ~ condit, data=k |&gt; filter(parType==\"Model\"))\n\n\nk2 &lt;- k |&gt; filter(parType==\"Model\") |&gt; mutate(id=reorder(id,Estimate), Estimate=Estimate+exam_coef) |&gt; \n  select(id,condit,Estimate) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\"))\n\nk3 &lt;- k |&gt; filter(parType==\"Model\") |&gt; mutate(id=reorder(id,Estimate), Estimate=Estimate+exam_coef) |&gt; \n  select(id,condit,Estimate) |&gt; left_join(testE1,by=c(\"id\",\"condit\"))\n\n\n# assess group differences, while controlling for the effect of the Model coefficient\n\nlme4::lmer(data=k2,distAvg ~ Estimate + bandInt +condit+ (1|id), REML=FALSE) |&gt; summary()\nlme4::lmer(data=k2,distAvg ~  bandType* condit+ (1|id), REML=FALSE) |&gt; summary()\n\nlme4::lmer(data=k3,dist ~ Estimate + bandInt +condit+ (1|id), REML=FALSE) |&gt; summary()\nlme4::lmer(data=k3,dist ~ Estimate * bandInt * condit+ (1|id), REML=FALSE) |&gt; summary()\nlme4::lmer(data=k3,dist ~ Estimate * bandType * condit+ (1|id), REML=FALSE) |&gt; summary()\nlme4::lmer(data=k3,dist ~ Estimate * bandType * condit+ (1|id) + (1|bandInt), REML=FALSE) |&gt; summary()\nlme4::lmer(data=k3,vx ~ Estimate * bandInt * condit+ (1|id), REML=FALSE) |&gt; summary()\n\n\n# bin k3 in quantiles based on value of Estimate\nk3 |&gt; group_by(condit) |&gt;  mutate(quantile=ntile(Estimate,4)) |&gt; group_by(quantile,condit) |&gt; summarise(dist=mean(dist),.groups=\"drop\")\n\n  k3 |&gt; group_by(condit) |&gt;  mutate(quantile=ntile(Estimate,4)) |&gt; ggplot(aes(x=vb,y=dist,fill=condit)) + stat_bar + facet_wrap(~quantile)\n\n  k2 |&gt; ggplot(aes(x=Estimate,y=distAvg,col=condit)) + geom_point() + geom_smooth(method=\"lm\",se=FALSE) + facet_wrap(~vb)\n  \nfe &lt;- fixef(e1_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e1_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e1_vxBMM)[,1][\"conditVaried:bandInt\"]\n\ne1_slopes &lt;- readRDS(paste0(here::here(\"data/model_cache/e1_testVxBand_RF_5k.rds\")))  \nre &lt;-data.frame(ranef(e1_slopes, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; tibble::rownames_to_column(\"id\") |&gt; \n  left_join(e1Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixef(e1_slopes)[,1][\"bandInt\"] + \n           fixef(e1_slopes)[,1][\"conditVaried:bandInt\"]*(condit==\"Varied\"),slope = Estimate + adjust )\n\nid_est &lt;- k |&gt; filter(parType==\"Model\") |&gt; transmute(id=reorder(id,Estimate), exam_est=Estimate+exam_coef) |&gt; \n  left_join(re,by=\"id\")\n\nid_est |&gt; ggplot(aes(x=slope,y=exam_est,fill=condit)) + geom_point() + geom_smooth(method=\"lm\",se=FALSE) + facet_wrap(~condit)\n\n# combine with testE1, and assess whether slope or exam_est is a better predictor of dist\nlme4::lmer(data=.,dist ~ slope + exam_est + (1|id), REML=FALSE) |&gt; summary()\n\n# assess whether the effect of slope on dist is different between conditions\nid_est |&gt; left_join(testE1,by=c(\"id\",\"condit\")) %&gt;%  lme4::lmer(data=.,dist ~ slope * condit + (1|id), REML=FALSE) |&gt; summary()\nid_est |&gt; left_join(testE1,by=c(\"id\",\"condit\")) %&gt;%  lme4::lmer(data=.,dist ~ exam_est * condit + (1|id), REML=FALSE) |&gt; summary()\n\n# see whether exam_est or slope can predict condit better (condit is a factor)\nid_est |&gt; left_join(testE1,by=c(\"id\",\"condit\")) %&gt;%  lme4::lmer(data=.,condit ~ slope + exam_est + (1|id), REML=FALSE) |&gt; summary()\n#Error in mkRespMod(fr, REML = REMLpass) : response must be numeric\n# try logistic regression or classification\nid_est |&gt; left_join(testE1,by=c(\"id\",\"condit\")) %&gt;% glm(data=.,condit ~ slope + exam_est, family=\"binomial\") |&gt; summary()\n# coefficients:\n#             Estimate Std. Error z value             Pr(&gt;|z|)    \n# (Intercept) 0.203035   0.046429   4.373            0.0000123 ***\n# slope       3.050911   0.107725  28.321 &lt; 0.0000000000000002 ***\n# exam_est    0.145142   0.003311  43.834 &lt; 0.0000000000000002 ***\n\n\n\nCodeet_sum_dist &lt;- post_dat_l |&gt;\n  group_by(id,condit, Fit_Method, Resp) |&gt;\n  summarise(val = mean(dist), .groups = 'drop') |&gt;\n  pivot_wider(\n    names_from = Resp,\n    values_from = val,\n    values_fill = list(val = NA)\n  ) |&gt;\n  mutate(\n    ALM_error = round(abs(ALM - Observed),1),\n    EXAM_error = round(abs(EXAM - Observed),1),\n    Best_Model = case_when(\n      ALM_error &lt; EXAM_error ~ \"ALM\",\n      EXAM_error &lt; ALM_error ~ \"EXAM\",\n      TRUE ~ NA_character_  # In case of a tie or missing data\n    )\n  ) |&gt;\n  group_by(condit, Fit_Method) %&gt;%\n  summarise(\n    Avg_ALM_error = round(mean(ALM_error, na.rm = TRUE),1),\n    Avg_EXAM_error = round(mean(EXAM_error, na.rm = TRUE),1),\n    N_Best_ALM = sum(Best_Model == \"ALM\", na.rm = TRUE),\n    N_Best_EXAM = sum(Best_Model == \"EXAM\", na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    Best_Model = case_when(\n      Avg_ALM_error &lt; Avg_EXAM_error ~ \"ALM\",\n      Avg_EXAM_error &lt; Avg_ALM_error ~ \"EXAM\",\n      TRUE ~ \"Tie\"  # In case of a tie or missing data\n    )\n  )\n\n\net_sum_vx &lt;- post_dat_l |&gt;\n  group_by(id,condit, Fit_Method, Resp) |&gt;\n  summarise(val = mean(val), .groups = 'drop') |&gt;\n  pivot_wider(\n    names_from = Resp,\n    values_from = val,\n    values_fill = list(val = NA)\n  ) |&gt;\n  mutate(\n    ALM_error = round(abs(ALM - Observed),1),\n    EXAM_error = round(abs(EXAM - Observed),1),\n    Best_Model = case_when(\n      ALM_error &lt; EXAM_error ~ \"ALM\",\n      EXAM_error &lt; ALM_error ~ \"EXAM\",\n      TRUE ~ NA_character_  # In case of a tie or missing data\n    )\n  ) |&gt;\n  group_by(condit, Fit_Method) %&gt;%\n  summarise(\n    Avg_ALM_error = round(mean(ALM_error, na.rm = TRUE),1),\n    Avg_EXAM_error = round(mean(EXAM_error, na.rm = TRUE),1),\n    N_Best_ALM = sum(Best_Model == \"ALM\", na.rm = TRUE),\n    N_Best_EXAM = sum(Best_Model == \"EXAM\", na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    Best_Model = case_when(\n      Avg_ALM_error &lt; Avg_EXAM_error ~ \"ALM\",\n      Avg_EXAM_error &lt; Avg_ALM_error ~ \"EXAM\",\n      TRUE ~ \"Tie\"  # In case of a tie or missing data\n    )\n  )\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 3 (i.e. these plots combine all the posterior samples from all of the subjects). There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Model residuals are shown in the upper panels of Figure 2. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 2) also show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that y axes are scaled different between plots). Unsurprisingly, the advantage of EXAM is strongest for extrapolation positions (the three smallest bands for both groups - as well as the two highest bands for the Constant group). Although EXAM tends to perform better for both Constant and Varied participants (see also Table 2), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods.\nPanel B of Figure 2 directly compares the aggregated observed data to the posterior predictive distributions for the testing stage. Of interest are a) the extent to which the median estimates of the ALM and EXAM posteriors deviate from the observed medians for each velocity band; b) the ability of ALM and EXAM to discriminate between velocity bands; c) the relative performance of models that are constrained by the training data (i.e. the ‘fit to train only’ and ‘fit to both’ models) compared to the ‘fit to test only’ models; and d) the extent to which the variance of the posterior predictive distributions mimics the variance of the observed data.\nConsidering first the models fit to only the testing data, which reflect the best possible performance of ALM and EXAM at capturing the group-aggregated testing patterns. For the varied group, both ALM and EXAM are able to capture the median values of the observed data within the 66% credible intervals, and the spread of model predictions generally matches that of the observed data. For the constant group, only EXAM is able to capture the median range of values across the velocity bands, with ALM generally underestimating human velocoties in the upper bands, and overestimating in the lower bands. In the case of band 100, the median ALM prediction appears to match that of our participants - however this is due to a large subset of participants have ALM predictions near 0 for band 100, a pattern we will explore further in our considertation of individual patterns below. Models fit to both training and testing data show a similar pattern to only the testing data display the same basic pattern as those fit to only the testing data, albeit with slightly larger residuals. However models fit to only the training data display markedly worse performance at accounting for the key testing patterns.\n\n** explain how the constant group ALM predictions for band 100 look deceptively good due to aggregation of a large subset of subjects having ALM predictions of 0 for vb100, and a large subset with ALM predictions close to their position 800 value. This is relected by much greater variance of the ALM esimates in the posterior predictive plot\n** comment on how much constrained by the training data has a worse impact on the EXAM predictions for varied than for constant - perhaps due to the varied training data being much noisier than the constant training data.\n** comment on EXAM doing a better job mimicing the within-condition variance of the observed data\n** comment on the % of Constant subjects being best accounted for by EXAM being higher.\n** does EXAM do better for the Constant group because the constant group performs better? Or does training with a single example encourage an exam sort of strategy?\n\n\nCode##| layout: [[45,-5, 45], [100]]\n##| fig-subcap: [\"Model Residuals - training data\", \"Model Residuals - testing data\",\"Full posterior predictive distributions vs. observed data from participants.\"]\ntrain_resid &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method, Block) |&gt; \n  summarise(y = mean(y), pred = mean(pred), error = y - pred) |&gt;\n  ggplot(aes(x = Block, y = abs(error), fill=Model)) + \n  stat_bar + \n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, scales=\"free\",ncol=2) +\n  scale_fill_manual(values=wes_palette(\"AsteroidCity2\"))+\n  labs(title=\"Model Residual Errors - Training Stage\", y=\"RMSE\", x= \"Training Block\") +\n  theme(legend.title = element_blank(), legend.position=\"top\")\n\ntest_resid &lt;-  post_dat |&gt; \n   group_by(id,condit,x,Model,Fit_Method,rank) |&gt;\n   summarize(error=mean(abs(y-pred)),n=n()) |&gt;\n   group_by(id,condit,x,Model,Fit_Method) |&gt;\n   summarize(error=mean(error)) |&gt;\n  mutate(vbLab = factor(paste0(x,\"-\",x+200))) |&gt;\n  ggplot(aes(x = vbLab, y = abs(error), fill=Model)) + \n  stat_bar + \n  scale_fill_manual(values=wes_palette(\"AsteroidCity2\"))+\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\") +\n  labs(title=\"Model Residual Errors - Testing Stage\",y=\"RMSE\", x=\"Velocity Band\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\ngroup_pred &lt;- post_dat_l |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200),levels=levels(testAvgE1$vb))) |&gt;\n  ggplot(aes(x=val,y=vbLab,col=Resp)) + \n  stat_pointinterval(position=position_dodge(.5), alpha=.9) + \n  scale_color_manual(values=wes_palette(\"AsteroidCity2\"))+\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\") +\n  labs(title=\"Posterior Predictions - Testing Stage\",y=\"Velocity Band (lower bound)\", x=\"X Velocity\") +\ntheme(legend.title=element_blank(),axis.text.y = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\n\n((train_resid | test_resid) / group_pred) +\n  plot_layout(heights=c(1,1.5)) & \n  plot_annotation(tag_levels = list(c('A1','A2','B')),tag_suffix = ') ') & \n  theme(plot.tag.position = c(0, 1))\n\n\n\n\n\n\nFigure 2: A) Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y axes are scaled differently between facets. B) Full posterior predictive distributions vs. observed data from participants.Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median.",
    "crumbs": [
      "Model",
      "HTW Model e1"
    ]
  },
  {
    "objectID": "Sections/combo1.html#model-fitting",
    "href": "Sections/combo1.html#model-fitting",
    "title": "HTW Modeling",
    "section": "\n2.2 Model Fitting",
    "text": "2.2 Model Fitting\nTo fit ALM and EXAM to our participant data, we employ a similar method to Mcdaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data in with separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase.Models to were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate the parameters of ALM and EXAM, we used approximate bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g. many neural network models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individuals velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.\n\n2.2.1 Modelling Results\n\n2.2.1.1 Group level Patterns\n\n\n\nTable 13: Models errors predicting empirical data - aggregated over all participants, posterior parameter values, and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n\nTask Stage\n      Fit Method\n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nTest\nFit to Test Data\n199.93\n103.36\n104.01\n85.68\n\n\nTest\nFit to Test & Training Data\n216.97\n170.28\n127.94\n144.86\n\n\nTest\nFit to Training Data\n467.73\n291.38\n273.30\n297.91\n\n\nTrain\nFit to Test Data\n297.82\n2,016.01\n53.90\n184.00\n\n\nTrain\nFit to Test & Training Data\n57.40\n132.32\n42.92\n127.90\n\n\nTrain\nFit to Training Data\n51.77\n103.48\n51.43\n107.03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y axes are scaled differently between facets. A) Residuals predicting each block of the training data. B) Residuals predicting each band during the testing stage. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 16, and model predictions are shown alongside the empirical data in Figure 18. There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Aggregated residuals are displayed in Figure 17. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences in training performance between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 17) also show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that y axes are scaled different between plots). Although EXAM tends to perform better for both Constant and Varied participants (see also Figure 19), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods. The primary predictive difference between ALM and EXAM is made clear in Figure 18, which directly compares the observed data against the posterior predictive distributions for both models. Regardless of how the models are fit, only EXAM can capture the pattern where participants are able to discriminate all 6 target bands.\n\n\n\n\n\n\n\nFigure 18: Empirical data and Model predictions for mean velocity across target bands. Fitting methods (Test Only, Test & Train, Train Only) - are separated across rows, and Training Condition (Constant vs. Varied) are separated by columns. Each facet contains the predictions of ALM and EXAM, alongside the observed data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: A-C) Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied). Lower values on the y axis indicate better model fit. D) Specific contrasts of model performance comparing 1) EXAM fits between constant and varied training; 2) ALM vs. EXAM for the varied group; 3) ALM fits between constant and varied. Negative error differences indicate that the term on the left side (e.g. EXAM Constant) tended to have smaller model residuals.\n\n\n\n\nTo quantitatively assess whether the differences in performance between models, we fit a bayesian regressions predicting the errors of the posterior predictions of each models as a function of the Model (ALM vs. EXAM) and training condition (Constant vs. Varied).\nModel errors were significantly lower for EXAM (\\(\\beta\\) = -37.54, 95% CrI [-60.4, -14.17], pd = 99.85%) than ALM. There was also a significant interaction between Model and Condition (\\(\\beta\\) = 60.42, 95% CrI [36.17, 83.85], pd = 100%), indicating that the advantage of EXAM over ALM was significantly greater for the constant group. To assess whether EXAM predicts constant performance significantly better for Constant than for Varied subjects, we calculated the difference in model error between the Constant and Varied conditions specifically for EXAM. The results indicated that the model error for EXAM was significantly lower in the Constant condition compared to the Varied condition, with a mean difference of -22.879 (95% CrI [-46.016, -0.968], pd = 0.981).\n\n\n\nTable 14: Models errors predicting empirical data - aggregated over all participants, posterior parameter values, and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n\n\n      \n        E2\n      \n      \n        E3\n      \n    \n\nTask Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nFit to Test Data\n    \n\nTest\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n    \n\nTest\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n    \n\nTest\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20: Empirical data and Model predictions from Experiment 2 and 3 for the testing stage. Observed data is shown on the right. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\n\n\n\nTable 15: Results of Bayesian Regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and the interaction between Model and Condition. The values represent the estimate coefficient for each term, with 95% credible intervals in brackets. The intercept reflects the baseline of ALM and Constant. The other estimates indicate deviations from the baseline for the EXAM mode and varied condition. Lower values indicate better model fit.\n\n\n\n\n\n\n\nExperiment\n      Term\n      Estimate\n      \n        Credible Interval\n      \n      pd\n    \n\n95% CrI Lower\n      95% CrI Upper\n    \n\n\n\nExperiment 1\n    \n\nExp 1\nIntercept\n176.30\n156.86\n194.59\n1.00\n\n\nExp 1\nModelEXAM\n−88.44\n−104.51\n−71.81\n1.00\n\n\nExp 1\nconditVaried\n−37.54\n−60.40\n−14.17\n1.00\n\n\nExp 1\nModelEXAM:conditVaried\n60.42\n36.17\n83.85\n1.00\n\n\nExperiment 2\n    \n\nExp 2\nIntercept\n245.87\n226.18\n264.52\n1.00\n\n\nExp 2\nModelEXAM\n−137.73\n−160.20\n−115.48\n1.00\n\n\nExp 2\nconditVaried\n−86.39\n−113.52\n−59.31\n1.00\n\n\nExp 2\nModelEXAM:conditVaried\n56.87\n25.26\n88.04\n1.00\n\n\nExperiment 3\n    \n\nExp 3\nIntercept\n164.83\n140.05\n189.44\n1.00\n\n\nExp 3\nModelEXAM\n−65.66\n−85.97\n−46.02\n1.00\n\n\nExp 3\nconditVaried\n−40.61\n−75.90\n−3.02\n0.98\n\n\nExp 3\nbandOrderReverse\n25.47\n−9.34\n58.68\n0.93\n\n\nExp 3\nModelEXAM:conditVaried\n41.90\n11.20\n72.54\n0.99\n\n\nExp 3\nModelEXAM:bandOrderReverse\n−7.32\n−34.53\n21.05\n0.70\n\n\nExp 3\nconditVaried:bandOrderReverse\n30.82\n−19.57\n83.56\n0.88\n\n\nExp 3\nModelEXAM:conditVaried:bandOrderReverse\n−60.60\n−101.80\n−18.66\n1.00\n\n\n\n\n\n\n\n\n\nModel Fits to Experiment 2 and 3. Data from Experiments 2 and 3 were fit to ALM and EXAM in the same manner as Experiment1 . For brevity, we only plot and discuss the results of the “fit to training and testing data” models - results from the other fitting methods can be found in the appendix. The model fitting results for Experiments 2 and 3 closely mirrored those observed in Experiment 1. The Bayesian regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and their interaction (see Table 15) revealed a consistent main effect of Model across all three experiments. The negative coefficients for the ModelEXAM term (Exp 2: \\(\\beta\\) = -86.39, 95% CrI -113.52, -59.31, pd = 100%; Exp 3: \\(\\beta\\) = -40.61, 95% CrI -75.9, -3.02, pd = 98.17%) indicate that EXAM outperformed ALM in both experiments. Furthermore, the interaction between Model and Condition was significant in both Experiment 2 (\\(\\beta\\) = 56.87, 95% CrI 25.26, 88.04, pd = 99.98%) and Experiment 3 (\\(\\beta\\) = 41.9, 95% CrI 11.2, 72.54, pd = 99.35%), suggesting that the superiority of EXAM over ALM was more pronounced for the Constant group compared to the Varied group, as was the case in Experiment 1. Recall that Experiment 3 included participants in both the original and reverse order conditions - and that this manipulation interacted with the effect of training condition. We thus also controleld for band order in our Bayesian Regression assessing the relative performance of EXAM and ALM in Experiment 3. There was a significant three way interaction between Model, Training Condition, and Band Order (\\(\\beta\\) = -60.6, 95% CrI -101.8, -18.66, pd = 99.83%), indicating that the relative advantage of EXAM over ALM was only more pronounced in the original order condition, and not the reverse order condition (see Figure 21).\n\n\n\n\n\n\n\nFigure 21: Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied) on Model Error for Experiment 2 and 3 data. Experiment 3 also includes a control for the order of training vs. testing bands (original order vs. reverse order).\n\n\n\n\nComputational Model Summary. Across the model fits to all three experiments, we found greater support for EXAM over ALM (negative coefficients on the ModelEXAM term in Table 15), and moreover that the constant participants were disproportionately well described by EXAM in comparison to ALM (positive coefficients on ModelEXAM:conditVaried terms in Table 15). This pattern is also clearly depicted in Figure 22, which plots the difference in model errors between ALM and EXAM for each individual participant. Both varied and constant conditions have a greater proportion of subjects better fit by EXAM (positive error differences), with the magnitude of EXAM’s advantage visibly greater for the constant group. It also bears mention that numerous participants were better fit by ALM, or did not show a clear preference for either model. A subset of these participants are shown in Figure 23.\n\n\n\n\n\n\n\nFigure 22: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands."
  },
  {
    "objectID": "Analysis/e1.html#e1-summary",
    "href": "Analysis/e1.html#e1-summary",
    "title": "Experiment 1",
    "section": "E1 Summary",
    "text": "E1 Summary\nIn Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions rresulted in lower final training performance is consistent with much of the prior researchon the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "Analyses",
      "Experiment 1"
    ]
  },
  {
    "objectID": "Sections/combo1.html#e1-summary",
    "href": "Sections/combo1.html#e1-summary",
    "title": "HTW Modeling",
    "section": "\n1.6 E1 Summary",
    "text": "1.6 E1 Summary\nIn Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions rresulted in lower final training performance is consistent with much of the prior researchon the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training."
  },
  {
    "objectID": "Sections/Discussion.html#overall-summary-of-experiments-1-3",
    "href": "Sections/Discussion.html#overall-summary-of-experiments-1-3",
    "title": "General Discussion",
    "section": "",
    "text": "Across three experiments, we investigated the impact of training variability on learning and extrapolation in a visuomotor function learning task. In Experiment 1, participants in the varied training condition, who experienced a wider range of velocity bands during training, showed lower accuracy at the end of training compared to those in the constant training condition.\nCrucially, during the testing phase, the varied group exhibited significantly larger deviations from the target velocity bands, particularly for the extrapolation bands that were not encountered during training. The varied group also showed less discrimination between velocity bands, as evidenced by shallower slopes when predicting response velocity from target velocity band.\nExperiment 2 extended these findings by reversing the order of the training and testing bands. Similar to Experiment 1, the varied group demonstrated poorer performance during both training and testing phases. However, unlike Experiment 1, the varied group did not show a significant difference in discrimination between bands compared to the constant group.\nIn Experiment 3, we provided only ordinal feedback during training, in contrast to the continuous feedback provided in the previous experiments. Participants were assigned to both an order condition (original or reverse) and a training condition (constant or varied). The varied condition showed larger deviations at the end of training, consistent with the previous experiments. Interestingly, there was a significant interaction between training condition and band order, with the varied condition showing greater accuracy in the reverse order condition. During testing, the varied group once again exhibited larger deviations, particularly for the extrapolation bands. The reverse order conditions showed smaller deviations compared to the original order conditions. Discrimination between velocity bands was poorer for the varied group in the original order condition, but not in the reverse order condition.\nAll three of our experiments yielded evidence that varied training conditions produced less learning by the end of training, a pattern consistent with much of the previous research on the influence of training variability (Catalano & Kleiner, 1984; Soderstrom & Bjork, 2015; Wrisberg et al., 1987). The sole exception to this pattern was the reverse order condition in Experiment 3, where the varied group was not significantly worse than the constant group. Neither the varied condition trained with the same reverse-order items in Experiment 2, nor the original-order varied condition trained with ordinal feedback in Experiment 3 were able to match the performance of their complementary constant groups by the end of training, suggesting that the relative success of the ordinal-reverse ordered varied group cannot be attributed to item or feedback effects alone.\nOur findings also diverge from the two previous studies to cleanly manipulate the variability of training items in a function learning task (DeLosh et al., 1997; van Dam & Ernst, 2015), although the varied training condition of van Dam & Ernst (2015) also exhibited less learning, neither of these previous studies observed any difference between training conditions in extrapolation to novel items. Like DeLosh et al. (1997) , our participants exhibited above chance extrapolation/discrimination of novel items, however they observed no difference between any of their three training conditions. A noteworthy difference difference between our studies is that DeLosh et al. (1997) trained participants with either 8, 20, or 50 unique items (all receiving the same total number of training trials). These larger sets of unique items, combined with the fact that participants achieved near ceiling level performance by the end of training - may have made it more difficult to observe any between-group differences of training variation in their study. van Dam & Ernst (2015) ’s variability manipulation was more similar to our own, as they trained participants with either 2 or 5 unique items. However, although the mapping between their input stimuli and motor responses was technically linear, the input dimension was more complex than our own, as it was defined by the degree of “spikiness” of the input shape. This entirely arbitrary mapping also would have preculded any sense of a “0” point, which may partially explain why neither of their training conditions were able to extrapolate linearly in the manner observed in the current study or in DeLosh et al. (1997).",
    "crumbs": [
      "Sections",
      "General Discussion"
    ]
  },
  {
    "objectID": "Sections/combo1.html#general-discussion",
    "href": "Sections/combo1.html#general-discussion",
    "title": "HTW Modeling",
    "section": "\n2.3 General Discussion",
    "text": "2.3 General Discussion"
  },
  {
    "objectID": "Sections/combo1.html#comparison-to-project-1",
    "href": "Sections/combo1.html#comparison-to-project-1",
    "title": "HTW Modeling",
    "section": "\n3.1 Comparison to Project 1",
    "text": "3.1 Comparison to Project 1\n\n3.1.1 Differences between the tasks\nThere are a number of differences between Project 1’s Hit The Target (HTT), and Project 2’s Hit The Wall (HTW) tasks.\n\nTask Space Complexity: In HTW, the task space is also almost perfectly smooth, at least for the continuous feedback subjects, if they throw 100 units too hard, they’ll be told that they were 100 units too hard. Whereas in HTT,  it was possible to produce xy velocity combinations that were technically closer to the empirical solution space than other throws, but which resulted in worse feedback due to striking the barrier.\nPerceptual Distinctiveness: HTT offers perceptually distinct varied conditions that directly relate to the task’s demands, which may increase the sallience between training positions encounted by the varied group. In contrast, HTW’s varied conditions differ only in the numerical values displayed, lacking the same level of perceptual differentiation. Conversely in HTW, the only difference between conditions for the varied group are the numbers displayed at the top of the screen which indicate the current target band(e.g. 800-1000, or 1000-1200)\nIn HTW, our primary testing stage of interest has no feedback, whereas in HTT testing always included feedback (the intermittent testing in HTT expt 1 being the only exception). Of course, we do collect testing with feedback data at the end of HTW, but we haven’t focused on that data at all in our modelling work thus far. It’s also interesting to recall that the gap between varied and constant in HTW does seem to close substantially in the testing-with-feedback stage. The difference between no-feedback and feedback testing might be relevant if the benefits of variation have anything to do with improving subsequent learning (as opposed to subsequent immediate performance), OR if the benefits of constant training rely on having the most useful anchor, having the most useful anchor might be a lot less helpful if you’re getting feedback from novel positions and can thus immediately begin to form position-specific anchors for the novelties, rather than relying on a training anchor. \nHTW and HTT both have a similar amount of training trials (~200), and thus the constant groups acquire a similar amount of experience with their single position/velocity in both experiments. However, the varied conditions in both HTT experiments train on 2 positions, whereas the varied group in HTW trains on 3 velocity bands. This means that in HTT the varied group gets half as much experience on any one position as the constant group, and in HTW they only get 1/3 as much experience in any one position. There are likely myriad ways in which this might impact the success of the varied group regardless of how you think the benefits of variation might be occurring, e.g. maybe they also need to develop a coherent anchor, maybe they need more experience in order to extract a function, or more experience in order to properly learn to tune their c parameter.",
    "crumbs": [
      "Sections",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Sections/combo1.html#modelling-interpretation",
    "href": "Sections/combo1.html#modelling-interpretation",
    "title": "HTW Modeling",
    "section": "\n3.2 Modelling Interpretation",
    "text": "3.2 Modelling Interpretation\n\nwhy does Constant do better\n\nkind of task that permits for prior knowledge about 0\nlearning - end of training\n\n\nwhat does it suggest that the constant group was disproportionately well explained by the EXAM model?\n\n\n\n3.2.1 Limitations\n\namount of training\nno mechanism to account for sequence effects in models\nmore rigorous model comparison to account for exams greater complexity\nknowledge of function",
    "crumbs": [
      "Sections",
      "HTW Modeling"
    ]
  },
  {
    "objectID": "Sections/Discussion.html#modelling-interpretation",
    "href": "Sections/Discussion.html#modelling-interpretation",
    "title": "General Discussion",
    "section": "",
    "text": "EXAM is the best model for both groups, but EXAM does relatively good at accounting for the constant group. May have seemed counterintuitive, if one assumed that multiple, varied, examples were necessary to extract a rule. But, EXAM is not a conventional rule model - it doesn’t require explictly abstract of a rule, but rather the rule-based response occurs during retrieval. The constant groups formation of a single, accurate, input-output association, in combination with the usefulness of the zero point, may have been sufficient for EXAM, and the constant group, to perform well.\nOne concern may have been that the assumption of participants making use of the zero point turned the extrapolation problem into an interpolation problem - however this concern is ameliorated by the consistency of the results across both the original and reverse order conditions.\n\nwhy does Constant do better\n\nkind of task that permits for prior knowledge about 0\nlearning - end of training\n\nwhat does it suggest that the constant group was disproportionately well explained by the EXAM model?\n\n\n\n\n\namount of training\nno mechanism to account for sequence effects in models\nmore rigorous model comparison to account for exams greater complexity\nknowledge of function",
    "crumbs": [
      "Sections",
      "General Discussion"
    ]
  },
  {
    "objectID": "Sections/htw_full.html",
    "href": "Sections/htw_full.html",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "",
    "text": "Link to project page\nWorking Draft of Manuscript\nrepo\n_______________________________________________________________________________",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#function-learning-and-extrapolation",
    "href": "Sections/htw_full.html#function-learning-and-extrapolation",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Function Learning and Extrapolation",
    "text": "Function Learning and Extrapolation\nThe study of human function learning investigates how people learn relationships between continuous input and output values. Function learning is studied both in tasks where individuals are exposed to a sequence of input/output pairs (DeLosh et al., 1997; McDaniel et al., 2013), or situations where observers are presented with an incomplete scatterplot or line graph and make predictions about regions of the plot that do not contain data (Ciccione & Dehaene, 2021; Courrieu, 2012; Said & Fischer, 2021; Schulz et al., 2020). Studies of function learning often compare the difficulty of learning functions of different underlying forms (e.g. linear, bi-linear, power, sinusoidal), and the extent to which participants can accurately respond to novel inputs that fall in-between previously experienced inputs (interpolation testing), or that fall outside the range of previously experienced inputs (extrapolation).\nCarroll (1963) conducted the earliest work on function learning. Input stimuli and output responses were both lines of varying length. The correct output response was related to the length of the input line by a linear, quadratic, or random function. Participants in the linear and quadratic performed above chance levels during extrapolation testing, with those in the linear condition performing the best overall. Carroll argued that these results were best explained by a rule-based model wherein learners form an abstract representation of the underlying function. Subsequent work by Brehmer (1974), testing a wider array of functional forms, provided further evidence for superior extrapolation in tasks with linear functions. Brehmer argued that individuals start out assuming a linear function, but given sufficient error will progressively test alternative hypotheses with polynomials of greater degree. Koh & Meyer (1991) employed a visuomotor function learning task, wherein participants were trained on examples from an unknown function relating the length of an input line to the duration of a response (time between keystrokes). In this domain, participants performed best when the relation between line length and response duration was determined by a power law, as opposed to linear function. Koh and Meyer developed the log-polynomial adaptive-regression model to account for their results.\nThe first significant challenge to rule-based accounts of function learning was put forth by DeLosh et al. (1997) . In their task, participants learned to associate stimulus magnitudes with response magnitudes that were related via either linear, exponential, or quadratic function. Participants approached ceiling performance by the end of training in each function condition, and were able to accurately respond on interpolation testing trials. All three conditions demonstrated some capacity for extrapolation, however participants in the linear condition tended to underestimate the true function, while exponential and quadratic participants reliably overestimated the true function on extrapolation trials. Extrapolation and interpolation performances are depicted in Figure 1.\nThe authors evaluated the rule-based models introduced in earlier research (with some modifications enabling trial-by-trial learning). The polynomial hypothesis testing model (Brehmer, 1974; Carroll, 1963) tended to mimic the true function closely in extrapolation, and thus offered a poor account of the under and over-estimation biases shown in the human data. The log-polynomial adaptive regression model (Koh & Meyer, 1991) was able to mimic some of the systematic deviations produced by human subjects, but also predicted overestimation in cases where underestimation occurred.\nThe authors also introduced two new function-learning models. The Associative Learning Model (ALM) and the extrapolation-association model (EXAM). ALM is a two layer connectionist model adapted from the ALCOVE model in the category learning literature (Kruschke, 1992). ALM belongs to the general class of radial-basis function neural networks, and can be considered a similarity-based model in the sense that the nodes in the input layer of the network are activated as a function of distance (see Figure 17). The EXAM model retains the same similarity-based activation and associative learning mechanisms as ALM, while being augmented with a linear rule response mechanism. When presented with novel stimuli, EXAM will retrieve the most similar input-output examples encountered during training, and from those examples compute a local slope. ALM was able to provide a good account of participants’ training and interpolation data in all three function conditions, however it was unable to extrapolate. EXAM, by contrast, was able to reproduce both the extrapolation underestimation, as well as the quadratic and exponential overestimation patterns exhibited by the human participants. Subsequent research identified some limitations in EXAM’s ability to account for cases where human participants learn and extrapolate a sinusoidal function (Bott & Heit, 2004) or to scenarios where different functions apply to different regions of the input space (Kalish et al., 2004), though EXAM has been shown to provide a good account of human learning and extrapolation in tasks with bi-linear, V-shaped input spaces (McDaniel et al., 2009).\n\nDisplay codepacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n  brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n  data.table, stringr, here,conflicted, gt, ggh4x, patchwork, knitr)\n#options(brms.backend=\"cmdstanr\",mc.cores=4)\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\",\"here\"), conflict_prefer_all, quiet = TRUE)\noptions(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\n\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\",\"here\"), conflict_prefer_all, quiet = TRUE)\nwalk(c(\"Display_Functions\",\"deLosh_data\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\", \"prep_model_data\",\"org_functions\"), ~source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n\n# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n# source(here::here(\"Functions\",\"deLosh_data.R\"))\n# source(here::here(\"Functions\",\"Display_Functions.R\"))\n\n\n\n\n\n\n\n\n\nFigure 1: The generalization patterns of human particpiants observed in DeLosh et al. (1997) (reproduced from Figure 3 in their manuscript). Dots represent the average responses of human participants, and solid lines represent the true functions. The dashed vertical lines indicate the lower and upper bounds of the trained examples. Stimulii that fall within the dashed lines are interpolations of the training examples, while those that fall outside the dashed lines are extrapolations.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#overview-of-present-study",
    "href": "Sections/htw_full.html#overview-of-present-study",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Overview Of Present Study",
    "text": "Overview Of Present Study\nThe present study investigates the influence of training variability on learning, generalization, and extrapolation in a uni-dimensional visuomotor function learning task. To the best of our knowledge, this research is the first to employ the classic constant vs. varied training manipulation, commonly used in the literature studying the benefits of variability, in the context of a uni-dimensional function learning task. Across three experiments, we compare constant and varied training conditions in terms of learning performance, extrapolation accuracy, and the ability to reliably discriminate between stimuli.\nTo account for the empirical results, we will apply a series of computational models, including the Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM). Notably, this study is the first to employ approximate Bayesian computation (ABC) to fit these models to individual subject data, enabling us to thoroughly investigate the full range of posterior predictions of each model, and to examine the ability of these influential models of function learning to account for both the group level and individual level data.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#methods",
    "href": "Sections/htw_full.html#methods",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Methods",
    "text": "Methods\nParticipants A total of 156 participants were recruited from Indiana University Introductory Psychology Courses. Participants were randomly assigned to one of two training conditions: varied training or constant training.\nTask. The “Hit The Wall” (HTW) visuomotor extrapolation task task was programmed in JavaScript, making use of the phaser.io game library. The HTW task involved launching a projectile such that it would strike the “wall” at the target speed indicated at the top of the screen (see Figure 2). The target velocities were given as a range, or band, of acceptable velocity values (e.g., band 800-1000). During the training stage, participants received feedback indicating whether they had hit the wall within the target velocity band, or how many units their throw was above or below the target band. Participants were instructed that only the x velocity component of the ball was relevant to the task. The y velocity, or the location at which the ball struck the wall, had no influence on the task feedback.\n\n\n\n\n\n\n\n\nFigure 2: The Hit the wall task. Participants launch the blue ball to hit the red wall at the target velocity band indicated at the top of the screen. The ball must be released from within the orange square - but the location of release, and the location at which the ball strikes the wall are both irrelevant to the task feedback.\n\n\n\n\nProcedure. All participants completed the task online. Participants were provided with a description of the experiment and indicated informed consent. Figure 3 illustrates the general procedure. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). Participants in the constant training condition trained on only one velocity band (800-1000) - the closest band to what would be the novel extrapolation bands in the testing stage.\nFollowing the training stage, participants proceeded immediately to the testing stage. Participants were tested from all six velocity bands, in two separate stages. In the novel extrapolation testing stage, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials. Participants were also tested from the three velocity bands that were trained by the varied condition (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training. The order in which participants completed the novel-extrapolation and testing-from-3-varied bands was counterbalanced across participants. A final training stage presented participants with “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 800-10001000-12001200-1400Test1\nTest  Novel Bands 100-300350-550600-800data1-&gt;Test1\ndata2\n Constant Training 800-1000data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  100-300350-550600-800Test2\n  Test   Varied Training Bands  800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 3: Experiment 1 Design. Constant and Varied participants complete different training conditions.\n\n\n\n\n\nDisplay code# pacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n#   brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n#   stringr, here,conflicted, patchwork, knitr)\n# #options(brms.backend=\"cmdstanr\",mc.cores=4)\n# options(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\n# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) \ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ntestE1 &lt;- e1 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE1 &lt;-  e1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE1_max &lt;- trainE1 |&gt; filter(Trial_Bin == nbins, bandInt==800)\ntrainE1_avg &lt;- trainE1_max |&gt; group_by(id,condit) |&gt; summarise(avg = mean(dist))",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#e1-summary",
    "href": "Sections/htw_full.html#e1-summary",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "E1 Summary",
    "text": "E1 Summary\nIn Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions resulted in lower final training performance are consistent with much of the prior research on the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#experiment-2",
    "href": "Sections/htw_full.html#experiment-2",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Experiment 2",
    "text": "Experiment 2\n\nCode# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne2 &lt;- readRDS(here(\"data/e2_08-04-23.rds\")) \ne2Sbjs &lt;- e2 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ntestE2 &lt;- e2 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE2 &lt;-  e2 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE2_max &lt;- trainE2 |&gt; filter(Trial_Bin == nbins, bandInt==600)\n\n# e2 |&gt; group_by(condit, bandOrder) |&gt; summarise(n_distinct(id))\n\n\nMethods & Procedure\nThe task and procedure of Experiment 2 was identical to Experiment 1, with the exception that the training and testing bands were reversed (see Figure 8). The Varied group trained on bands 100-300, 350-550, 600-800, and the constant group trained on band 600-800. Both groups were tested from all six bands. A total of 110 participants completed the experiment (Varied: 55, Constant: 55).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 100-300350-550600-800Test1\nTest  Novel Bands  800-10001000-12001200-1400data1-&gt;Test1\ndata2\n Constant Training 600-800data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  800-10001000-12001200-1400Test2\n  Test   Varied Training Bands  100-300350-550600-800Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 8: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.\n\n\n\n\nResults\n\nCodep1 &lt;- trainE2 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e2_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\nFigure 9: Experiment 2 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\nCodebmm_e2_train &lt;- trainE2_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e2_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\nmtr2 &lt;- as.data.frame(describe_posterior(bmm_e2_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\ncdtr2 &lt;- get_coef_details(bmm_e2_train, \"conditVaried\")\n# mtr2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\n\n\n\nTable 5: Experiment 2 - End of training performance. The Intercept represents the average of the baseline condition (constant training), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n91.01\n80.67\n101.26\n1\n\n\nconditVaried\n36.15\n16.35\n55.67\n1\n\n\n\n\n\n\n\nTraining. Figure 9 presents the deviations across training blocks for both constant and varied training groups. We again compared training performance on the band common to both groups (600-800). The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, ( \\(\\beta\\) = 36.15, 95% CrI [16.35, 55.67]; pd = 99.95%).\n\nCodemodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e2_dist_Cond_Type_RF_2\")\nbmtd2 &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE2, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted2 &lt;- as.data.frame(describe_posterior(bmtd2, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\ncd2ted1 &lt;- get_coef_details(bmtd2, \"conditVaried\")\ncd2ted2 &lt;-get_coef_details(bmtd2, \"bandTypeExtrapolation\")\ncd2ted3 &lt;-get_coef_details(bmtd2, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\nTable 6: Experiment 2 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition (constant training & trained bands). Larger coefficients indicate larger deviations from the baselines - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\n \nTesting Accuracy. The analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 6, reveal no significant main effect of training condition (\\(\\beta\\) = -20.58, 95% CrI [-72.94, 33.08]; pd = 77.81%). However, the interaction between training condition and band type was significant (\\(\\beta\\) = 82, 95% CrI [41.89, 121.31]; pd = 100%), with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands (see Figure 10).\n\nCodecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\npe2td &lt;- testE2 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\npe2ce &lt;- bmtd2 |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\np2 &lt;- (pe2td + pe2ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\nFigure 10: Experiment 2 Testing Accuracy. A) Empricial Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\nCode##| label: tbl-e2-bmm-vx\n##| tbl-cap: \"Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne2_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e2_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e2_vxBMM ) |&gt; kable(escape=F,booktabs=T, caption=\"Fit to all 6 bands\")\n\ncd2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried\")\nsc2 &lt;- get_coef_details(e2_vxBMM, \"bandInt\")\nintCoef2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\nTable 7: Experiment 2 Testing Discrimination. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients for the Band term reflect a larger slope, or greater sensitivity/discrimination. The interaction between condit and Band indicates the difference between constant and varied slopes. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate)\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\nTesting Discrimination. Finally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. The full model results are shown in Table 7. The overall slope on target velocity band predictor was significantly positive, (\\(\\beta\\) = 0.71, 95% CrI [0.58, 0.84]; pd= 100%), indicating that participants exhibited discrimination between bands. The interaction between slope and condition was not significant, (\\(\\beta\\) = -0.06, 95% CrI [-0.24, 0.13]; pd= 72.67%), suggesting that the two conditions did not differ in their ability to discriminate between bands (see Figure 11 and Figure 12).\n\nCodetestE2 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\nFigure 11: Experiment 2. Empirical distribution of velocities produced in the testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\nCodecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n}\n\npe2vce &lt;- e2_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e2_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e2_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e2_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e2_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e2Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\npid_den2 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes2 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n      theme(legend.title=element_blank(), \n        axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe2vce + pid_den2 + pid_slopes2) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\n\n\n\n\nFigure 12: Experiment 2 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.\n\n\n\n\nExperiment 2 Summary\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied group did not show a significant difference in discrimination between bands.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#experiment-3",
    "href": "Sections/htw_full.html#experiment-3",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Experiment 3",
    "text": "Experiment 3\n\nCodee3 &lt;- readRDS(here(\"data/e3_08-04-23.rds\")) |&gt; \n    mutate(trainCon=case_when(\n    bandOrder==\"Original\" ~ \"800\",\n    bandOrder==\"Reverse\" ~ \"600\",\n    TRUE ~ NA_character_\n    ), trainCon=as.numeric(trainCon)) \ne3Sbjs &lt;- e3 |&gt; group_by(id,condit,bandOrder) |&gt; summarise(n=n())\ntestE3 &lt;- e3 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE3 &lt;-  e3 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit,bandOrder, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE3_max &lt;- trainE3 |&gt; filter(Trial_Bin == nbins, bandInt==trainCon)\n\n\nMethods & Procedure\nThe major adjustment of Experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the prior experiments. After each training throw, participants are informed whether a throw was too soft, too hard, or correct (i.e. within the target velocity range). All other aspects of the task and design are identical to Experiments 1 and 2. We utilized the order of training and testing bands from both of the prior experiments, thus assigning participants to both an order condition (Original or Reverse) and a training condition (Constant or Varied). Participants were once again recruited from the online Indiana University Introductory Psychology Course pool. Following exclusions, 195 participants were included in the final analysis, n=51 in the Constant-Original condition, n=59 in the Constant-Reverse condition, n=39 in the Varied-Original condition, and n=46 in the Varied-Reverse condition.\nResults\n\nCodebmm_e3_train &lt;- trainE3_max %&gt;% \n  brm(dist ~ condit*bandOrder, \n      file=here(\"data/model_cache/e3_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\n# mtr3 &lt;- as.data.frame(describe_posterior(bmm_e3_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mtr3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mtr3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\ncd3tr1 &lt;- get_coef_details(bmm_e3_train, \"conditVaried\")\ncd3tr2 &lt;-get_coef_details(bmm_e3_train, \"bandOrderReverse\")\ncd3tr3 &lt;-get_coef_details(bmm_e3_train, \"conditVaried:bandOrderReverse\")\n\n\n\n\nTable 8: Experiment 3 - End of training performance. The Intercept represents the average of the baseline condition (constant training & original band order), the conditVaried coefficient reflects the difference between the constant and varied groups, and the bandOrderReverse coefficient reflects the difference between original and reverse order. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group. The negative value for the interaction between condit and bandOrder indicates that varied condition with reverse order had significantly lower deviations than the varied condition with the original band order\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n121.86\n109.24\n134.60\n1.00\n\n\nconditVaried\n64.93\n36.99\n90.80\n1.00\n\n\nbandOrderReverse\n1.11\n-16.02\n18.16\n0.55\n\n\nconditVaried:bandOrderReverse\n-77.02\n-114.16\n-39.61\n1.00\n\n\n\n\n\n\nTraining. Figure 13 displays the average deviations from the target band across training blocks, and Table 8 shows the results of the Bayesian regression model predicting the deviation from the common band at the end of training (600-800 for reversed order, and 800-1000 for original order conditions). The main effect of training condition is significant, with the varied condition showing larger deviations ( \\(\\beta\\) = 64.93, 95% CrI [36.99, 90.8]; pd = 100%). The main effect of band order is not significant \\(\\beta\\) = 1.11, 95% CrI [-16.02, 18.16]; pd = 55.4%, however the interaction between training condition and band order is significant, with the varied condition showing greater accuracy in the reverse order condition ( \\(\\beta\\) = -77.02, 95% CrI [-114.16, -39.61]; pd = 100%).\n\nCodep1 &lt;- trainE3 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    ggh4x::facet_nested_wrap(~bandOrder*vb,ncol=3)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Absolute Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e3_train_deviation.png\"), p1, width = 9, height = 8,bg=\"white\")\np1\n\n\n\n\n\n\nFigure 13: Experiment 3 training. Deviations from target band during training. Shown separately for groups trained with the orginal order (used in E1) and reverse order (used in E2).\n\n\n\n\n\nCode#options(brms.backend=\"cmdstanr\",mc.cores=4)\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e3_dist_Cond_Type_RF_2\")\nbmtd3 &lt;- brm(dist ~ condit * bandType*bandOrder + (1|bandInt) + (1|id), \n    data=testE3, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted3 &lt;- as.data.frame(describe_posterior(bmtd3, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\n#ce_bmtd3 &lt;- plot(conditional_effects(bmtd3),points=FALSE,plot=FALSE)\n#wrap_plots(ce_bmtd3)\n\n#ggsave(here::here(\"Assets/figs\", \"e3_cond_effects_dist.png\"), wrap_plots(ce_bmtd3), width=11, height=11, bg=\"white\")\n\ncd3ted1 &lt;- get_coef_details(bmtd3, \"conditVaried\")\ncd3ted2 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation\")\ncd3ted3 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation\")\ncd3ted4 &lt;-get_coef_details(bmtd3, \"bandOrderReverse\")\ncd3ted5 &lt;-get_coef_details(bmtd3, \"conditVaried:bandOrderReverse\")\ncd3ted6 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation:bandOrderReverse\")\ncd3ted7 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation:bandOrderReverse\")\n\n\n\n\nTable 9: Experiment 3 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition, (constant training, trained bands & original order), and the remaining coefficients reflect the deviation from that baseline. Positive coefficients thus represent worse performance relative to the baseline, - and a positive interaction coefficient indicates disproportionate deviation for the varied condition or reverse order condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n288.65\n199.45\n374.07\n1.00\n\n\nconditVaried\n-40.19\n-104.68\n23.13\n0.89\n\n\nbandTypeExtrapolation\n-23.35\n-57.28\n10.35\n0.92\n\n\nbandOrderReverse\n-73.72\n-136.69\n-11.07\n0.99\n\n\nconditVaried:bandTypeExtrapolation\n52.66\n14.16\n90.23\n1.00\n\n\nconditVaried:bandOrderReverse\n-37.48\n-123.28\n49.37\n0.80\n\n\nbandTypeExtrapolation:bandOrderReverse\n80.69\n30.01\n130.93\n1.00\n\n\nconditVaried:bandTypeExtrapolation:bandOrder\n30.42\n-21.00\n81.65\n0.87\n\n\n\n\n\n\nTesting Accuracy. Table 9 presents the results of the Bayesian mixed efects model predicting absolute deviation from the target band during the testing stage. There was no significant main effect of training condition,\\(\\beta\\) = -40.19, 95% CrI [-104.68, 23.13]; pd = 89.31%, or band type,\\(\\beta\\) = -23.35, 95% CrI [-57.28, 10.35]; pd = 91.52%. However the effect of band order was significant, with the reverse order condition showing lower deviations, \\(\\beta\\) = -73.72, 95% CrI [-136.69, -11.07]; pd = 98.89%. The interaction between training condition and band type was also significant \\(\\beta\\) = 52.66, 95% CrI [14.16, 90.23]; pd = 99.59%, with the varied condition showing disproprionately large deviations on the extrapolation bands compared to the constant group. There was also a significant interaction between band type and band order, \\(\\beta\\) = 80.69, 95% CrI [30.01, 130.93]; pd = 99.89%, such that the reverse order condition showed larger deviations on the extrapolation bands. No other interactions were significant.\n\n\nCodecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3td &lt;- testE3 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n    facet_wrap(~bandOrder,ncol=1) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\npe3ce &lt;- bmtd3 |&gt; emmeans( ~condit *bandOrder*bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\") + \n facet_wrap(~bandOrder,ncol=1)\n\np2 &lt;- pe3td + pe3ce + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e3_test-dev.png\"), p2, width=9, height=8, bg=\"white\")\np2\n\n\n\n\n\n\nFigure 14: Experiment 3 Testing Accuracy. A) Empricial Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Shown separately for groups trained with the orginal order (used in E1) and reverse order (used in E2). Error bars represent 95% credible intervals.\n\n\n\n\n\nCode##| label: tbl-e3-bmm-vx\n##| tbl-cap: \"Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne3_vxBMM &lt;- brm(vx ~ condit * bandOrder * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e3_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n# m1 &lt;- as.data.frame(describe_posterior(e3_vxBMM, centrality = \"Mean\"))\n# m2 &lt;- fixef(e3_vxBMM)\n# mp3 &lt;- m1[, c(1,2,4,5,6)]\n# colnames(mp3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")                       \n# mp3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_replace_all(Term, \"b_bandInt\", \"Band\")) |&gt;\n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T)\n\n#wrap_plots(plot(conditional_effects(e3_vxBMM),points=FALSE,plot=FALSE))\n\ncd1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried:bandInt\")\nintCoef2 &lt;- get_coef_details(e3_vxBMM, \"bandOrderReverse:bandInt\")\ncoef3 &lt;- get_coef_details(e3_vxBMM,\"conditVaried:bandOrderReverse:bandInt\")\n\n\n\n\nTable 10: Experiment 3 testing discrimination. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band. The Intercept represents the baseline condition (constant training & original order), and the Band coefficient represents the slope for the baseline condition. The interaction terms which include condit and Band (e.g., conditVaried:Band & conditVaried:bandOrderReverse:band) respectively indicate the how the slopes of the varied-original condition differed from the baseline condition, and how varied-reverse condition differed from the varied-original condition\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nconditVaried:Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:band\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:band\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\nTesting Discrimination. The full results of the discrimination model are presented in Table 9. For the purposes of assessing group differences in discrimination, only the coefficients including the band variable are of interest. The baseline effect of band represents the slope cofficient for the constant training - original order condition, this effect was significant \\(\\beta\\) = 0.49, 95% CrI [0.36, 0.62]; pd = 100%. Neither of the two way interactions reached significance, \\(\\beta\\) = -0.04, 95% CrI [-0.23, 0.15]; pd = 66.63%, \\(\\beta\\) = -0.1, 95% CrI [-0.27, 0.08]; pd = 86.35%. However, the three way interaction between training condition, band order, and target band was significant, \\(\\beta\\) = 0.42, 95% CrI [0.17, 0.7]; pd = 99.96% - indicating a greater slope for the varied condition trained with reverse order bands. This interaction is shown in Figure 15, where the steepness of the best fitting line for the varied-reversed condition is noticably steeper than the other conditions.\n\nCode##| column: screen-inset-right\n# testE3 |&gt; filter(bandOrder==\"Original\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit()\n# testE3 |&gt; filter(bandOrder==\"Reverse\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit() +ggtitle(\"test\")\n\ntestE3 |&gt; group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + \n   ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\n\n\n\n\nFigure 15: e3 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\nCode# pe3tv &lt;- testE3 %&gt;% group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3vce &lt;- e3_vxBMM |&gt; emmeans( ~condit* bandOrder* bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  facet_wrap(~bandOrder,ncol=1) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e3_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e3_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction1 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandInt\"]\nfixed_effect_interaction2 &lt;- fixef(e3_vxBMM)[,1][\"bandOrderReverse:bandInt\"]\nfixed_effect_interaction3 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandOrderReverse:bandInt\"]\n\nre &lt;- data.frame(ranef(e3_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e3Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction1*(condit==\"Varied\") + \n           fixed_effect_interaction2*(bandOrder==\"Reverse\") + \n           fixed_effect_interaction3*(condit==\"Varied\" & bandOrder==\"Reverse\"),\n  slope = Estimate + adjust )\n\npid_den3 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\") +\n  facet_wrap(~bandOrder,ncol=1)\n\npid_slopes3 &lt;- re |&gt;  \n    mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n    geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n    theme(legend.title=element_blank(), \n      axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_nested_wrap(bandOrder~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe3vce + pid_den3 + pid_slopes3) + plot_annotation(tag_levels= 'A')\n\n#ggsave(here::here(\"Assets/figs\", \"e3_test-vx.png\"), p3,width=11,height=13, bg=\"white\",dpi=800)\np3\n\n\n\n\n\n\nFigure 16: Experiment 3 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.\n\n\n\n\nExperiment 3 Summary\nIn Experiment 3, we investigated the effects of training condition (constant vs. varied) and band type (training vs. extrapolation) on participants’ accuracy and discrimination during the testing phase. Unlike the previous experiments, participants received ordinal feedback during the training phase. Additionally, Experiment 3 included both the original order condition from Experiment 1 and the reverse order condition from Experiment 2. The results revealed no significant main effects of training condition on testing accuracy, nor was there a significant difference between groups in band discrimination. However, we observed a significant three-way interaction for the discrimination analysis, indicating that the varied condition showed a steeper slope coefficient on the reverse order bands compared to the constant condition. This result suggests that varied training enhanced participants’ ability to discriminate between velocity bands, but only when the band order was reversed during testing.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#computational-model",
    "href": "Sections/htw_full.html#computational-model",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Computational Model",
    "text": "Computational Model\n\nCode###| cache: true\ninvisible(list2env(load_sbj_data(), envir = .GlobalEnv))\ninvisible(list2env(load_e1(), envir = .GlobalEnv))\ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ne2_model &lt;- load_e2()\ne3_model &lt;- load_e3()\n#options(contrasts = initial_contrasts)\n\n\n\nCodealm_plot()\n\n\n\n\n\n\nFigure 17: The Associative Learning Model (ALM). The diagram illustrates the basic structure of the ALM model as used in the present work. Input nodes are activated as a function of their similarity to the lower-boundary of the target band. The generalization parameter, \\(c\\), determines the degree to which nearby input nodes are activated. The output nodes are activated as a function of the weighted sum of the input nodes - weights are updated via the delta rule.\n\n\n\n\nThe modeling goal is to implement a full process model capable of both 1) producing novel responses and 2) modeling behavior in both the learning and testing stages of the experiment. For this purpose, we will apply the associative learning model (ALM) and the EXAM model of function learning (DeLosh et al., 1997). ALM is a simple connectionist learning model which closely resembles Kruschke’s ALCOVE model (Kruschke, 1992), with modifications to allow for the generation of continuous responses.\nALM & Exam\nALM is a localist neural network model (Page, 2000), with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus ( a_i(X) = exp(-c(X - X_i)^2) ). So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on the value of the generalization parameter, the nearby units (e.g., 54 and 56; 53 and 57) may also activate to some degree. The units in the input layer activate as a function of their similarity to a presented stimulus. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nThe EXAM model is an extension of ALM, with the same learning rule and representational scheme for input and output units. EXAM differs from ALM only in its response rule, as it includes a linear extrapolation mechanism for generating novel responses. When a novel test stimulus, \\(X\\), is presented, EXAM first identifies the two nearest training stimuli, \\(X_1\\) and \\(X_2\\), that bracket \\(X\\). This is done based on the Gaussian activation of input nodes, similar to ALM, but focuses on identifying the closest known points for extrapolation.\nSlope Calculation: EXAM calculates a local slope, \\(S\\), using the responses associated with \\(X_1\\) and \\(X_2\\). This is computed as:\n\\[\n   S = \\frac{m(X_{1}) - m(X_{2})}{X_{1} - X_{2}}\n   \\]\nwhere \\(m(X_1)\\) and \\(m(X_2)\\) are the output values from ALM corresponding to the \\(X_1\\) and \\(X_2\\) inputs.\nResponse Generation: The response for the novel stimulus \\(X\\) is then extrapolated using the slope \\(S\\):\n\\[\n   E[Y|X] = m(X_1) + S \\cdot |X - X_1|\n   \\]\nHere, \\(m(X_1)\\) is the ALM response value from the training data for the stimulus closest to \\(X\\), and \\((X - X_1)\\) represents the distance between the novel stimulus and the nearest training stimulus.\nAlthough this extrapolation rule departs from a strictly similarity-based generalization mechanism, EXAM is distinct from pure rule-based models in that it remains constrained by the weights learned during training. EXAM retrieves the two nearest training inputs, and the ALM responses associated with those inputs, and computes the slope between these two points. The slope is then used to extrapolate the response to the novel test stimulus. Because EXAM requires at least two input-output pairs to generate a response, additional assumptions were required in order for it to generate resposnes for the constant group. We assumed that participants come to the task with prior knowledge of the origin point (0,0), which can serve as a reference point necessary for the model to generate responses for the constant group. This assumption is motivated by previous function learning research (Brown & Lacroix, 2017), which through a series of manipulations of the y intercept of the underlying function, found that participants consistently demonstrated knowledge of, or a bias towards, the origin point (see Kwantes & Neal (2006) for additional evidence of such a bias in function learning tasks).\nSee Table 11 for a full specification of the equations that define ALM and EXAM, and Figure 17 for a visual representation of the ALM model.\n\n\n\nTable 11: ALM & EXAM Equations\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).\n\n\n\nEXAM Extrapolation\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\n\nSlope Computation\n\n\\(S =\\) \\(\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\n\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nFinal EXAM response is the ALM response for the nearest training stimulus, \\(m(X_i)\\), adjusted by local slope \\(S\\).\n\n\n\n\n\n\n\nModel Fitting\nTo fit ALM and EXAM to our participant data, we employ a similar method to Mcdaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data with three separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase. Models were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate the parameters of ALM and EXAM, we used approximate bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g., many neural network models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individuals velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.\nModelling Results\n\nCodepost_tabs &lt;- abc_tables(post_dat,post_dat_l)\ntrain_tab &lt;- abc_train_tables(pd_train,pd_train_l)\n\n\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\n\n\ne1_tab &lt;- rbind(post_tabs$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Test\"), train_tab$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; mutate(Fit_Method=rename_fm(Fit_Method)) \n\nif (primary) {\ne1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = c(`Task Stage`)) %&gt;%\n  cols_label(\n    `Task Stage` = \"Task Stage\"\n  ) %&gt;%\n  fmt_number(\n    columns = starts_with(\"ALM\") | starts_with(\"EXAM\"),\n    decimals = 2\n  ) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"white\"),\n     locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = \"top\", color = \"black\", weight = px(1)),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n  )\n} else {\n\n  e1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;% kable(escape=F,booktabs=T)\n\n  }\n\n\nTable 12: Models errors predicting empirical data from Experiment 1 - aggregated over the full posterior distribution for each participant. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n\nTask Stage\n      Fit Method\n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nTest\nFit to Test Data\n199.93\n103.36\n104.01\n85.68\n\n\nTest\nFit to Test & Training Data\n216.97\n170.28\n127.94\n144.86\n\n\nTest\nFit to Training Data\n467.73\n291.38\n273.30\n297.91\n\n\nTrain\nFit to Test Data\n297.82\n2,016.01\n53.90\n184.00\n\n\nTrain\nFit to Test & Training Data\n57.40\n132.32\n42.92\n127.90\n\n\nTrain\nFit to Training Data\n51.77\n103.48\n51.43\n107.03\n\n\n\n\n\n\n\n\n\n\nCodec_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n\n\n\nFigure 18: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y-axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\n\nCodetrain_resid &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method, Block,x) |&gt; \n  summarise(y=mean(y), pred=mean(pred), mean_error=abs(y-pred)) |&gt;\n  group_by(id,condit,Model,Fit_Method,Block) |&gt;\n  summarise(mean_error=mean(mean_error)) |&gt;\n  ggplot(aes(x=interaction(Block,Model), y = mean_error, fill=factor(Block))) + \n  stat_bar + \n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, scales=\"free\",ncol=2) +\n   scale_x_discrete(guide = \"axis_nested\") +\n  scale_fill_manual(values=c(\"gray10\",\"gray50\",\"gray92\"))+\n  labs(title=\"Model Residual Errors - Training Stage\", y=\"RMSE\", x= \"Model\",fill=\"Training Block\") +\n  theme(legend.position=\"top\")\n\ntest_resid &lt;-  post_dat |&gt; \n   group_by(id,condit,x,Model,Fit_Method,bandType) |&gt;\n    summarise(y=mean(y), pred=mean(pred), error=abs(y-pred)) |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200))) |&gt;\n  ggplot(aes(x = Model, y = abs(error), fill=vbLab,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n  #scale_fill_manual(values=wes_palette(\"AsteroidCity2\"))+\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\") +\n  labs(title=\"Model Residual Errors - Testing Stage\",y=\"RMSE\", x=\"Velocity Band\") \n\n(train_resid / test_resid) +\n  #plot_layout(heights=c(1,1.5)) & \n  plot_annotation(tag_levels = list(c('A','B')),tag_suffix = ') ') \n\n\n\n\n\n\nFigure 19: Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y-axes are scaled differently between facets. A) Residuals predicting each block of the training data. B) Residuals predicting each band during the testing stage. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 18, and model predictions are shown alongside the empirical data in Figure 20. There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Aggregated residuals are displayed in Figure 19. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences in training performance between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 19) also show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that y axes are scaled different between plots). Although EXAM tends to perform better for both Constant and Varied participants (see also Figure 21), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods. The primary predictive difference between ALM and EXAM is made clear in Figure 20, which directly compares the observed data against the posterior predictive distributions for both models. Regardless of how the models are fit, only EXAM can capture the pattern where participants are able to discriminate all 6 target bands.\n\nCodepost_dat_l |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; \n #left_join(testAvgE1, by=join_by(id,condit,x==bandInt)) |&gt;\n ggplot(aes(x=Resp,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n    facet_wrap(~rename_fm(Fit_Method)+condit, ncol=2,strip.position = \"top\", scales = \"free_x\") +\n        scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n         strip.background = element_blank(),\n         strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(10,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions - Experiment 1 Data\", y=\"Vx\")\n\n\n\n\n\n\nFigure 20: Empirical data and Model predictions for mean velocity across target bands. Fitting methods (Test Only, Test & Train, Train Only) - are separated across rows, and Training Condition (Constant vs. Varied) are separated by columns. Each facet contains the predictions of ALM and EXAM, alongside the observed data.\n\n\n\n\n\nCode###| eval: false\n\npacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable,ggstance, htmltools,\n               ggdist,ggh4x,brms,tidybayes,emmeans,bayestestR, gt)\n\npdl &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n  filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\n# aerror is model error, which is predicted by Model(ALM vs. EXAM) & condit (Constant vs. Varied)\ne1_ee_brm_ae &lt;- brm(data=pdl,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e1_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbct_e1 &lt;- as.data.frame(bayestestR::describe_posterior(e1_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\n#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\n\np1 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n#p_ce_1 &lt;- (p1 + p2+ p3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n# plot_custom_effects &lt;- function(model) {\n#   # Extract posterior samples for fixed effects\n#   post_samples &lt;- posterior_samples(model, pars = c(\"b_Intercept\", \"b_ModelEXAM\", \"b_conditVaried\", \"b_ModelEXAM:conditVaried\"))\n  \n#   # Calculate conditional effects\n#   post_samples &lt;- post_samples %&gt;%\n#     mutate(\n#       ALM_Constant = b_Intercept,\n#       EXAM_Constant = b_Intercept + b_ModelEXAM,\n#       ALM_Varied = b_Intercept + b_conditVaried,\n#       EXAM_Varied = b_Intercept + b_ModelEXAM + b_conditVaried + `b_ModelEXAM:conditVaried`\n#     )\n  \n#   # Reshape data for plotting\n#   plot_data &lt;- post_samples %&gt;%\n#     select(ALM_Constant, EXAM_Constant, ALM_Varied, EXAM_Varied) %&gt;%\n#     pivot_longer(everything(), names_to = \"Condition\", values_to = \"Estimate\") %&gt;%\n#     separate(Condition, into = c(\"Model\", \"Condit\"), sep = \"_\")\n  \n#   # Plot conditional effects\n#   ggplot(plot_data, aes(x = Model, y = Estimate, color = Condit)) +\n#     geom_boxplot() +\n#     theme_minimal() +\n#     labs(x = \"Model\", y = \"Estimate\", color = \"Condition\")\n# }\n# p_ce_1 &lt;- plot_custom_effects(e1_ee_brm_ae)\n\n\n\n\nbm1 &lt;- get_coef_details(e1_ee_brm_ae, \"conditVaried\")\nbm2 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM\")\nbm3 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nposterior_estimates &lt;- as.data.frame(e1_ee_brm_ae) %&gt;%\n  select(starts_with(\"b_\")) %&gt;%\n  setNames(c(\"Intercept\", \"ModelEXAM\", \"conditVaried\", \"ModelEXAM_conditVaried\"))\n\nconstant_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM\nvaried_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM + posterior_estimates$conditVaried + posterior_estimates$ModelEXAM_conditVaried\ncomparison_EXAM &lt;- constant_EXAM - varied_EXAM\nsummary_EXAM &lt;- bayestestR::describe_posterior(comparison_EXAM, centrality = \"Mean\")\n\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NULL)\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NA)\n\n# full set of Model x condit contrasts\n# ALM - EXAM\n# btw_model &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model | condit, re_formula=NULL)  |&gt; \n#   pluck(\"contrasts\") |&gt; \n#   gather_emmeans_draws() |&gt; \n#   group_by(contrast,.draw,condit) |&gt; summarise(value=mean(.value), n=n()) \n\n# btw_model |&gt; ggplot(aes(x=value,y=contrast,fill=condit)) +stat_halfeye()\n\n# Constant - Varied\n# emm_condit &lt;- e1_ee_brm_ae |&gt; emmeans(~ condit | Model, re_formula = NULL)\n# btw_con &lt;- emm_condit |&gt;  pairs() |&gt; gather_emmeans_draws() |&gt; \n#   group_by(contrast,.draw, Model) |&gt; summarise(value=mean(.value), n=n()) \n# # btw_con |&gt; ggplot(aes(x=value,y=Model,fill=Model)) +stat_halfeye()                              \n\np_em_1 &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model*condit, re_formula=NA)  |&gt; \n  pluck(\"contrasts\") |&gt;\n  gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw) |&gt; summarise(value=mean(.value), n=n()) |&gt; \n  filter(!(contrast %in% c(\"ALM Constant - EXAM Constant\",\"ALM Constant - EXAM Varied\",\"ALM Varied - EXAM Varied \", \"EXAM Constant - ALM Varied\" ))) |&gt; \n  ggplot(aes(x=value,y=contrast,fill=contrast)) +stat_halfeye() + labs(x=\"Model Error Difference\",y=\"Contrast\") + theme(legend.position=\"none\") \n\n#p_ce_1 / p_em_1\n\n(p1 + p2+ p3) /p_em_1 + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\nFigure 21: A-C) Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied). Lower values on the y axis indicate better model fit. D) Specific contrasts of model performance comparing 1) EXAM fits between constant and varied training; 2) ALM vs. EXAM for the varied group; 3) ALM fits between constant and varied. Negative error differences indicate that the term on the left side (e.g., EXAM Constant) tended to have smaller model residuals.\n\n\n\n\nTo quantitatively assess whether the differences in performance between models, we fit a Bayesian regression predicting the errors of the posterior predictions of each models as a function of the Model (ALM vs. EXAM) and training condition (Constant vs. Varied).\nModel errors were significantly lower for EXAM (\\(\\beta\\) = -37.54, 95% CrI [-60.4, -14.17], pd = 99.85%) than ALM. There was also a significant interaction between Model and Condition (\\(\\beta\\) = 60.42, 95% CrI [36.17, 83.85], pd = 100%), indicating that the advantage of EXAM over ALM was significantly greater for the constant group. To assess whether EXAM predicts constant performance significantly better for Constant than for Varied subjects, we calculated the difference in model error between the Constant and Varied conditions specifically for EXAM. The results indicated that the model error for EXAM was significantly lower in the Constant condition compared to the Varied condition, with a mean difference of -22.88 (95% CrI [-46.02, -0.97], pd = 0.98).\n\nCodeout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\n\npost_tabs2 &lt;- abc_tables(e2_model$post_dat,e2_model$post_dat_l)\ntrain_tab2 &lt;- abc_train_tables(e2_model$pd_train,e2_model$pd_train_l)\n\npdl2 &lt;- e2_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne2_tab &lt;- rbind(post_tabs2$agg_pred_full |&gt;\n mutate(\"Task Stage\"=\"Test\"), train_tab2$agg_pred_full |&gt; \n mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\npost_tabs3 &lt;- abc_tables(e3_model$post_dat,e3_model$post_dat_l)\ntrain_tab3 &lt;- abc_train_tables(e3_model$pd_train,e3_model$pd_train_l)\n\npdl3 &lt;- e3_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne3_tab &lt;- rbind(post_tabs3$agg_pred_full |&gt; \n  mutate(\"Task Stage\"=\"Test\"), train_tab3$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\ne23_tab &lt;- rbind(e2_tab |&gt; mutate(Exp=\"E2\"), e3_tab |&gt; mutate(Exp=\"E3\")) \n\nif (primary) {\ngt_table &lt;- e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = `Task Stage`) %&gt;%\n  cols_label(`Task Stage` = \"Task Stage\") %&gt;%\n  fmt_number(columns = matches(\"E2|E3\"), decimals = 1) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n  )\ngt_table\n} else {\n  e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  kable(escape=F,booktabs=T)\n}\n\n\nTable 13: Models errors predicting empirical data - aggregated over all participants, posterior parameter values, and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n\n\n      \n        E2\n      \n      \n        E3\n      \n    \n\nTask Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nFit to Test Data\n    \n\nTest\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n    \n\nTest\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n    \n\nTest\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n\n\n\n\n\n   \n\nCoderbind(e2_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\"), \n e3_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb,bandOrder) |&gt;\n  summarize(vx=median(val)) |&gt; mutate(Exp=\"E3\")) |&gt;\n  ggplot( aes(x=condit,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) +\n  stat_bar + \n    facet_nested_wrap(~Exp+bandOrder+Resp, strip.position = \"top\", scales = \"free_x\") +\n    scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .7), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n        #  strip.background = element_blank(),\n        #  strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(20,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions Experiment 2 & 3\", y=\"vx\")\n\n\n\n\n\n\nFigure 22: Empirical data and Model predictions from Experiment 2 and 3 for the testing stage. Observed data is shown on the right. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\n\n\nCodee2_ee_brm_ae &lt;- brm(data=pdl2,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e2_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"conditVaried\")\nbm2_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM\")\nbm3_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nbct_e2 &lt;- as.data.frame(bayestestR::describe_posterior(e2_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) # %&gt;% kable(booktabs = TRUE)\n\ne3_ee_brm_ae &lt;- brm(data=pdl3,\n  aerror ~  Model * condit*bandOrder + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e3_ae_modelCondBo_RFint2.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e3 &lt;- get_coef_details(e3_ee_brm_ae, \"conditVaried\")\nbm2_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM\")\nbm3_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried\")\nbm4_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried:bandOrderReverse\")\n\n\nbct_e3  &lt;- as.data.frame(bayestestR::describe_posterior(e3_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\nbct &lt;- rbind(bct_e1 |&gt; mutate(exp=\"Exp 1\"),bct_e2 |&gt; \n               mutate(exp= \"Exp 2\"),bct_e3 |&gt; mutate(exp=\"Exp 3\")) |&gt; \n  relocate(exp, .before=Term)\n\n\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\nif (primary) {\nbct_table &lt;- bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;%\n  gt() %&gt;%\n  # tab_header(\n  #   title = \"Bayesian Model Results\",\n  #   subtitle = \"Estimates and Credible Intervals for Each Term Across Experiments\"\n  # ) %&gt;%\n  cols_label(\n    exp = \"Experiment\",\n    Term = \"Term\",\n    Estimate = \"Estimate\",\n    `95% CrI Lower` = \"95% CrI Lower\",\n    `95% CrI Upper` = \"95% CrI Upper\",\n    pd = \"pd\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Estimate, `95% CrI Lower`, `95% CrI Upper`),\n    decimals = 1\n  ) %&gt;%\n  fmt_number(\n    columns = pd,\n    decimals = 2\n  ) %&gt;%\n  tab_spanner(\n    label = \"Credible Interval\",\n    columns = c(`95% CrI Lower`, `95% CrI Upper`)\n  ) %&gt;%\n  tab_style(\n    style = list(\n      #cell_fill(color = \"lightgray\"),\n      cell_text(weight = \"bold\"), \n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(\n      columns = c(Estimate, pd),\n      rows = Term==\"ModelEXAM:conditVaried\"\n    )\n  ) %&gt;%\n   tab_row_group(\n    label = \"Experiment 3\",\n    rows = exp == \"Exp 3\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 2\",\n    rows = exp == \"Exp 2\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 1\",\n    rows = exp == \"Exp 1\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n    #row_group.background.color = \"gray95\"\n  )\nbct_table\n} else {\n  bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;% kable(booktabs = TRUE)\n\n}\n\n\nTable 14: Results of Bayesian Regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and the interaction between Model and Condition. The values represent the estimate coefficient for each term, with 95% credible intervals in brackets. The intercept reflects the baseline of ALM and Constant. The other estimates indicate deviations from the baseline for the EXAM mode and varied condition. Lower values indicate better model fit.\n\n\n\n\n\n\n\nExperiment\n      Term\n      Estimate\n      \n        Credible Interval\n      \n      pd\n    \n\n95% CrI Lower\n      95% CrI Upper\n    \n\n\n\nExperiment 1\n    \n\nExp 1\nIntercept\n176.3\n156.9\n194.6\n1.00\n\n\nExp 1\nModelEXAM\n−88.4\n−104.5\n−71.8\n1.00\n\n\nExp 1\nconditVaried\n−37.5\n−60.4\n−14.2\n1.00\n\n\nExp 1\nModelEXAM:conditVaried\n60.4\n36.2\n83.8\n1.00\n\n\nExperiment 2\n    \n\nExp 2\nIntercept\n245.9\n226.2\n264.5\n1.00\n\n\nExp 2\nModelEXAM\n−137.7\n−160.2\n−115.5\n1.00\n\n\nExp 2\nconditVaried\n−86.4\n−113.5\n−59.3\n1.00\n\n\nExp 2\nModelEXAM:conditVaried\n56.9\n25.3\n88.0\n1.00\n\n\nExperiment 3\n    \n\nExp 3\nIntercept\n164.8\n140.1\n189.4\n1.00\n\n\nExp 3\nModelEXAM\n−65.7\n−86.0\n−46.0\n1.00\n\n\nExp 3\nconditVaried\n−40.6\n−75.9\n−3.0\n0.98\n\n\nExp 3\nbandOrderReverse\n25.5\n−9.3\n58.7\n0.93\n\n\nExp 3\nModelEXAM:conditVaried\n41.9\n11.2\n72.5\n0.99\n\n\nExp 3\nModelEXAM:bandOrderReverse\n−7.3\n−34.5\n21.1\n0.70\n\n\nExp 3\nconditVaried:bandOrderReverse\n30.8\n−19.6\n83.6\n0.88\n\n\nExp 3\nModelEXAM:conditVaried:bandOrderReverse\n−60.6\n−101.8\n−18.7\n1.00\n\n\n\n\n\n\n\n\n\nModel Fits to Experiment 2 and 3. Data from Experiments 2 and 3 were fit to ALM and EXAM in the same manner as Experiment1 . For brevity, we only plot and discuss the results of the “fit to training and testing data” models - results from the other fitting methods can be found in the appendix. The model fitting results for Experiments 2 and 3 closely mirrored those observed in Experiment 1. The Bayesian regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and their interaction (see Table 14) revealed a consistent main effect of Model across all three experiments. The negative coefficients for the ModelEXAM term (Exp 2: \\(\\beta\\) = -86.39, 95% CrI -113.52, -59.31, pd = 100%; Exp 3: \\(\\beta\\) = -40.61, 95% CrI -75.9, -3.02, pd = 98.17%) indicate that EXAM outperformed ALM in both experiments. Furthermore, the interaction between Model and Condition was significant in both Experiment 2 (\\(\\beta\\) = 56.87, 95% CrI 25.26, 88.04, pd = 99.98%) and Experiment 3 (\\(\\beta\\) = 41.9, 95% CrI 11.2, 72.54, pd = 99.35%), suggesting that the superiority of EXAM over ALM was more pronounced for the Constant group compared to the Varied group, as was the case in Experiment 1. Recall that Experiment 3 included participants in both the original and reverse order conditions - and that this manipulation interacted with the effect of training condition. We thus also controlled for band order in our Bayesian Regression assessing the relative performance of EXAM and ALM in Experiment 3. There was a significant three way interaction between Model, Training Condition, and Band Order (\\(\\beta\\) = -60.6, 95% CrI -101.8, -18.66, pd = 99.83%), indicating that the relative advantage of EXAM over ALM was only more pronounced in the original order condition, and not the reverse order condition (see Figure 23).\n\nCode#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\np1 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\") + labs(title=\"E2. Model Error\")\np2 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n  p_e2 &lt;- (p1 + p2+ p3) \n# #wrap_plots(plot(conditional_effects(e3_ee_brm_ae),points=FALSE,plot=FALSE))\n\np_e3 &lt;- plot(conditional_effects(e3_ee_brm_ae, \n                         effects = \"Model:condit\", \n                         conditions=make_conditions(e3_ee_brm_ae,vars=c(\"bandOrder\"))),\n     points=FALSE,plot=FALSE)$`Model:condit` + \n     labs(x=\"Model\",y=\"Model Error\", title=\"E3. Model Error\", fill=NULL, col=NULL) + \n     theme(legend.position=\"right\") + \n     scale_color_manual(values=wes_palette(\"Darjeeling1\")) \n\np1 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n p2 &lt;- (p1 + p2+ p3)\n (p_e2 / p_e3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\nFigure 23: Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied) on Model Error for Experiment 2 and 3 data. Experiment 3 also includes a control for the order of training vs. testing bands (original order vs. reverse order).\n\n\n\n\nComputational Model Summary. Across all three experiments, the model fits consistently favored the Extrapolation-Association Model (EXAM) over the Associative Learning Model (ALM). This preference for EXAM was particularly pronounced for participants in the constant training conditions (note the positive coefficients on ModelEXAM:conditVaried interaction terms Table 14). This pattern is clearly illustrated in Figure 24, which plots the difference in model errors between ALM and EXAM for each individual participant. Both varied and constant conditions have a greater proportion of subjects better fit by EXAM (positive error differences), with the magnitude of EXAM’s advantage visibly larger for the constant group.\nThe superior performance of EXAM, especially for the constant training groups, may initially seem counterintuitive. One might assume that exposure to multiple, varied examples would be necessary to extract an abstract rule. However, EXAM is not a conventional rule-based model; it does not require the explicit abstraction of a rule. Instead, rule-based responses emerge during the retrieval process. The constant groups’ formation of a single, accurate input-output association, combined with the usefulness of the zero point, may have been sufficient for EXAM to capture their performance. A potential concern is that the assumption of participants utilizing the zero point essentially transforms the extrapolation problem into an interpolation problem. However, this concern is mitigated by the consistency of the results across both the original and reversed order conditions (the testing extrapolation bands fall in between the constant training band and the 0 point in experiment 1, but not in experiment 2).\nThe fits to the individual participants also reveal a number of interesting cases where the models struggle to capture the data (Figure 25). For example participant 68 exhibits a strong a strong non-monotonicity in the highest velocity band, a pattern which ALM can mimic, but which EXAM cannot capture, given it’s to enforce a simple linear relationship between target velocity and response. Participant 70 (lower right corner of Figure 25) had a roughly parabolic response pattern in their observed data, a pattern which neither model can properly reproduce, but which causes EXAM to perform particularly poorly.\nModeling Limitations. The present work compared models based on their ability to predict the observed data, without employing conventional model fit indices such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). These indices, which penalize models based on their number of free parameters, would have been of limited utility in this case, as both ALM and EXAM have two free parameters. However, despite having the same number of free parameters, EXAM could still be considered the more complex model, as it incorporates all the components of ALM plus an additional mechanism for rule-based responding. A more comprehensive model comparison approach might involve performing cross-validation with a held-out subset of the data (Mezzadri et al., 2022) or penalizing models based on the range of patterns they can produce (Dome & Wills, 2023).\n\n\nCodetid1 &lt;- post_dat  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-pred_dist, -dist) |&gt;\n  rbind(e2_model$post_dat |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat |&gt; mutate(Exp=\"E3\")) |&gt;\n  filter(Fit_Method==\"Test_Train\") |&gt;\n  group_by(id,condit,Model,Fit_Method,x, Exp) |&gt; \n    mutate(e2=abs(y-pred)) |&gt; \n    summarise(y1=median(y), pred1=median(pred),mean_error=abs(y1-pred1)) |&gt;\n    group_by(id,condit,Model,Fit_Method,Exp) |&gt; \n    summarise(mean_error=mean(mean_error)) |&gt; \n    arrange(id,condit,Fit_Method) |&gt;\n    round_tibble(1) \n\nbest_id &lt;- tid1 |&gt; \n  group_by(id,condit,Fit_Method) |&gt; \n  mutate(best=ifelse(mean_error==min(mean_error),1,0)) \n\nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method, Exp) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\nerror_difference&lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(mean_error)) %&gt;%\n  mutate(Error_difference = (ALM - EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; \n  left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; \n  mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\nfull_comparison |&gt; \n  filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n  geom_col() +\n  #ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  ggh4x::facet_nested_wrap(~condit+Exp,scales=\"free\") + \n  theme(axis.text.y = element_text(size=8)) +\n  labs(fill=\"Best Model\",\n  x=\"Mean Model Error Difference (ALM - EXAM)\",\n  y=\"Participant\")\n\n\n\n\n\n\nFigure 24: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\n\nCodecId_tr &lt;- c(137, 181, 11)\nvId_tr &lt;- c(14, 193, 47)\ncId_tt &lt;- c(11, 93, 35)\nvId_tt &lt;- c(1,14,74)\ncId_new &lt;- c(175, 68, 93, 74)\n# filter(id %in% (filter(bestTestEXAM,group_rank&lt;=9, Fit_Method==\"Test\")\n\ne1_sbjs &lt;- c(49,68,155, 175,74)\ne3_sbjs &lt;-  c(245, 280, 249)\ne2_sbjs &lt;- c(197, 157, 312, 334)\ncFinal &lt;- c(49, 128,202 )\nvFinal &lt;- c(68,70,245)\n\n\nindv_post_l &lt;- post_dat_l  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-signed_dist) |&gt;\n  rbind(e2_model$post_dat_l |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat_l |&gt; mutate(Exp=\"E3\") |&gt; select(-fb)) |&gt;\n  filter(Fit_Method==\"Test_Train\", id %in% c(cFinal,vFinal))\n\ntestIndv &lt;- indv_post_l |&gt; \n#filter(id %in% c(cId_tt,vId_tt,cId_new), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=vb, col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar_sd + \n  ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) + \n  labs(title=\"Individual Participant fits from Test & Train Fitting Method\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\ntestIndv \n\n\n\n\n\n\nFigure 25: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#alm-exam-description",
    "href": "Sections/htw_full.html#alm-exam-description",
    "title": "HTW Full",
    "section": "ALM & Exam Description",
    "text": "ALM & Exam Description\nALM is a localist neural network model (Page, 2000), with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on the value of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter. The EXAM model is an extension of ALM, with the same learning rule and representational scheme for input and output units. EXAM differs from ALM only in its response rule, as it includes a linear extrapolation mechanism for generating novel responses. Although this extrapolation rule departs from a strictly similarity-based generalization mechanism, EXAM is distinct from pure rule-based models in that it remains constrained by the weights learned during training. EXAM retrieves the two nearest training inputs, and the ALM responses associated with those inputs, and computes the slope between these two points. The slope is then used to extrapolate the response to the novel test stimulus. Because EXAM requires at least two input-output pairs to generate a response, additional assumptions were required in order for it to generate resposnes for the constant group. We assumed that participants come to the task with prior knowledge of the origin point (0,0), which can serve as a reference point necessary for the model to generate responses for the constant group. This assumption is motivated by previous function learning research (Brown & Lacroix (2017)), which through a series of manipulations of the y intercept of the underlying function, found that participants consistently demonstrated knowledge of, or a bias towards, the origin point (see Kwantes & Neal (2006) for additional evidence of such a bias in function learning tasks).\nSee Table 12 for a full specification of the equations that define ALM and EXAM, and Figure 15 for a visual representation of the ALM model.\n\n\n\nTable 12: ALM & EXAM Equations\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).\n\n\n\nEXAM Extrapolation\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\n\nSlope Computation\n\n\\(S =\\) \\(\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\n\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nALM response \\(m(X_i)\\) adjusted by slope.",
    "crumbs": [
      "Sections",
      "HTW Full"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#model-fitting",
    "href": "Sections/htw_full.html#model-fitting",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Model Fitting",
    "text": "Model Fitting\nTo fit ALM and EXAM to our participant data, we employ a similar method to McDaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data with three separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase. Models were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate Bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate the parameters of ALM and EXAM, we used approximate Bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g., many neural network models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until the desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individual’s velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#comparison-to-project-1",
    "href": "Sections/htw_full.html#comparison-to-project-1",
    "title": "HTW Full",
    "section": "Comparison to Project 1",
    "text": "Comparison to Project 1\nDifferences between the tasks\nThere are a number of differences between Project 1’s Hit The Target (HTT), and Project 2’s Hit The Wall (HTW) tasks.\n\nTask Space Complexity: In HTW, the task space is also almost perfectly smooth, at least for the continuous feedback subjects, if they throw 100 units too hard, they’ll be told that they were 100 units too hard. Whereas in HTT,  it was possible to produce xy velocity combinations that were technically closer to the empirical solution space than other throws, but which resulted in worse feedback due to striking the barrier.\nPerceptual Distinctiveness: HTT offers perceptually distinct varied conditions that directly relate to the task’s demands, which may increase the sallience between training positions encounted by the varied group. In contrast, HTW’s varied conditions differ only in the numerical values displayed, lacking the same level of perceptual differentiation. Conversely in HTW, the only difference between conditions for the varied group are the numbers displayed at the top of the screen which indicate the current target band(e.g. 800-1000, or 1000-1200)\nIn HTW, our primary testing stage of interest has no feedback, whereas in HTT testing always included feedback (the intermittent testing in HTT expt 1 being the only exception). Of course, we do collect testing with feedback data at the end of HTW, but we haven’t focused on that data at all in our modelling work thus far. It’s also interesting to recall that the gap between varied and constant in HTW does seem to close substantially in the testing-with-feedback stage. The difference between no-feedback and feedback testing might be relevant if the benefits of variation have anything to do with improving subsequent learning (as opposed to subsequent immediate performance), OR if the benefits of constant training rely on having the most useful anchor, having the most useful anchor might be a lot less helpful if you’re getting feedback from novel positions and can thus immediately begin to form position-specific anchors for the novelties, rather than relying on a training anchor. \nHTW and HTT both have a similar amount of training trials (~200), and thus the constant groups acquire a similar amount of experience with their single position/velocity in both experiments. However, the varied conditions in both HTT experiments train on 2 positions, whereas the varied group in HTW trains on 3 velocity bands. This means that in HTT the varied group gets half as much experience on any one position as the constant group, and in HTW they only get 1/3 as much experience in any one position. There are likely myriad ways in which this might impact the success of the varied group regardless of how you think the benefits of variation might be occurring, e.g. maybe they also need to develop a coherent anchor, maybe they need more experience in order to extract a function, or more experience in order to properly learn to tune their c parameter.",
    "crumbs": [
      "Sections",
      "HTW Full"
    ]
  },
  {
    "objectID": "Sections/Results.html#e1-summary",
    "href": "Sections/Results.html#e1-summary",
    "title": "Experiment 3",
    "section": "",
    "text": "In Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions rresulted in lower final training performance is consistent with much of the prior researchon the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "Sections",
      "Results"
    ]
  },
  {
    "objectID": "Sections/combo1.html",
    "href": "Sections/combo1.html",
    "title": "HTW Modeling",
    "section": "",
    "text": "Link to HTW project page\nWorking Draft of HTW Manuscript\nrepo"
  },
  {
    "objectID": "Sections/combo1.html#introduction",
    "href": "Sections/combo1.html#introduction",
    "title": "HTW Modeling",
    "section": "",
    "text": "Link to HTW project page\nWorking Draft of HTW Manuscript\nrepo"
  },
  {
    "objectID": "Sections/htw_full.html#experiment-1",
    "href": "Sections/htw_full.html#experiment-1",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Experiment 1",
    "text": "Experiment 1\nMethods\nParticipants A total of 156 participants were recruited from the Indiana University Introductory Psychology Course. Participants were randomly assigned to one of two training conditions: varied training or constant training.\nTask. The “Hit The Wall” (HTW) visuomotor extrapolation task task was programmed in Javascript, making heavy use of the phaser.io game library. The HTW task involved launching a projectile such that it would strike the “wall” at target speed indicated at the top of the screen (see Figure 2). The target velocities were given as a range, or band, of acceptable velocity values (e.g. band 800-1000). During the training stage, participants received feedback indicating whether they had hit the wall within the target velocity band, or how many units their throw was above or below from the target band. Participants were instructed that only the x velocity component of the ball was relevant to the task. The y velocity, or the location at which the ball struck the wall, had no influence on the task feedback.\n\n\n\n\n\n\n\n\nFigure 2: The Hit the wall task. Participants launch the blue ball to hit the red wall at the target velocity band indicated at the top of the screen. The ball must be released from within the orange square - but the location of release, and the location at which the ball strikes the wall are both irrelevant to the task feedback.\n\n\n\n\nProcedure. All participants completed the task online. Participants were provided with a description of the experiment and indicated informed consent. Figure 3 illustrates the general procedure. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). Participants in the constant training condition trained on only one velocity band (800-1000) - the closest band to what would be the novel extrapolation bands in the testing stage.\nFollowing the training stage, participants proceeded immediately to the testing stage. Participants were tested from all six velocity bands, in two separate stages. In the novel extrapolation testing stage, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials. Participants were also tested from the three velocity bands that were trained by the varied condition (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training. The order in which participants completed the novel-extrapolation and testing-from-3-varied bands was counterbalanced across participants. A final training stage presented participants with “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 800-10001000-12001200-1400Test1\nTest  Novel Bands 100-300350-550600-800data1-&gt;Test1\ndata2\n Constant Training 800-1000data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  100-300350-550600-800Test2\n  Test   Varied Training Bands  800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 3: Experiment 1 Design. Constant and Varied participants complete different training conditions.\n\n\n\n\n\nCode# pacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n#   brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n#   stringr, here,conflicted, patchwork, knitr)\n# #options(brms.backend=\"cmdstanr\",mc.cores=4)\n# options(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\n# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) \ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ntestE1 &lt;- e1 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE1 &lt;-  e1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE1_max &lt;- trainE1 |&gt; filter(Trial_Bin == nbins, bandInt==800)\ntrainE1_avg &lt;- trainE1_max |&gt; group_by(id,condit) |&gt; summarise(avg = mean(dist))\n\n\nAnalyses Strategy\nAll data processing and statistical analyses were performed in R version 4.32 (Team, 2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R (Bürkner, 2017), and descriptive stats and tables were extracted with the BayestestR package (Makowski et al., 2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as avoiding convergence issues common to the frequentist analogues of our mixed models.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 discarded as warmup chains. Rhat values were within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for for the random effects. For each model, we report 1) the mean values of the posterior distribution for the parameters of interest, 2) the lower and upper credible intervals (CrI), and the probability of direction value (pd).\n\n\nTable 1: Statistical Model Specifications. The specifications for the Bayesian regression models used in the analyses of each of the 3 experiments. Comparisons of accuracy use abosulte deviation as the dependent variable, while comparisons of discrimination use the raw velocities produced by participants as the dependent variable.\n\n\n\n\n\n\n\n\nGroup Comparison\nCode\nData\n\n\n\nEnd of Training Accuracy\nbrm(Abs. Deviation ~ condit)\nFinal Training Block\n\n\nTest Accuracy\nbrm(Abs. Deviation ~ condit * bandType + (1|id) + (1|bandInt)\nAll Testing trials\n\n\nBand Discrimination\nbrm(vx ~ condit * band +(1 + bandInt|id)\nAll Testing Trials\n\n\n\n\n\n\n\nIn each experiment we compare varied and constant conditions in terms of 1) accuracy in the final training block; 2) testing accuracy as a function of band type (trained vs. extrapolation bands); 3) extent of discrimination between all six testing bands. We quantified accuracy as the absolute deviation between the response velocity and the nearest boundary of the target band. Thus, when the target band was velocity 600-800, throws of 400, 650, and 900 would result in deviation values of 200, 0, and 100, respectively. The degree of discrimination between bands was index by fitting a linear model predicting the response velocity as a function of the target velocity. Participants who reliably discriminated between velocity bands tended to haves slope values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.\nResults\n\nCodep1 &lt;- trainE1 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e1_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\nFigure 4: Experiment 1 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\nCode##| label: tbl-e1-train-dist\n##| tbl-cap: \"Experiment 1 - Learning curves. \"\n##| output: asis\n\nbmm_e1_train&lt;- trainE1_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e1_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\nmtr1 &lt;- as.data.frame(describe_posterior(bmm_e1_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n# mtr1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#    kable(booktabs = TRUE)\n\ncdtr1 &lt;- get_coef_details(bmm_e1_train, \"conditVaried\")\n\n\n\n\nTable 2: Experiment 1 - End of training performance. Comparing final training block accuracy in band common to both groups. The Intercept represents the average of the baseline condition (constant training), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive estimates indicates a greater deviation (lower accuracy) for the varied group. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n106.34\n95.46\n117.25\n1\n\n\nconditVaried\n79.64\n57.92\n101.63\n1\n\n\n\n\n\n\n\nTraining. Figure 4 displays the average deviations across training blocks for the varied group, which trained on three velocity bands, and the constant group, which trained on one velocity band. To compare the training conditions at the end of training, we analyzed performance on the 800-1000 velocity band, which both groups trained on. The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, (\\(\\beta\\) = 79.64, 95% CrI [57.92, 101.63]; pd = 100%).\n\nCode##| label: tbl-e1-bmm-dist\n##| tbl-cap: \"E1. Training vs. Extrapolation\"\n#| \nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e1_dist_Cond_Type_RF_2\")\nbmtd &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE1, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted1 &lt;- as.data.frame(describe_posterior(bmtd, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n# \n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_id:bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n\n# mted1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE)\ncdted1 &lt;- get_coef_details(bmtd, \"conditVaried\")\ncdted2 &lt;-get_coef_details(bmtd, \"bandTypeExtrapolation\")\ncdted3 &lt;-get_coef_details(bmtd, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\nTable 3: Experiment 1 testing accuracy. Main effects of condition and band type (training vs. extrapolation bands), and the interaction between the two factors. The Intercept represents the baseline condition (constant training & trained bands). Larger coefficients indicate larger deviations from the baselines - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n152.55\n70.63\n229.85\n1.0\n\n\nconditVaried\n39.00\n-21.10\n100.81\n0.9\n\n\nbandTypeExtrapolation\n71.51\n33.24\n109.60\n1.0\n\n\nconditVaried:bandTypeExtrapolation\n66.46\n32.76\n99.36\n1.0\n\n\n\n\n\n\nTesting. To compare accuracy between groups in the testing stage, we fit a Bayesian mixed effects model predicting deviation from the target band as a function of training condition (varied vs. constant) and band type (trained vs. extrapolation), with random intercepts for participants and bands. The model results are shown in Table 3. The main effect of training condition was not significant (\\(\\beta\\) = 39, 95% CrI [-21.1, 100.81]; pd = 89.93%). The extrapolation testing items had a significantly greater deviation than the training bands (\\(\\beta\\) = 71.51, 95% CrI [33.24, 109.6]; pd = 99.99%). Most importantly, the interaction between training condition and band type was significant (\\(\\beta\\) = 66.46, 95% CrI [32.76, 99.36]; pd = 99.99%), As shown in Figure 5, the varied group had disproportionately larger deviations compared to the constant group in the extrapolation bands.\n\nCodepe1td &lt;- testE1 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target Band\")\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe1ce &lt;- bmtd |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Target Band\", x=\"Band Type\")\n\np2 &lt;- (pe1td + pe1ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e1_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\nFigure 5: Experiment 1 Testing Accuracy. A) Empricial Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\nCode##| label: tbl-e1-bmm-vx\n##| tbl-cap: \"Experiment 1. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\ne1_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e1_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e1_vxBMM) |&gt; kable(booktabs = TRUE)\n\ncd1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e1_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\n\nTable 4: Experiment 1 Testing Discrimination. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients for the Band term reflect a larger slope, or greater sensitivity/discrimination. The interaction between condit and Band indicates the difference between constant and varied slopes. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\nFinally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. See Table 4 for the full model results. The estimated coefficient for training condition (\\(\\beta\\) = 164.05, 95% CrI [45.5, 278.85], pd = 99.61%) suggests that the varied group tends to produce harder throws than the constant group, though is not in and of itself useful for assessing discrimination. Most relevant to the issue of discrimination is the coefficient on the Band predictor (\\(\\beta\\) = 0.71 95% CrI [0.62, 0.8], pd = 100%). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The significant negative estimate for the interaction between slope and condition (\\(\\beta\\) = -0.14, 95% CrI [-0.26, -0.01], pd = 98.39%), suggests that the discrimination was modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition (see Figure 6 and Figure 7).\n\nCodetestE1 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\nFigure 6: Experiment 1. Empirical distribution of velocities producing in testing stage. Translucent bands with dashed lines indicate the correct range for each velocity band.\n\n\n\n\n\nCodepe1vce &lt;- e1_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE1$vb), \n                     limits = c(0, 1400)) + \n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e1_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e1_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e1_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e1_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e1Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\n\npid_den1 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes1 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n     theme(legend.title=element_blank(), \n           axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\n\np3 &lt;- (pe1vce + pid_den1 + pid_slopes1) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e1_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\n\n\n\n\nFigure 7: Experiment 1 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.\n\n\n\n\nExperiment 1 Summary\nIn Experiment 1, we investigated how variability in training influenced participants’ ability learn and extrapolate in a visuomotor task. Our findings that training with variable conditions resulted in lower final training performance are consistent with much of the prior research on the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and is particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#analyses-strategy",
    "href": "Sections/htw_full.html#analyses-strategy",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Analyses Strategy",
    "text": "Analyses Strategy\nAll data processing and statistical analyses were performed in R version 4.32 (Team, 2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R (Bürkner, 2017), and descriptive stats and tables were extracted with the BayestestR package (Makowski et al., 2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as avoid convergence issues common to the frequentist analogues of our mixed models.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 discarded as warmup chains. Rhat values were within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for the random effects. For each model, we report 1) the mean values of the posterior distribution for the parameters of interest, 2) the lower and upper credible intervals (CrI), and the probability of direction value (pd).\n\n\nTable 1: Statistical Model Specifications. The specifications for the Bayesian regression models used in the analyses of each of the 3 experiments. Comparisons of accuracy use absolute deviation as the dependent variable, while comparisons of discrimination use the raw velocities produced by participants as the dependent variable.\n\n\n\n\n\n\n\n\nGroup Comparison\nCode\nData\n\n\n\nEnd of Training Accuracy\nbrm(Abs. Deviation ~ condit)\nFinal Training Block\n\n\nTest Accuracy\nbrm(Abs. Deviation ~ condit * bandType + (1|id) + (1|bandInt)\nAll Testing trials\n\n\nBand Discrimination\nbrm(vx ~ condit * band +(1 + bandInt|id)\nAll Testing Trials\n\n\n\n\n\n\n\nIn each experiment we compare varied and constant conditions in terms of 1) accuracy in the final training block; 2) testing accuracy as a function of band type (trained vs. extrapolation bands); 3) extent of discrimination between all six testing bands. We quantified accuracy as the absolute deviation between the response velocity and the nearest boundary of the target band. Thus, when the target band was velocity 600-800, throws of 400, 650, and 900 would result in deviation values of 200, 0, and 100, respectively. The degree of discrimination between bands was measured by fitting a linear model predicting the response velocity as a function of the target velocity. Participants who reliably discriminated between velocity bands tended to have slope values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#results",
    "href": "Sections/htw_full.html#results",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Results",
    "text": "Results\n\nDisplay codep1 &lt;- trainE1 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e1_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\nFigure 4: Experiment 1 - Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\nDisplay code##| label: tbl-e1-train-dist\n##| tbl-cap: \"Experiment 1 - Learning curves. \"\n##| output: asis\n\nbmm_e1_train&lt;- trainE1_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e1_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\nmtr1 &lt;- as.data.frame(describe_posterior(bmm_e1_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n# mtr1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#    kable(booktabs = TRUE)\n\ncdtr1 &lt;- get_coef_details(bmm_e1_train, \"conditVaried\")\n\n\n\n\nTable 2: Experiment 1 - End of training performance. Comparing final training block accuracy in the band common to both groups. The Intercept represents the average of the baseline condition (constant training), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive estimates indicates a greater deviation (lower accuracy) for the varied group. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n106.34\n95.46\n117.25\n1\n\n\nconditVaried\n79.64\n57.92\n101.63\n1\n\n\n\n\n\n\n\nTraining. Figure 4 displays the average deviations across training blocks for the varied group, which trained on three velocity bands, and the constant group, which trained on one velocity band. To compare the training conditions at the end of training, we analyzed performance on the 800-1000 velocity band, which both groups trained on. The full model results are shown in Table 1. The varied group had a significantly greater deviation from the target band than the constant group in the final training block, (\\(\\beta\\) = 79.64, 95% CrI [57.92, 101.63]; pd = 100%).\n\nDisplay code##| label: tbl-e1-bmm-dist\n##| tbl-cap: \"E1. Training vs. Extrapolation\"\n#| \nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e1_dist_Cond_Type_RF_2\")\nbmtd &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE1, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted1 &lt;- as.data.frame(describe_posterior(bmtd, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n# \n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_id:bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n\n# mted1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE)\ncdted1 &lt;- get_coef_details(bmtd, \"conditVaried\")\ncdted2 &lt;-get_coef_details(bmtd, \"bandTypeExtrapolation\")\ncdted3 &lt;-get_coef_details(bmtd, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\nTable 3: Experiment 1 testing accuracy. Main effects of condition and band type (training vs. extrapolation bands), and the interaction between the two factors. The Intercept represents the baseline condition (constant training & trained bands). Larger coefficients indicate larger deviations from the baselines - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n152.55\n70.63\n229.85\n1.0\n\n\nconditVaried\n39.00\n-21.10\n100.81\n0.9\n\n\nbandTypeExtrapolation\n71.51\n33.24\n109.60\n1.0\n\n\nconditVaried:bandTypeExtrapolation\n66.46\n32.76\n99.36\n1.0\n\n\n\n\n\n\nTesting. To compare accuracy between groups in the testing stage, we fit a Bayesian mixed effects model predicting deviation from the target band as a function of training condition (varied vs. constant) and band type (trained vs. extrapolation), with random intercepts for participants and bands. The model results are shown in Table 3. The main effect of training condition was not significant (\\(\\beta\\) = 39, 95% CrI [-21.1, 100.81]; pd = 89.93%). The extrapolation testing items had a significantly greater deviation than the training bands (\\(\\beta\\) = 71.51, 95% CrI [33.24, 109.6]; pd = 99.99%). Most importantly, the interaction between training condition and band type was significant (\\(\\beta\\) = 66.46, 95% CrI [32.76, 99.36]; pd = 99.99%), As shown in Figure 5, the varied group had disproportionately larger deviations compared to the constant group in the extrapolation bands.\n\nDisplay codepe1td &lt;- testE1 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target Band\")\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe1ce &lt;- bmtd |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Target Band\", x=\"Band Type\")\n\np2 &lt;- (pe1td + pe1ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e1_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\nFigure 5: Experiment 1 Testing Accuracy. A) Empirical Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\nDisplay code##| label: tbl-e1-bmm-vx\n##| tbl-cap: \"Experiment 1. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\ne1_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e1_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e1_vxBMM) |&gt; kable(booktabs = TRUE)\n\ncd1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e1_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\n\nTable 4: Experiment 1 Testing Discrimination. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients for the Band term reflect a larger slope, or greater sensitivity/discrimination. The interaction between condit and Band indicates the difference between constant and varied slopes. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\nFinally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. See Table 4 for the full model results. The estimated coefficient for training condition (\\(\\beta\\) = 164.05, 95% CrI [45.5, 278.85], pd = 99.61%) suggests that the varied group tends to produce harder throws than the constant group, though this is not, in and of itself, useful for assessing discrimination. Most relevant to the issue of discrimination is the coefficient on the Band predictor (\\(\\beta\\) = 0.71 95% CrI [0.62, 0.8], pd = 100%). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The significant negative estimate for the interaction between slope and condition (\\(\\beta\\) = -0.14, 95% CrI [-0.26, -0.01], pd = 98.39%), indicates that the discrimination was modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition (see Figure 6 and Figure 7).\n\nDisplay codetestE1 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\nFigure 6: Experiment 1. Empirical distribution of velocities produced in the testing stage. Translucent bands with dashed lines indicate the correct range for each velocity band.\n\n\n\n\n\nDisplay codepe1vce &lt;- e1_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE1$vb), \n                     limits = c(0, 1400)) + \n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e1_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e1_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e1_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e1_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e1Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\n\npid_den1 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes1 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n     theme(legend.title=element_blank(), \n           axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\n\np3 &lt;- (pe1vce + pid_den1 + pid_slopes1) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e1_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\n\n\n\n\nFigure 7: Experiment 1 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination between target bands. C) Individual participant slopes. Error bars represent 95% HDI.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#experiment-1-summary",
    "href": "Sections/htw_full.html#experiment-1-summary",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Experiment 1 Summary",
    "text": "Experiment 1 Summary\nIn Experiment 1, we investigated how variability in training influenced participants’ ability to learn and extrapolate in a visuomotor task. Our findings that training with variable conditions resulted in lower final training performance are consistent with much of the prior research on the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and are particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#methods-procedure",
    "href": "Sections/htw_full.html#methods-procedure",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Methods & Procedure",
    "text": "Methods & Procedure\nThe task and procedure of Experiment 2 was identical to Experiment 1, with the exception that the training and testing bands were reversed (see Figure 8). The Varied group trained on bands 100-300, 350-550, 600-800, and the constant group trained on band 600-800. Both groups were tested from all six bands. A total of 110 participants completed the experiment (Varied: 55, Constant: 55).\n\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 100-300350-550600-800Test1\nTest  Novel Bands  800-10001000-12001200-1400data1-&gt;Test1\ndata2\n Constant Training 600-800data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  800-10001000-12001200-1400Test2\n  Test   Varied Training Bands  100-300350-550600-800Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\nFigure 8: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#results-1",
    "href": "Sections/htw_full.html#results-1",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Results",
    "text": "Results\n\nDisplay codep1 &lt;- trainE2 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e2_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\nFigure 9: Experiment 2 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\nDisplay codebmm_e2_train &lt;- trainE2_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e2_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\nmtr2 &lt;- as.data.frame(describe_posterior(bmm_e2_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\ncdtr2 &lt;- get_coef_details(bmm_e2_train, \"conditVaried\")\n# mtr2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\n\n\n\nTable 5: Experiment 2 - End of training performance. The Intercept represents the average of the baseline condition (constant training), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n91.01\n80.67\n101.26\n1\n\n\nconditVaried\n36.15\n16.35\n55.67\n1\n\n\n\n\n\n\n\nTraining. Figure 9 presents the deviations across training blocks for both constant and varied training groups. We again compared training performance on the band common to both groups (600-800). The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, ( \\(\\beta\\) = 36.15, 95% CrI [16.35, 55.67]; pd = 99.95%).\n\nDisplay codemodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e2_dist_Cond_Type_RF_2\")\nbmtd2 &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE2, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted2 &lt;- as.data.frame(describe_posterior(bmtd2, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\ncd2ted1 &lt;- get_coef_details(bmtd2, \"conditVaried\")\ncd2ted2 &lt;-get_coef_details(bmtd2, \"bandTypeExtrapolation\")\ncd2ted3 &lt;-get_coef_details(bmtd2, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\nTable 6: Experiment 2 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition (constant training & trained bands). Larger coefficients indicate larger deviations from the baselines - and a positive interaction coefficient indicates disproportionate deviation for the varied condition on the extrapolation bands. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\n \nTesting Accuracy. The analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 6, reveal no significant main effect of training condition (\\(\\beta\\) = -20.58, 95% CrI [-72.94, 33.08]; pd = 77.81%). However, the interaction between training condition and band type was significant (\\(\\beta\\) = 82, 95% CrI [41.89, 121.31]; pd = 100%), with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands (see Figure 10).\n\nDisplay codecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\npe2td &lt;- testE2 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\npe2ce &lt;- bmtd2 |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\np2 &lt;- (pe2td + pe2ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\nFigure 10: Experiment 2 Testing Accuracy. A) Empirical Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\nDisplay code##| label: tbl-e2-bmm-vx\n##| tbl-cap: \"Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne2_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e2_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e2_vxBMM ) |&gt; kable(escape=F,booktabs=T, caption=\"Fit to all 6 bands\")\n\ncd2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried\")\nsc2 &lt;- get_coef_details(e2_vxBMM, \"bandInt\")\nintCoef2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\nTable 7: Experiment 2 Testing Discrimination. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients for the Band term reflect a larger slope, or greater sensitivity/discrimination. The interaction between condition and Band indicates the difference between constant and varied slopes. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate)\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\nTesting Discrimination. Finally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. The full model results are shown in Table 7. The overall slope on target velocity band predictor was significantly positive, (\\(\\beta\\) = 0.71, 95% CrI [0.58, 0.84]; pd= 100%), indicating that participants exhibited discrimination between bands. The interaction between slope and condition was not significant, (\\(\\beta\\) = -0.06, 95% CrI [-0.24, 0.13]; pd= 72.67%), suggesting that the two conditions did not differ in their ability to discriminate between bands (see Figure 11 and Figure 12).\n\nDisplay codetestE2 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\nFigure 11: Experiment 2. Empirical distribution of velocities produced in the testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\nDisplay codecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n}\n\npe2vce &lt;- e2_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e2_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e2_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e2_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e2_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e2Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\npid_den2 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes2 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n      theme(legend.title=element_blank(), \n        axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe2vce + pid_den2 + pid_slopes2) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\n\n\n\n\nFigure 12: Experiment 2 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#experiment-2-summary",
    "href": "Sections/htw_full.html#experiment-2-summary",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Experiment 2 Summary",
    "text": "Experiment 2 Summary\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied and Constant groups did not show a significant difference in their discrimination between bands.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#methods-procedure-1",
    "href": "Sections/htw_full.html#methods-procedure-1",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Methods & Procedure",
    "text": "Methods & Procedure\nThe major adjustment of Experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the prior experiments. After each training throw, participants are informed whether a throw was too soft, too hard, or correct (i.e. within the target velocity range). All other aspects of the task and design are identical to Experiments 1 and 2. We utilized the order of training and testing bands from both of the prior experiments, thus assigning participants to both an order condition (Original or Reverse) and a training condition (Constant or Varied). Participants were once again recruited from the online Indiana University Introductory Psychology Course pool. Following exclusions, 195 participants were included in the final analysis, n=51 in the Constant-Original condition, n=59 in the Constant-Reverse condition, n=39 in the Varied-Original condition, and n=46 in the Varied-Reverse condition.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#results-2",
    "href": "Sections/htw_full.html#results-2",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Results",
    "text": "Results\n\nDisplay codebmm_e3_train &lt;- trainE3_max %&gt;% \n  brm(dist ~ condit*bandOrder, \n      file=here(\"data/model_cache/e3_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\n# mtr3 &lt;- as.data.frame(describe_posterior(bmm_e3_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mtr3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mtr3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\ncd3tr1 &lt;- get_coef_details(bmm_e3_train, \"conditVaried\")\ncd3tr2 &lt;-get_coef_details(bmm_e3_train, \"bandOrderReverse\")\ncd3tr3 &lt;-get_coef_details(bmm_e3_train, \"conditVaried:bandOrderReverse\")\n\n\n\n\nTable 8: Experiment 3 - End of training performance. The Intercept represents the average of the baseline condition (constant training & original band order), the conditVaried coefficient reflects the difference between the constant and varied groups, and the bandOrderReverse coefficient reflects the difference between original and reverse order. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group. The negative value for the interaction between condit and bandOrder indicates that varied condition with reverse order had significantly lower deviations than the varied condition with the original band order\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n121.86\n109.24\n134.60\n1.00\n\n\nconditVaried\n64.93\n36.99\n90.80\n1.00\n\n\nbandOrderReverse\n1.11\n-16.02\n18.16\n0.55\n\n\nconditVaried:bandOrderReverse\n-77.02\n-114.16\n-39.61\n1.00\n\n\n\n\n\n\nTraining. Figure 13 displays the average deviations from the target band across training blocks, and Table 8 shows the results of the Bayesian regression model predicting the deviation from the common band at the end of training (600-800 for reversed order, and 800-1000 for original order conditions). The main effect of training condition is significant, with the varied condition showing larger deviations ( \\(\\beta\\) = 64.93, 95% CrI [36.99, 90.8]; pd = 100%). The main effect of band order is not significant \\(\\beta\\) = 1.11, 95% CrI [-16.02, 18.16]; pd = 55.4%, however the interaction between training condition and band order is significant, with the varied condition showing greater accuracy in the reverse order condition ( \\(\\beta\\) = -77.02, 95% CrI [-114.16, -39.61]; pd = 100%).\n\nDisplay codep1 &lt;- trainE3 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    ggh4x::facet_nested_wrap(~bandOrder*vb,ncol=3)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Absolute Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e3_train_deviation.png\"), p1, width = 9, height = 8,bg=\"white\")\np1\n\n\n\n\n\n\nFigure 13: Experiment 3 training. Deviations from target band during training, shown separately for groups trained with the original order (used in E1) and reverse order (used in E2).\n\n\n\n\n\nDisplay code#options(brms.backend=\"cmdstanr\",mc.cores=4)\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e3_dist_Cond_Type_RF_2\")\nbmtd3 &lt;- brm(dist ~ condit * bandType*bandOrder + (1|bandInt) + (1|id), \n    data=testE3, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted3 &lt;- as.data.frame(describe_posterior(bmtd3, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\n#ce_bmtd3 &lt;- plot(conditional_effects(bmtd3),points=FALSE,plot=FALSE)\n#wrap_plots(ce_bmtd3)\n\n#ggsave(here::here(\"Assets/figs\", \"e3_cond_effects_dist.png\"), wrap_plots(ce_bmtd3), width=11, height=11, bg=\"white\")\n\ncd3ted1 &lt;- get_coef_details(bmtd3, \"conditVaried\")\ncd3ted2 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation\")\ncd3ted3 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation\")\ncd3ted4 &lt;-get_coef_details(bmtd3, \"bandOrderReverse\")\ncd3ted5 &lt;-get_coef_details(bmtd3, \"conditVaried:bandOrderReverse\")\ncd3ted6 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation:bandOrderReverse\")\ncd3ted7 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation:bandOrderReverse\")\n\n\n\n\nTable 9: Experiment 3 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition, (constant training, trained bands & original order), and the remaining coefficients reflect the deviation from that baseline. Positive coefficients thus represent worse performance relative to the baseline, and a positive interaction coefficient indicates disproportionate deviation for the varied condition or reverse order condition.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n288.65\n199.45\n374.07\n1.00\n\n\nconditVaried\n-40.19\n-104.68\n23.13\n0.89\n\n\nbandTypeExtrapolation\n-23.35\n-57.28\n10.35\n0.92\n\n\nbandOrderReverse\n-73.72\n-136.69\n-11.07\n0.99\n\n\nconditVaried:bandTypeExtrapolation\n52.66\n14.16\n90.23\n1.00\n\n\nconditVaried:bandOrderReverse\n-37.48\n-123.28\n49.37\n0.80\n\n\nbandTypeExtrapolation:bandOrderReverse\n80.69\n30.01\n130.93\n1.00\n\n\nconditVaried:bandTypeExtrapolation:bandOrder\n30.42\n-21.00\n81.65\n0.87\n\n\n\n\n\n\nTesting Accuracy. Table 9 presents the results of the Bayesian mixed effects model predicting absolute deviation from the target band during the testing stage. There was no significant main effect of training condition,\\(\\beta\\) = -40.19, 95% CrI [-104.68, 23.13]; pd = 89.31%, or band type,\\(\\beta\\) = -23.35, 95% CrI [-57.28, 10.35]; pd = 91.52%. However the effect of band order was significant, with the reverse order condition showing lower deviations, \\(\\beta\\) = -73.72, 95% CrI [-136.69, -11.07]; pd = 98.89%. The interaction between training condition and band type was also significant \\(\\beta\\) = 52.66, 95% CrI [14.16, 90.23]; pd = 99.59%, with the varied condition showing disproprionately large deviations on the extrapolation bands compared to the constant group. There was also a significant interaction between band type and band order, \\(\\beta\\) = 80.69, 95% CrI [30.01, 130.93]; pd = 99.89%, such that the reverse order condition showed larger deviations on the extrapolation bands. No other interactions were significant.\n\n\nDisplay codecondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3td &lt;- testE3 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n    facet_wrap(~bandOrder,ncol=1) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\npe3ce &lt;- bmtd3 |&gt; emmeans( ~condit *bandOrder*bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\") + \n facet_wrap(~bandOrder,ncol=1)\n\np2 &lt;- pe3td + pe3ce + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e3_test-dev.png\"), p2, width=9, height=8, bg=\"white\")\np2\n\n\n\n\n\n\nFigure 14: Experiment 3 Testing Accuracy. A) Empirical Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Shown separately for groups trained with the original order (used in E1) and reverse order (used in E2). Error bars represent 95% credible intervals.\n\n\n\n\n\nDisplay code##| label: tbl-e3-bmm-vx\n##| tbl-cap: \"Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne3_vxBMM &lt;- brm(vx ~ condit * bandOrder * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e3_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n# m1 &lt;- as.data.frame(describe_posterior(e3_vxBMM, centrality = \"Mean\"))\n# m2 &lt;- fixef(e3_vxBMM)\n# mp3 &lt;- m1[, c(1,2,4,5,6)]\n# colnames(mp3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")                       \n# mp3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_replace_all(Term, \"b_bandInt\", \"Band\")) |&gt;\n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T)\n\n#wrap_plots(plot(conditional_effects(e3_vxBMM),points=FALSE,plot=FALSE))\n\ncd1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried:bandInt\")\nintCoef2 &lt;- get_coef_details(e3_vxBMM, \"bandOrderReverse:bandInt\")\ncoef3 &lt;- get_coef_details(e3_vxBMM,\"conditVaried:bandOrderReverse:bandInt\")\n\n\n\n\nTable 10: Experiment 3 testing discrimination. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band. The Intercept represents the baseline condition (constant training & original order), and the Band coefficient represents the slope for the baseline condition. The interaction terms which include condit and Band (e.g., conditVaried:Band & conditVaried:bandOrderReverse:band) respectively indicate how the slopes of the varied-original condition differed from the baseline condition, and how varied-reverse condition differed from the varied-original condition\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nconditVaried:Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:band\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:band\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\nTesting Discrimination. The full results of the discrimination model are presented in Table 9. For the purposes of assessing group differences in discrimination, only the coefficients including the band variable are of interest. The baseline effect of band represents the slope coefficient for the constant training - original order condition, this effect was significant \\(\\beta\\) = 0.49, 95% CrI [0.36, 0.62]; pd = 100%. Neither of the two way interactions reached significance, \\(\\beta\\) = -0.04, 95% CrI [-0.23, 0.15]; pd = 66.63%, \\(\\beta\\) = -0.1, 95% CrI [-0.27, 0.08]; pd = 86.35%. However, the three way interaction between training condition, band order, and target band was significant, \\(\\beta\\) = 0.42, 95% CrI [0.17, 0.7]; pd = 99.96% - indicating a greater slope for the varied condition trained with reverse order bands. This interaction is shown in Figure 15, where the steepness of the best fitting line for the varied-reversed condition is noticeably steeper than the other conditions.\n\nDisplay code##| column: screen-inset-right\n# testE3 |&gt; filter(bandOrder==\"Original\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit()\n# testE3 |&gt; filter(bandOrder==\"Reverse\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit() +ggtitle(\"test\")\n\ntestE3 |&gt; group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + \n   ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\n\n\n\n\nFigure 15: Experiment 3. Empirical distribution of velocities produced in the testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\nDisplay code# pe3tv &lt;- testE3 %&gt;% group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3vce &lt;- e3_vxBMM |&gt; emmeans( ~condit* bandOrder* bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  facet_wrap(~bandOrder,ncol=1) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e3_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e3_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction1 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandInt\"]\nfixed_effect_interaction2 &lt;- fixef(e3_vxBMM)[,1][\"bandOrderReverse:bandInt\"]\nfixed_effect_interaction3 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandOrderReverse:bandInt\"]\n\nre &lt;- data.frame(ranef(e3_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e3Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction1*(condit==\"Varied\") + \n           fixed_effect_interaction2*(bandOrder==\"Reverse\") + \n           fixed_effect_interaction3*(condit==\"Varied\" & bandOrder==\"Reverse\"),\n  slope = Estimate + adjust )\n\npid_den3 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\") +\n  facet_wrap(~bandOrder,ncol=1)\n\npid_slopes3 &lt;- re |&gt;  \n    mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n    geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n    theme(legend.title=element_blank(), \n      axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_nested_wrap(bandOrder~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe3vce + pid_den3 + pid_slopes3) + plot_annotation(tag_levels= 'A')\n\n#ggsave(here::here(\"Assets/figs\", \"e3_test-vx.png\"), p3,width=11,height=13, bg=\"white\",dpi=800)\np3\n\n\n\n\n\n\nFigure 16: Experiment 3 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#experiment-3-summary",
    "href": "Sections/htw_full.html#experiment-3-summary",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Experiment 3 Summary",
    "text": "Experiment 3 Summary\nIn Experiment 3, we investigated the effects of training condition (constant vs. varied) and band type (training vs. extrapolation) on participants’ accuracy and discrimination during the testing phase. Unlike the previous experiments, participants received only ordinal, not continuous valued, feedback during the training phase. Additionally, Experiment 3 included both the original order condition from Experiment 1 and the reverse order condition from Experiment 2. The results revealed no significant main effects of training condition on testing accuracy, nor was there a significant difference between groups in band discrimination. However, we observed a significant three-way interaction for the discrimination analysis, indicating that the varied condition showed a steeper slope coefficient on the reverse order bands compared to the constant condition. This result suggests that varied training enhanced participants’ ability to discriminate between velocity bands, but only when the band order was reversed during testing.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#alm-exam",
    "href": "Sections/htw_full.html#alm-exam",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "ALM & Exam",
    "text": "ALM & Exam\nALM is a localist neural network model (Page, 2000), with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus ( a_i(X) = exp(-c(X - X_i)^2) ). So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on the value of the generalization parameter, the nearby units (e.g., 54 and 56; 53 and 57) may also activate to some degree. The units in the input layer activate as a function of their similarity to a presented stimulus. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nThe EXAM model is an extension of ALM, with the same learning rule and representational scheme for input and output units. EXAM differs from ALM only in its response rule, as it includes a linear extrapolation mechanism for generating novel responses. When a novel test stimulus, \\(X\\), is presented, EXAM first identifies the two nearest training stimuli, \\(X_1\\) and \\(X_2\\), that bracket \\(X\\). This is done based on the Gaussian activation of input nodes, similar to ALM, but focuses on identifying the closest known points for extrapolation.\nSlope Calculation: EXAM calculates a local slope, \\(S\\), using the responses associated with \\(X_1\\) and \\(X_2\\). This is computed as:\n\\[\n   S = \\frac{m(X_{1}) - m(X_{2})}{X_{1} - X_{2}}\n   \\]\nwhere \\(m(X_1)\\) and \\(m(X_2)\\) are the output values from ALM corresponding to the \\(X_1\\) and \\(X_2\\) inputs.\nResponse Generation: The response for the novel stimulus \\(X\\) is then extrapolated using the slope \\(S\\):\n\\[\n   E[Y|X] = m(X_1) + S \\cdot |X - X_1|\n   \\]\nHere, \\(m(X_1)\\) is the ALM response value from the training data for the stimulus closest to \\(X\\), and \\((X - X_1)\\) represents the distance between the novel stimulus and the nearest training stimulus.\nAlthough this extrapolation rule departs from a strictly similarity-based generalization mechanism, EXAM is distinct from pure rule-based models in that it remains constrained by the weights learned during training. EXAM retrieves the two nearest training inputs, and the ALM responses associated with those inputs, and computes the slope between these two points. The slope is then used to extrapolate the response to the novel test stimulus. Because EXAM requires at least two input-output pairs to generate a response, additional assumptions were required in order for it to generate resposnes for the constant group. We assumed that participants come to the task with prior knowledge of the origin point (0,0), which can serve as a reference point necessary for the model to generate responses for the constant group. This assumption is motivated by previous function learning research (Brown & Lacroix, 2017), which through a series of manipulations of the y intercept of the underlying function, found that participants consistently demonstrated knowledge of, or a bias towards, the origin point (see Kwantes & Neal (2006) for additional evidence of such a bias in function learning tasks).\nSee Table 11 for a full specification of the equations that define ALM and EXAM, and Figure 17 for a visual representation of the ALM model.\n\n\n\nTable 11: ALM & EXAM Equations\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).\n\n\n\nEXAM Extrapolation\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\n\nSlope Computation\n\n\\(S =\\) \\(\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\n\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nFinal EXAM response is the ALM response for the nearest training stimulus, \\(m(X_i)\\), adjusted by local slope \\(S\\).",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#modelling-results",
    "href": "Sections/htw_full.html#modelling-results",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Modelling Results",
    "text": "Modelling Results\n\nDisplay codepost_tabs &lt;- abc_tables(post_dat,post_dat_l)\ntrain_tab &lt;- abc_train_tables(pd_train,pd_train_l)\n\n\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\n\n\ne1_tab &lt;- rbind(post_tabs$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Test\"), train_tab$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; mutate(Fit_Method=rename_fm(Fit_Method)) \n\nif (primary) {\ne1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = c(`Task Stage`)) %&gt;%\n  cols_label(\n    `Task Stage` = \"Task Stage\"\n  ) %&gt;%\n  fmt_number(\n    columns = starts_with(\"ALM\") | starts_with(\"EXAM\"),\n    decimals = 2\n  ) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"white\"),\n     locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = \"top\", color = \"black\", weight = px(1)),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n  )\n} else {\n\n  e1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;% kable(escape=F,booktabs=T)\n\n  }\n\n\nTable 12: Model errors predicting empirical data from Experiment 1 - aggregated over the full posterior distribution for each participant. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n\nTask Stage\n      Fit Method\n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nTest\nFit to Test Data\n199.93\n103.36\n104.01\n85.68\n\n\nTest\nFit to Test & Training Data\n216.97\n170.28\n127.94\n144.86\n\n\nTest\nFit to Training Data\n467.73\n291.38\n273.30\n297.91\n\n\nTrain\nFit to Test Data\n297.82\n2,016.01\n53.90\n184.00\n\n\nTrain\nFit to Test & Training Data\n57.40\n132.32\n42.92\n127.90\n\n\nTrain\nFit to Training Data\n51.77\n103.48\n51.43\n107.03\n\n\n\n\n\n\n\n\n\n\nDisplay codec_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n\n\n\nFigure 18: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y-axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\n\nDisplay codetrain_resid &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method, Block,x) |&gt; \n  summarise(y=mean(y), pred=mean(pred), mean_error=abs(y-pred)) |&gt;\n  group_by(id,condit,Model,Fit_Method,Block) |&gt;\n  summarise(mean_error=mean(mean_error)) |&gt;\n  ggplot(aes(x=interaction(Block,Model), y = mean_error, fill=factor(Block))) + \n  stat_bar + \n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, scales=\"free\",ncol=2) +\n   scale_x_discrete(guide = \"axis_nested\") +\n  scale_fill_manual(values=c(\"gray10\",\"gray50\",\"gray92\"))+\n  labs(title=\"Model Residual Errors - Training Stage\", y=\"RMSE\", x= \"Model\",fill=\"Training Block\") +\n  theme(legend.position=\"top\")\n\ntest_resid &lt;-  post_dat |&gt; \n   group_by(id,condit,x,Model,Fit_Method,bandType) |&gt;\n    summarise(y=mean(y), pred=mean(pred), error=abs(y-pred)) |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200))) |&gt;\n  ggplot(aes(x = Model, y = abs(error), fill=vbLab,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n  #scale_fill_manual(values=wes_palette(\"AsteroidCity2\"))+\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\") +\n  labs(title=\"Model Residual Errors - Testing Stage\",y=\"RMSE\", x=\"Velocity Band\") \n\n(train_resid / test_resid) +\n  #plot_layout(heights=c(1,1.5)) & \n  plot_annotation(tag_levels = list(c('A','B')),tag_suffix = ') ') \n\n\n\n\n\n\nFigure 19: Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y-axes are scaled differently between facets. A) Residuals predicting each block of the training data. B) Residuals predicting each band during the testing stage. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 18, and model predictions are shown alongside the empirical data in Figure 20. There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Aggregated residuals are displayed in Figure 19. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences in training performance between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 19) show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that Y-axes are scaled different between plots). Although EXAM tends to perform better for both Constant and Varied participants (see also Figure 21), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods. The primary predictive difference between ALM and EXAM is made clear in Figure 20, which directly compares the observed data against the posterior predictive distributions for both models. Regardless of how the models are fit, only EXAM can capture the pattern where participants are able to discriminate all 6 target bands.\n\nDisplay codepost_dat_l |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; \n #left_join(testAvgE1, by=join_by(id,condit,x==bandInt)) |&gt;\n ggplot(aes(x=Resp,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n    facet_wrap(~rename_fm(Fit_Method)+condit, ncol=2,strip.position = \"top\", scales = \"free_x\") +\n        scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n         strip.background = element_blank(),\n         strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(10,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions - Experiment 1 Data\", y=\"Vx\")\n\n\n\n\n\n\nFigure 20: Empirical data and Model predictions for mean velocity across target bands. Fitting methods (Test Only, Test & Train, Train Only) - are separated across rows, and Training Condition (Constant vs. Varied) are separated by columns. Each facet contains the predictions of ALM and EXAM, alongside the observed data.\n\n\n\n\n\nDisplay code###| eval: false\n\npacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable,ggstance, htmltools,\n               ggdist,ggh4x,brms,tidybayes,emmeans,bayestestR, gt)\n\npdl &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n  filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\n# aerror is model error, which is predicted by Model(ALM vs. EXAM) & condit (Constant vs. Varied)\ne1_ee_brm_ae &lt;- brm(data=pdl,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e1_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbct_e1 &lt;- as.data.frame(bayestestR::describe_posterior(e1_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\n#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\n\np1 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n#p_ce_1 &lt;- (p1 + p2+ p3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n# plot_custom_effects &lt;- function(model) {\n#   # Extract posterior samples for fixed effects\n#   post_samples &lt;- posterior_samples(model, pars = c(\"b_Intercept\", \"b_ModelEXAM\", \"b_conditVaried\", \"b_ModelEXAM:conditVaried\"))\n  \n#   # Calculate conditional effects\n#   post_samples &lt;- post_samples %&gt;%\n#     mutate(\n#       ALM_Constant = b_Intercept,\n#       EXAM_Constant = b_Intercept + b_ModelEXAM,\n#       ALM_Varied = b_Intercept + b_conditVaried,\n#       EXAM_Varied = b_Intercept + b_ModelEXAM + b_conditVaried + `b_ModelEXAM:conditVaried`\n#     )\n  \n#   # Reshape data for plotting\n#   plot_data &lt;- post_samples %&gt;%\n#     select(ALM_Constant, EXAM_Constant, ALM_Varied, EXAM_Varied) %&gt;%\n#     pivot_longer(everything(), names_to = \"Condition\", values_to = \"Estimate\") %&gt;%\n#     separate(Condition, into = c(\"Model\", \"Condit\"), sep = \"_\")\n  \n#   # Plot conditional effects\n#   ggplot(plot_data, aes(x = Model, y = Estimate, color = Condit)) +\n#     geom_boxplot() +\n#     theme_minimal() +\n#     labs(x = \"Model\", y = \"Estimate\", color = \"Condition\")\n# }\n# p_ce_1 &lt;- plot_custom_effects(e1_ee_brm_ae)\n\n\n\n\nbm1 &lt;- get_coef_details(e1_ee_brm_ae, \"conditVaried\")\nbm2 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM\")\nbm3 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nposterior_estimates &lt;- as.data.frame(e1_ee_brm_ae) %&gt;%\n  select(starts_with(\"b_\")) %&gt;%\n  setNames(c(\"Intercept\", \"ModelEXAM\", \"conditVaried\", \"ModelEXAM_conditVaried\"))\n\nconstant_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM\nvaried_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM + posterior_estimates$conditVaried + posterior_estimates$ModelEXAM_conditVaried\ncomparison_EXAM &lt;- constant_EXAM - varied_EXAM\nsummary_EXAM &lt;- bayestestR::describe_posterior(comparison_EXAM, centrality = \"Mean\")\n\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NULL)\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NA)\n\n# full set of Model x condit contrasts\n# ALM - EXAM\n# btw_model &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model | condit, re_formula=NULL)  |&gt; \n#   pluck(\"contrasts\") |&gt; \n#   gather_emmeans_draws() |&gt; \n#   group_by(contrast,.draw,condit) |&gt; summarise(value=mean(.value), n=n()) \n\n# btw_model |&gt; ggplot(aes(x=value,y=contrast,fill=condit)) +stat_halfeye()\n\n# Constant - Varied\n# emm_condit &lt;- e1_ee_brm_ae |&gt; emmeans(~ condit | Model, re_formula = NULL)\n# btw_con &lt;- emm_condit |&gt;  pairs() |&gt; gather_emmeans_draws() |&gt; \n#   group_by(contrast,.draw, Model) |&gt; summarise(value=mean(.value), n=n()) \n# # btw_con |&gt; ggplot(aes(x=value,y=Model,fill=Model)) +stat_halfeye()                              \n\np_em_1 &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model*condit, re_formula=NA)  |&gt; \n  pluck(\"contrasts\") |&gt;\n  gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw) |&gt; summarise(value=mean(.value), n=n()) |&gt; \n  filter(!(contrast %in% c(\"ALM Constant - EXAM Constant\",\"ALM Constant - EXAM Varied\",\"ALM Varied - EXAM Varied \", \"EXAM Constant - ALM Varied\" ))) |&gt; \n  ggplot(aes(x=value,y=contrast,fill=contrast)) +stat_halfeye() + labs(x=\"Model Error Difference\",y=\"Contrast\") + theme(legend.position=\"none\") \n\n#p_ce_1 / p_em_1\n\n(p1 + p2+ p3) /p_em_1 + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\nFigure 21: A-C) Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied). Lower values on the y axis indicate better model fit. D) Specific contrasts of model performance comparing 1) EXAM fits between constant and varied training; 2) ALM vs. EXAM for the varied group; 3) ALM fits between constant and varied. Negative error differences indicate that the term on the left side (e.g., EXAM Constant) tended to have smaller model residuals.\n\n\n\n\nTo quantitatively assess the differences in performance between models, we fit a Bayesian regression model predicting the errors of the posterior predictions of each models as a function of the Model (ALM vs. EXAM) and training condition (Constant vs. Varied).\nModel errors were significantly lower for EXAM (\\(\\beta\\) = -37.54, 95% CrI [-60.4, -14.17], pd = 99.85%) than ALM. There was also a significant interaction between Model and Condition (\\(\\beta\\) = 60.42, 95% CrI [36.17, 83.85], pd = 100%), indicating that the advantage of EXAM over ALM was significantly greater for the constant group. To assess whether EXAM predicts performance significantly better for Constant than for Varied subjects, we calculated the difference in model error between the Constant and Varied conditions specifically for EXAM. The results indicated that the model error for EXAM was significantly lower in the Constant condition compared to the Varied condition, with a mean difference of -22.88 (95% CrI [-46.02, -0.97], pd = 0.98).\n\nDisplay codeout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\n\npost_tabs2 &lt;- abc_tables(e2_model$post_dat,e2_model$post_dat_l)\ntrain_tab2 &lt;- abc_train_tables(e2_model$pd_train,e2_model$pd_train_l)\n\npdl2 &lt;- e2_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne2_tab &lt;- rbind(post_tabs2$agg_pred_full |&gt;\n mutate(\"Task Stage\"=\"Test\"), train_tab2$agg_pred_full |&gt; \n mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\npost_tabs3 &lt;- abc_tables(e3_model$post_dat,e3_model$post_dat_l)\ntrain_tab3 &lt;- abc_train_tables(e3_model$pd_train,e3_model$pd_train_l)\n\npdl3 &lt;- e3_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne3_tab &lt;- rbind(post_tabs3$agg_pred_full |&gt; \n  mutate(\"Task Stage\"=\"Test\"), train_tab3$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\ne23_tab &lt;- rbind(e2_tab |&gt; mutate(Exp=\"E2\"), e3_tab |&gt; mutate(Exp=\"E3\")) \n\nif (primary) {\ngt_table &lt;- e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = `Task Stage`) %&gt;%\n  cols_label(`Task Stage` = \"Task Stage\") %&gt;%\n  fmt_number(columns = matches(\"E2|E3\"), decimals = 1) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n  )\ngt_table\n} else {\n  e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  kable(escape=F,booktabs=T)\n}\n\n\nTable 13: Models errors predicting empirical data - aggregated over all participants, posterior parameter values, and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n\n\n      \n        E2\n      \n      \n        E3\n      \n    \n\nTask Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n\nConstant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n\n\n\nFit to Test Data\n    \n\nTest\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n\n\nTrain\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n\n\nFit to Test & Training Data\n    \n\nTest\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n\n\nTrain\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n\n\nFit to Training Data\n    \n\nTest\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n\n\nTrain\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n\n\n\n\n\n\n\n\n\n   \n\nDisplay coderbind(e2_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\"), \n e3_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb,bandOrder) |&gt;\n  summarize(vx=median(val)) |&gt; mutate(Exp=\"E3\")) |&gt;\n  ggplot( aes(x=condit,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) +\n  stat_bar + \n    facet_nested_wrap(~Exp+bandOrder+Resp, strip.position = \"top\", scales = \"free_x\") +\n    scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .7), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n        #  strip.background = element_blank(),\n        #  strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(20,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions Experiment 2 & 3\", y=\"vx\")\n\n\n\n\n\n\nFigure 22: Empirical data and Model predictions from Experiment 2 and 3 for the testing stage. Observed data is shown on the right. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\n\n\nDisplay codee2_ee_brm_ae &lt;- brm(data=pdl2,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e2_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"conditVaried\")\nbm2_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM\")\nbm3_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nbct_e2 &lt;- as.data.frame(bayestestR::describe_posterior(e2_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) # %&gt;% kable(booktabs = TRUE)\n\ne3_ee_brm_ae &lt;- brm(data=pdl3,\n  aerror ~  Model * condit*bandOrder + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e3_ae_modelCondBo_RFint2.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e3 &lt;- get_coef_details(e3_ee_brm_ae, \"conditVaried\")\nbm2_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM\")\nbm3_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried\")\nbm4_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried:bandOrderReverse\")\n\n\nbct_e3  &lt;- as.data.frame(bayestestR::describe_posterior(e3_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\nbct &lt;- rbind(bct_e1 |&gt; mutate(exp=\"Exp 1\"),bct_e2 |&gt; \n               mutate(exp= \"Exp 2\"),bct_e3 |&gt; mutate(exp=\"Exp 3\")) |&gt; \n  relocate(exp, .before=Term)\n\n\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\nif (primary) {\nbct_table &lt;- bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;%\n  gt() %&gt;%\n  # tab_header(\n  #   title = \"Bayesian Model Results\",\n  #   subtitle = \"Estimates and Credible Intervals for Each Term Across Experiments\"\n  # ) %&gt;%\n  cols_label(\n    exp = \"Experiment\",\n    Term = \"Term\",\n    Estimate = \"Estimate\",\n    `95% CrI Lower` = \"95% CrI Lower\",\n    `95% CrI Upper` = \"95% CrI Upper\",\n    pd = \"pd\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Estimate, `95% CrI Lower`, `95% CrI Upper`),\n    decimals = 1\n  ) %&gt;%\n  fmt_number(\n    columns = pd,\n    decimals = 2\n  ) %&gt;%\n  tab_spanner(\n    label = \"Credible Interval\",\n    columns = c(`95% CrI Lower`, `95% CrI Upper`)\n  ) %&gt;%\n  tab_style(\n    style = list(\n      #cell_fill(color = \"lightgray\"),\n      cell_text(weight = \"bold\"), \n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(\n      columns = c(Estimate, pd),\n      rows = Term==\"ModelEXAM:conditVaried\"\n    )\n  ) %&gt;%\n   tab_row_group(\n    label = \"Experiment 3\",\n    rows = exp == \"Exp 3\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 2\",\n    rows = exp == \"Exp 2\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 1\",\n    rows = exp == \"Exp 1\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n    #row_group.background.color = \"gray95\"\n  )\nbct_table\n} else {\n  bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;% kable(booktabs = TRUE)\n\n}\n\n\nTable 14: Results of Bayesian Regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and the interaction between Model and Condition. The values represent the estimated coefficient for each term, with 95% credible intervals in brackets. The intercept reflects the baseline of ALM and Constant. The other estimates indicate deviations from the baseline for the EXAM mode and varied condition. Lower values indicate better model fit.\n\n\n\n\n\n\n\nExperiment\n      Term\n      Estimate\n      \n        Credible Interval\n      \n      pd\n    \n\n95% CrI Lower\n      95% CrI Upper\n    \n\n\n\nExperiment 1\n    \n\nExp 1\nIntercept\n176.3\n156.9\n194.6\n1.00\n\n\nExp 1\nModelEXAM\n−88.4\n−104.5\n−71.8\n1.00\n\n\nExp 1\nconditVaried\n−37.5\n−60.4\n−14.2\n1.00\n\n\nExp 1\nModelEXAM:conditVaried\n60.4\n36.2\n83.8\n1.00\n\n\nExperiment 2\n    \n\nExp 2\nIntercept\n245.9\n226.2\n264.5\n1.00\n\n\nExp 2\nModelEXAM\n−137.7\n−160.2\n−115.5\n1.00\n\n\nExp 2\nconditVaried\n−86.4\n−113.5\n−59.3\n1.00\n\n\nExp 2\nModelEXAM:conditVaried\n56.9\n25.3\n88.0\n1.00\n\n\nExperiment 3\n    \n\nExp 3\nIntercept\n164.8\n140.1\n189.4\n1.00\n\n\nExp 3\nModelEXAM\n−65.7\n−86.0\n−46.0\n1.00\n\n\nExp 3\nconditVaried\n−40.6\n−75.9\n−3.0\n0.98\n\n\nExp 3\nbandOrderReverse\n25.5\n−9.3\n58.7\n0.93\n\n\nExp 3\nModelEXAM:conditVaried\n41.9\n11.2\n72.5\n0.99\n\n\nExp 3\nModelEXAM:bandOrderReverse\n−7.3\n−34.5\n21.1\n0.70\n\n\nExp 3\nconditVaried:bandOrderReverse\n30.8\n−19.6\n83.6\n0.88\n\n\nExp 3\nModelEXAM:conditVaried:bandOrderReverse\n−60.6\n−101.8\n−18.7\n1.00\n\n\n\n\n\n\n\n\n\nModel Fits to Experiment 2 and 3. Data from Experiments 2 and 3 were fit to ALM and EXAM in the same manner as Experiment 1. For brevity, we only plot and discuss the results of the “fit to training and testing data” models - results from the other fitting methods can be found in the appendix. The model fitting results for Experiments 2 and 3 closely mirrored those observed in Experiment 1. The Bayesian regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and their interaction (see Table 14) revealed a consistent main effect of Model across all three experiments. The negative coefficients for the ModelEXAM term (Exp 2: \\(\\beta\\) = -86.39, 95% CrI -113.52, -59.31, pd = 100%; Exp 3: \\(\\beta\\) = -40.61, 95% CrI -75.9, -3.02, pd = 98.17%) indicate that EXAM outperformed ALM in both experiments. Furthermore, the interaction between Model and Condition was significant in both Experiment 2 (\\(\\beta\\) = 56.87, 95% CrI 25.26, 88.04, pd = 99.98%) and Experiment 3 (\\(\\beta\\) = 41.9, 95% CrI 11.2, 72.54, pd = 99.35%), suggesting that the superiority of EXAM over ALM was more pronounced for the Constant group compared to the Varied group, as was the case in Experiment 1. Recall that Experiment 3 included participants in both the original and reverse order conditions - and that this manipulation interacted with the effect of training condition. We thus also controlled for band order in our Bayesian Regression assessing the relative performance of EXAM and ALM in Experiment 3. There was a significant three way interaction between Model, Training Condition, and Band Order (\\(\\beta\\) = -60.6, 95% CrI -101.8, -18.66, pd = 99.83%), indicating that the relative advantage of EXAM over ALM was only more pronounced in the original order condition, and not the reverse order condition (see Figure 23).\n\nDisplay code#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\np1 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\") + labs(title=\"E2. Model Error\")\np2 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n  p_e2 &lt;- (p1 + p2+ p3) \n# #wrap_plots(plot(conditional_effects(e3_ee_brm_ae),points=FALSE,plot=FALSE))\n\np_e3 &lt;- plot(conditional_effects(e3_ee_brm_ae, \n                         effects = \"Model:condit\", \n                         conditions=make_conditions(e3_ee_brm_ae,vars=c(\"bandOrder\"))),\n     points=FALSE,plot=FALSE)$`Model:condit` + \n     labs(x=\"Model\",y=\"Model Error\", title=\"E3. Model Error\", fill=NULL, col=NULL) + \n     theme(legend.position=\"right\") + \n     scale_color_manual(values=wes_palette(\"Darjeeling1\")) \n\np1 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n p2 &lt;- (p1 + p2+ p3)\n (p_e2 / p_e3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\nFigure 23: Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied) on Model Error for Experiments 2 and 3 data. Experiment 3 also includes a condition for the order of training vs. testing bands (original order vs. reverse order).\n\n\n\n\nComputational Model Summary.\nAcross all three experiments, the model fits consistently favored the Extrapolation-Association Model (EXAM) over the Associative Learning Model (ALM). This preference for EXAM was particularly pronounced for participants in the constant training conditions (note the positive coefficients on ModelEXAM:conditVaried interaction terms Table 14). This pattern is clearly illustrated in Figure 24, which plots the difference in model errors between ALM and EXAM for each individual participant. Both varied and constant conditions have a greater proportion of subjects better fit by EXAM (positive error differences), with the magnitude of EXAM’s advantage visibly larger for the constant group.\nThe superior performance of EXAM, especially for the constant training groups, may initially seem counterintuitive. One might assume that exposure to multiple, varied examples would be necessary to extract an abstract rule. However, EXAM is not a conventional rule-based model; it does not require the explicit abstraction of a rule. Instead, rule-based responses emerge during the retrieval process. The constant groups’ formation of a single, accurate input-output association, combined with the usefulness of the zero point, seem to have been sufficient for EXAM to capture their performance. A potential concern is that the assumption of participants utilizing the zero point essentially transforms the extrapolation problem into an interpolation problem. However, this concern is mitigated by the consistency of the results across both the original and reversed order conditions (the testing extrapolation bands fall in between the constant training band and the 0 point in experiment 1, but not in experiment 2).\nThe fits to the individual participants also reveal a number of interesting cases where the models struggle to capture the data (Figure 25). For example participant 68 exhibits a strong non-monotonicity in the highest velocity band, a pattern which ALM can mimic, but which EXAM cannot capture, given that it enforces a simple linear relationship between target velocity and response. Participant 70 (lower right corner of Figure 25) had a roughly parabolic response pattern in their observed data, a pattern which neither model can properly reproduce, but which causes EXAM to perform particularly poorly.\nModeling Limitations. The present work compared models based on their ability to predict the observed data, without employing conventional model fit indices such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). These indices, which penalize models based on their number of free parameters, would have been of limited utility in the current case, as both ALM and EXAM have two free parameters. However, despite having the same number of free parameters, EXAM could still be considered the more complex model, as it incorporates all the components of ALM plus an additional mechanism for rule-based responding. A more comprehensive model comparison approach might involve performing cross-validation with a held-out subset of the data (Mezzadri et al., 2022) or penalizing models based on the range of patterns they can produce (Dome & Wills, 2023), under the assumption that more constrained models are more impressive when they do adequately fit a given pattern of results.\n\n\nDisplay codetid1 &lt;- post_dat  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-pred_dist, -dist) |&gt;\n  rbind(e2_model$post_dat |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat |&gt; mutate(Exp=\"E3\")) |&gt;\n  filter(Fit_Method==\"Test_Train\") |&gt;\n  group_by(id,condit,Model,Fit_Method,x, Exp) |&gt; \n    mutate(e2=abs(y-pred)) |&gt; \n    summarise(y1=median(y), pred1=median(pred),mean_error=abs(y1-pred1)) |&gt;\n    group_by(id,condit,Model,Fit_Method,Exp) |&gt; \n    summarise(mean_error=mean(mean_error)) |&gt; \n    arrange(id,condit,Fit_Method) |&gt;\n    round_tibble(1) \n\nbest_id &lt;- tid1 |&gt; \n  group_by(id,condit,Fit_Method) |&gt; \n  mutate(best=ifelse(mean_error==min(mean_error),1,0)) \n\nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method, Exp) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\nerror_difference&lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(mean_error)) %&gt;%\n  mutate(Error_difference = (ALM - EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; \n  left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; \n  mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\nfull_comparison |&gt; \n  filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n  geom_col() +\n  #ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  ggh4x::facet_nested_wrap(~condit+Exp,scales=\"free\") + \n  theme(axis.text.y = element_text(size=8)) +\n  labs(fill=\"Best Model\",\n  x=\"Mean Model Error Difference (ALM - EXAM)\",\n  y=\"Participant\")\n\n\n\n\n\n\nFigure 24: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\n\nDisplay codecId_tr &lt;- c(137, 181, 11)\nvId_tr &lt;- c(14, 193, 47)\ncId_tt &lt;- c(11, 93, 35)\nvId_tt &lt;- c(1,14,74)\ncId_new &lt;- c(175, 68, 93, 74)\n# filter(id %in% (filter(bestTestEXAM,group_rank&lt;=9, Fit_Method==\"Test\")\n\ne1_sbjs &lt;- c(49,68,155, 175,74)\ne3_sbjs &lt;-  c(245, 280, 249)\ne2_sbjs &lt;- c(197, 157, 312, 334)\ncFinal &lt;- c(49, 128,202 )\nvFinal &lt;- c(68,70,245)\n\n\nindv_post_l &lt;- post_dat_l  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-signed_dist) |&gt;\n  rbind(e2_model$post_dat_l |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat_l |&gt; mutate(Exp=\"E3\") |&gt; select(-fb)) |&gt;\n  filter(Fit_Method==\"Test_Train\", id %in% c(cFinal,vFinal))\n\ntestIndv &lt;- indv_post_l |&gt; \n#filter(id %in% c(cId_tt,vId_tt,cId_new), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=vb, col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar_sd + \n  ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) + \n  labs(title=\"Individual Participant fits from Test & Train Fitting Method\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\ntestIndv \n\n\n\n\n\n\nFigure 25: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/htw_full.html#variability-and-function-learning",
    "href": "Sections/htw_full.html#variability-and-function-learning",
    "title": "Impact of Training Variability on Visuomotor Function Learning and Extrapolation",
    "section": "Variability and Function Learning",
    "text": "Variability and Function Learning\nThe influence of variability on function learning tasks has received relatively little attention. The study by DeLosh et al. (1997) (described in detail above) did include a variability manipulation (referred to as density in their paper), wherein participants were trained with either 8, 20, or 50 unique input-output pairs, with the total number of training trials held constant. They found a minimal influence of variability on training performance, and no difference between groups in interpolation or extrapolation, with all three variability conditions displaying accurate interpolation, and linearly biased extrapolation that was well accounted for by the EXAM model.\nIn the domain of visuomotor learning, van Dam & Ernst (2015) employed a task which required participants to learn a linear function between the spikiness of shape stimuli and the correct horizontal position to make a rapid pointing response. The shapes ranged from very spiky to completely circular at the extreme ends of the space. Participants trained with intermediate shapes having lower variation (2 shapes) or higher variation (5 shapes) condition, with the 2 items of the lower variation condition matching the items used on the extreme ends of the higher variation training space. Learning was significantly slower in the higher variation group. However, the two conditions did not differ when tested with novel shapes, with both groups producing extrapolation responses of comparable magnitude to the most similar training item, rather than in accordance with the true linear function. The authors accounted for both learning and extrapolation performance with a Bayesian learning model. Similar to ALM, the model assumes that generalization occurs as a Gaussian function of the distance between stimuli. However, unlike ALM, the Bayesian learning model utilizes more elaborate probabilistic stimulus representations, with a separate Kalman Filter for each shape stimulus.",
    "crumbs": [
      "Sections",
      "HTW Full w/Code"
    ]
  },
  {
    "objectID": "Sections/Intro.html#variability-and-function-learning",
    "href": "Sections/Intro.html#variability-and-function-learning",
    "title": "",
    "section": "Variability and Function Learning",
    "text": "Variability and Function Learning\nThe influence of variability on function learning tasks has received relatively little attention. The study by DeLosh et al. (1997) (described in detail above) did include a variability manipulation (referred to as density in their paper), wherein participants were trained with either 8, 20, or 50 unique input-output pairs, with the total number of training trials held constant. They found a minimal influence of variability on training performance, and no difference between groups in interpolation or extrapolation, with all three variability conditions displaying accurate interpolation, and linearly biased extrapolation that was well accounted for by the EXAM model.\nIn the domain of visuomotor learning, van Dam & Ernst (2015) employed a task which required participants to learn a linear function between the spikiness of shape stimuli and the correct horizontal position to make a rapid pointing response. The shapes ranged from very spiky to completely circular at the extreme ends of the space. Participants trained with intermediate shapes having lower variation (2 shapes) or higher variation (5 shapes) condition, with the 2 items of the lower variation condition matching the items used on the extreme ends of the higher variation training space. Learning was significantly slower in the higher variation group. However, the two conditions did not differ when tested with novel shapes, with both groups producing extrapolation responses of comparable magnitude to the most similar training item, rather than in accordance with the true linear function. The authors accounted for both learning and extrapolation performance with a Bayesian learning model. Similar to ALM, the model assumes that generalization occurs as a Gaussian function of the distance between stimuli. However, unlike ALM, the Bayesian learning model utilizes more elaborate probabilistic stimulus representations, with a separate Kalman Filter for each shape stimulus.",
    "crumbs": [
      "Sections",
      "Introduction"
    ]
  }
]