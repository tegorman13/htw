[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HTW Project",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nModeling\n\n\nALM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\n\nALM\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\n\nALM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#modeling-empircal-data",
    "href": "index.html#modeling-empircal-data",
    "title": "HTW Project",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nModeling\n\n\nALM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\n\nALM\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\n\nALM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#simulations",
    "href": "index.html#simulations",
    "title": "HTW Project",
    "section": "Simulations",
    "text": "Simulations\n\n\n\n\n\n\n\n\nALM Learning\n\n\n\nSimulation\n\n\nALM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking\n\n\n\nSimulation\n\n\nALM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Simulations\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Recovery Simulations\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating DeLosh 1997\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#analyses",
    "href": "index.html#analyses",
    "title": "HTW Project",
    "section": "Analyses",
    "text": "Analyses\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nBayesian Mixed Effects Models - Training Data\n\n\n\nAnalysis\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing, High/Low performers\n\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDistributional_Explorations\n\n\n\nAnalysis\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nE1 Discrimination Analysis\n\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nE1 Discrimination via Clustering\n\n\n\nAnalysis\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nE1 Testing\n\n\n\nAnalysis\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nE1 Testing - Bayesian Mixed Models\n\n\n\nAnalysis\n\n\nR\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 1 Analysis\n\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 1 Testing\n\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\nAug 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Analysis\n\n\n\nAnalysis\n\n\nLearning-Curve\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Data Dictionary\n\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Test Analysis\n\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOJS data exploration\n\n\n\nAnalysis\n\n\nLearning-Curve\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Discrimination Analysis\n\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#misc",
    "href": "index.html#misc",
    "title": "HTW Project",
    "section": "Misc",
    "text": "Misc\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Categories\n        \n     \n  \n\n\n\n\n\n\n\n\nALM Shiny App Code\n\n\n\nSimulation\n\n\nALM\n\n\nEXAM\n\n\nShiny\n\n\nInteractive\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHTW Task\n\n\n\nTask\n\n\njs\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMermaid with R + consort\n\n\n\nVisualization\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nModel Visualization\n\n\n\nVisualization\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOJS ALM\n\n\n\nSimulation\n\n\nALM\n\n\nOJS\n\n\nInteractive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOJS data exploration\n\n\n\nAnalysis\n\n\nLearning-Curve\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntikz\n\n\n\nVisualization\n\n\nR\n\n\nOJS\n\n\n\n\n\n\n\n\n\n\nJul 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Simulations/alm_learning.html",
    "href": "Simulations/alm_learning.html",
    "title": "ALM Learning",
    "section": "",
    "text": "pacman::p_load(tidyverse,data.table,patchwork,glue,knitr,kableExtra,here)\noptions(dplyr.summarise.inform=FALSE)\npurrr::walk(here(c(\"Functions/alm_functions.R\",\"Functions/Display_Functions.R\")),source)\n\n\nupdate.weights.with_noise &lt;- function(x.new, y.new, weights, c, lr, noise_sd){\n  y.feedback.activation &lt;- exp(-1 * c * (y.new - outputNodes)^2)\n  x.feedback.activation &lt;- output.activation(x.new, weights, c)\n  weight_updates &lt;- lr * (y.feedback.activation - x.feedback.activation) %*% t(input.activation(x.new, c))\n  noise &lt;- matrix(rnorm(nrow(weight_updates) * ncol(weight_updates), sd = noise_sd), \n                  nrow = nrow(weight_updates), ncol = ncol(weight_updates))\n  updated_weights &lt;- weights + weight_updates + noise\n  return(updated_weights)\n}\n\n\nupdate.weights&lt;-function(x.new, y.new, weights, c, lr, noise_sd = NULL){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\nsim_data &lt;- function(dat, c=0.5, lr=0.2, inNodes=7, outNodes=32, trainVec=c(5,6,7),noise_sd=0,update_func=\"update.weights\" ) {\n  inputNodes &lt;&lt;- seq(1,7,length.out=inNodes*1)  \n  outputNodes &lt;&lt;- seq(50,1600,length.out=outNodes*1) \n  #print(length(outputNodes))\n  wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n  # wm=matrix(rnorm(length(outputNodes)*length(inputNodes),.1,5),nrow=length(outputNodes),ncol=length(inputNodes))\n  tt&lt;-trainTest.alm(dat, c, lr, wm, trainVec, update_func, noise_sd)\n}   \n\ntrainTest.alm &lt;- function(dat, c=0.05, lr=0.5, weights, testVec, update_func, noise_sd) {\n   alm.train &lt;- rep(NA, nrow(dat))  \n   update_func=get(update_func)\n   decay_factor = 0.79\n  for (i in 1:nrow(dat)) {\n    #lr = lr * decay_factor^i\n    resp = mean.prediction(dat$input[i], weights, c)\n    weights &lt;- update_func(dat$input[i], dat$vx[i], weights, c, lr, noise_sd)\n    alm.train[i] = resp\n    weights[weights&lt;0] = 0\n  }\n  almPred &lt;- sapply(testVec, mean.prediction, weights, c)\n  examPred &lt;- sapply(testVec, exam.prediction, weights, c, trainVec=c(1,sort(unique(dat$input))))\n  list(almTrain=alm.train, almPred=almPred, examPred=examPred,weights=weights)\n}\n\nModel learning, and resulting weights, across range of parameter values\n\ntibble(crossing(\n  c = c(.5,5),lr = c(.05,1),noise = c(0),\n  inNodes=c(7),outNodes=c(32),\n  trainVec=list(list(5,6,7)),trainRep = c(9),\n  lossFun = list(\"MAE\"),\n  simNum = 1:1,\n  update_func = list(\"update.weights\"),update_func_name = c(\"uW\"),\n  noise_sd = c(0)\n)) %&gt;%   mutate(id=seq(1,nrow(.)),td = pmap(list(trainVec,trainRep,noise),~gen_train(trainVec=.x,trainRep=..2,noise=..3) )) %&gt;% \n  ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr, update_func,noise_sd,inNodes,outNodes), \n                  ~sim_data(dat = .x, c = ..2, lr = ..3, update_func = ..4, noise_sd = ..5,inNodes=..6,outNodes=..7)),\n         almTrainDat = map(d, \"almTrain\"),weights=map(d,\"weights\"))%&gt;%\n  unnest(c(almTrainDat, td)) %&gt;% select(-d) %&gt;% mutate(input=as.factor(input)) %T&gt;%\n  {pf(.) } %&gt;% trainTab\n\ncleaner imlementation of above\n\n# Define parameters\nparams &lt;- tibble(crossing(\n  c = c(.5,5),\n  lr = c(.05,1),\n  noise = c(0),\n  inNodes = c(7),\n  outNodes = c(32),\n  trainVec = list(list(5,6,7)),\n  trainRep = c(9),\n  lossFun = list(\"MAE\"),\n  simNum = 1:1,\n  update_func = list(\"update.weights\"),\n  update_func_name = c(\"uW\"),\n  noise_sd = c(0)\n))\n\n# Generate training data\nparams &lt;- params %&gt;% \n  mutate(\n    id = seq(1, nrow(.)),\n    td = pmap(list(trainVec, trainRep, noise), ~gen_train(trainVec = .x, trainRep = ..2, noise = ..3))\n  )\n\n# Run simulations\nparams &lt;- params %&gt;% \n  mutate(\n    d = pmap(list(td, c, lr, update_func, noise_sd, inNodes, outNodes), \n              ~sim_data(dat = .x, c = ..2, lr = ..3, update_func = ..4, noise_sd = ..5, inNodes = ..6, outNodes = ..7)),\n    almTrainDat = map(d, \"almTrain\"),\n    weights = map(d, \"weights\")\n  )\n\n# Unnest and select relevant columns\nparams &lt;- params %&gt;% \n  unnest(c(almTrainDat, td)) %&gt;% \n  select(-d) %&gt;% \n  mutate(input = as.factor(input))\n\n# Apply pf function and trainTab\nresult &lt;- params %T&gt;% \n  {pf(.) } %&gt;% \n  trainTab\n\n\n# Define a function to fit the model and return the parameters\nfit_model &lt;- function(data, initial_c, initial_lr) {\n  # Fit the model here and extract the parameters\n  # This is a placeholder and should be replaced with your actual model fitting code\n  fit_c = initial_c #+ rnorm(1, 0, 0.1)\n  fit_lr = initial_lr #+ rnorm(1, 0, 0.1)\n  \n  tibble(\n    gen_c = initial_c,\n    gen_lr = initial_lr,\n    fit_c = fit_c,\n    fit_lr = fit_lr,\n    error_c = fit_c - initial_c,\n    error_lr = fit_lr - initial_lr\n  )\n}\n\nparams &lt;- tibble(crossing(\n  c = seq(.01,2,length.out=10),\n  lr = seq(.01,2,length.out=10)\n))\n# Run the simulations\nresults &lt;- params %&gt;%\n  mutate(simulation = map2(c, lr, ~fit_model(td, .x, .y))) %&gt;%\n  unnest(simulation)\n\n\nfit_model &lt;- function(data, initial_c, initial_lr) {\n  # Simulate data from the ALM model\n  sim_data &lt;- sim_data(dat = data, c = initial_c, lr = initial_lr, \n                       update_func = \"update.weights\", noise_sd = 0, \n                       inNodes = 7, outNodes = 32)\n  \n  # Extract the fitted parameters\n  fit_c = sim_data$c\n  fit_lr = sim_data$lr\n  \n  tibble(\n    gen_c = initial_c,\n    gen_lr = initial_lr,\n    fit_c = fit_c,\n    fit_lr = fit_lr,\n    error_c = fit_c - initial_c,\n    error_lr = fit_lr - initial_lr\n  )\n}\n\n# Run the simulations\nresults &lt;- params %&gt;%\n  mutate(simulation = map2(c, lr, ~fit_model(td, .x, .y))) %&gt;%\n  unnest(simulation)\n\n# Print the results\nprint(results)\n\n# Print the results\nprint(results %&gt;% select(c,lr,gen_c,gen_lr,fit_c,fit_lr,error_c,error_lr))\n\nggplot(results, aes(x = gen_c, y = fit_c)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(x = \"Generating c\", y = \"Fitted c\", title = \"Parameter Recovery for c\")\n\n# Plot for lr parameter\nggplot(results, aes(x = gen_lr, y = fit_lr)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(x = \"Generating lr\", y = \"Fitted lr\", title = \"Parameter Recovery for lr\")\n\nSelf contained\n\npacman::p_load(tidyverse,data.table,knitr,kableExtra,glue)\ninput.activation&lt;-function(x.target, c){return(exp((-1*c)*(x.target-inputNodes)^2))}\noutput.activation&lt;-function(x.target, weights, c){return(weights%*%input.activation(x.target, c))}\nmean.prediction&lt;-function(x.target, weights, c){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  } \n  list(almTrain = alm.train, weights = weights)\n  }\nsim_train &lt;- function(dat, c=0.5, lr=0.2, inNodes=7, outNodes=32, trainVec=c(5,6,7),noise_sd=0,update_func=\"update.weights\" ) {\n  inputNodes &lt;&lt;- seq(1,7,length.out=inNodes*1)  \n  outputNodes &lt;&lt;- seq(50,1600,length.out=outNodes*1) \n  wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n  tt&lt;-train.alm(dat, c, lr, wm)\n} \ngen_train &lt;- function(trainVec = c(5, 6, 7), trainRep = 3, noise = 0) {\n    bandVec &lt;- c(0, 100, 350, 600, 800, 1000, 1200)\n    if (class(trainVec) == \"list\") {trainVec &lt;- unlist(trainVec)}\n    ts &lt;- rep(seq(1, length(trainVec)), trainRep)\n    noiseVec &lt;- rnorm(length(ts), mean = 0) * noise\n    if (noise == 0) {noiseVec &lt;- noiseVec * 0}\n    tibble(trial = seq(1, length(ts)), input = trainVec[ts], vx = bandVec[trainVec[ts]] + noiseVec)\n}\n\ntibble(crossing(\n    c = c(.5, 5), lr = c(.05, 1), noise = c(0),\n    inNodes = c(7), outNodes = c(32),\n    trainVec = list(list(5, 6, 7)), trainRep = c(9),\n    lossFun = list(\"MAE\"),\n    simNum = 1:1,\n)) %&gt;%\n    mutate(id = seq(1, nrow(.)), td = pmap(list(trainVec, trainRep, noise), ~ gen_train(trainVec = .x, trainRep = ..2, noise = ..3))) %&gt;%\n    ungroup() %&gt;%\n    mutate(\n        d = pmap(\n            list(td, c, lr,inNodes, outNodes),\n            ~ sim_train(dat = .x, c = ..2, lr = ..3, inNodes = ..4, outNodes = ..5)\n        ),\n        almTrainDat = map(d, \"almTrain\"), weights = map(d, \"weights\")\n    ) %&gt;%\n    unnest(c(almTrainDat, td)) %&gt;%\n    select(-d) %&gt;%\n    mutate(input = as.factor(input)) %&gt;%\n    trainTab()\n\ntibble(crossing(\n    c = c(.5, 5), lr = c(.05, 1), noise = c(0),\n    inNodes = c(7), outNodes = c(32),\n    trainVec = list(list(5, 6, 7)), trainRep = c(9),\n    lossFun = list(\"MAE\"),\n    simNum = 1:1,\n)) %&gt;%\n    mutate(id = seq(1, nrow(.)), td = pmap(list(trainVec, trainRep, noise), ~ gen_train(trainVec = .x, trainRep = ..2, noise = ..3))) %&gt;%\n    ungroup() %&gt;%\n    mutate(\n        d = pmap(\n            list(td, c, lr,inNodes, outNodes),\n            ~ sim_train(dat = .x, c = ..2, lr = ..3, inNodes = ..4, outNodes = ..5)\n        ),\n        almTrainDat = map(d, \"almTrain\"), weights = map(d, \"weights\")\n    ) %&gt;%\n    unnest(c(almTrainDat, td)) %&gt;%\n    select(-d) %&gt;%\n    mutate(input = as.factor(input)) %&gt;%\n    trainTab()\n\nOptimize for single decay curve\ngenerate data that follows an exponetial decay function of error over trials, inspect ability of model to fit that data.\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=18) %&gt;% mutate(cor=vx,err=(400-0)*exp(-.1*seq(1,n()))+0,vx=cor-err)\n\nbias &lt;- 1000; \ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=228,noise=0) %&gt;% mutate(\n  cor = vx,\n  err = (bias - 0) * exp(-.005 * seq(1, n())) + 0,\n  en = map2_dbl(err,cor, ~rnorm(n = 1, mean = .y, sd = .x/2)),\n  enAvg = map2_dbl(err,cor, ~mean(rnorm(n = 1, mean = .y, sd = .x))),\n  weight = (seq(1, n()) - 1) / (n() - 1),\n  vx = (weight*en)+bias*(1-weight),\n  vx=en\n)\ngt %&gt;% ggplot(aes(x = trial, y = vx, color = as.factor(input))) +\n  geom_line() + ylim(c(-10,1600))\n\n\nk=wrap_optim(gt,lossFun = \"MAE\")\ns=sim_data(dat=mutate(gt,vx=cor),c=k %&gt;% pluck(\"c\"),lr= k %&gt;% pluck(\"lr\"))\nggp &lt;- gt %&gt;% mutate(pred=s %&gt;% pluck(\"almTrain\"),c=k %&gt;% pluck(\"c\"),lr= k %&gt;% pluck(\"lr\"),input=as.factor(input)) %&gt;%  \n  ggplot(aes(x = trial, y = pred, color = input)) +\n  geom_line() + ylim(c(0,1600))\nggo &lt;-  gt %&gt;% ggplot(aes(x = trial, y = vx, color = as.factor(input))) +\n  geom_line() + ylim(c(-400,1600))\n\nggo+ggp\n\n\n#k = t[1,]\n#image(matrix(unlist(k$weights),nrow=7))\n# mutate(md=map(weights,~matrix(unlist(.),nrow=7)))\n\nwms &lt;- t %&gt;% filter(trial==1) %&gt;% \n  mutate(k=map(weights,~ as.data.frame(matrix(unlist(.),nrow=7)) %&gt;% \n                 rownames_to_column(\"Input\") %&gt;%\n                 pivot_longer(-c(Input), names_to = \"Output\",\n                              names_prefix = \"V\", values_to = \"weight\") %&gt;%  mutate(Output= fct_relevel(Output,unique(.$Output)))))\n\nwms %&gt;% unnest(k) %&gt;% ggplot(.,aes(x=Input, y=Output, fill=weight)) + \n  geom_raster() + \n  scale_fill_viridis_c()+facet_wrap(~id+c)\n\n\n\nmd=matrix(unlist(k$weights),nrow=7)\nkd=as.data.frame(matrix(unlist(k$weights),nrow=7))%&gt;%\n  rownames_to_column(\"Input\") %&gt;%\n  pivot_longer(-c(Input), names_to = \"Output\",names_prefix = \"V\", values_to = \"weight\")\n\nkd %&gt;% \n  mutate(Output= fct_relevel(Output,unique(kd$Output))) %&gt;%\n  ggplot(aes(x=Input, y=Output, fill=weight)) + \n  geom_raster() + \n  scale_fill_viridis_c()\n\n\npv &lt;- t &lt;- parmVec &lt;- tibble(crossing(\n  c = c(0.00003),\n  lr = c(0.051),\n  noise = c(0),\n  inNodes=c(7),\n  outNodes=c(32),\n  trainVec=list(list(1,5,6,7),list(5,6,7)),\n  trainRep = c(4),\n  lossFun = list(\"MAE\"),\n  simNum = 1:1,\n  update_func = list(\"update.weights\"),\n  update_func_name = c(\"update.weights\"),\n  noise_sd = c(0)\n)) %&gt;% mutate(id=seq(1,nrow(.)))\n\ninspect learning\n\nparmVec &lt;- tibble(crossing(\n  c = c(0.1),\n  lr = c(0.1, 0.4, 1),\n  noise = c(10),\n  trainRep = c(20),\n  lossFun = list(\"MAE\"),\n  simNum = 1:10,\n  update_func = list(\"update.weights\", \"update.weights.with_noise\"),\n  update_func_name = c(\"update.weights\", \"update.weights.with_noise\"),\n  noise_sd = c(0.1, 0.5)\n))\n\n\n\nt &lt;- parmVec %&gt;%\n  group_by(simNum, c, lr, update_func,noise_sd) %&gt;%\n  mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;%\n  ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr, update_func,noise_sd), \n                  ~sim_data(dat = .x, c = ..2, lr = ..3, update_func = ..4, noise_sd = ..5)),\n         almTrainDat = map(d, \"almTrain\"))%&gt;%\n  unnest(almTrainDat, td) %&gt;%\n  select(-d) %&gt;% \n  mutate(trial = rep(seq(1, nrow(.)/10), 10),input=as.factor(input))\n\nt %&gt;% group_by(lr, update_func_name, simNum, trial, input) %&gt;%\n  summarize(almTrainDat = mean(almTrainDat), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = trial, y = almTrainDat, color = input)) +\n  geom_line() + ylim(c(0,1300))+\n  facet_grid(lr ~ update_func_name)\n\n\nparmVec &lt;- tibble(crossing(c = c(0.1), lr = c(0.1,0.4,1), \n                           noise = c(10), trainRep = c(20), lossFun = list(\"MAE\"), simNum = 1))\n\nt &lt;- parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;% ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr), ~sim_data(dat = .x, c = ..2, lr = ..3)),\n         almTrainDat = map(d, \"almTrain\"))\n\n# extract and plot each almTrainDat \nalmTrainDat &lt;- parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;% ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr), ~sim_data(dat = .x, c = ..2, lr = ..3)),\n         almTrainDat = map(d, \"almTrain\")) %&gt;% unnest(almTrainDat) %&gt;% select(-d)\n\n# For each unique value of lr, plot the learning curve, showing almTrainDat as a function of trial number, color by input, and facet by lr. Convert input to factor first. \n\nalmTrainDat %&gt;% group_by(lr) %&gt;% mutate(trial = seq(1,n()),input= as.factor(input)) %&gt;% ggplot(aes(x=trial,y=almTrainDat,color=input)) + geom_line() + facet_wrap(~lr)\n\n\n\nparmVec &lt;- tibble(crossing(c = c(0.1), lr = c(.01,.05,0.1), noise = c(5), trainRep = c(20), lossFun = list(\"MAE\"), simNum = 1:30))\n\n# The rest of the code remains the same\nalmTrainDat &lt;- parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;% ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr), ~sim_data(dat = .x, c = ..2, lr = ..3)),\n         almTrainDat = map(d, \"almTrain\")) %&gt;% unnest(almTrainDat,td) %&gt;% select(-d) %&gt;%\n  mutate(trial = rep(seq(1, nrow(.)/30), 30),input=as.factor(input))\n\n\nmean_sd_almTrainDat &lt;- almTrainDat %&gt;%\n  group_by(lr,input, trial) %&gt;%\n  summarise(avg_almTrainDat = mean(almTrainDat), sd_almTrainDat = sd(almTrainDat), .groups = 'drop')\n\n# Check the results\nhead(mean_sd_almTrainDat)\n\n\nggplot(mean_sd_almTrainDat, aes(x = trial, y = avg_almTrainDat,color=input)) +\n   geom_line() +\n  geom_errorbar(aes(ymin = avg_almTrainDat - sd_almTrainDat, ymax = avg_almTrainDat + sd_almTrainDat), width = 0.2) +\n  facet_wrap(~lr) +\n  labs(title = \"ALM Train Data: Mean and Variance Over Trials\",\n       x = \"Trial\",\n       y = \"Average ALM Train Data\") + ylim(c(0,1300))+\n  theme_minimal()\n\n\n#plot(seq(1,90), (200-.50)*exp(-.1*seq(1,90))+.50)\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=18) %&gt;% mutate(cor=vx,err=(400-0)*exp(-.1*seq(1,n()))+0,vx=cor-err)\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=10) %&gt;% mutate(cor=vx,err=(100-0)*exp(-.03*seq(1,n()))+0,en=map_dbl(err,~rnorm(n=1,mean=0,sd=.x)),vx1=cor-err,vx2=cor-en)\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=0) %&gt;% group_by(input) %&gt;% mutate(cor=vx,err=(200--10)*exp(-.01*seq(1,n()))+(-10),en=map(err,~rnorm(n=1,mean=0,sd=.x)),vx=cor-err)\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=10) %&gt;% mutate(cor=vx,\n            err=ifelse(seq(1, n()) &lt;= n()/2, (700-0)*exp(-.01*seq(1,n()))+0, (700-0)*exp(-.06*(seq(1,n())-n()/2))+0),\n            en=map(err,~rnorm(n=1,mean=0,sd=.x)),\n            vx=cor-err)\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=10) %&gt;%\n  mutate(\n    cor = vx,\n    err = (700 - 0) * exp(-0.03 * seq(1, n()) * (input / max(input))) + 0,\n    en = map(err, ~rnorm(n = 1, mean = 0, sd = .x)),\n    vx = cor - err\n  )\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=28,noise=10) %&gt;% mutate(\n  cor = vx,\n  err = (700 - 1) * exp(-.03 * seq(1, n())) + 1,\n  en = map(err, ~rnorm(n = 1, mean = 0, sd = .x)),\n  weight = (seq(1, n()) - 1) / (n() - 1),\n  vx = cor * weight - err + abs(min(cor * weight - err)) + 1\n)"
  },
  {
    "objectID": "Simulations/DeLosh97_Sim.html",
    "href": "Simulations/DeLosh97_Sim.html",
    "title": "Simulating DeLosh 1997",
    "section": "",
    "text": "Code#lapply(c('tidyverse','data.table','igraph','ggraph','kableExtra'),library,character.only=TRUE))\npacman::p_load(tidyverse,data.table,igraph,ggraph,kableExtra)\nCode#https://nrennie.rbind.io/blog/2022-06-06-creating-flowcharts-with-ggplot2/\ninNodes &lt;- seq(1,6,1) %&gt;% as.integer()\noutNodes &lt;- seq(300,1000,50)%&gt;% as.integer()\nstim &lt;- \"Stim\"\nresp &lt;- \"Response\"\ninFlow &lt;- tibble(expand.grid(from=stim,to=inNodes)) %&gt;% mutate_all(as.character)\noutFlow &lt;- tibble(expand.grid(from=outNodes,to=resp)) %&gt;% mutate_all(as.character)\n\ngd &lt;- tibble(expand.grid(from=inNodes,to=outNodes)) %&gt;% mutate_all(as.character) %&gt;%\n  rbind(inFlow,.) %&gt;% rbind(.,outFlow)\n\ng = graph_from_data_frame(gd,directed=TRUE)\ncoords2=layout_as_tree(g)\ncolnames(coords2)=c(\"y\",\"x\")\nodf &lt;- as_tibble(coords2) %&gt;% \n  mutate(label=vertex_attr(g,\"name\"),\n         type=c(\"stim\",rep(\"Input\",length(inNodes)),rep(\"Output\",length(outNodes)),\"Resp\"),\n         x=x*-1) %&gt;%\n  mutate(y=ifelse(type==\"Resp\",0,y),xmin=x-.05,xmax=x+.05,ymin=y-.35,ymax=y+.35)\n\nplot_edges = gd %&gt;% mutate(id=row_number()) %&gt;%\n  pivot_longer(cols=c(\"from\",\"to\"),names_to=\"s_e\",values_to=(\"label\")) %&gt;%\n                 mutate(label=as.character(label)) %&gt;% \n  group_by(id) %&gt;%\n  mutate(weight=sqrt(rnorm(1,mean=0,sd=10)^2)/10) %&gt;%\n  left_join(odf,by=\"label\") %&gt;%\n  mutate(xmin=xmin+.02,xmax=xmax-.02)\n\nggplot() + geom_rect(data = odf,\n            mapping = aes(xmin = xmin, ymin = ymin, \n                          xmax = xmax, ymax = ymax, \n                          fill = type, colour = type),alpha = 0.5) +\n  geom_text(data=odf,aes(x=x,y=y,label=label,size=3)) +\n  geom_path(data=plot_edges,mapping=aes(x=x,y=y,group=id,alpha=weight)) +\n  # geom_rect(aes(xmin=-1.05,xmax=-.95,ymin=-10,ymax=5),color=\"red\",alpha=.1)+\n  # geom_rect(aes(xmin=-0.05,xmax=.05,ymin=-10,ymax=5),color=\"blue\",alpha=.1) +\n  theme_void()"
  },
  {
    "objectID": "Simulations/DeLosh97_Sim.html#alm-definition",
    "href": "Simulations/DeLosh97_Sim.html#alm-definition",
    "title": "Simulating DeLosh 1997",
    "section": "ALM Definition",
    "text": "ALM Definition\n\n\nInput Activation\n\\[\na_i(X)=\\exp \\left|-\\gamma \\cdot\\left[X-X_i\\right]^2\\right|\n\\]\nOutput activation\n\\[\no_j(X)=\\Sigma_{i=1, M} w_{j i} \\cdot a_i(X)\n\\]\nOutput Probability\n\\[\nP\\left[Y_j \\mid X\\right]=o_j(X) / \\Sigma_{k=1, L} o_k(X)\n\\]\nMean Response\n\\[\nm(X)=\\Sigma_{j=1, L} Y_j \\cdot P\\left[Y_j \\mid X\\right]\n\\]\n\nGenerate Response\n\nToggle Codealm.response &lt;- function(input=1,c) {\ninput.activation &lt;- exp(-c*(input.layer - input)^2)\ninput.activation &lt;&lt;- input.activation/sum(input.activation)\n#print(length(input.activation)); print(dim(weight.mat))\noutput.activation &lt;&lt;- weight.mat %*% input.activation\noutput.probability &lt;&lt;- output.activation/sum(output.activation)\nmean.response &lt;&lt;- sum(output.layer * output.probability)\nmean.response\n}\n\n\n     \nUpdate Weights Based on Feedback\n\nToggle Codealm.update &lt;- function(corResp,c,lr){\n  fz &lt;- exp(-c*(output.layer - corResp)^2)\n  teacherSignal &lt;- (fz - output.activation)*lr\n  #print(length(teacherSignal)); print(length(fz))\n  wChange &lt;- teacherSignal %*% t(input.activation)\n  weight.mat &lt;&lt;- weight.mat + (wChange)\n  weight.mat[weight.mat&lt;0]=0 # prevent negative values\n # weight.mat[weight.mat&gt;1]=1\n  weight.mat &lt;&lt;- weight.mat\n}\n\nalm.trial &lt;- function(input, corResp,c,lr){\n  alm.response(input,c)\n  alm.update(corResp,c,lr)\n # print(paste0(\"input=\",input,\"; corResp=\",corResp,\"; mean.response=\",mean.response))\n  mean.response\n}\n\n\n\n\nFeedback Signal\n\\[\nf_j(Z)=e^{-c\\cdot(Z-Y_j)^2}\n\\]\nWeight Updates\n\\[\nw_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t))\n\\]\n\nExam Generalization\n\nToggle Codeexam.response &lt;- function(input,c){\n  # Find the index of the input node with the highest activation\n  trainVec = sort(unique(xt))\n  nearestTrain &lt;- trainVec[which.min(abs(input - trainVec))]\n  aresp &lt;- alm.response(nearestTrain,c)\n  #max.index &lt;- which.max(input.activation)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n\n  mUnder &lt;- alm.response(xUnder,c)\n  mOver &lt;- alm.response(xOver,c)\n \n   exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (input - nearestTrain), 3)\n  # Determine the input nodes and associated weights for computing the slope\n  exam.output\n}\n\n\n\n\nInput node actvation\n\\[\nP[X_i|X] = \\frac{a_i(X)}{\\\\sum_{k=1}^Ma_k(X)}\n\\]\nSlope Computation\n\\[\nE[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg]\\cdot[X-X_i]\n\\]\n\nPrepare Simulation Data\n\nToggle Code# function to generate data, from either linear, exponential, quadratic or sinusoidal functions\n\ngenerate.data &lt;- function(x, type = \"linear\", noise = NA) {\n  if (type == \"linear\") {\n    y &lt;- round(2.2*x + 30,0)\n  }\n  else if (type == \"exponential\") {\n    y &lt;- round(200*(1-exp(-x/25)),0)\n  }\n  else if (type == \"sinusoidal\") {\n    y &lt;- sin(2 * pi * x) \n  }\n  else if (type == \"quadratic\") {\n    y &lt;- round(210 - ((x-50)^2)/12,0)\n  }\n  else {\n    stop(\"type must be linear, exponential, quadratic, or sinusoidal\")\n  }\n  # if noise is specified, add noise to the y values\n  if(!is.na(noise)) {\n    y &lt;- y + round(rnorm(length(y), 0, noise),2)\n  }\n  data.frame(x, y,type)\n}\n\nenvTypes &lt;- c(\"linear\", \"exponential\", \"quadratic\")\n\nlowDensityTrainBlock &lt;- c(30.5, 36.0, 41.0, 46.5, 53.5, 59.0, 64.0, 69.5)\nmedDensityTrainBlock &lt;- c(\n    30.0, 31.5, 33.0, 34.5, 36.5, 38.5, 41.0, 43.5, 46.0,\n    48.5, 51.5, 54.0, 56.5, 59.0, 61.5, 63.5, 65.5, 67.0, 68.5, 70.0\n)\nhighDensityTrainBlock &lt;- c(\n    30.0, 30.5, 31.0, 32.0, 33.0, 33.5, 34.5, 35.5,\n    36.5, 37.0, 38.0, 38.5, 39.5, 40.5, 41.5, 42.0, 43.0,\n    43.5, 44.5, 45.5, 46.5, 47.0, 48.0, 48.5, 49.0, 51.0, 51.5, 52.0,\n    53.0, 53.5, 54.5, 55.5, 56.5, 57.0, 58.0, 58.5, 59.5, 60.5, 61.5, \n    62.0, 63.0,63.5, 64.5, 65.5, 66.5, 67.0, 68.0, 69.0, 69.5, 70.0\n)\n# all density conditions have the same # of training trials, but differ in the # of repetitions per items, or blocks,  low density has 25 training blocks, medium has 10 blocks, high has 4 blocks. \n\n# generate training data, for each combination of environment type and density. Use purrr map functions. Rep each dataset by its number of blocks.\nlowTrain &lt;- map_dfr(envTypes, ~ generate.data(rep(lowDensityTrainBlock,25), type = .x)) %&gt;% group_by(type) %&gt;% mutate(block = rep(1:25, each = 8),trial=seq(1,200))\n\nmedTrain &lt;- map_dfr(envTypes, ~ generate.data(rep(medDensityTrainBlock,10), type = .x)) %&gt;% group_by(type) %&gt;% mutate(block = rep(1:10, each = 20),trial=seq(1,200))\n\nhighTrain &lt;- map_dfr(envTypes, ~ generate.data(rep(highDensityTrainBlock,4), type = .x)) %&gt;% group_by(type) %&gt;% mutate(block = rep(1:4, each = 50),trial=seq(1,200))\n\n# nTrainExamples &lt;- 10\n# trainLowBound &lt;-20; trainHIghBound &lt;- 80\n# nBlock &lt;- 5 # number of times each training example is presented\n# sample training data from function, nTrainExamples., between trainLowBound and trainHIghBound\n# xt &lt;- runif(nTrainExamples, trainLowBound, trainHIghBound)\n# trainVec &lt;- rep(xt, nBlock)\n\n\nSimulation Functions\n\nCode# simulation function\nalm.sim &lt;- function(dat, c, lr,testRange=seq(0,100,.5)){\n  \ninput.layer &lt;&lt;- matrix(seq(0,100,.5) ) # half step units for inputs, from 0 to 100\noutput.layer &lt;&lt;- matrix(seq(0,250,1)) # single step units for outputs, from 0 to 250\nweight.mat &lt;&lt;- matrix(0.0000000,nrow=length(output.layer),ncol=length(input.layer )) # weights initialized to 0 (as in Delosh 1997)\n\nxt&lt;&lt;-dat$x\n# run training\nst &lt;- map2_dbl(dat$x, dat$y, ~alm.trial(.x,.y,c,lr))\n# append training data to the data frame\ndat &lt;- dat %&gt;% mutate(almResp = st)\n\nreturn(list(d=dat,wm=weight.mat,c=c,lr=lr)) # final weightmat is probs incorrect for all but last\n}\n\n\nsimOrganize &lt;- function(simOut){\n  dat &lt;- simOut$d\n  weight.mat &lt;&lt;- simOut$wm\n  c &lt;- simOut$c\n  lr &lt;- simOut$lr\n\n  trainX &lt;- unique(dat$x)\n  xt &lt;&lt;- trainX\n  \n almResp &lt;- generate.data(seq(0,100,.5), type = first(dat$type)) %&gt;% rowwise() %&gt;% \n mutate(model=\"ALM\",resp = alm.response(x,c))\n\n examResp &lt;- generate.data(seq(0,100,.5), type = first(dat$type)) %&gt;% rowwise() %&gt;% \n mutate(model=\"EXAM\",resp = exam.response(x,c))\n\n bind_rows(almResp,examResp) %&gt;% \n mutate(type=first(dat$type),\n        c=c,lr=lr,\n type=factor(type,levels=c(\"linear\",\"exponential\",\"quadratic\"))) %&gt;%\n # compute test_region, equal to \"train\" if x is within trainX, interpolate if within trainX range, else \"extrapolate\"\n  mutate(test_region = ifelse(x %in% trainX, \"train\", ifelse(x &gt; min(trainX) & x &lt; max(trainX), \"interpolate\", \"extrapolate\")))\n\n}\n\n\nSimulate Training:\n\nCode# split by type, then send each training dataset to simulation function\nlowSim &lt;- map(envTypes, ~ alm.sim(lowTrain %&gt;% filter(type == .x), c = 1.4, lr = .4))\n\nmedSim &lt;- map(envTypes, ~ alm.sim(medTrain %&gt;% filter(type == .x), c = 1.4, lr = .4))\n\nhighSim &lt;- map(envTypes, ~ alm.sim(highTrain %&gt;% filter(type == .x), c = 1.4, lr = .4))\n\n\nsimAll &lt;- rbind(bind_rows(lowSim %&gt;% map(\"d\")) %&gt;% mutate(density = \"low\"), \n                bind_rows(medSim %&gt;% map(\"d\")) %&gt;% mutate(density = \"med\"), \n                bind_rows(highSim %&gt;% map(\"d\")) %&gt;% mutate(density = \"high\"))\n\n\n\nsimAll &lt;- simAll %&gt;% mutate(stage=as.numeric(cut(trial,breaks=20,labels=seq(1,20))),\n                                      dev=sqrt((y-almResp)^2),\n                                  #reorder density factor levels\n                            density=factor(density,levels=c(\"low\",\"med\",\"high\")),\n                            type=factor(type,levels=c(\"linear\",\"exponential\",\"quadratic\"))) %&gt;%\n                            dplyr::relocate(density,type,stage)\n\nsimAll %&gt;% ggplot(aes(x=block,y=dev,color=type)) + stat_summary(geom=\"line\",fun=mean,alpha=.4)+\n  stat_summary(geom=\"point\",fun=mean,alpha=.4)+\n  stat_summary(geom=\"errorbar\",fun.data=mean_cl_normal,alpha=.4)+facet_wrap(~density, scales=\"free_x\")\n\n\n\n\nPredictions for Generalization\n\nCodelowSimTest &lt;- map_dfr(lowSim,simOrganize) %&gt;% mutate(density = \"low\")\nmedSimTest &lt;- map_dfr(medSim,simOrganize) %&gt;% mutate(density = \"med\")\nhighSimTest &lt;- map_dfr(highSim,simOrganize) %&gt;% mutate(density = \"high\")\n\nsimTestAll &lt;- rbind(lowSimTest,medSimTest,highSimTest) %&gt;% group_by(type,density,model) %&gt;%\n  mutate(type=factor(type,levels=c(\"linear\",\"exponential\",\"quadratic\")),\n         density=factor(density,levels=c(\"low\",\"med\",\"high\"))) %&gt;%\n  dplyr::relocate(density,type,test_region)\n\nsimTestAll %&gt;% ggplot(aes(x=x,y=y)) + \n  geom_point(aes(x=x,y=resp,shape=model,color=model),alpha=.7,size=1) + \n  geom_line(aes(x=x,y=y),alpha=.4)+ \n  #geom_point(aes(x=x,y=y,color=test_region),alpha=.2)+ \n  geom_point(data=simTestAll %&gt;% filter(test_region==\"train\"),aes(x=x,y=y),color=\"black\",size=1,alpha=1) +\n # geom_point(data=simTestAll %&gt;% filter(test_region %in% c(\"interpolate\",\"extrapolate\")),aes(x=x,y=y,color=test_region),alpha=.6) +\n  # geom_point(data=simTestAll %&gt;% filter(test_region==\"extrapolate\"),aes(x=x,y=y),color=\"purple\",alpha=.3) +\n  facet_grid(density~type) + \n  theme_bw() + theme(legend.position=\"bottom\")\n\n\n\nCode#lowSimTest %&gt;% filter(model==\"EXAM\" & type==\"linear\")\n#rm(list= ls()[sapply(ls(), function(x) class(get(x))) != 'function'])\n#rm(weight.mat,input.activation,output.probability,output.activation,mean.response,xt)\n# \n# simAll %&gt;% ggplot(aes(x=block,y=dev)) + stat_summary(geom=\"line\",fun=mean,alpha=.3)+stat_summary(geom=\"point\",fun=mean)+\n#   stat_summary(geom=\"errorbar\",fun.data=mean_cl_normal)+facet_wrap(density~type, scales=\"free_x\")\n\n\nCollpasing Across Density Levels gives us:\n\nCodesimTestAll %&gt;% group_by(type,model,x,y) %&gt;% summarise(resp=mean(resp))  %&gt;% ggplot(aes(x=x,y=y)) + \n  geom_point(aes(x=x,y=resp,shape=model,color=model),alpha=.7,size=1) + \n  geom_line(aes(x=x,y=y),alpha=.4)+ \n  facet_grid(~type) + \n  theme_bw() + theme(legend.position=\"bottom\")\n\n\nMaster Function for full simulation\n\nCode# Function that goes through every step of generating data, simulating training, and simulating generalization\nfull.sim &lt;- function(c,lr,noise)\n{\n  \nenvTypes &lt;- c(\"linear\", \"exponential\", \"quadratic\")\nlowDensityTrainBlock &lt;- c(30.5, 36.0, 41.0, 46.5, 53.5, 59.0, 64.0, 69.5)\nmedDensityTrainBlock &lt;- c(\n    30.0, 31.5, 33.0, 34.5, 36.5, 38.5, 41.0, 43.5, 46.0,\n    48.5, 51.5, 54.0, 56.5, 59.0, 61.5, 63.5, 65.5, 67.0, 68.5, 70.0\n)\nhighDensityTrainBlock &lt;- c(\n    30.0, 30.5, 31.0, 32.0, 33.0, 33.5, 34.5, 35.5,\n    36.5, 37.0, 38.0, 38.5, 39.5, 40.5, 41.5, 42.0, 43.0,\n    43.5, 44.5, 45.5, 46.5, 47.0, 48.0, 48.5, 49.0, 51.0, 51.5, 52.0,\n    53.0, 53.5, 54.5, 55.5, 56.5, 57.0, 58.0, 58.5, 59.5, 60.5, 61.5, \n    62.0, 63.0,63.5, 64.5, 65.5, 66.5, 67.0, 68.0, 69.0, 69.5, 70.0\n)\n# low density has 25 training blocks, medium has 10 blocks, high has 4 blocks. \n# generate training data, for each combination of environment type and density. Use purrr map functions. Rep each dataset by its number of blocks.\nlowTrain &lt;- map_dfr(envTypes, ~ generate.data(rep(lowDensityTrainBlock,25), type = .x, noise)) %&gt;% group_by(type) %&gt;% mutate(block = rep(1:25, each = 8),trial=seq(1,200))\nmedTrain &lt;- map_dfr(envTypes, ~ generate.data(rep(medDensityTrainBlock,10), type = .x, noise)) %&gt;% group_by(type) %&gt;% mutate(block = rep(1:10, each = 20),trial=seq(1,200))\nhighTrain &lt;- map_dfr(envTypes, ~ generate.data(rep(highDensityTrainBlock,4), type = .x, noise)) %&gt;% group_by(type) %&gt;% mutate(block = rep(1:4, each = 50),trial=seq(1,200))\n  \nlowSim &lt;- map(envTypes, ~ alm.sim(lowTrain %&gt;% filter(type == .x), c = 1.4, lr = .4))\nmedSim &lt;- map(envTypes, ~ alm.sim(medTrain %&gt;% filter(type == .x), c = 1.4, lr = .4))\nhighSim &lt;- map(envTypes, ~ alm.sim(highTrain %&gt;% filter(type == .x), c = 1.4, lr = .4))\n\nsimAll &lt;- rbind(bind_rows(lowSim %&gt;% map(\"d\")) %&gt;% mutate(density = \"low\"), \n                bind_rows(medSim %&gt;% map(\"d\")) %&gt;% mutate(density = \"med\"), \n                bind_rows(highSim %&gt;% map(\"d\")) %&gt;% mutate(density = \"high\"))\n\nsimAll &lt;- simAll %&gt;% mutate(stage=as.numeric(cut(trial,breaks=20,labels=seq(1,20))),\n                                      dev=sqrt((y-almResp)^2),\n                                  #reorder density factor levels\n                            density=factor(density,levels=c(\"low\",\"med\",\"high\")),\n                            type=factor(type,levels=c(\"linear\",\"exponential\",\"quadratic\"))) %&gt;%\n                            dplyr::relocate(density,type,stage)\n\nlowSimTest &lt;- map_dfr(lowSim,simOrganize) %&gt;% mutate(density = \"low\")\nmedSimTest &lt;- map_dfr(medSim,simOrganize) %&gt;% mutate(density = \"med\")\nhighSimTest &lt;- map_dfr(highSim,simOrganize) %&gt;% mutate(density = \"high\")\n\nsimTestAll &lt;- rbind(lowSimTest,medSimTest,highSimTest) %&gt;% group_by(type,density,model) %&gt;%\n  mutate(type=factor(type,levels=c(\"linear\",\"exponential\",\"quadratic\")),\n         density=factor(density,levels=c(\"low\",\"med\",\"high\"))) %&gt;%\n  dplyr::relocate(density,type,test_region)\n\nreturn(list(simAll=list(simAll),simTestAll=list(simTestAll)))\n  \n}\n\n\nSimulations with noise\n\nCodek = full.sim(c=1.4,lr=.4,noise=2.0)\nk4 = full.sim(c=1.4,lr=.4,noise=4.0)\n\n# run simulation with noise=10, 3 times, average results together. \nk10 = map_dfr(1:3, ~ full.sim(c=1.4,lr=.4,noise=10.0)) %&gt;% group_by(type,density,model) %&gt;%\n  mutate(type=factor(type,levels=c(\"linear\",\"exponential\",\"quadratic\")),\n         density=factor(density,levels=c(\"low\",\"med\",\"high\"))) %&gt;%\n  dplyr::relocate(density,type,test_region)\n\n\n\nk %&gt;% pluck(\"simAll\") %&gt;% ggplot(aes(x=block,y=dev,color=type)) + stat_summary(geom=\"line\",fun=mean,alpha=.4)+\n  stat_summary(geom=\"point\",fun=mean,alpha=.4)+\n  stat_summary(geom=\"errorbar\",fun.data=mean_cl_normal,alpha=.4)+facet_wrap(~density, scales=\"free_x\")\n\n\nk4 %&gt;% pluck(\"simAll\") %&gt;% ggplot(aes(x=block,y=dev,color=type)) + stat_summary(geom=\"line\",fun=mean,alpha=.4)+\n  stat_summary(geom=\"point\",fun=mean,alpha=.4)+\n  stat_summary(geom=\"errorbar\",fun.data=mean_cl_normal,alpha=.4)+facet_wrap(~density, scales=\"free_x\")\n\n\nk %&gt;% pluck(\"simTestAll\") %&gt;%ggplot(aes(x=x,y=y)) + \n  geom_point(aes(x=x,y=resp,shape=model,color=model),alpha=.7,size=1) + \n  geom_line(aes(x=x,y=y),alpha=.4)+ \n  geom_point(data=simTestAll %&gt;% filter(test_region==\"train\"),aes(x=x,y=y),color=\"black\",size=1,alpha=1) +\n  facet_grid(density~type) + \n  theme_bw() + theme(legend.position=\"bottom\")\n\nk4 %&gt;% pluck(\"simTestAll\") %&gt;%ggplot(aes(x=x,y=y)) + \n  geom_point(aes(x=x,y=resp,shape=model,color=model),alpha=.7,size=1) + \n  geom_line(aes(x=x,y=y),alpha=.4)+ \n  geom_point(data=simTestAll %&gt;% filter(test_region==\"train\"),aes(x=x,y=y),color=\"black\",size=1,alpha=1) +\n  facet_grid(density~type) + \n  theme_bw() + theme(legend.position=\"bottom\")\n\nk10 %&gt;% pluck(\"simTestAll\") %&gt;%ggplot(aes(x=x,y=y)) + \n  geom_point(aes(x=x,y=resp,shape=model,color=model),alpha=.7,size=1) + \n  geom_line(aes(x=x,y=y),alpha=.4)+ \n  geom_point(data=simTestAll %&gt;% filter(test_region==\"train\"),aes(x=x,y=y),color=\"black\",size=1,alpha=1) +\n  facet_grid(density~type) + \n  theme_bw() + theme(legend.position=\"bottom\")\n\n\n\nCode# label each each simulation with its density level (low, med, high), then combine all 3\nlowSimTest &lt;- lowSimTest %&gt;% mutate(density = \"low\")\nmedSimTest &lt;- medSimTest %&gt;% mutate(density = \"med\")\nhighSimTest &lt;- highSimTest %&gt;% mutate(density = \"high\")\n\nsimTest &lt;- bind_rows(lowSimTest,medSimTest,highSimTest)\n\n# extract element d from each list, and bind rows, remove Nan's, group by type, mutate new variable \"Stage\", which is set to first for block 1, last for final block, and middle for all other glocks, pipe to ggplot, plotting y and almResp in different colors, facet by type and stage (only first and last block)\nls2=bind_rows(lowSim %&gt;% map(\"d\")) %&gt;% filter(!is.na(almResp)) %&gt;% \n  group_by(type) %&gt;% \n  mutate(stage = ifelse(block == 1, \"first\", ifelse(block == 25, \"last\", \"middle\")),\n         stage2=cut(trial,breaks=20,labels=seq(1,20)),\n         dev=sqrt((y-almResp)^2))\n \nls2 %&gt;% ggplot() + geom_point(aes(x=stage2,y=dev))+facet_grid(~type)\n\n\n\nls2 %&gt;% filter(stage %in% c(\"first\",\"last\")) %&gt;% ggplot() + geom_point(aes(x=x,y=y),color=\"red\",alpha=.3) + \ngeom_point(aes(x=x,y=almResp),color=\"blue\",alpha=.4)+ \nfacet_grid(type~stage) + theme_bw() + theme(legend.position=\"bottom\")\n\n\nlowSimTest %&gt;% ggplot() + geom_point(aes(x=x,y=resp,color=model)) + geom_line(aes(x=x,y=y),alpha=.3)+ facet_grid(~type) + theme_bw() + theme(legend.position=\"bottom\")\n\n# make grid of each combination of train dataset and envType\nsimGrid &lt;- expand.grid(envTypes=envTypes,trainData=c(\"lowTrain\",\"medTrain\",\"highTrain\"))\n\n# map over the grid, for each row of simGrid, filter the training dataset by the envType, and then run the simulation function, and then bind rows of the output data frame.\nsimOut &lt;- map2(c(\"lowTrain\",\"medTrain\",\"highTrain\"),envTypes, ~ alm.sim(get(.x) %&gt;% filter(type == .y), c = 1.4, lr = .4))\n\n# simOut &lt;- map(c(\"lowTrain\",\"medTrain\",\"highTrain\"), ~ map_dfr(envTypes, ~ alm.sim(get(.) %&gt;% filter(type == .x), c = 1.4, lr = .4) %&gt;% simOrganize))\n# \n# simOut &lt;- map_dfr(simGrid, ~ alm.sim(get(.x$trainData) %&gt;% filter(type == .x$envTypes), c = 1.4, lr = .4) %&gt;% simOrganize)\n# \n# for each training dataset in c(lowTrain,medTrain,highTrain), run simulation function, separately for each type, and then bind rows of the output data frame.\n\n\nPrimary Functions"
  },
  {
    "objectID": "Model/group_fits.html",
    "href": "Model/group_fits.html",
    "title": "Group Level Fits",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,data.table,here)\noptions(dplyr.summarise.inform=FALSE)\n\n# select first row for each id in d, then create histogram for nTrain\n#  d  %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ggplot(aes(nTrain)) + geom_histogram() + facet_wrap(~condit)\n\nd &lt;- readRDS(here(\"data/dPrune-01-19-23.rds\"))\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\n# unique(dtest[dtest$nd==4,]$sbjCode) # 7 in wrong condition\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\n# for any id that has at least 1 nBand &gt;=5, remove all rows with that id. \ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,catOrder,feedbackType,vb,band,lowBound,highBound,input) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")\n\nds &lt;- d %&gt;% filter(expMode %in% c(\"train\",\"train-Nf\",\"test-Nf\",\"test-train-nf\")) %&gt;% \nfilter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) %&gt;% \nselect(id,condit,catOrder,feedbackType,expMode,trial,gt.train,vb,band,bandInt,lowBound,highBound,input,vx,dist,vxb) \nhead(ds,6)\n\n# A tibble: 6 × 16\n     id condit catOrder feedbackType expMode trial gt.train vb     band  bandInt\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1     1 Varied orig     continuous   train       2        1 1000-… 5        1000\n2     1 Varied orig     continuous   train       3        2 1200-… 6        1200\n3     1 Varied orig     continuous   train       4        3 800-1… 4         800\n4     1 Varied orig     continuous   train       5        4 1000-… 5        1000\n5     1 Varied orig     continuous   train       6        5 800-1… 4         800\n6     1 Varied orig     continuous   train       7        6 1000-… 5        1000\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, input &lt;dbl&gt;, vx &lt;dbl&gt;,\n#   dist &lt;dbl&gt;, vxb &lt;dbl&gt;\n\nCodedst &lt;- ds %&gt;% filter(expMode==\"train\",catOrder==\"orig\")\nhead(dst)\n\n# A tibble: 6 × 16\n     id condit catOrder feedbackType expMode trial gt.train vb     band  bandInt\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1     1 Varied orig     continuous   train       2        1 1000-… 5        1000\n2     1 Varied orig     continuous   train       3        2 1200-… 6        1200\n3     1 Varied orig     continuous   train       4        3 800-1… 4         800\n4     1 Varied orig     continuous   train       5        4 1000-… 5        1000\n5     1 Varied orig     continuous   train       6        5 800-1… 4         800\n6     1 Varied orig     continuous   train       7        6 1000-… 5        1000\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, input &lt;dbl&gt;, vx &lt;dbl&gt;,\n#   dist &lt;dbl&gt;, vxb &lt;dbl&gt;\n\nCodecolnames(dst)\n\n [1] \"id\"           \"condit\"       \"catOrder\"     \"feedbackType\" \"expMode\"     \n [6] \"trial\"        \"gt.train\"     \"vb\"           \"band\"         \"bandInt\"     \n[11] \"lowBound\"     \"highBound\"    \"input\"        \"vx\"           \"dist\"        \n[16] \"vxb\"         \n\nCodevTrainTrial &lt;- dst %&gt;% filter(condit==\"Varied\",gt.train&lt;=84) %&gt;% group_by(gt.train,vb) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=5,labels=c(1:5)))\n\nbinTrainTrial &lt;- dst %&gt;% filter(gt.train&lt;=83) %&gt;% group_by(gt.train,vb,condit) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=6,labels=c(1:6)))\n\n\ntMax=84\nbandVec &lt;- rep(c(800,1000,1200),each=tMax/3)\nbandVec &lt;- bandVec[sample(1:length(bandVec),tMax,replace=FALSE)]\n\ntrainTrials &lt;- dst %&gt;% filter(gt.train&lt;=tMax) %&gt;% group_by(condit,gt.train,vb,bandInt,input) %&gt;% summarise(vx=mean(vx)) \n\ntv &lt;- trainTrials %&gt;% filter(condit==\"Varied\") %&gt;% group_by(gt.train) %&gt;% mutate(bandInt2=bandVec[gt.train]) %&gt;% filter(bandInt==bandInt2) %&gt;% select(-bandInt2) %&gt;% \n  rbind(.,trainTrials %&gt;% filter(condit==\"Constant\"))\n\nhead(dst)\n\n# A tibble: 6 × 16\n     id condit catOrder feedbackType expMode trial gt.train vb     band  bandInt\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1     1 Varied orig     continuous   train       2        1 1000-… 5        1000\n2     1 Varied orig     continuous   train       3        2 1200-… 6        1200\n3     1 Varied orig     continuous   train       4        3 800-1… 4         800\n4     1 Varied orig     continuous   train       5        4 1000-… 5        1000\n5     1 Varied orig     continuous   train       6        5 800-1… 4         800\n6     1 Varied orig     continuous   train       7        6 1000-… 5        1000\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, input &lt;dbl&gt;, vx &lt;dbl&gt;,\n#   dist &lt;dbl&gt;, vxb &lt;dbl&gt;\n\nCodedst[1:2,]\n\n# A tibble: 2 × 16\n     id condit catOrder feedbackType expMode trial gt.train vb     band  bandInt\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1     1 Varied orig     continuous   train       2        1 1000-… 5        1000\n2     1 Varied orig     continuous   train       3        2 1200-… 6        1200\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, input &lt;dbl&gt;, vx &lt;dbl&gt;,\n#   dist &lt;dbl&gt;, vxb &lt;dbl&gt;\n\n\n\n\nCode# grouping by condit, display count of each trial in dst\n# dst %&gt;% group_by(condit,trial) %&gt;% summarise(n=n())\n# dst %&gt;% group_by(condit,gt.train) %&gt;% summarise(n=n())\n\n# display histogram with count on x axis\n#dst %&gt;% ggplot(aes(gt.train)) + geom_histogram() + facet_wrap(~condit)\n\nggplot(dst %&gt;% filter(gt.train&lt;=84), aes(x = gt.train, y = vx,color=vb)) +\n  geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n  stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+facet_wrap(~condit)\n\n\n\nCodeggplot(binTrainTrial, aes(x = gt.trainBin, y = vx,color=vb)) +\n  geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n  stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+facet_wrap(~condit)\n\n\n\nCode# ggplot(vTrainTrial, aes(x = gt.train, y = vx,color=vb)) +\n#   geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n#   stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n#   stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)\n# \n# # plot vx and sdVx over gt.train, separate facets for vx and sdVx\n# vTrainTrial %&gt;% pivot_longer(cols=c(vx,sdVx),names_to=\"vxType\",values_to=\"vxVal\") %&gt;% \n#   ggplot(aes(gt.train,vxVal,color=vb)) + geom_line() + \n#   stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n#   stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+\n#   facet_wrap(~vxType, scale=\"free_y\")\n\n\n\nCodeinput.activation&lt;-function(x.target, c){\n  return(exp(-1*c*(x.target-inputNodes)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, c){\n  return(weights%*%input.activation(x.target, c))\n}\n\nmean.prediction&lt;-function(x.target, weights, association.parameter){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, c,trainVec){\n  #trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, c)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, c)\n  mOver = mean.prediction(xOver, weights, c)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n  \nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  alm.train\n}\n\ntrainTest.alm&lt;-function(dat, c=0.05, lr=0.5, weights,testVec){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  almPred &lt;- sapply(testVec,mean.prediction,weights,c)\n  examPred &lt;- sapply(testVec,exam.prediction,weights,c,trainVec=c(1,sort(unique(dat$input))))\n  list(almPred=almPred,examPred=examPred)\n}\n\nwrap_alm &lt;- function(parms,dat, weights){\n    c=parms[1]; lr=parms[2]\n   pred=train.alm(dat, c=c, lr=lr, weights=weights)\n   sqrt(mean((dat$vx -pred)^2))\n}\n\nwrap_optim &lt;- function(dat,wm){\n  bounds_lower &lt;- c(.0000001, .00001)\n  bounds_upper &lt;- c(5, 5)\n\n optim(c(.1, .2),\n   fn = wrap_alm,\n   dat = dat, weights = wm,\n   method = \"L-BFGS-B\",\n   lower = bounds_lower,\n   upper = bounds_upper,\n   control = list(maxit = 1e4, pgtol = 0, factr = 0)\n )\n}\n\n\n\nCodeinputNodes = seq(1,7,1)  # \noutputNodes = seq(50,1600,50)\nwm=matrix(.00001,nrow=length(outputNodes),ncol=length(inputNodes))\n\n\nfitVaried &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% wrap_optim(.,wm)\nfitConstant &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% wrap_optim(.,wm)\n\n# call train.alm with the optimized parameters\nvPred &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% cbind(., pred=train.alm(., c=fitVaried$par[1], lr=fitVaried$par[2], weights=wm))\n\ncPred &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% cbind(., pred=train.alm(., c=fitConstant$par[1], lr=fitConstant$par[2], weights=wm))\n\n# plot the results, showing gt.train on x axis, and separate lines with vx and pred\nvPred %&gt;% ggplot(aes(gt.train,vx)) + geom_line() + \ngeom_line(aes(y=pred),color=\"red\") + facet_wrap(~bandInt, scale=\"free_y\")+ggtitle(\"Varied training and predictions\")\n\ncPred %&gt;% ggplot(aes(gt.train,vx)) + geom_line() +\ngeom_line(aes(y=pred),color=\"red\") + facet_wrap(~bandInt, scale=\"free_y\")+ggtitle(\"Constant training and predictions\")\n\n# pred &lt;- train.alm(dat, c = .05, lr = .2, wm)\n# sqrt(mean((dat$vx - pred)^2))\n\n# pred &lt;- train.alm(dat, c = .113, lr = .048, wm)\n# sqrt(mean((dat$vx - pred)^2))\n\n# c=.05; lr=.5; y.new=dat$vx[1]; x.new=dat$input[1]; weights=wm; i=1\n#dat=tv %&gt;% filter(condit==\"Varied\")\n\n\n\nCodetestVec=seq(2,7)\nvarTestPred &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;%\n  trainTest.alm(.,.113,.0488,wm,testVec)\n  #trainTest.alm(.,fitVaried$par[1],fitVaried$par[2],wm,testVec)\nvarTestPred\n\n\ntestVec=seq(2,7)\nconTestPred &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;%\n  trainTest.alm(.,.0008,.144,wm,testVec)\n  #trainTest.alm(.,fitVaried$par[1],fitVaried$par[2],wm,testVec)\nconTestPred"
  },
  {
    "objectID": "Model/group_fits.html#test-fold",
    "href": "Model/group_fits.html#test-fold",
    "title": "Group Level Fits",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,data.table,here)\noptions(dplyr.summarise.inform=FALSE)\n\n# select first row for each id in d, then create histogram for nTrain\n#  d  %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ggplot(aes(nTrain)) + geom_histogram() + facet_wrap(~condit)\n\nd &lt;- readRDS(here(\"data/dPrune-01-19-23.rds\"))\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\n# unique(dtest[dtest$nd==4,]$sbjCode) # 7 in wrong condition\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\n# for any id that has at least 1 nBand &gt;=5, remove all rows with that id. \ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,catOrder,feedbackType,vb,band,lowBound,highBound,input) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")\n\nds &lt;- d %&gt;% filter(expMode %in% c(\"train\",\"train-Nf\",\"test-Nf\",\"test-train-nf\")) %&gt;% \nfilter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) %&gt;% \nselect(id,condit,catOrder,feedbackType,expMode,trial,gt.train,vb,band,bandInt,lowBound,highBound,input,vx,dist,vxb) \nhead(ds,6)\n\n# A tibble: 6 × 16\n     id condit catOrder feedbackType expMode trial gt.train vb     band  bandInt\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1     1 Varied orig     continuous   train       2        1 1000-… 5        1000\n2     1 Varied orig     continuous   train       3        2 1200-… 6        1200\n3     1 Varied orig     continuous   train       4        3 800-1… 4         800\n4     1 Varied orig     continuous   train       5        4 1000-… 5        1000\n5     1 Varied orig     continuous   train       6        5 800-1… 4         800\n6     1 Varied orig     continuous   train       7        6 1000-… 5        1000\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, input &lt;dbl&gt;, vx &lt;dbl&gt;,\n#   dist &lt;dbl&gt;, vxb &lt;dbl&gt;\n\nCodedst &lt;- ds %&gt;% filter(expMode==\"train\",catOrder==\"orig\")\nhead(dst)\n\n# A tibble: 6 × 16\n     id condit catOrder feedbackType expMode trial gt.train vb     band  bandInt\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1     1 Varied orig     continuous   train       2        1 1000-… 5        1000\n2     1 Varied orig     continuous   train       3        2 1200-… 6        1200\n3     1 Varied orig     continuous   train       4        3 800-1… 4         800\n4     1 Varied orig     continuous   train       5        4 1000-… 5        1000\n5     1 Varied orig     continuous   train       6        5 800-1… 4         800\n6     1 Varied orig     continuous   train       7        6 1000-… 5        1000\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, input &lt;dbl&gt;, vx &lt;dbl&gt;,\n#   dist &lt;dbl&gt;, vxb &lt;dbl&gt;\n\nCodecolnames(dst)\n\n [1] \"id\"           \"condit\"       \"catOrder\"     \"feedbackType\" \"expMode\"     \n [6] \"trial\"        \"gt.train\"     \"vb\"           \"band\"         \"bandInt\"     \n[11] \"lowBound\"     \"highBound\"    \"input\"        \"vx\"           \"dist\"        \n[16] \"vxb\"         \n\nCodevTrainTrial &lt;- dst %&gt;% filter(condit==\"Varied\",gt.train&lt;=84) %&gt;% group_by(gt.train,vb) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=5,labels=c(1:5)))\n\nbinTrainTrial &lt;- dst %&gt;% filter(gt.train&lt;=83) %&gt;% group_by(gt.train,vb,condit) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=6,labels=c(1:6)))\n\n\ntMax=84\nbandVec &lt;- rep(c(800,1000,1200),each=tMax/3)\nbandVec &lt;- bandVec[sample(1:length(bandVec),tMax,replace=FALSE)]\n\ntrainTrials &lt;- dst %&gt;% filter(gt.train&lt;=tMax) %&gt;% group_by(condit,gt.train,vb,bandInt,input) %&gt;% summarise(vx=mean(vx)) \n\ntv &lt;- trainTrials %&gt;% filter(condit==\"Varied\") %&gt;% group_by(gt.train) %&gt;% mutate(bandInt2=bandVec[gt.train]) %&gt;% filter(bandInt==bandInt2) %&gt;% select(-bandInt2) %&gt;% \n  rbind(.,trainTrials %&gt;% filter(condit==\"Constant\"))\n\nhead(dst)\n\n# A tibble: 6 × 16\n     id condit catOrder feedbackType expMode trial gt.train vb     band  bandInt\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1     1 Varied orig     continuous   train       2        1 1000-… 5        1000\n2     1 Varied orig     continuous   train       3        2 1200-… 6        1200\n3     1 Varied orig     continuous   train       4        3 800-1… 4         800\n4     1 Varied orig     continuous   train       5        4 1000-… 5        1000\n5     1 Varied orig     continuous   train       6        5 800-1… 4         800\n6     1 Varied orig     continuous   train       7        6 1000-… 5        1000\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, input &lt;dbl&gt;, vx &lt;dbl&gt;,\n#   dist &lt;dbl&gt;, vxb &lt;dbl&gt;\n\nCodedst[1:2,]\n\n# A tibble: 2 × 16\n     id condit catOrder feedbackType expMode trial gt.train vb     band  bandInt\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1     1 Varied orig     continuous   train       2        1 1000-… 5        1000\n2     1 Varied orig     continuous   train       3        2 1200-… 6        1200\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, input &lt;dbl&gt;, vx &lt;dbl&gt;,\n#   dist &lt;dbl&gt;, vxb &lt;dbl&gt;\n\n\n\n\nCode# grouping by condit, display count of each trial in dst\n# dst %&gt;% group_by(condit,trial) %&gt;% summarise(n=n())\n# dst %&gt;% group_by(condit,gt.train) %&gt;% summarise(n=n())\n\n# display histogram with count on x axis\n#dst %&gt;% ggplot(aes(gt.train)) + geom_histogram() + facet_wrap(~condit)\n\nggplot(dst %&gt;% filter(gt.train&lt;=84), aes(x = gt.train, y = vx,color=vb)) +\n  geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n  stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+facet_wrap(~condit)\n\n\n\nCodeggplot(binTrainTrial, aes(x = gt.trainBin, y = vx,color=vb)) +\n  geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n  stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+facet_wrap(~condit)\n\n\n\nCode# ggplot(vTrainTrial, aes(x = gt.train, y = vx,color=vb)) +\n#   geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n#   stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n#   stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)\n# \n# # plot vx and sdVx over gt.train, separate facets for vx and sdVx\n# vTrainTrial %&gt;% pivot_longer(cols=c(vx,sdVx),names_to=\"vxType\",values_to=\"vxVal\") %&gt;% \n#   ggplot(aes(gt.train,vxVal,color=vb)) + geom_line() + \n#   stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n#   stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+\n#   facet_wrap(~vxType, scale=\"free_y\")\n\n\n\nCodeinput.activation&lt;-function(x.target, c){\n  return(exp(-1*c*(x.target-inputNodes)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, c){\n  return(weights%*%input.activation(x.target, c))\n}\n\nmean.prediction&lt;-function(x.target, weights, association.parameter){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, c,trainVec){\n  #trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, c)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, c)\n  mOver = mean.prediction(xOver, weights, c)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n  \nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  alm.train\n}\n\ntrainTest.alm&lt;-function(dat, c=0.05, lr=0.5, weights,testVec){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  almPred &lt;- sapply(testVec,mean.prediction,weights,c)\n  examPred &lt;- sapply(testVec,exam.prediction,weights,c,trainVec=c(1,sort(unique(dat$input))))\n  list(almPred=almPred,examPred=examPred)\n}\n\nwrap_alm &lt;- function(parms,dat, weights){\n    c=parms[1]; lr=parms[2]\n   pred=train.alm(dat, c=c, lr=lr, weights=weights)\n   sqrt(mean((dat$vx -pred)^2))\n}\n\nwrap_optim &lt;- function(dat,wm){\n  bounds_lower &lt;- c(.0000001, .00001)\n  bounds_upper &lt;- c(5, 5)\n\n optim(c(.1, .2),\n   fn = wrap_alm,\n   dat = dat, weights = wm,\n   method = \"L-BFGS-B\",\n   lower = bounds_lower,\n   upper = bounds_upper,\n   control = list(maxit = 1e4, pgtol = 0, factr = 0)\n )\n}\n\n\n\nCodeinputNodes = seq(1,7,1)  # \noutputNodes = seq(50,1600,50)\nwm=matrix(.00001,nrow=length(outputNodes),ncol=length(inputNodes))\n\n\nfitVaried &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% wrap_optim(.,wm)\nfitConstant &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% wrap_optim(.,wm)\n\n# call train.alm with the optimized parameters\nvPred &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% cbind(., pred=train.alm(., c=fitVaried$par[1], lr=fitVaried$par[2], weights=wm))\n\ncPred &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% cbind(., pred=train.alm(., c=fitConstant$par[1], lr=fitConstant$par[2], weights=wm))\n\n# plot the results, showing gt.train on x axis, and separate lines with vx and pred\nvPred %&gt;% ggplot(aes(gt.train,vx)) + geom_line() + \ngeom_line(aes(y=pred),color=\"red\") + facet_wrap(~bandInt, scale=\"free_y\")+ggtitle(\"Varied training and predictions\")\n\ncPred %&gt;% ggplot(aes(gt.train,vx)) + geom_line() +\ngeom_line(aes(y=pred),color=\"red\") + facet_wrap(~bandInt, scale=\"free_y\")+ggtitle(\"Constant training and predictions\")\n\n# pred &lt;- train.alm(dat, c = .05, lr = .2, wm)\n# sqrt(mean((dat$vx - pred)^2))\n\n# pred &lt;- train.alm(dat, c = .113, lr = .048, wm)\n# sqrt(mean((dat$vx - pred)^2))\n\n# c=.05; lr=.5; y.new=dat$vx[1]; x.new=dat$input[1]; weights=wm; i=1\n#dat=tv %&gt;% filter(condit==\"Varied\")\n\n\n\nCodetestVec=seq(2,7)\nvarTestPred &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;%\n  trainTest.alm(.,.113,.0488,wm,testVec)\n  #trainTest.alm(.,fitVaried$par[1],fitVaried$par[2],wm,testVec)\nvarTestPred\n\n\ntestVec=seq(2,7)\nconTestPred &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;%\n  trainTest.alm(.,.0008,.144,wm,testVec)\n  #trainTest.alm(.,fitVaried$par[1],fitVaried$par[2],wm,testVec)\nconTestPred"
  },
  {
    "objectID": "Misc/quartile_splits.html",
    "href": "Misc/quartile_splits.html",
    "title": "Comparing, High/Low performers",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,lme4,emmeans,here,knitr,kableExtra,gt,gghalves)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\n\ntrain &lt;- e1 |&gt; filter(expMode %in% c(\"train\"))\ntrainAvg &lt;- train %&gt;% group_by(id, condit, vb, bandInt,trainStage) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist))\ntest &lt;- e1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\"))\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist),sdist=mean(sdist))\n\n\n\nCodetestAvg %&gt;% \n  group_by(condit,vb) %&gt;%  \n  reframe(enframe(quantile(dist, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"dist\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=dist,names_prefix=\"Q_\") |&gt;\n  group_by(vb,condit) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean)))\n\n# A tibble: 12 × 7\n# Groups:   vb [6]\n   vb    condit `Q_0%_mean` `Q_25%_mean` `Q_50%_mean` `Q_75%_mean` `Q_100%_mean`\n   &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 100-… Const…       0            110.          205.         393.          849.\n 2 100-… Varied       1.81          82.4         249.         634.         1260.\n 3 350-… Const…       7.34          91.5         149.         254.          757.\n 4 350-… Varied       4.21          81.2         180.         433.          998.\n 5 600-… Const…       9.52          78.9         117.         196.          523.\n 6 600-… Varied      28.4          105.          182.         284.          787.\n 7 800-… Const…      12.9           86.3         145.         252.          896.\n 8 800-… Varied       0.670        128.          195.         308.          634.\n 9 1000… Const…       2.99         122.          200.         317.          824.\n10 1000… Varied      14.5          111.          182.         298.          540.\n11 1200… Const…      26.9          158.          240.         398.          909.\n12 1200… Varied       3.65         141.          209.         320.          594.\n\nCodetestAvg %&gt;% \n  group_by(condit,vb) %&gt;%  \n  reframe(enframe(quantile(dist, seq(0,1,1/8)), \"quantile\", \"dist\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=dist,names_prefix=\"Q_\") |&gt;\n  group_by(vb,condit) |&gt;\n  summarise(across(starts_with(\"Q\"),round,1)) %&gt;% kable(format=\"html\",escape=F) %&gt;% kable_styling() \n\n\n\nvb\ncondit\nQ_0%\nQ_12.5%\nQ_25%\nQ_37.5%\nQ_50%\nQ_62.5%\nQ_75%\nQ_87.5%\nQ_100%\n\n\n\n100-300\nConstant\n0.0\n46.1\n109.7\n141.1\n205.0\n261.5\n393.2\n603.3\n849.4\n\n\n100-300\nVaried\n1.8\n32.1\n82.4\n170.8\n248.9\n520.7\n633.9\n852.8\n1259.7\n\n\n350-550\nConstant\n7.3\n50.9\n91.5\n111.8\n149.5\n187.3\n253.9\n440.1\n757.5\n\n\n350-550\nVaried\n4.2\n41.4\n81.2\n130.9\n180.1\n279.5\n433.1\n699.3\n997.8\n\n\n600-800\nConstant\n9.5\n59.2\n78.9\n98.7\n117.0\n148.5\n195.6\n305.8\n522.6\n\n\n600-800\nVaried\n28.4\n68.8\n104.9\n139.1\n181.7\n211.6\n284.2\n478.9\n786.6\n\n\n800-1000\nConstant\n12.9\n45.2\n86.3\n111.7\n145.0\n197.9\n251.9\n364.7\n896.1\n\n\n800-1000\nVaried\n0.7\n70.6\n127.8\n156.8\n195.1\n226.9\n308.2\n385.5\n633.8\n\n\n1000-1200\nConstant\n3.0\n83.0\n122.2\n156.1\n200.1\n236.3\n317.2\n441.5\n823.7\n\n\n1000-1200\nVaried\n14.5\n69.4\n110.7\n147.3\n181.7\n217.6\n298.3\n368.9\n539.9\n\n\n1200-1400\nConstant\n26.9\n125.5\n157.9\n193.0\n240.5\n308.4\n397.7\n503.8\n908.9\n\n\n1200-1400\nVaried\n3.6\n96.7\n141.3\n170.6\n208.7\n287.1\n320.2\n389.0\n593.9\n\n\n\n\n\n\nCoderaw_table &lt;- testAvg %&gt;% \n  group_by(condit,vb) %&gt;%  \n  reframe(enframe(quantile(dist, seq(0,1,1/8)), \"quantile\", \"dist\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=dist,names_prefix=\"Q_\") |&gt;\n  group_by(vb,condit) |&gt;\n  summarise(across(starts_with(\"Q\"), round,1))\n\n\nlong_data &lt;- raw_table %&gt;% \n  pivot_longer(\n    cols = starts_with(\"Q\"),\n    names_to = \"Quartile\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(Quart = str_remove_all(Quartile, \"Q_|%\"), # Remove \"Q_\" and \"%\"\n         Quart = as.numeric(Quart), # Convert Quart to numeric\n         Quart = factor(Quart, levels = sort(unique(Quart)))) \n\n\n\n\n\n\nbold_lower &lt;- function(data, by_group) {\n  ifelse(data &lt; by_group, cell_spec(data, \"html\", bold = T), as.character(data))\n}\n\n# Separate data by condition\nconstant_data &lt;- raw_table %&gt;% filter(condit == \"Constant\")\nvaried_data &lt;- raw_table %&gt;% filter(condit == \"Varied\")\n\n# Apply function to varied data\nvaried_data &lt;- varied_data %&gt;%\n  group_by(vb) %&gt;%\n  mutate(across(starts_with(\"Q\"), function(.x) {\n    col_name &lt;- cur_column()\n    by_group &lt;- constant_data[[col_name]][constant_data$vb == first(vb)]\n    bold_lower(.x, by_group)\n  }, .names = \"{.col}\"))\n\n# Format the constant_data to match the varied_data\nconstant_data &lt;- constant_data %&gt;%\n  group_by(vb) %&gt;%\n  mutate(across(starts_with(\"Q\"), ~as.character(.x), .names = \"{.col}\"))\n\n# Join the constant and varied data frames back together\nfinal_table &lt;- bind_rows(constant_data, varied_data) %&gt;%\n  select(vb, condit, ends_with(\"%\")) %&gt;%\n  arrange(vb, condit)\n\n# Print the table\nfinal_table %&gt;% kable(\"html\", escape = F) %&gt;% kable_styling() %&gt;%\n  pack_rows(index = table(final_table$vb))\n\n\n\nvb\ncondit\nQ_0%\nQ_12.5%\nQ_25%\nQ_37.5%\nQ_50%\nQ_62.5%\nQ_75%\nQ_87.5%\nQ_100%\n\n\n\n100-300\n\n\n100-300\nConstant\n0\n46.1\n109.7\n141.1\n205\n261.5\n393.2\n603.3\n849.4\n\n\n100-300\nVaried\n1.8\n32.1\n82.4\n170.8\n248.9\n520.7\n633.9\n852.8\n1259.7\n\n\n350-550\n\n\n350-550\nConstant\n7.3\n50.9\n91.5\n111.8\n149.5\n187.3\n253.9\n440.1\n757.5\n\n\n350-550\nVaried\n4.2\n41.4\n81.2\n130.9\n180.1\n279.5\n433.1\n699.3\n997.8\n\n\n600-800\n\n\n600-800\nConstant\n9.5\n59.2\n78.9\n98.7\n117\n148.5\n195.6\n305.8\n522.6\n\n\n600-800\nVaried\n28.4\n68.8\n104.9\n139.1\n181.7\n211.6\n284.2\n478.9\n786.6\n\n\n800-1000\n\n\n800-1000\nConstant\n12.9\n45.2\n86.3\n111.7\n145\n197.9\n251.9\n364.7\n896.1\n\n\n800-1000\nVaried\n0.7\n70.6\n127.8\n156.8\n195.1\n226.9\n308.2\n385.5\n633.8\n\n\n1000-1200\n\n\n1000-1200\nConstant\n3\n83\n122.2\n156.1\n200.1\n236.3\n317.2\n441.5\n823.7\n\n\n1000-1200\nVaried\n14.5\n69.4\n110.7\n147.3\n181.7\n217.6\n298.3\n368.9\n539.9\n\n\n1200-1400\n\n\n1200-1400\nConstant\n26.9\n125.5\n157.9\n193\n240.5\n308.4\n397.7\n503.8\n908.9\n\n\n1200-1400\nVaried\n3.6\n96.7\n141.3\n170.6\n208.7\n287.1\n320.2\n389\n593.9\n\n\n\n\n\n\nCodeggplot(long_data, aes(x = Quart, y = Value, color = condit, group = condit)) +\n  facet_wrap(~ vb) +\n  geom_line() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  labs(title = \"Comparison of Varied and Constant Conditions across Octiles\", \n       x = \"Octile\", \n       y = \"Distance from target\")\n\n\n\n\n\nCodemodel &lt;- lmer(Value ~ condit * Quart + (1|vb), data = long_data)\nsummary(model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Value ~ condit * Quart + (1 | vb)\n   Data: long_data\n\nREML criterion at convergence: 1115.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5486 -0.4061  0.0253  0.3715  4.0652 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n vb       (Intercept) 2096     45.79   \n Residual             8970     94.71   \nNumber of obs: 108, groups:  vb, 6\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)               9.933     42.946   0.231\nconditVaried             -1.067     54.680  -0.020\nQuart12.5                58.383     54.680   1.068\nQuart25                  97.817     54.680   1.789\nQuart37.5               125.467     54.680   2.295\nQuart50                 166.250     54.680   3.040\nQuart62.5               213.383     54.680   3.902\nQuart75                 291.650     54.680   5.334\nQuart87.5               433.267     54.680   7.924\nQuart100                783.100     54.680  14.321\nconditVaried:Quart12.5   -4.083     77.330  -0.053\nconditVaried:Quart25      1.367     77.330   0.018\nconditVaried:Quart37.5   18.250     77.330   0.236\nconditVaried:Quart50     24.250     77.330   0.314\nconditVaried:Quart62.5   68.317     77.330   0.883\nconditVaried:Quart75     79.133     77.330   1.023\nconditVaried:Quart87.5   86.933     77.330   1.124\nconditVaried:Quart100     9.983     77.330   0.129\n\nCodelibrary(multcomp)\ncomparison &lt;- glht(model, linfct = mcp(condit = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = Value ~ condit * Quart + (1 | vb), data = long_data)\n\nLinear Hypotheses:\n                       Estimate Std. Error z value Pr(&gt;|z|)\nVaried - Constant == 0   -1.067     54.680   -0.02    0.984\n(Adjusted p values reported -- single-step method)\n\n\n\nCoderaw_table &lt;- test %&gt;% \n  group_by(id,condit,vb) %&gt;%  \n  reframe(enframe(quantile(dist, seq(0,1,1/6)), \"quantile\", \"dist\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=dist,names_prefix=\"Q_\") |&gt;\n  group_by(id,vb,condit) |&gt;\n  summarise(across(starts_with(\"Q\"), round,1))\n\n\nlong_data &lt;- raw_table %&gt;% \n  pivot_longer(\n    cols = starts_with(\"Q\"),\n    names_to = \"Quartile\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(Quart = str_remove_all(Quartile, \"Q_|%\"), # Remove \"Q_\" and \"%\"\n         Quart = as.numeric(Quart), # Convert Quart to numeric\n         Quart = factor(Quart, levels = sort(unique(Quart)))) \n\n\nggplot(long_data, aes(x = Quart, y = Value, fill = condit, color=condit,group = condit)) +\n  facet_wrap(~ vb) +\n  stat_summary(geom=\"line\",fun=mean)+\n  stat_summary(geom=\"errorbar\",fun.data=mean_se)+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  labs(title = \"Comparison of Varied and Constant Conditions across Octiles\", \n       x = \"Octile\", \n       y = \"Distance from target\")\n\n\n\nCodemodel &lt;- lmer(Value ~ condit * Quart + (1|vb)+(1|id), data = long_data)\nsummary(model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Value ~ condit * Quart + (1 | vb) + (1 | id)\n   Data: long_data\n\nREML criterion at convergence: 91516.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5591 -0.5577 -0.0522  0.4545  7.2509 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 16898    129.99  \n vb       (Intercept)  2466     49.66  \n Residual             36196    190.25  \nNumber of obs: 6832, groups:  id, 166; vb, 6\n\nFixed effects:\n                           Estimate Std. Error t value\n(Intercept)                   32.90      25.86   1.273\nconditVaried                  11.29      23.65   0.477\nQuart16.66667                 44.20      11.80   3.746\nQuart33.33333                 96.86      11.80   8.209\nQuart50                      158.69      11.80  13.449\nQuart66.66667                234.59      11.80  19.882\nQuart83.33333                334.62      11.80  28.360\nQuart100                     521.83      11.80  44.226\nconditVaried:Quart16.66667    17.30      17.26   1.002\nconditVaried:Quart33.33333    26.67      17.26   1.545\nconditVaried:Quart50          28.93      17.26   1.676\nconditVaried:Quart66.66667    30.17      17.26   1.748\nconditVaried:Quart83.33333    39.94      17.26   2.314\nconditVaried:Quart100         40.22      17.26   2.330\n\nCodecomparison &lt;- glht(model, linfct = mcp(condit = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = Value ~ condit * Quart + (1 | vb) + (1 | id), \n    data = long_data)\n\nLinear Hypotheses:\n                       Estimate Std. Error z value Pr(&gt;|z|)\nVaried - Constant == 0    11.29      23.65   0.477    0.633\n(Adjusted p values reported -- single-step method)"
  },
  {
    "objectID": "Misc/gam_discrim.html",
    "href": "Misc/gam_discrim.html",
    "title": "GAM",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,lme4,emmeans,here,knitr,kableExtra,gt,mgcv,data.table)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\n\n\ntest &lt;- e1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\"))\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist)) %&gt;% data.table()\n\n\n\nCodetestAvg %&gt;% ggplot(aes(x=bandInt,y=vx))+geom_point(color=\"grey\")+geom_line()+theme_bw()\n\ntm &lt;- gam(vx ~s(bandInt,k=3),sp=0,data=testAvg)\nsummary(tm)\n\n\nresults &lt;- tibble(k = integer(), r_sq = numeric(), edf = numeric(), p_value = numeric(), scale = numeric(), deviance = numeric(), null_deviance = numeric())\n\n# Iterate through different k values\nfor (k in 1:6) {\n  temp_result &lt;- tryCatch({\n    # Fit the GAM model\n    tm &lt;- gam(vx ~ s(bandInt, k = k), sp = 0, data = testAvg)\n    model_sum &lt;- summary(tm)\n    \n    # Extract additional metrics from the model summary and the model itself\n    edf &lt;- model_sum$s.table[1]\n    p_value &lt;- model_sum$s.pv[1]\n    scale &lt;- tm$scale\n    deviance &lt;- tm$deviance\n    null_deviance &lt;- tm$null.deviance\n    \n    # Return data frame with results\n    tibble(k = k, r_sq = model_sum$r.sq, edf = edf, p_value = p_value, scale = scale, deviance = deviance, null_deviance = null_deviance)\n  }, error = function(e) {\n    # Print error message\n    cat(paste(\"Skipping k =\", k, \"due to error:\", conditionMessage(e)), \"\\n\")\n    \n    # Return data frame with NA values\n    tibble(k = k, r_sq = NA_real_, edf = NA_real_, p_value = NA_real_, scale = NA_real_, deviance = NA_real_, null_deviance = NA_real_)\n  })\n  \n  # Add results to data frame\n  results &lt;- bind_rows(results, temp_result)\n}\n\nprint(results)\n\n# Plot R-squared\nresults %&gt;%\n  ggplot(aes(x = k, y = r_sq)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"k\", y = \"R-squared\") +\n  theme_minimal()\n\n# Plot deviance\nresults %&gt;%\n  ggplot(aes(x = k, y = deviance)) +\n  geom_line(color = \"red\") +\n  labs(x = \"k\", y = \"Deviance\") +\n  theme_minimal()\n\n\n\nCode# Iterate through different k values\nfor (k in 1:6) {\n  for (group in unique(testAvg$condit)) {\n    \n    temp_result &lt;- tryCatch({\n      # Filter data by group\n      group_data &lt;- testAvg %&gt;% filter(condit == group)\n      \n      # Fit the GAM model with interaction and more flexible splines\n      tm &lt;- gam(vx ~ s(bandInt, k = k, bs = \"cr\"), data = group_data)\n      model_sum &lt;- summary(tm)\n      \n      # Extract additional metrics from the model summary and the model itself\n      edf &lt;- model_sum$s.table[1]\n      p_value &lt;- model_sum$s.pv[1]\n      scale &lt;- tm$scale\n      deviance &lt;- tm$deviance\n      null_deviance &lt;- tm$null.deviance\n      threshold &lt;- NA_real_\n      \n      # Attempt to calculate discrimination threshold\n      tryCatch({\n        # Value of bandInt where the derivative of the fitted spline is highest\n        bandInt_values &lt;- seq(min(group_data$bandInt), max(group_data$bandInt), length.out = 1000)\n        fitted_spline &lt;- predict(tm, newdata = data.frame(bandInt = bandInt_values, condit_num = ifelse(group == \"Varied\", 1, 0)), type = \"terms\")\n        fitted_derivative &lt;- diff(fitted_spline[,1]) / diff(bandInt_values)\n        threshold &lt;- bandInt_values[which.max(fitted_derivative)]\n      }, error = function(e) {\n        cat(paste(\"Failed to calculate threshold for k =\", k, \"in group\", group, \"due to error:\", conditionMessage(e)), \"\\n\")\n      })\n      \n      # Return data frame with results\n      tibble(group = group, k = k, r_sq = model_sum$r.sq, edf = edf, p_value = p_value, scale = scale, deviance = deviance, null_deviance = null_deviance, threshold = threshold)\n    }, error = function(e) {\n      # Print error message\n      cat(paste(\"Skipping k =\", k, \"for group\", group, \"due to error:\", conditionMessage(e)), \"\\n\")\n      \n      # Return data frame with NA values\n      tibble(group = group, k = k, r_sq = NA_real_, edf = NA_real_, p_value = NA_real_, scale = NA_real_, deviance = NA_real_, null_deviance = NA_real_, threshold = NA_real_)\n    })\n    \n    # Add results to data frame\n    results &lt;- bind_rows(results, temp_result)\n  }\n}\nprint(results)\n\n\n\nCodeest_gam &lt;- function(k, train_data, test_data, group) \n{\n  # Filter by group\n  train_data_group &lt;- train_data %&gt;% filter(condit == group)\n  test_data_group &lt;- test_data %&gt;% filter(condit == group)\n \n  # Train a model  \n  trained_model &lt;- gam(vx ~ s(bandInt, k = k, bs = \"tp\"), data = train_data_group)\n \n  # Print out the number of rows in train_data_group\n  print(paste(\"Number of rows in train_data_group: \", nrow(train_data_group)))\n  \n  # Predict y for the train datasets\n  predicted_values_train &lt;- predict(trained_model, newdata = train_data_group)\n  print(paste(\"Number of predicted values for train data: \", length(predicted_values_train)))\n\n  # Add the predicted values as a new column\n  train_data_group &lt;- train_data_group %&gt;% mutate(vx_hat = predicted_values_train, type = \"Train\")\n \n  # Repeat the same for the test datasets\n  predicted_values_test &lt;- predict(trained_model, newdata = test_data_group)\n  print(paste(\"Number of predicted values for test data: \", length(predicted_values_test)))\n\n  test_data_group &lt;- test_data_group %&gt;% mutate(vx_hat = predicted_values_test, type = \"Test\")\n \n  # Combine before returning\n  return_data &lt;- bind_rows(train_data_group, test_data_group) %&gt;% \n                 mutate(num_knots = k, group = group)\n \n  return(return_data)\n}\n\n# define k-values\nk_values &lt;- 1:5\n\n# Split the data into training and testing datasets\nset.seed(123)\ndata_split &lt;- testAvg %&gt;% \n  resample_partition(c(Train = 0.75, Test = 0.25))\n\n# Extract the data frames from the resample objects\ntrain_data &lt;- as.data.frame(data_split$Train)\ntest_data &lt;- as.data.frame(data_split$Test)\n\n# Use a nested loop to apply the est_gam function over all combinations of groups and k_values\nresults &lt;- map_df(unique(testAvg$condit), ~{\n  group &lt;- .\n  map_df(k_values, ~{\n    est_gam(.x, train_data %&gt;% filter(condit == group), \n            test_data %&gt;% filter(condit == group), group)\n  })\n})\n\n\n\nCode# Create an empty data frame to store results\nresults &lt;- tibble(group = character(), k = integer(), r_sq = numeric(), edf = numeric(), p_value = numeric(), scale = numeric(), deviance = numeric(), null_deviance = numeric())\n\n# Iterate through different k values\nfor (k in 1:6) {\n  # Iterate through different conditions\n  for (condit in unique(testAvg$condit)) {\n    temp_result &lt;- tryCatch({\n      # Subset the data for the current condition\n      data_subset &lt;- testAvg[testAvg$condit == condit, ]\n      \n      # Fit the GAM model\n      tm &lt;- gam(vx ~ s(bandInt, k = k), sp = 0, data = data_subset)\n      model_sum &lt;- summary(tm)\n      \n      # Extract additional metrics from the model summary and the model itself\n      edf &lt;- model_sum$s.table[1]\n      p_value &lt;- model_sum$s.pv[1]\n      scale &lt;- tm$scale\n      deviance &lt;- tm$deviance\n      null_deviance &lt;- tm$null.deviance\n      \n      # Return data frame with results\n      tibble(group = condit, k = k, r_sq = model_sum$r.sq, edf = edf, p_value = p_value, scale = scale, deviance = deviance, null_deviance = null_deviance)\n    }, error = function(e) {\n      # Print error message\n      cat(paste(\"Skipping k =\", k, \"in group\", condit, \"due to error:\", conditionMessage(e)), \"\\n\")\n      \n      # Return data frame with NA values\n      tibble(group = condit, k = k, r_sq = NA_real_, edf = NA_real_, p_value = NA_real_, scale = NA_real_, deviance = NA_real_, null_deviance = NA_real_)\n    })\n    \n    # Add results to data frame\n    results &lt;- bind_rows(results, temp_result)\n  }\n}\n\nprint(results)\n\n# Plot R-squared\nresults %&gt;%\n  ggplot(aes(x = k, y = r_sq, color = group)) +\n  geom_line() +\n  labs(x = \"k\", y = \"R-squared\") +\n  theme_minimal()\n\n# Plot deviance\nresults %&gt;%\n  ggplot(aes(x = k, y = deviance, color = group)) +\n  geom_line() +\n  labs(x = \"k\", y = \"Deviance\") +\n  theme_minimal()\n\n# Add additional plots as necessary\n\n\n\nCode# Create an empty data frame to store results\nresults &lt;- data.frame(k = numeric(), r_sq = numeric(), deviance_explained = numeric())\n\n# Iterate through different k values\nfor (k in 1:6) {\n  temp_result &lt;- tryCatch({\n    # Fit the GAM model\n    tm &lt;- gam(vx ~ s(bandInt, k = k), sp = 0, data = testAvg)\n    model_sum &lt;- summary(tm)\n    \n    # Return data frame with results\n    data.frame(k = k, r_sq = model_sum$r.sq.adj, deviance_explained = model_sum$dev.expl)\n  }, error = function(e) {\n    # Print error message\n    cat(paste(\"Skipping k =\", k, \"due to error:\", conditionMessage(e)), \"\\n\")\n\n    # Return data frame with NA values\n    data.frame(k = k, r_sq = NA, deviance_explained = NA)\n  })\n\n  # Add results to data frame\n  results &lt;- rbind(results, temp_result)\n}\n\n# Check if we have successfully fitted models and results have more than zero rows \nif(nrow(results) &gt; 0){\n  # No need to set column names here, you've already set them when creating the data frames inside the loop\n\n  # Plotting\n  plot(results$k, results$r_sq, type = \"l\", xlab = \"k\", ylab = \"R-squared\", col = \"blue\")\n  lines(results$k, results$deviance_explained, type = \"l\", col = \"red\") \n  legend(\"topright\", legend = c(\"R-squared\", \"Deviance Explained\"), col = c(\"blue\", \"red\"), lty = 1)\n}\n\n\n\nCodegam_fit &lt;- gam(vx ~ s(bandInt, k = 6, bs = \"cr\"), data = testAvg)\ngam_fit$coefficient\n\nbasis_data &lt;- gam_fit %&gt;%  predict(., type = \"lpmatrix\") %&gt;%\n  data.table() %&gt;% \n  .[, bandInt := testAvg[, bandInt]] %&gt;% \n  melt(id.var = \"bandInt\")\n\n\n\n\ndata.table(\n  variable = unique(basis_data$variable)[-1],\n  coef = gam_fit$coefficient[-1]\n) %&gt;% \n.[basis_data, on = \"variable\"] %&gt;% \n.[, .(y_no_int = sum(coef * value)), by = bandInt] %&gt;% \n.[, y_hat := gam_fit$coefficient[1] + vx] %&gt;% \nggplot(data = .) +\n  geom_line(aes(y = vx, x = bandInt, color = \"gam-fitted\")) +\n  geom_line(data = data, aes(y = vx, x = bandInt, color = \"True\")) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"gam-fitted\" = \"red\", \"True\" = \"blue\")\n  ) +\n  ylab(\"y\") +\n  xlab(\"x\") +\n  theme_bw()"
  },
  {
    "objectID": "Misc/data_organize.html",
    "href": "Misc/data_organize.html",
    "title": "Organization",
    "section": "",
    "text": "pacman::p_load(tidyverse,here,knitr,kableExtra,reactable)\noptions(dplyr.summarise.inform=FALSE)\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\")) %&gt;% ungroup()\n\n\nd &lt;- d |&gt; mutate(bandType=case_when(\n  vb %in% c(\"1000-1200\",\"1200-1400\") & condit==\"Constant\" & bandOrder==\"Original\" ~ \"Extrapolation\",\n  vb %in% c(\"100-300\",\"350-550\") & condit==\"Constant\" & bandOrder==\"Reverse\" ~ \"Extrapolation\",\n  (vb %in% c(\"100-300\",\"350-550\",\"600-800\") & bandOrder==\"Original\") |\n  (vb %in% c(\"1200-1400\",\"1000-1200\",\"800-1000\") & bandOrder==\"Reverse\") ~ \"Extrapolation\",\n  \n  (vb %in% c(\"800-1000\",\"1000-1200\",\"1200-1400\") & bandOrder==\"Original\") |\n  (vb %in% c(\"100-300\",\"350-550\",\"600-800\") & bandOrder==\"Reverse\") ~ \"Trained\",\n  TRUE ~ NA_character_\n))\n\nd$bandType &lt;- factor(d$bandType,levels=c(\"Trained\",\"Extrapolation\"))\nd&lt;- d %&gt;% relocate(bandOrder,bandType,.after=bandInt)\n\n\nd &lt;- d |&gt; mutate(Exp=case_when(\n  fb==\"Continuous\" & bandOrder==\"Original\" ~ \"E1\",\n  fb==\"Continuous\" & bandOrder==\"Reverse\" ~ \"E2\",\n  fb==\"Ordinal\" ~ \"E3\",\n  TRUE ~ NA_character_\n))\n\n\nd &lt;- d |&gt; mutate(vxC = ifelse(vx&gt;1800,1800,vx))\nd &lt;- d %&gt;% relocate(vxC,.after=vx)\nd &lt;- d |&gt; select(-goodThrow, -trainVec, -fullCond)\n\n# test &lt;- d |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\"))\n# rtest &lt;- test |&gt; filter(bandOrder==\"Reverse\")\n# rtest |&gt; group_by(vb,bandType,condit,tOrder) |&gt; summarise(n=n()) \n\n\ne1 &lt;- d |&gt; filter(fb==\"Continuous\" & bandOrder==\"Original\") |&gt; mutate(id=factor(id,levels=unique(id)))\ne2 &lt;- d |&gt; filter(fb==\"Continuous\" & bandOrder==\"Reverse\") |&gt; mutate(id=factor(id,levels=unique(id)))\ne3 &lt;- d |&gt; filter(fb==\"Ordinal\") |&gt; mutate(id=factor(id,levels=unique(id)))\n\ndate.append=\"08-04-23\"\n\n# saveRDS(d,here(paste0(\"data/dAll_\",date.append,\".rds\")))\n# saveRDS(e1,here(paste0(\"data/e1_\",date.append,\".rds\")))\n# saveRDS(e2,here(paste0(\"data/e2_\",date.append,\".rds\")))\n# saveRDS(e3,here(paste0(\"data/e3_\",date.append,\".rds\")))\n\n# save csv versions\n#d %&gt;% write_csv(here(paste0(\"data/dAll_\",date.append,\".csv\")))\n# e1 %&gt;% write_csv(here(paste0(\"data/e1_\",date.append,\".csv\")))\n# e2 %&gt;% write_csv(here(paste0(\"data/e2_\",date.append,\".csv\")))\n# e3 %&gt;% write_csv(here(paste0(\"data/e3_\",date.append,\".csv\")))\n\nhead(d) colnames(d) d %&gt;% select_if(is.factor) %&gt;% colnames()\n\nd %&gt;% select_if(is.factor) %&gt;% select(-sbjCode,-id) %&gt;% map(levels)\n\n$condit\n[1] \"Constant\" \"Varied\"  \n\n$fb\n[1] \"Continuous\" \"Ordinal\"   \n\n$tOrder\n[1] \"testFirst\"  \"trainFirst\"\n\n$expMode2\n[1] \"Train\"    \"Train-Nf\" \"Test\"     \"Test-Fb\" \n\n$band\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n$vb\n[1] \"100-300\"   \"350-550\"   \"600-800\"   \"800-1000\"  \"1000-1200\" \"1200-1400\"\n\n$bandOrder\n[1] \"Original\" \"Reverse\" \n\n$bandType\n[1] \"Trained\"       \"Extrapolation\"\n\n$vxCat\n [1] \"0\"      \"100\"    \"btw1\"   \"350\"    \"btw2\"   \"600\"    \"800\"    \"1000\"  \n [9] \"1200\"   \"&gt;=1401\"\n\n$prev\n[1] \"100\"  \"350\"  \"600\"  \"800\"  \"1000\" \"1200\"\n\n$bandSeq\n [1] \"100-&gt;100\"   \"350-&gt;100\"   \"600-&gt;100\"   \"100-&gt;350\"   \"350-&gt;350\"  \n [6] \"600-&gt;350\"   \"100-&gt;600\"   \"350-&gt;600\"   \"600-&gt;600\"   \"800-&gt;800\"  \n[11] \"1000-&gt;800\"  \"1200-&gt;800\"  \"800-&gt;1000\"  \"1000-&gt;1000\" \"1200-&gt;1000\"\n[16] \"800-&gt;1200\"  \"1000-&gt;1200\" \"1200-&gt;1200\"\n\n$lowBound\n[1] \"100\"  \"350\"  \"600\"  \"800\"  \"1000\" \"1200\"\n\n$stage\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\"\n\n$expMode\n[1] \"train\"         \"train-Nf\"      \"test-Nf\"       \"test-train-nf\"\n[5] \"test-feedback\"\n\n$trainStage\n[1] \"Beginning\" \"Middle\"    \"End\"       \"Test\"     \n\n$feedback\n[1] \"0\" \"1\"\n\nd %&gt;% select_if(is.factor) %&gt;% select(-sbjCode,-id) %&gt;% droplevels %&gt;% map(levels)\n\n$condit\n[1] \"Constant\" \"Varied\"  \n\n$fb\n[1] \"Continuous\" \"Ordinal\"   \n\n$tOrder\n[1] \"testFirst\"  \"trainFirst\"\n\n$expMode2\n[1] \"Train\"    \"Train-Nf\" \"Test\"     \"Test-Fb\" \n\n$band\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n$vb\n[1] \"100-300\"   \"350-550\"   \"600-800\"   \"800-1000\"  \"1000-1200\" \"1200-1400\"\n\n$bandOrder\n[1] \"Original\" \"Reverse\" \n\n$bandType\n[1] \"Trained\"       \"Extrapolation\"\n\n$vxCat\n [1] \"0\"      \"100\"    \"btw1\"   \"350\"    \"btw2\"   \"600\"    \"800\"    \"1000\"  \n [9] \"1200\"   \"&gt;=1401\"\n\n$prev\n[1] \"100\"  \"350\"  \"600\"  \"800\"  \"1000\" \"1200\"\n\n$bandSeq\n [1] \"100-&gt;100\"   \"350-&gt;100\"   \"600-&gt;100\"   \"100-&gt;350\"   \"350-&gt;350\"  \n [6] \"600-&gt;350\"   \"100-&gt;600\"   \"350-&gt;600\"   \"600-&gt;600\"   \"800-&gt;800\"  \n[11] \"1000-&gt;800\"  \"1200-&gt;800\"  \"800-&gt;1000\"  \"1000-&gt;1000\" \"1200-&gt;1000\"\n[16] \"800-&gt;1200\"  \"1000-&gt;1200\" \"1200-&gt;1200\"\n\n$lowBound\n[1] \"100\"  \"350\"  \"600\"  \"800\"  \"1000\" \"1200\"\n\n$stage\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\"\n\n$expMode\n[1] \"train\"         \"train-Nf\"      \"test-Nf\"       \"test-train-nf\"\n[5] \"test-feedback\"\n\n$trainStage\n[1] \"Beginning\" \"Middle\"    \"End\"       \"Test\"     \n\n$feedback\n[1] \"0\" \"1\"\n\n\n\nd %&gt;% select_if(is.numeric) |&gt; colnames()\n\n [1] \"trial\"        \"bandInt\"      \"vx\"           \"vxC\"          \"dist\"        \n [6] \"sdist\"        \"vxb\"          \"vxi\"          \"vy\"           \"launchX\"     \n[11] \"launchY\"      \"highBound\"    \"nGoodTrial\"   \"nTrain\"       \"nTestNf\"     \n[16] \"nInt\"         \"nTestF\"       \"nTotal\"       \"gt.train\"     \"gt.bandStage\"\n[21] \"gt.stage\"     \"lastTrain\"    \"lastTrial\"   \n\n\n\nd %&gt;%\n  distinct(id, condit, fb, bandOrder, tOrder) %&gt;%\n  group_by(condit, fb, bandOrder, tOrder) %&gt;%\n  summarise(n = n()) %&gt;%\n  kable()\n\n\n\ncondit\nfb\nbandOrder\ntOrder\nn\n\n\n\nConstant\nContinuous\nOriginal\ntestFirst\n52\n\n\nConstant\nContinuous\nOriginal\ntrainFirst\n38\n\n\nConstant\nContinuous\nReverse\ntestFirst\n28\n\n\nConstant\nContinuous\nReverse\ntrainFirst\n27\n\n\nConstant\nOrdinal\nOriginal\ntestFirst\n51\n\n\nConstant\nOrdinal\nReverse\ntestFirst\n31\n\n\nConstant\nOrdinal\nReverse\ntrainFirst\n28\n\n\nVaried\nContinuous\nOriginal\ntestFirst\n39\n\n\nVaried\nContinuous\nOriginal\ntrainFirst\n37\n\n\nVaried\nContinuous\nReverse\ntestFirst\n25\n\n\nVaried\nContinuous\nReverse\ntrainFirst\n30\n\n\nVaried\nOrdinal\nOriginal\ntestFirst\n39\n\n\nVaried\nOrdinal\nReverse\ntestFirst\n28\n\n\nVaried\nOrdinal\nReverse\ntrainFirst\n18\n\n\n\n\n\n\n# Average trials per subject by condition  \nd %&gt;%\n  group_by(condit, fb, bandOrder, tOrder, id) %&gt;%\n  summarise(n = n()) %&gt;%\n  group_by(condit, fb, bandOrder, tOrder) %&gt;%\n  summarise(mean_trials = mean(n)) %&gt;%\n  kable()\n\n\n\ncondit\nfb\nbandOrder\ntOrder\nmean_trials\n\n\n\nConstant\nContinuous\nOriginal\ntestFirst\n197.6346\n\n\nConstant\nContinuous\nOriginal\ntrainFirst\n199.1316\n\n\nConstant\nContinuous\nReverse\ntestFirst\n198.0357\n\n\nConstant\nContinuous\nReverse\ntrainFirst\n196.2222\n\n\nConstant\nOrdinal\nOriginal\ntestFirst\n198.5294\n\n\nConstant\nOrdinal\nReverse\ntestFirst\n198.6452\n\n\nConstant\nOrdinal\nReverse\ntrainFirst\n198.1429\n\n\nVaried\nContinuous\nOriginal\ntestFirst\n197.7949\n\n\nVaried\nContinuous\nOriginal\ntrainFirst\n197.1351\n\n\nVaried\nContinuous\nReverse\ntestFirst\n198.4400\n\n\nVaried\nContinuous\nReverse\ntrainFirst\n197.9000\n\n\nVaried\nOrdinal\nOriginal\ntestFirst\n199.9744\n\n\nVaried\nOrdinal\nReverse\ntestFirst\n197.5714\n\n\nVaried\nOrdinal\nReverse\ntrainFirst\n196.2222\n\n\n\n\nd %&gt;%\n  group_by(condit, fb, bandOrder, tOrder, id, expMode) %&gt;% \n  summarise(n = n()) %&gt;%\n  pivot_wider(names_from = expMode, values_from = n, names_prefix = \"n_\") %&gt;%\n  group_by(condit, fb, bandOrder, tOrder) %&gt;%\n  summarise(across(starts_with(\"n_\"), ~mean(., na.rm = TRUE))) %&gt;%\n  kable()\n\n\n\ncondit\nfb\nbandOrder\ntOrder\nn_train\nn_train-Nf\nn_test-Nf\nn_test-train-nf\nn_test-feedback\n\n\n\nConstant\nContinuous\nOriginal\ntestFirst\n84.61538\n25.98077\n44.00000\n16.59615\n26.44231\n\n\nConstant\nContinuous\nOriginal\ntrainFirst\n85.52632\n26.13158\n43.84211\n17.00000\n26.63158\n\n\nConstant\nContinuous\nReverse\ntestFirst\n86.00000\n24.92857\n43.78571\n17.42857\n25.89286\n\n\nConstant\nContinuous\nReverse\ntrainFirst\n85.77778\n24.77778\n43.11111\n17.33333\n25.22222\n\n\nConstant\nOrdinal\nOriginal\ntestFirst\n85.45098\n25.86275\n44.23529\n16.52941\n26.45098\n\n\nConstant\nOrdinal\nReverse\ntestFirst\n85.93548\n25.25806\n43.58065\n17.64516\n26.22581\n\n\nConstant\nOrdinal\nReverse\ntrainFirst\n85.67857\n25.25000\n43.60714\n17.67857\n25.92857\n\n\nVaried\nContinuous\nOriginal\ntestFirst\n84.05128\n26.10256\n43.84615\n17.38462\n26.41026\n\n\nVaried\nContinuous\nOriginal\ntrainFirst\n83.81081\n25.70270\n44.05405\n16.97297\n26.59459\n\n\nVaried\nContinuous\nReverse\ntestFirst\n86.24000\n25.04000\n43.64000\n17.64000\n25.88000\n\n\nVaried\nContinuous\nReverse\ntrainFirst\n85.90000\n24.90000\n43.33333\n17.70000\n26.06667\n\n\nVaried\nOrdinal\nOriginal\ntestFirst\n85.28205\n26.43590\n44.10256\n17.56410\n26.58974\n\n\nVaried\nOrdinal\nReverse\ntestFirst\n86.25000\n24.53571\n43.28571\n17.39286\n26.10714\n\n\nVaried\nOrdinal\nReverse\ntrainFirst\n84.88889\n24.77778\n43.05556\n17.55556\n25.94444\n\n\n\n\n\n\n# Define column defs function \ncol_defs &lt;- function(data) {\n  \n  cols &lt;- colnames(data)\n  \n  defs &lt;- lapply(cols, function(x) {\n    \n    if(is.factor(data[[x]])) {\n      colDef(sortable = TRUE, filterable = TRUE,minWidth=108) \n    } else {\n      colDef(sortable = TRUE, filterable = FALSE,minWidth=90)\n    }\n    \n  })\n  \n  setNames(defs, cols)\n}\n\nd %&gt;%\n  group_by(id,condit, fb, bandOrder, tOrder, expMode) %&gt;%\n  summarise(n = n()) %&gt;%\n  pivot_wider(names_from = expMode, values_from = n, names_prefix = \"n_\") %&gt;%\n  \n  # Pass original data too \n  reactable(columns = col_defs(.), \n            highlight = TRUE,\n            defaultPageSize = 25)\n\n\n\n\n\n\nd %&gt;% filter(nGoodTrial==1,nTotal&gt;100) %&gt;% ggplot(aes(nTotal)) + geom_histogram() + facet_wrap(~condit)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nd %&gt;% filter(nGoodTrial==1,nTotal&gt;100) %&gt;% ggplot(aes(nTestNf)) + geom_histogram() + facet_wrap(~condit)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nd %&gt;% filter(nGoodTrial==1,nTotal&gt;100) %&gt;% ggplot(aes(nTrain)) + geom_histogram() + facet_wrap(~condit)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Misc/clust_gmm.html",
    "href": "Misc/clust_gmm.html",
    "title": "E1 Discrimination via Clustering",
    "section": "",
    "text": "https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/clustering-analysis.html#gaussian-mixture-models\nhttps://cran.r-project.org/web/packages/mclust/vignettes/mclust.html\nhttps://joshuamrosenberg.com/posts/lpa-in-r-using-mclust/\n\nCodepacman::p_load(tidyverse,here, mclust,furrr,future,broom)\nselect &lt;- dplyr::select\nmutate &lt;- dplyr::mutate\nfilter &lt;- dplyr::filter\nmap &lt;- purrr::map\n\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\n\n\ntest &lt;- e1 |&gt; filter(expMode2 == \"Test\")\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(nHits=sum(dist==0),vxMean=mean(vx),vxMed=median(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n\n\nget_mclust_param &lt;- function(model, param) {\n  if (!is.null(model$parameters[[param]])) {\n    return(model$parameters[[param]])\n  } else {\n    return(NA_real_)\n  }\n}\n\n\nplan(multisession, workers = 8)\n\n\n\nmcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n\n\nmcFree2 &lt;- get_mcFree(mcFree, \"id\", \"condit\", c(\"bandInt\", \"vb\", \"vx\", \"vy\"))\n\n\nmcFit &lt;- function(df, id_var, condit_var, clustvars,G=1:9) {\n  df %&gt;%\n    select({{ id_var }}, {{ condit_var }}, !!!clustvars) %&gt;%\n    ungroup() %&gt;%\n    group_by({{ id_var }}, {{ condit_var }}) %&gt;%\n    nest(data = clustvars) %&gt;%\n    mutate(mc = future_map(data, ~Mclust(.x[c(clustvars)],G=G))) %&gt;%\n    mutate(\n      bic= as.numeric(map(mc,\"bic\")),\n      Gfit = as.numeric(map(mc, \"G\")),\n      means = map(mc, get_mclust_param, param = \"mean\"),\n      proportions = map(mc, get_mclust_param, param = \"pro\")\n    )\n}\n\nmcFree1 &lt;- mcFit(test, id, condit, c(\"vx\"))\nmcFreeVxb &lt;- mcFit(test, id, condit, c(\"vxb\"))\nmcFree2 &lt;- mcFit(test, id, condit, c(\"vx\",\"vy\"))\n\nmcFreeVb &lt;- mcFit(test, id, condit, c(\"vx\",\"bandInt\"))\n\n\ndf &lt;- test |&gt; select(id,condit,bandInt,vx) |&gt; group_by(id,bandInt) |&gt; mutate(m=mean(vx),sd=sd(vx),n=n())\n\n\n\n\n\n# test1 &lt;- dfs |&gt; filter(id==1 || id==2) |&gt; mutate(samps = map2(m,sd, ~rnorm(n=10,mean=.x,sd=.y)))\n# test1 &lt;- test1 %&gt;% unnest(samps)\n\n\ndfs &lt;- test |&gt; select(id,condit,bandInt,vx,vb) |&gt; group_by(id,vb,condit) |&gt; summarise(m=mean(vx),sd=sd(vx),med=median(vx),n=n(),se=sd/sqrt(n))\n\nbdf &lt;- dfs |&gt; mutate(samps = map2(m,se, ~rnorm(n=100,mean=.x,sd=.y))) |&gt; unnest(samps) %&gt;% ungroup()\nmcBoot2 &lt;- mcFit(bdf, id, condit, c(\"samps\"))\n\nmcBoot2 |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nSample Stat bootstrap\n\nCodedfs &lt;- test |&gt; select(id,condit,bandInt,vx,vb) |&gt; group_by(id,vb,condit) |&gt; summarise(m=mean(vx),sd=sd(vx),med=median(vx),n=n(),se=sd/sqrt(n))\n\nbdf &lt;- dfs |&gt; mutate(samps = map2(m,se, ~rnorm(n=100,mean=.x,sd=.y))) |&gt; unnest(samps) %&gt;% ungroup()\nmcBoot2 &lt;- mcFit(bdf, id, condit, c(\"samps\"))\n\nmcBoot2 |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\n\ndfs2 &lt;- test |&gt; select(id,condit,bandInt,vx,vb,vxb) |&gt; group_by(id,vb,condit) |&gt; summarise(m=mean(vxb),sd=sd(vxb),med=median(vxb),n=n(),se=sd/sqrt(n))\n\nbdf2 &lt;- dfs2 |&gt; mutate(samps = map2(m,se, ~rnorm(n=100,mean=.x,sd=.y))) |&gt; unnest(samps) %&gt;% ungroup()\nmcBoot2b &lt;- mcFit(bdf2, id, condit, c(\"samps\"))\n\nmcBoot2b |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nEmpirical Bootstrap\n\nCode# Define the bootstrap function\nbootstrap &lt;- function(x, n) {\n  sample(x, size = n, replace = TRUE)\n}\n\n# Apply the bootstrap function to your data\nbdf2 &lt;- test %&gt;%\n  select(id, condit, bandInt, vx, vb, vxb) %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(vxb_bootstrap = list(bootstrap(vx, 20)), .groups = \"drop\") %&gt;%\n  unnest(vxb_bootstrap)\n\neb1 &lt;- mcFit(bdf2, id, condit, c(\"vxb_bootstrap\"))\n\neb1 |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\neb2&lt;- test %&gt;%\n  select(id, condit, bandInt, vx, vb, vxb) %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(vxb_bootstrap = list(bootstrap(vx, 100)), .groups = \"drop\") %&gt;%\n  unnest(vxb_bootstrap) %&gt;% mcFit(.,id,condit,c(\"vxb_bootstrap\"))\n\neb2 |&gt; group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nPopulation Clustering\n\nCodebic &lt;- mclustBIC(test$vx)\nplot(bic)\nmod1 &lt;- Mclust(test$vx,x=bic)\nsummary(mod1,parameters=TRUE)\n\nbic &lt;- mclustBIC(c(test$vx,test$vy))\nplot(bic)\n\nbic &lt;- mclustBIC(c(test$sdist))\nplot(bic)\n\nbic &lt;- mclustBIC(c(test$dist))\nplot(bic)\n\nbic &lt;- mclustBIC(c(test$dist,test$vx))\nplot(bic)\n\n\n\nbic &lt;- mclustBIC(c(test$vx,test$bandInt))\nplot(bic)\nsummary(bic)\n\n\n\nCodemv4 &lt;- mclustBIC(subset(test,select = c(vx,bandInt)),\n                 G=1:7,\n                 modelNames=c(\"EII\", \"VVI\", \"EEE\", \"VVV\"))\n y &lt;- mv4%&gt;%\n        as.data.frame.matrix() %&gt;%\n        rownames_to_column(\"n_mixtures\") %&gt;%\n        rename(`Constrained variance, fixed covariance` = EII, \n               `Freed variance, fixed covariance` = VVI,\n               `Constrained variance, constrained covariance` = EEE,\n               `Freed variance, freed covariance` = VVV)\n \nto_plot &lt;- y %&gt;%  gather(`Covariance matrix structure`, val, -n_mixtures) %&gt;% \n    mutate(`Covariance matrix structure` = as.factor(`Covariance matrix structure`),\n           val = abs(val)) # this is to make the BIC values positive (to align with more common formula / interpretation of BIC)\nggplot(to_plot, aes(x = n_mixtures, y = val, color = `Covariance matrix structure`, group = `Covariance matrix structure`)) +\n    geom_line() +\n    geom_point() +\n    ylab(\"BIC (smaller value is better)\") \n\n\n\nCodem = Mclust(test$vx)\ntest$vx\n\n\nsummary(m)\n\nmv &lt;- Mclust(subset(test, condit == \"Varied\", select = vx))\nsummary(mv)\nplot(mv, what = \"classification\")\n\nmc &lt;- Mclust(subset(test, condit == \"Constant\", select = vx))\nsummary(mc)\nplot(mc, what = \"classification\")\n\n\n\nmv2 &lt;- densityMclust(subset(test, condit == \"Varied\", select = c(vxb,vy)))\nsummary(mv2)\nplot(mv2, what = \"density\",type=\"persp\")\nplot(mv2, what = \"BIC\")\n\n\nmv3 &lt;- mclustBIC(subset(test, condit == \"Varied\", select = vx),\n                 G=1:7)\n y &lt;- mv3%&gt;%\n        as.data.frame.matrix() %&gt;%\n        rownames_to_column(\"n_mixtures\") %&gt;%\n        rename(`Constrained variance, constrained covariance` = E,\n               `Freed variance, freed covariance` = V)\n\nplot(mc, what = \"classification\")\n\nmc &lt;- Mclust(subset(test, condit == \"Constant\", select = vx),G=6)\nsummary(mc)\ntidy(mc)\nglance(mc)\n\n\nVx and bandInt\n\nCodemcFreeVb &lt;- mcFit(test, id, condit, c(\"vx\",\"bandInt\"),G=1:6)\n\n\n\nmcFreeVb %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\nmcFreeVb2 &lt;- mcFit(test, id, condit, c(\"vxb\",\"bandInt\"),G=1:6)\n\nmcFreeVb2 %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nmcFreeVb3 &lt;- mcFit(test, id, condit, c(\"vxb\",\"bandInt\",\"vy\"),G=1:6)\n\nmcFreeVb3 %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\n\nCodemcFreeVb1 &lt;- mcFit(test, id, condit, c(\"vx\"),G=1:6)\n\nmcFreeVb1 %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\nmcFreeVb1b &lt;- mcFit(test, id, condit, c(\"vxb\"),G=1:6)\n\nmcFreeVb1b %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\nboot1 &lt;- MclustBootstrap(mcFreeVb1b[[4]][[1]], nboot = 1000, type = \"bs\")\nsummary(boot1)\n\n\n\nCodemcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcFree &lt;- mcFree %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  mutate(mc = future_map(data, ~MclustBootstrap(.x$vx))) \n\n\nmcFree &lt;- mcFree %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\n\nFit free to each id\n\nCodemcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcFree &lt;- mcFree %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  mutate(mc = future_map(data, ~Mclust(.x$vx))) \n\n\nmcFree &lt;- mcFree %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nggplot(mcFree,aes(x=condit,y=Gfit))+geom_boxplot()+geom_jitter()\n\nggplot(mcFree,aes(x=Gfit,fill=condit))+geom_bar(position=position_dodge())\n\n# mcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n# mcFree &lt;- mcFree |&gt; group_by(id) |&gt; nest(id) |&gt; mutate(mc=map(vx, ~Mclust(.x$vx))) \n#   \n\nplot(mcCondit6[[2]][[1]], what = \"classification\")\nplot(mcCondit6[[2]][[2]], what = \"classification\")\n\nk=summary(mcCondit6[[2]][[2]],parameters=TRUE)\n\n\nFit xy free to each id\n\nCodemcFree2 &lt;- test |&gt; select(id,condit,bandInt,vb,vx,vy) %&gt;% ungroup()\n\nmcFree2 &lt;- mcFree2 %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx,vy)) %&gt;%\n  mutate(mc = future_map(data, ~Mclust(c(.x$vx,.x$vy)))) \n\n\nmcFree2 &lt;- mcFree2 %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nggplot(mcFree2,aes(x=condit,y=Gfit))+geom_boxplot()+geom_jitter()\n\nggplot(mcFree2,aes(x=Gfit,fill=condit))+geom_bar(position=position_dodge())\n\n# mcFree &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n# mcFree &lt;- mcFree |&gt; group_by(id) |&gt; nest(id) |&gt; mutate(mc=map(vx, ~Mclust(.x$vx))) \n#   \n\nplot(mcCondit6[[2]][[1]], what = \"classification\")\nplot(mcCondit6[[2]][[2]], what = \"classification\")\n\nk=summary(mcCondit6[[2]][[2]],parameters=TRUE)\n\n\nFit xy free to each id\n\nCodemcFree2b &lt;- test |&gt; select(id,condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcFree2b &lt;- mcFree2b %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  mutate(mc = future_map(data, ~Mclust(c(.x$vx,.x$bandInt)))) \n\n\nmcFree2b &lt;- mcFree2b %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nggplot(mcFree2b,aes(x=condit,y=Gfit))+geom_boxplot()+geom_jitter()\nggplot(mcFree2b,aes(x=Gfit,fill=condit))+geom_bar(position=position_dodge())\n\nmcFree2b %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\n\nCodemcFree2b &lt;- test |&gt; select(id,condit,bandInt,vb,vx,vy) %&gt;% ungroup()\n\nmcFree2b &lt;- mcFree2b %&gt;%\n  group_by(id,condit) %&gt;%\n  nest(data = c(bandInt, vb, vx,vy)) %&gt;%\n  mutate(mc = future_map(data, ~Mclust(c(.x$vx,.x$bandInt,.x$vy)))) \n\n\nmcFree2b &lt;- mcFree2b %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nggplot(mcFree2b,aes(x=condit,y=Gfit))+geom_boxplot()+geom_jitter()\nggplot(mcFree2b,aes(x=Gfit,fill=condit))+geom_bar(position=position_dodge())\n\nmcFree2b %&gt;% group_by(condit) |&gt; mutate(n=n()) |&gt; \n   group_by(condit,Gfit) |&gt; mutate(cg=n(),pg=cg/n) |&gt;\n  ggplot(aes(x=Gfit,fill=condit))+geom_col(aes(y=pg),position=position_dodge())+\n  scale_x_continuous(breaks=seq(1,9))\n\n\n\nCodemcConditFree &lt;- test |&gt; select(condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcConditFree &lt;- mcConditFree %&gt;%\n  group_by(condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  reframe(mc = future_map(data, ~Mclust(.x$vx))) \n\n\nmcConditFree &lt;- mcConditFree %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\nmcCondit6 &lt;- test |&gt; select(condit,bandInt,vb,vx) %&gt;% ungroup()\n\nmcCondit6 &lt;- mcCondit6 %&gt;%\n  group_by(condit) %&gt;%\n  nest(data = c(bandInt, vb, vx)) %&gt;%\n  reframe(mc = future_map(data, ~Mclust(.x$vx,G=6))) \n\n\nmcCondit6 &lt;- mcCondit6 %&gt;% \n  mutate(\n    Gfit = as.numeric(map(mc, \"G\")),\n    bic=as.numeric(map(mc,\"bic\")),\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\n\ntable(class,mcCondit6[[2]][[2]]$classification)\n\nbic &lt;- mclustBIC(test$vx)\n\n\n\nCodedf &lt;- test %&gt;%\n    select(id, condit, bandInt, vb, vx) %&gt;%\n    group_by(id) %&gt;%\n    nest(data = c(condit, bandInt, vb, vx)) \n\n# Define a function to apply the Mclust operation to one group\nprocess_group &lt;- function(group_data, group_id) {\n    print(paste(\"Processing group\", group_id))\n    if(length(unique(group_data$vx))&gt;=12) {\n        Mclust(group_data$vx, G=6)\n    } else {\n        Mclust(rep(0, 12), G=6)  # dummy Mclust model\n    }\n}\n\n# Apply the function to each group one by one\ndf$mc &lt;- map2(df$data, df$id, process_group)\n\n\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    means = map(mc, get_mclust_param, param = \"mean\"),\n    proportions = map(mc, get_mclust_param, param = \"pro\")\n  )\n\n\n\nCodecenters &lt;- tibble(\n  cluster = factor(1:3),\n  # number points in each cluster\n  num_points = c(100, 150, 50),\n  # x1 coordinate of cluster center\n  x1 = c(5, 0, -3),\n  # x2 coordinate of cluster center\n  x2 = c(-1, 1, -2)\n)\n\npoints &lt;- centers %&gt;%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %&gt;%\n  select(-num_points, -cluster) %&gt;%\n  unnest(c(x1, x2))\n\n\nm &lt;- Mclust(points)\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    mclust_success = map_lgl(mc, ~ !is.null(.x$parameters))\n  ) %&gt;%\n  filter(mclust_success)\n\n\n\nCodelibrary(mixR)\n\nmr1 &lt;- mixfit(test$vx,ncomp=3)\nplot(mr1)\nmr1\n\nmr2 &lt;- mixfit(test$vx,ncomp=6,family=\"gamma\")\nplot(mr2)\nmr2\n\n\n\nCodelibrary(flexmix)\n#https://rdrr.io/cran/flexmix/man/flexmix.html\n\nfm1=flexmix(vx~1|id,data=test,k=6)\nsummary(fm1)\nplot(fm1)\nparameters(fm1)\n\ntest %&gt;% group_by(vb) %&gt;% summarise(mean(vx))\n\n\n\nCode#http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-known-methods/\n\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\ntest &lt;- e1 |&gt; filter(expMode2 == \"Test\") |&gt; ungroup()\nlibrary(factoextra)\nlibrary(NbClust)\n\n\n\nfviz_nbclust(test |&gt; select(vx), kmeans,method = \"gap_stat\")\n\nNbClust(test |&gt; select(vx),distance=\"euclidean\",min.nc=2,max.nc=15,method=\"average\")\n\n\n\nCodelibrary(brms)\n\ntestS &lt;- test |&gt; filter(id %in% 1:20)\n\noptions(mc.cores = 2, brms.backend = \"cmdstanr\")\n\nmix &lt;- mixture(gaussian, gaussian, gaussian,gaussian,gaussian,gaussian)\nprior &lt;- c(\n  prior(normal(200, 100), Intercept, dpar = mu1),\n  prior(normal(450, 100), Intercept, dpar = mu2),\n  prior(normal(700, 100), Intercept, dpar = mu3),\n  prior(normal(900, 100), Intercept, dpar = mu4),\n  prior(normal(1100, 100), Intercept, dpar = mu5),\n  prior(normal(1300, 100), Intercept, dpar = mu6)\n)\nfit1 &lt;- brm(bf(vxC ~ 1), testS, family = mix,\n            prior = prior, chains = 2) \n\nsummary(fit1)\npp_check(fit1,type=\"stat_grouped\",ndraws=500,group=\"bandInt\",stat=\"mean\")\n\n# saveRDS(object = fit1,\n#         file = here(\"data/model_cache/mix6_vxC_test6.rda\"), \n#         compress = \"xz\")\n\n\n7x speedup for scaled vx\n\nCodelibrary(brms)\n\ntest$scaleVx &lt;- scale(test$vx)\ntest %&gt;% group_by(vb) %&gt;% summarise(mean(scaleVx))\n# vb        `mean(scaleVx)`\n#   &lt;fct&gt;               &lt;dbl&gt;\n# 1 100-300           -0.544 \n# 2 350-550           -0.273 \n# 3 600-800           -0.0194\n# 4 800-1000           0.449 \n# 5 1000-1200          0.774 \n# 6 1200-1400          1.00  \n\ntestS &lt;- test |&gt; filter(id %in% 1:20)\n\noptions(mc.cores = 2, brms.backend = \"cmdstanr\")\n\nmix &lt;- mixture(gaussian, gaussian, gaussian,gaussian,gaussian,gaussian)\nprior &lt;- c(\n  prior(normal(-.5, 10), Intercept, dpar = mu1),\n  prior(normal(-.25, 10), Intercept, dpar = mu2),\n  prior(normal(-.02, 10), Intercept, dpar = mu3),\n  prior(normal(.4, 10), Intercept, dpar = mu4),\n  prior(normal(.7, 10), Intercept, dpar = mu5),\n  prior(normal(1, 10), Intercept, dpar = mu6)\n)\nfitScale &lt;- brm(bf(scaleVx ~ 1), testS, family = mix,\n            prior = prior, chains = 2) \n\nsummary(fitScale)\npp_check(fit1,type=\"stat_grouped\",ndraws=500,group=\"bandInt\",stat=\"mean\")\n\n# saveRDS(object = fit1,\n#         file = here(\"data/model_cache/mix6_vxC_test6.rda\"), \n#         compress = \"xz\")\n\n\n\nCodeoptions(mc.cores = 4, brms.backend = \"cmdstanr\")\n\nmix &lt;- mixture(gaussian, gaussian, gaussian,gaussian,gaussian,gaussian)\nprior &lt;- c(\n  prior(normal(-.5, 10), Intercept, dpar = mu1),\n  prior(normal(-.25, 10), Intercept, dpar = mu2),\n  prior(normal(-.02, 10), Intercept, dpar = mu3),\n  prior(normal(.4, 10), Intercept, dpar = mu4),\n  prior(normal(.7, 10), Intercept, dpar = mu5),\n  prior(normal(1, 10), Intercept, dpar = mu6)\n)\n\nfitScaleInt &lt;- brm(bf(scaleVx ~ 1 + (1|id)), test, family = mix,\n            prior = prior, chains = 2,iter=1000) \n\nsummary(fitScaleInt)\npp_check(fitScaleInt,type=\"stat_grouped\",ndraws=500,stat=\"mean\")\n\n# saveRDS(object = fit1,\n#         file = here(\"data/model_cache/mix6_vxC_test6.rda\"), \n#         compress = \"xz\")"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html",
    "href": "Misc/Visuals_Interactives/tikz_net.html",
    "title": "tikz",
    "section": "",
    "text": "https://www.andrewheiss.com/blog/2021/08/27/tikz-knitr-html-svg-fun/ https://gist.github.com/andrewheiss/4ece621813a27dfdcaef7f1c2d773237\nCode```{r}\n#lapply(c('tidyverse','data.table','igraph','ggraph','kableExtra'),library,character.only=TRUE))\npacman::p_load(tikzDevice, knitr) \n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#intro",
    "href": "Misc/Visuals_Interactives/tikz_net.html#intro",
    "title": "tikz",
    "section": "Intro",
    "text": "Intro\nHere is a TikZ picture\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n\\tikzset{\n    declare function={\n        sig = 0.1;\n        mu = 0;\n        g(\\x) = 1/(sig*sqrt(2*pi)) * exp(-1/2 * ((\\x-mu)/sig)^2);\n    }\n}\n\n\n\n\\begin{tikzpicture}[\n    shorten &gt;=1pt,\n    -&gt;,\n    draw=black!50,\n    node distance=2.5cm,\n    scale=1.5,\n    every pin edge/.style={&lt;-,shorten &lt;=1pt},\n    neuron/.style={\n        circle,fill=black!25,minimum size=17pt,inner sep=0pt,\n        path picture={\n            \\draw[red,thick,-] plot[domain=-0.3:0.3,samples=11,smooth] ({\\x},{0.05*g(\\x)});\n        },\n    },\n    input neuron/.style={neuron, fill=green!50},\n    output neuron/.style={neuron, fill=red!50},\n    hidden neuron/.style={neuron, fill=blue!50},\n    annot/.style={text width=4em, text centered},\n]\n\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,...,4}\n    % This is the same as writing \\foreach \\name / \\y in {1/1,2/2,3/3,4/4}\n        \\node[input neuron, pin=left:Input \\y] (I-\\name) at (0,-\\y) {};\n    \n    % Draw the hidden layer nodes\n    \\foreach \\name / \\y in {1,...,5}\n        \\path[yshift=0.5cm]\n            node[hidden neuron] (H-\\name) at (2.5cm,-\\y cm) {};\n    \n    % Draw the output layer node\n    \\node[output neuron,pin={[pin edge={-&gt;}]right:Output}, right of=H-3] (O) {};\n    \n    % Connect every node in the input layer with every node in the\n    % hidden layer.\n    \\foreach \\source in {1,...,4}\n        \\foreach \\dest in {1,...,5}\n            \\path (I-\\source) edge (H-\\dest);\n    \n    % Connect every node in the hidden layer with the output layer\n    \\foreach \\source in {1,...,5}\n        \\path (H-\\source) edge (O);\n    \n    % Annotate the layers\n    \\node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};\n    \\node[annot,above of=I-1, node distance=1cm] {Input layer};\n    \\node[annot,above of=O] {Output layer};\n\n\n\\end{tikzpicture}\n\n```\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\n\\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}\n\n\\begin{tikzpicture}[\n    init/.style={\n      draw,\n      circle,\n      inner sep=2pt,\n      font=\\Huge,\n      join = by -latex\n    },\n    squa/.style={\n      draw,\n      inner sep=2pt,\n      font=\\Large,\n      join = by -latex\n    },\n    start chain=2,node distance=13mm\n    ]\n    \\node[on chain=2] \n      (x2) {$x_2$};\n    \\node[on chain=2,join=by o-latex] \n      {$w_2$};\n    \\node[on chain=2,init] (sigma) \n      {$\\displaystyle\\Sigma$};\n    \\node[on chain=2,squa,label=above:{\\parbox{2cm}{\\centering Activate \\\\ function}}]   \n      {$f$};\n    \\node[on chain=2,label=above:Output,join=by -latex] \n      {$y$};\n    \\begin{scope}[start chain=1]\n    \\node[on chain=1] at (0,1.5cm) \n      (x1) {$x_1$};\n    \\node[on chain=1,join=by o-latex] \n      (w1) {$w_1$};\n    \\end{scope}\n    \\begin{scope}[start chain=3]\n    \\node[on chain=3] at (0,-1.5cm) \n      (x3) {$x_3$};\n    \\node[on chain=3,label=below:Weights,join=by o-latex] \n      (w3) {$w_3$};\n    \\end{scope}\n    \\node[label=above:\\parbox{2cm}{\\centering Bias \\\\ $b$}] at (sigma|-w1) (b) {};\n    \n    \\draw[-latex] (w1) -- (sigma);\n    \\draw[-latex] (w3) -- (sigma);\n    \\draw[o-latex] (b) -- (sigma);\n    \n    \\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);\n\\end{tikzpicture}\n\n```\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{positioning}\n\n\\tikzset{basic/.style={draw,fill=blue!20,text width=1em,text badly centered}}\n\\tikzset{input/.style={basic,circle}}\n\\tikzset{weights/.style={basic,rectangle}}\n\\tikzset{functions/.style={basic,circle,fill=blue!10}}\n\n\n\\begin{tikzpicture}\n    \\node[functions] (center) {};\n    \\node[below of=center,font=\\scriptsize,text width=4em] {Activation function};\n    \\draw[thick] (0.5em,0.5em) -- (0,0.5em) -- (0,-0.5em) -- (-0.5em,-0.5em);\n    \\draw (0em,0.75em) -- (0em,-0.75em);\n    \\draw (0.75em,0em) -- (-0.75em,0em);\n    \\node[right of=center] (right) {};\n        \\path[draw,-&gt;] (center) -- (right);\n    \\node[functions,left=3em of center] (left) {$\\sum$};\n        \\path[draw,-&gt;] (left) -- (center);\n    \\node[weights,left=3em of left] (2) {$w_2$} -- (2) node[input,left of=2] (l2) {$x_2$};\n        \\path[draw,-&gt;] (l2) -- (2);\n        \\path[draw,-&gt;] (2) -- (left);\n    \\node[below of=2] (dots) {$\\vdots$} -- (dots) node[left of=dots] (ldots) {$\\vdots$};\n    \\node[weights,below of=dots] (n) {$w_n$} -- (n) node[input,left of=n] (ln) {$x_n$};\n        \\path[draw,-&gt;] (ln) -- (n);\n        \\path[draw,-&gt;] (n) -- (left);\n    \\node[weights,above of=2] (1) {$w_1$} -- (1) node[input,left of=1] (l1) {$x_1$};\n        \\path[draw,-&gt;] (l1) -- (1);\n        \\path[draw,-&gt;] (1) -- (left);\n    \\node[weights,above of=1] (0) {$w_0$} -- (0) node[input,left of=0] (l0) {$1$};\n        \\path[draw,-&gt;] (l0) -- (0);\n        \\path[draw,-&gt;] (0) -- (left);\n    \\node[below of=ln,font=\\scriptsize] {inputs};\n    \\node[below of=n,font=\\scriptsize] {weights};\n\\end{tikzpicture}\n\n```\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{%\n  calc,\n  fit,\n  shapes,\n  backgrounds\n}\n% the next macro is useful to create a table\n\\newcommand\\tabins[3]{%\n \\tikz[baseline=(Tab.base)] \n           \\node  [rectangle split, \n                   rectangle split parts=3, \n                   draw, \n                   align=right,\n                   inner sep=.5em,\n                   rectangle split horizontal] (Tab)\n                           {\\hbox to 4ex{#1}\n           \\nodepart{two}  {\\hbox to 8ex{\\hfill #2\\$}}  \n           \\nodepart{three}{\\hbox to 3ex{#3}}}; \n}\n\n\n\\parindent=0pt\n\n\\begin{tikzpicture}[%\n    %every node/.style={transform shape},% now is not necessary but good for a poster\n    x=1.25cm,y=2cm,  \n    font=\\footnotesize,\n    % every group of nodes have a style except for main, the style is named by a letter\n    main/.style={draw,fill=yellow,inner sep=.5em},\n    R/.style={draw,fill=purple!40!blue!30,inner sep=.5em},\n    M/.style={draw,fill=green!80!yellow,inner sep=.5em},\n    S/.style={anchor=east},\n    V/.style={anchor=west},\n    P/.style={anchor=center},\n    F/.style={anchor=west}\n    ]\n\n  % main node the reference Shuffle \n  \\node[main] (shuffle) {Group};\n  %group R reducer\n  \\node[R] at ($(shuffle)+(8,1)$)    (R1+) {Reduce};\n  \\node[R] at ($(shuffle)+(8, 0)$)   (R0)  {Reduce};\n  \\node[R] at ($(shuffle)+(8,-1)$)   (R1-) {Reduce};\n  % group M Mapper\n  \\node[M] at ($(shuffle)+(-6,+2.5)$)   (M3+)  {Map};\n  \\node[M] at ($(shuffle)+(-6,+ 1.5)$)  (M2+)  {Map};\n  \\node[M] at ($(shuffle)+(-6,+ .5)$)   (M1+)  {Map};\n  \\node[M] at ($(shuffle)+(-6,- .5)$)   (M1-)  {Map};\n  \\node[M] at ($(shuffle)+(-6,- 1.5)$)  (M2-)  {Map};\n  \\node[M] at ($(shuffle)+(-6,-2.5)$)   (M3-)  {Map};\n  % group S Start the first nodes\n  \\node[S] at ($(M3+)+(-1.5,0)$)  (S3+) {\\Big($k_1$,\\tabins{4711}{59.90}{NY}\\Big)};\n  \\node[S] at ($(M2+)+(-1.5,0)$)  (S2+) {\\Big($k_2$,\\tabins{4713}{142.99}{CA}\\Big)};\n  \\node[S] at ($(M1+)+(-1.5,0)$)  (S1+) {\\Big($k_3$,\\tabins{4714}{72.00}{NY}\\Big)}; \n  \\node[S] at ($(M1-)+(-1.5,0)$)  (S1-) {\\Big($k_4$,\\tabins{4715}{108.75}{NY}\\Big)}; \n  \\node[S] at ($(M2-)+(-1.5,0)$)  (S2-) {\\Big($k_5$,\\tabins{4718}{19.89}{WA}\\Big)};  \n  \\node[S] at ($(M3-)+(-1.5,0)$)  (S3-) {\\Big($k_6$,\\tabins{4719}{36.60}{CA}\\Big)};  \n  % group V  why not\n  \\node[V] at ($(M3+)+(1.5,0)$)  (V3+) {\\Big(NY,59.90\\$\\Big)};\n  \\node[V] at ($(M2+)+(1.5,0)$)  (V2+) {\\Big(CA,142.99\\$\\Big)};\n  \\node[V] at ($(M1+)+(1.5,0)$)  (V1+) {\\Big(NY,72.00\\$\\Big)}; \n  \\node[V] at ($(M1-)+(1.5,0)$)  (V1-) {\\Big(NY,108.75\\$\\Big)}; \n  \\node[V] at ($(M2-)+(1.5,0)$)  (V2-) {\\Big(WA,19.89\\$\\Big)};  \n  \\node[V] at ($(M3-)+(1.5,0)$)  (V3-) {\\Big(CA,36.60\\$\\Big)};   \n\n  \\node[P] at ($(R1+)+(-4,0)$) (P1+) {\\Big(CA,\\big[142.99\\$,36.60\\$\\big]\\Big)};\n  \\node[P] at ($(R0) +(-4,0)$) (P0)  {\\Big(NY,\\big[59.90\\$,72.00\\$,108.75\\big]\\Big)};\n  \\node[P] at ($(R1-)+(-4,0)$) (P1-) {\\Big(WA,\\big[19.89\\$\\big]\\Big)}; \n\n  \\node[F] (F1+) at ($(R1+)+(1.5,0)$) {(CA,89.80\\$)};\n  \\node[F] (F0)  at ($(R0) +(1.5,0)$) {(NY,80.22\\$)}; \n  \\node[F] (F1-) at ($(R1-)+(1.5,0)$) {(WA,72.00\\$)}; \n\n  % wrappers\n  \\begin{scope}[on background layer]\n      \\node[fill=lightgray!50,inner sep = 4mm,fit=(shuffle),label=above:Shuffle] {}; \n  \\end{scope} \n  \\begin{scope}[on background layer]\n      \\node[fill=lightgray!50,inner sep = 4mm,fit=(R1+)(R1-),label=above:Reducer] {}; \n  \\end{scope}  \n  \\begin{scope}[on background layer]\n      \\node[fill=lightgray!50,inner sep = 4mm,fit=(M3+)(M3-),label=above:Mapper] {}; \n  \\end{scope}\n\n  %edges\n\n  \\foreach \\indice in {3+,2+,1+,1-,2-,3-} \\draw[-&gt;] (S\\indice.east) -- (M\\indice.west); \n  \\foreach \\indice in {3+,2+,1+,1-,2-,3-} \\draw[-&gt;] (M\\indice.east) -- (V\\indice.west);\n  \\foreach \\indice in {3+,2+,1+,1-,2-,3-} \\draw[-&gt;] (V\\indice.east) to [out=0,in=180] (shuffle.west); \n  \\foreach \\indice in {1+,0,1-} \\draw[-&gt;] (shuffle.east) to [out=0,in=180] (P\\indice.west);  \n  \\foreach \\indice in {1+,0,1-} \\draw[-&gt;] (P\\indice.east) -- (R\\indice.west);\n  \\foreach \\indice in {1+,0,1-} \\draw[-&gt;] (R\\indice.east) -- (F\\indice.west);   \n\\end{tikzpicture} \n\n```\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n#|\n\\def\\layersep{3cm}\n\\def\\nodeinlayersep{1.5cm}\n\n\\begin{tikzpicture}\n  [\n    shorten &gt;=1pt,-&gt;,\n    draw=black!50,\n    node distance=\\layersep,\n    every pin edge/.style={&lt;-,shorten &lt;=1pt},\n    neuron/.style={circle,fill=black!25,minimum size=17pt,inner sep=0pt},\n    input neuron/.style={neuron, fill=green!50,},\n    output neuron/.style={neuron, fill=red!50},\n    hidden neuron/.style={neuron, fill=blue!50},\n    annot/.style={text width=4em, text centered},\n    bias/.style={neuron, fill=yellow!50,minimum size=4em},%&lt;-- added %%%\n  ]\n\n  % Draw the input layer nodes\n  \\foreach \\name / \\y in {1,...,3}\n    \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y-2.5) {};  \n\n  % set number of hidden layers\n  \\newcommand\\Nhidden{2}\n\n  % Draw the hidden layer nodes\n  \\foreach \\N in {0,...,\\Nhidden} {\n      \\foreach \\y in {0,...,5} { % &lt;-- added 0 instead of 1 %%%%%\n      \\ifnum \\y=4\n      \\ifnum \\N&gt;0 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n        \\node at (\\N*\\layersep,-\\y*\\nodeinlayersep) {$\\vdots$};  % add dots\n        \\else\\fi %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n      \\else\n          \\ifnum \\y=0 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n          \\ifnum \\N&lt;3 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n            \\node[bias] (H\\N-\\y) at (\\N*\\layersep,-\\y*\\nodeinlayersep ) {Bias}; %&lt;-- added\n            \\else\\fi %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n          \\else %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n            \\ifnum \\N&gt;0 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%%%\n            % print function\n            \\node[hidden neuron] (H\\N-\\y) at (\\N*\\layersep,-\\y*\\nodeinlayersep ) {$\\frac{1}{1+e^{-x}}$}; %&lt;-- added %%%%%%%%%%%\n                \\else\\fi %&lt;-- added %%%%%%%%%%%%\n          \\fi %&lt;-- added %%%%%%%\n      \\fi\n    }\n      \\ifnum \\N&gt;0 %&lt;-- added %%%%%%\n      % print hidden layer labels at the top\n    \\node[annot,above of=H\\N-1, node distance=1cm,yshift=2cm] (hl\\N) {Hidden layer \\N}; % &lt;- added yshift=2cm %%%%%%%%%%%%\n    \\else\\fi %&lt;-- added %%%%%\n  }\n\n  % Draw the output layer node and label\n  \\node[output neuron,pin={[pin edge={-&gt;}]right:Output}, right of=H\\Nhidden-3] (O) {}; \n  \n  % Connect bias every node in the input layer with every node in the\n  % hidden layer.\n  \\foreach \\source in {1,...,3}\n      \\foreach \\dest in {1,...,3,5} {\n        % \\path[yellow] (H-0) edge (H1-\\dest);\n        \\path[dashed,orange] (H0-0) edge (H1-\\dest); %&lt;-- added %%%%%\n          \\path[green!50] (I-\\source) edge (H1-\\dest);  % change to green, yellow gets blended\n    };\n\n  % connect all hidden stuff\n  \\foreach [remember=\\N as \\lastN (initially 1)] \\N in {2,...,\\Nhidden}\n      \\foreach \\source in {0,...,3,5} \n          \\foreach \\dest in {1,...,3,5}{\n              \\ifnum \\source=0 %&lt;-- added %%%%%%%%%%%%%%%%%%%%%%%\n          \\path[dashed,red](H\\lastN-\\source) edge (H\\N-\\dest);%&lt;-- added \n            \\else %&lt;-- added %%%\n            \\path[blue!50] (H\\lastN-\\source) edge (H\\N-\\dest);%&lt;-- added \n            \\fi %&lt;-- added %%%\n            }; %&lt;-- added %%%%\n\n\n  % Connect every node in the hidden layer with the output layer\n  \\foreach \\source in {1,...,3,5}\n    \\path[green!50] (H\\Nhidden-\\source) edge (O);\n    \\path[dashed,red] (H2-0) edge (O); %&lt;-- added %%%%\n\n% Annotate the input and output layers\n  \\node[annot,left of=hl1] {Input layer};\n  \\node[annot,right of=hl\\Nhidden] {Output layer};  \n\\end{tikzpicture}\n\n```\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{positioning}\n\n\\tikzstyle{inputNode}=[draw,circle,minimum size=10pt,inner sep=0pt]\n\\tikzstyle{stateTransition}=[-stealth, thick]\n\n\\begin{tikzpicture}\n    \\node[draw,circle,minimum size=25pt,inner sep=0pt] (x) at (0,0) {$\\Sigma$ $\\sigma$};\n\n    \\node[inputNode] (x0) at (-2, 1.5) {$\\tiny +1$};\n    \\node[inputNode] (x1) at (-2, 0.75) {$\\tiny x_1$};\n    \\node[inputNode] (x2) at (-2, 0) {$\\tiny x_2$};\n    \\node[inputNode] (x3) at (-2, -0.75) {$\\tiny x_3$};\n    \\node[inputNode] (xn) at (-2, -1.75) {$\\tiny x_n$};\n\n    \\draw[stateTransition] (x0) to[out=0,in=120] node [midway, sloped, above] {$w_0$} (x);\n    \\draw[stateTransition] (x1) to[out=0,in=150] node [midway, sloped, above] {$w_1$} (x);\n    \\draw[stateTransition] (x2) to[out=0,in=180] node [midway, sloped, above] {$w_2$} (x);\n    \\draw[stateTransition] (x3) to[out=0,in=210] node [midway, sloped, above] {$w_3$} (x);\n    \\draw[stateTransition] (xn) to[out=0,in=240] node [midway, sloped, above] {$w_n$} (x);\n    \\draw[stateTransition] (x) -- (4,0) node [midway,above] {$\\sigma\\left(w_0 + \\sum\\limits_{i=1}^{n}{w_ix_i}\\right)$};\n    \\draw[dashed] (0,-0.43) -- (0,0.43);\n    \\node (dots) at (-2, -1.15) {$\\vdots$};\n    \\node[inputNode, thick] (i1) at (6, 0.75) {};\n    \\node[inputNode, thick] (i2) at (6, 0) {};\n    \\node[inputNode, thick] (i3) at (6, -0.75) {};\n    \n    \\node[inputNode, thick] (h1) at (8, 1.5) {};\n    \\node[inputNode, thick] (h2) at (8, 0.75) {};\n    \\node[inputNode, thick] (h3) at (8, 0) {};\n    \\node[inputNode, thick] (h4) at (8, -0.75) {};\n    \\node[inputNode, thick] (h5) at (8, -1.5) {};\n    \n    \\node[inputNode, thick] (o1) at (10, 0.75) {};\n    \\node[inputNode, thick] (o2) at (10, -0.75) {};\n    \n    \\draw[stateTransition] (5, 0.75) -- node[above] {$I_1$} (i1);\n    \\draw[stateTransition] (5, 0) -- node[above] {$I_2$} (i2);\n    \\draw[stateTransition] (5, -0.75) -- node[above] {$I_3$} (i3);\n    \n    \\draw[stateTransition] (i1) -- (h1);\n    \\draw[stateTransition] (i1) -- (h2);\n    \\draw[stateTransition] (i1) -- (h3);\n    \\draw[stateTransition] (i1) -- (h4);\n    \\draw[stateTransition] (i1) -- (h5);\n    \\draw[stateTransition] (i2) -- (h1);\n    \\draw[stateTransition] (i2) -- (h2);\n    \\draw[stateTransition] (i2) -- (h3);\n    \\draw[stateTransition] (i2) -- (h4);\n    \\draw[stateTransition] (i2) -- (h5);\n    \\draw[stateTransition] (i3) -- (h1);\n    \\draw[stateTransition] (i3) -- (h2);\n    \\draw[stateTransition] (i3) -- (h3);\n    \\draw[stateTransition] (i3) -- (h4);\n    \\draw[stateTransition] (i3) -- (h5);\n    \n    \\draw[stateTransition] (h1) -- (o1);\n    \\draw[stateTransition] (h1) -- (o2);\n    \\draw[stateTransition] (h2) -- (o1);\n    \\draw[stateTransition] (h2) -- (o2);\n    \\draw[stateTransition] (h3) -- (o1);\n    \\draw[stateTransition] (h3) -- (o2);\n    \\draw[stateTransition] (h4) -- (o1);\n    \\draw[stateTransition] (h4) -- (o2);\n    \\draw[stateTransition] (h5) -- (o1);\n    \\draw[stateTransition] (h5) -- (o2);\n    \n    \\node[above=of i1, align=center] (l1) {Input \\\\ layer};\n    \\node[right=2.3em of l1, align=center] (l2) {Hidden \\\\ layer};\n    \\node[right=2.3em of l2, align=center] (l3) {Output \\\\ layer};\n    \n    \\draw[stateTransition] (o1) -- node[above] {$O_1$} (11, 0.75);\n    \\draw[stateTransition] (o2) -- node[above] {$O_2$} (11, -0.75);\n    \n    \\path[dashed, double, ultra thick, gray] (x.north) edge[bend left=0] (h5.north);\n    \\path[dashed, double, ultra thick, gray] (x.south) edge[bend right=0] (h5.south);\n\\end{tikzpicture}\n\n```\n\n\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n\n\\usetikzlibrary{positioning,decorations.pathreplacing,shapes}\n\n\n\\newcommand*{\\cancer}{\\text{cancer}}\n\\newcommand*{\\testp}{\\text{test}+}\n\n\n\\begin{tikzpicture}[%\n   % common options for blocks:\n   block/.style = {draw, fill=blue!30, align=center, anchor=west,\n               minimum height=0.65cm, inner sep=0},\n   % common options for the circles:\n   ball/.style = {circle, draw, align=center, anchor=north, inner sep=0}]\n\n   % circle illustrating all women\n   \\node[ball,text width=3cm,fill=purple!20] (all) at (6,0) {All women};\n\n   % two circles showing split of p{cancer} and p{~cancer}\n   \\node[ball,fill=red!70,text width=0.1cm,anchor=base] (pcan) at (3.5,-5.5) {};\n   \\node[ball,fill=blue!40,text width=2.9cm,anchor=base] (pncan) at (8.5,-6)\n      {Women without cancer\\\\\n      $\\p({\\sim}\\cancer) = 99\\%$};\n\n   % arrows showing split from all women to cancer and ~cancer\n   \\draw[-&gt;,thick,draw=red!50] (all.south) to [out=270,in=90] (pcan.north);\n   \\draw[-&gt;,thick,draw=blue!80] (all.south) to [out=270,in=110] (pncan.100);\n\n   % transition from all women to actual cancer rates\n   \\node[anchor=north,text width=10cm,inner sep=.05cm,align=center,fill=white]\n   (why1) at (6,-3.7) {In measuring, we find:};\n\n   % note illustration the p{cancer} circle (text wont fit inside)\n   \\node[inner sep=0,anchor=east,text width=3.3cm] (note1) at (3.2,-5.5) {\n      Women with cancer $\\p(\\cancer) = 1\\%$};\n\n   % draw the sieves\n   \\node[block,anchor=north,text width=4.4cm,fill=green!50] (tray1) at\n      (3.5,-8.8) {\\small{$\\p(\\testp\\mid\\cancer)=0.8$}};\n\n   \\node[block,anchor=north,text width=4.4cm,fill=green!50] (tray2) at\n      (8.5,-8.8) {$\\p(\\testp\\mid{\\sim}\\cancer)=0.096$};\n\n   % text explaining how p{cancer} and p{~cancer} behave as they\n   % pass through the sieves\n   \\node[anchor=west,text width=6cm] (note1) at (-6,-9.1) {\n      Now we pass both groups through the sieve; note that both\n      sieves are \\emph{the same}; they just behave differently\n      depending on which group is passing through. \\\\ \n      Let $\\testp=$ a positve mammography.};\n\n   % arrows showing the circles passing through the seives\n   \\draw[-&gt;,thick,draw=red!80] (3.5,-5.9) -- (3.5,-8.6);\n   \\draw[-&gt;,thick,draw=blue!50] (8.5,-8.1) -- (8.5,-8.6);\n\n   % numerator\n   \\node[ball,text width=0.05cm,fill=red!70] (can) at (6,-10.5) {};\n\n   % dividing line\n   \\draw[thick] (5,-11) -- (7,-11);\n\n   % demoniator\n   \\node[ball,text width=0.39cm,fill=blue!40,anchor=base] (ncan) at (6.5,-11.5) {};\n   \\node[ball,text width=0.05cm,fill=red!70,anchor=base] (can2) at (5.5,-11.5) {};\n\n   % plus sign in denominator\n   \\draw[thick] (5.9,-11.4) -- (5.9,-11.6);\n   \\draw[thick] (5.8,-11.5) -- (6,-11.5);\n\n   % arrows showing the output of the sieves formed the fraction\n   \\draw[-&gt;,thick,draw=red!80] (tray1.south) to [out=280,in=180] (can);\n   \\draw[-&gt;,thick,draw=red!80] (tray1.south) to [out=280,in=180] (can2);\n   \\node[anchor=north,inner sep=.1cm,align=center,fill=white] (why2) at\n      (3.8,-9.8) {$1\\% * 80\\%$};\n\n   \\draw[-&gt;,thick,draw=blue!50] (tray2.south) to [out=265,in=0] (ncan);\n   \\node[anchor=north,inner sep=.1cm,align=center,fill=white] (why2) at\n      (8.4,-9.8) {$99\\% * 9.6\\%$};\n\n   % explanation of final formula\n   \\node[anchor=north west,text width=6.5cm] (note2) at (-6,-12.5)\n      {Finally, to find the probability that a positive test\n         \\emph{actually means cancer}, we look at those who passed\n         through the sieve \\emph{with cancer}, and divide by all who\n         received a positive test, cancer or not.}; \n\n   % illustrated fraction turned into math\n   \\node[anchor=north,text width=10cm] (solution) at (6,-12.5) {\n   \\begin{align*}\n         \\frac{\\p(\\testp\\mid\\cancer)}{\\p(\\testp\\mid\\cancer)\n         + \\p(\\testp\\mid{\\sim}\\cancer)} &= \\\\\n         \\frac{1\\% * 80\\%}{(1\\% * 80\\%) + (99\\% * 9.6\\%)} &= 7.8\\%\n         = \\p(\\cancer\\mid\\testp)\n      \\end{align*}};\n\\end{tikzpicture}\n\n```\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n\n\n# \\usetikzlibrary{arrows}\n# \\usetikzlibrary{positioning}\n# \\usetikzlibrary{calc}\n# \\usetikzlibrary{arrows.meta}\n# \\usetikzlibrary{decorations.pathreplacing}\n\n# \\begin{tikzpicture}\n# \\draw (0,0)node(a){} -- (10,0) node (b) {} ;\n# \\foreach \\x in  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10} % edit here for the vertical lines\n# \\draw[shift={(\\x,0)},color=black] (0pt,3pt) -- (0pt,-3pt);\n# \\foreach \\x in {0, 0.2, 0.4, 0.6, 0.8, 1} % edit here for the numbers\n# \\draw[shift={(\\x*10,0)},color=black] (0pt,0pt) -- (0pt,-3pt) node[below]\n# {$\\x$};\n# \\node at (8, 0.5) (eq1) {$\\textcolor{red}{\\boldsymbol{SQ}}$};\n# \\node at (4, 0.5) (eq2) {$\\textcolor{purple}{\\boldsymbol{G_i(0)}}$}; \n# \\node at (7, 0.5) (eq2) {$\\textcolor{purple}{\\boldsymbol{G_i(1)}}$}; \n# \\node at (3, 0.5) (eq3) {$\\textcolor{blue}{\\boldsymbol{P}}$};\n# \\node at (0, 0.5) (eq4) {$\\textcolor{black}{\\boldsymbol{x_i}}$};\n# \\draw[decorate, decoration={brace, amplitude=6pt, mirror},] ([yshift=0.5cm]4,0.5)-- node[above=0.25cm]\n# {\\shortstack{Text}}([yshift=0.5cm]3,0.5);\n# \\draw[decorate, decoration={brace, amplitude=6pt},] ([yshift=-1cm]7,0)-- node[below=0.25cm]\n# {\\shortstack{Text}}([yshift=-1cm]3,0);\n# \\end{tikzpicture}\n```\n\n\nTikz Neural Networks\nhttps://tikz.net/neural_networks/\n\nCode```{r, engine = 'tikz', engine.opts=font_opts2, cache=TRUE}\n#| cache: true\n\n\\usetikzlibrary{arrows.meta} % for arrow size\n\n\\tikzset{&gt;=latex} % for LaTeX arrow head\n\\colorlet{myred}{red!80!black}\n\\colorlet{myblue}{blue!80!black}\n\\colorlet{mygreen}{green!60!black}\n\\colorlet{myorange}{orange!70!red!60!black}\n\\colorlet{mydarkred}{red!30!black}\n\\colorlet{mydarkblue}{blue!40!black}\n\\colorlet{mydarkgreen}{green!30!black}\n\\tikzstyle{node}=[thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]\n\\tikzstyle{node in}=[node,green!20!black,draw=mygreen!30!black,fill=mygreen!25]\n\\tikzstyle{node hidden}=[node,blue!20!black,draw=myblue!30!black,fill=myblue!20]\n\\tikzstyle{node convol}=[node,orange!20!black,draw=myorange!30!black,fill=myorange!20]\n\\tikzstyle{node out}=[node,red!20!black,draw=myred!30!black,fill=myred!20]\n\\tikzstyle{connect}=[thick,mydarkblue] %,line cap=round\n\\tikzstyle{connect arrow}=[-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten &lt;=0.5,shorten &gt;=1]\n\\tikzset{ % node styles, numbered for easy mapping with \\nstyle\n  node 1/.style={node in},\n  node 2/.style={node hidden},\n  node 3/.style={node out},\n}\n\\def\\nstyle{int(\\lay&lt;\\Nnodlen?min(2,\\lay):3)} % map layer number onto 1, 2, or 3\n\n\n\\begin{tikzpicture}[x=2.7cm,y=1.6cm]\n  \\message{^^JNeural network activation}\n  \\def\\NI{5} % number of nodes in input layers\n  \\def\\NO{4} % number of nodes in output layers\n  \\def\\yshift{0.4} % shift last node for dots\n  \n  % INPUT LAYER\n  \\foreach \\i [evaluate={\\c=int(\\i==\\NI); \\y=\\NI/2-\\i-\\c*\\yshift; \\index=(\\i&lt;\\NI?int(\\i):\"n\");}]\n              in {1,...,\\NI}{ % loop over nodes\n    \\node[node in,outer sep=0.6] (NI-\\i) at (0,\\y) {$a_{\\index}^{(0)}$};\n  }\n  \n  % OUTPUT LAYER\n  \\foreach \\i [evaluate={\\c=int(\\i==\\NO); \\y=\\NO/2-\\i-\\c*\\yshift; \\index=(\\i&lt;\\NO?int(\\i):\"m\");}]\n    in {\\NO,...,1}{ % loop over nodes\n    \\ifnum\\i=1 % high-lighted node\n      \\node[node hidden]\n        (NO-\\i) at (1,\\y) {$a_{\\index}^{(1)}$};\n      \\foreach \\j [evaluate={\\index=(\\j&lt;\\NI?int(\\j):\"n\");}] in {1,...,\\NI}{ % loop over nodes in previous layer\n        \\draw[connect,white,line width=1.2] (NI-\\j) -- (NO-\\i);\n        \\draw[connect] (NI-\\j) -- (NO-\\i)\n          node[pos=0.50] {\\contour{white}{$w_{1,\\index}$}};\n      }\n    \\else % other light-colored nodes\n      \\node[node,blue!20!black!80,draw=myblue!20,fill=myblue!5]\n        (NO-\\i) at (1,\\y) {$a_{\\index}^{(1)}$};\n      \\foreach \\j in {1,...,\\NI}{ % loop over nodes in previous layer\n        %\\draw[connect,white,line width=1.2] (NI-\\j) -- (NO-\\i);\n        \\draw[connect,myblue!20] (NI-\\j) -- (NO-\\i);\n      }\n    \\fi\n  }\n  \n  % DOTS\n  \\path (NI-\\NI) --++ (0,1+\\yshift) node[midway,scale=1.2] {$\\vdots$};\n  \\path (NO-\\NO) --++ (0,1+\\yshift) node[midway,scale=1.2] {$\\vdots$};\n  \n  % EQUATIONS\n  \\def\\agr#1{{\\color{mydarkgreen}a_{#1}^{(0)}}}\n  \\node[below=17,right=11,mydarkblue,scale=0.95] at (NO-1)\n    {$\\begin{aligned} %\\underset{\\text{bias}}{b_1}\n       &= \\color{mydarkred}\\sigma\\left( \\color{black}\n            w_{1,0}\\agr{0} + w_{1,1}\\agr{1} + \\ldots + w_{1,n}\\agr{n} + b_1^{(0)}\n          \\color{mydarkred}\\right)\\\\\n       &= \\color{mydarkred}\\sigma\\left( \\color{black}\n            \\sum_{i=1}^{n} w_{1,i}\\agr{i} + b_1^{(0)}\n           \\color{mydarkred}\\right)\n     \\end{aligned}$};\n  \\node[right,scale=0.9] at (1.3,-1.3)\n    {$\\begin{aligned}\n      {\\color{mydarkblue}\n      \\begin{pmatrix}\n        a_{1}^{(1)} \\\\[0.3em]\n        a_{2}^{(1)} \\\\\n        \\vdots \\\\\n        a_{m}^{(1)}\n      \\end{pmatrix}}\n      &=\n      \\color{mydarkred}\\sigma\\left[ \\color{black}\n      \\begin{pmatrix}\n        w_{1,0} & w_{1,1} & \\ldots & w_{1,n} \\\\\n        w_{2,0} & w_{2,1} & \\ldots & w_{2,n} \\\\\n        \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n        w_{m,0} & w_{m,1} & \\ldots & w_{m,n}\n      \\end{pmatrix}\n      {\\color{mydarkgreen}\n      \\begin{pmatrix}\n        a_{1}^{(0)} \\\\[0.3em]\n        a_{2}^{(0)} \\\\\n        \\vdots \\\\\n        a_{n}^{(0)}\n      \\end{pmatrix}}\n      +\n      \\begin{pmatrix}\n        b_{1}^{(0)} \\\\[0.3em]\n        b_{2}^{(0)} \\\\\n        \\vdots \\\\\n        b_{m}^{(0)}\n      \\end{pmatrix}\n      \\color{mydarkred}\\right]\\\\[0.5em]\n      {\\color{mydarkblue}a^{(1)}}\n      &= \\color{mydarkred}\\sigma\\left( \\color{black}\n           \\mathbf{W}^{(0)} {\\color{mydarkgreen}a^{(0)}}+\\mathbf{b}^{(0)}\n         \\color{mydarkred}\\right)\n         %\\color{black},\\quad \\mathbf{W}^{(0)} \\in \\mathbb{R}^{m\\times n}\n    \\end{aligned}$};\n  \n\\end{tikzpicture}\n\n```\n\n\n\n\nAutoencoder\nhttps://tikz.net/autoencoder/\n\nCode```{r, engine = 'tikz', engine.opts=font_opts}\n#| eval: true\n#| cache: true\n\n\\newcommand{\\xin}[2]{$x_#2$}\n\\newcommand{\\xout}[2]{$\\hat x_#2$}\n\n\\begin{neuralnetwork}[height=8]\n  \\tikzstyle{input neuron}=[neuron, fill=orange!70];\n  \\tikzstyle{output neuron}=[neuron, fill=blue!60!black, text=white];\n\n  \\inputlayer[count=8, bias=false, title=Input Layer, text=\\xin]\n\n  \\hiddenlayer[count=5, bias=false]\n  \\linklayers\n\n  \\hiddenlayer[count=3, bias=false, title=Latent\\\\Representation]\n  \\linklayers\n\n  \\hiddenlayer[count=5, bias=false]\n  \\linklayers\n\n  \\outputlayer[count=8, title=Output Layer, text=\\xout]\n  \\linklayers\n\n\\end{neuralnetwork}\n\n```\n\n\n\n\nVAE\nhttps://tikz.net/vae/\n\nCode```{r, engine = 'tikz', engine.opts=font_opts, cache=TRUE}\n#| eval: true\n\n\\usetikzlibrary{fit,positioning}\n\n\\newcommand\\drawNodes[2]{\n  % #1 (str): namespace\n  % #2 (list[list[str]]): list of labels to print in the node of each neuron\n  \\foreach \\neurons [count=\\lyrIdx] in #2 {\n    \\StrCount{\\neurons}{,}[\\lyrLength] % use xstring package to save each layer size into \\lyrLength macro\n    \\foreach \\n [count=\\nIdx] in \\neurons\n      \\node[neuron] (#1-\\lyrIdx-\\nIdx) at (2*\\lyrIdx, \\lyrLength/2-1.4*\\nIdx) {\\n};\n  }\n}\n\n\\newcommand\\denselyConnectNodes[2]{\n  % #1 (str): namespace\n  % #2 (list[int]): number of nodes in each layer\n  \\foreach \\n [count=\\lyrIdx, remember=\\lyrIdx as \\previdx, remember=\\n as \\prevn] in #2 {\n    \\foreach \\y in {1,...,\\n} {\n      \\ifnum \\lyrIdx &gt; 1\n        \\foreach \\x in {1,...,\\prevn}\n          \\draw[-&gt;] (#1-\\previdx-\\x) -- (#1-\\lyrIdx-\\y);\n      \\fi\n    }\n  }\n}\n\n\\begin{tikzpicture}[\n    shorten &gt;=1pt, shorten &lt;=1pt,\n    neuron/.style={circle, draw, minimum size=4ex, thick},\n    legend/.style={font=\\large\\bfseries},\n  ]\n\n  % encoder\n  \\drawNodes{encoder}{{{,,,,}, {,,,}, {,,}}}\n  \\denselyConnectNodes{encoder}{{5, 4, 3}}\n\n  % decoder\n  \\begin{scope}[xshift=11cm]\n    \\drawNodes{decoder}{{{,,}, {,,,}, {,,,,}}}\n    \\denselyConnectNodes{decoder}{{3, 4, 5}}\n  \\end{scope}\n\n  % mu, sigma, sample nodes\n  \\foreach \\idx in {1,...,3} {\n      \\coordinate[neuron, right=2 of encoder-3-2, yshift=\\idx cm,, fill=yellow, fill opacity=0.2] (mu-\\idx);\n      \\coordinate[neuron, right=2 of encoder-3-2, yshift=-\\idx cm, fill=blue, fill opacity=0.1] (sigma-\\idx);\n      \\coordinate[neuron, right=4 of encoder-3-2, yshift=\\idx cm-2cm, fill=green, fill opacity=0.1] (sample-\\idx);\n    }\n\n  % mu, sigma, sample boxes\n  \\node [label=$\\mu$, fit=(mu-1) (mu-3), draw, fill=yellow, opacity=0.45] (mu) {};\n  \\node [label=$\\sigma$, fit=(sigma-1) (sigma-3), draw, fill=blue, opacity=0.3] (sigma) {};\n  \\node [label=sample, fit=(sample-1) (sample-3), draw, fill=green, opacity=0.3] (sample) {};\n\n  % mu, sigma, sample connections\n  \\draw[-&gt;] (mu.east) edge (sample.west) (sigma.east) -- (sample.west);\n  \\foreach \\a in {1,2,3}\n  \\foreach \\b in {1,2,3} {\n      \\draw[-&gt;] (encoder-3-\\a) -- (mu-\\b);\n      \\draw[-&gt;] (encoder-3-\\a) -- (sigma-\\b);\n      \\draw[-&gt;] (sample-\\a) -- (decoder-1-\\b);\n    }\n\n  % input + output labels\n  \\foreach \\idx in {1,...,5} {\n      \\node[left=0 of encoder-1-\\idx] {$x_\\idx$};\n      \\node[right=0 of decoder-3-\\idx] {$\\hat x_\\idx$};\n    }\n  \\node[above=0.1 of encoder-1-1] {input};\n  \\node[above=0.1 of decoder-3-1] {output};\n\n\\end{tikzpicture}\n\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#bayes-vs-regular-nn",
    "href": "Misc/Visuals_Interactives/tikz_net.html#bayes-vs-regular-nn",
    "title": "tikz",
    "section": "bayes vs regular nn",
    "text": "bayes vs regular nn\nhttps://tikz.net/regular-vs-bayes-nn/\n\nCode```{r, engine = 'tikz', engine.opts=font_opts}\n#| cache: true\n#| fig-align: \"center\"\n\n\n\\usetikzlibrary{calc}\n\\def\\layersep{3cm}\n\n\\newcommand\\nn[1]{\n    % Input layer\n    \\foreach \\y in {1,...,2}\n        \\node[neuron, fill=green!40] (i\\y-#1) at (0,\\y+1) {$i\\y$};\n\n    % Hidden layer\n    \\foreach \\y in {1,...,4}\n        \\path node[neuron, fill=blue!40] (h\\y-#1) at (\\layersep,\\y) {$h\\y$};\n\n    % Output node\n    \\node[neuron, fill=red!40] (o-#1) at (2*\\layersep,2.5) {$o$};\n\n    % Connect every node in the input layer with every node in the hidden layer.\n    \\foreach \\source in {1,...,2}\n        \\foreach \\dest in {1,...,4}\n            \\path (i\\source-#1) edge (h\\dest-#1);\n\n    % Connect every node in the hidden layer with the output layer\n    \\foreach \\source in {1,...,4}\n        \\path (h\\source-#1) edge (o-#1);\n}\n\n\n\\begin{tikzpicture}[\n    scale=1.2,\n    shorten &gt;=1pt,-&gt;,draw=black!70, node distance=\\layersep,\n    neuron/.style={circle,fill=black!25,minimum size=20,inner sep=0},\n    edge/.style 2 args={pos={(mod(#1+#2,2)+1)*0.33}, font=\\tiny},\n    distro/.style 2 args={\n        edge={#1}{#2}, node contents={}, minimum size=0.6cm, path picture={\\draw[double=orange,white,thick,double distance=1pt,shorten &gt;=0pt] plot[variable=\\t,domain=-1:1,samples=51] ({\\t},{0.2*exp(-100*(\\t-0.05*(#1-1))^2 - 3*\\t*#2))});}\n      },\n    weight/.style 2 args={\n        edge={#1}{#2}, node contents={\\pgfmathparse{0.35*#1-#2*0.15}\\pgfmathprintnumber[fixed]{\\pgfmathresult}}, fill=white, inner sep=2pt\n      }\n  ]\n  \\nn{regular}\n\n  \\begin{scope}[xshift=8cm]\n    \\nn{bayes}\n  \\end{scope}\n\n  % Draw weights for all regular edges.\n  \\foreach \\i in {1,...,2}\n  \\foreach \\j in {1,...,4}\n  \\path (i\\i-regular) -- (h\\j-regular) node[weight={\\i}{\\j}];\n  \\foreach \\i in {1,...,4}\n  \\path (h\\i-regular) -- (o-regular) node[weight={\\i}{1}];\n\n  % Draw distros for all Bayesian edges.\n  \\foreach \\i in {1,...,2}\n  \\foreach \\j in {1,...,4}\n  \\path (i\\i-bayes) -- (h\\j-bayes) node[distro={\\i}{\\j}];\n  \\foreach \\i in {1,...,4}\n  \\path (h\\i-bayes) -- (o-bayes) node[distro={\\i}{1}];\n\\end{tikzpicture}\n\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#nndiagram",
    "href": "Misc/Visuals_Interactives/tikz_net.html#nndiagram",
    "title": "tikz",
    "section": "nndiagram",
    "text": "nndiagram\nhttps://github.com/ccfang2/nndiagram\nCode```{r}\n#| results: 'asis'\n#| eval: false\n\nlibrary(nndiagram)\nnnd &lt;- nndiagram(input=3, hidden=c(4,4,4))\ncat(paste(nnd,\"\\n\"))\n```\n\nCode```{r, engine = 'tikz'}\n#| eval: true\n#| cache: true\n\n\\def\\layersep{2.5cm} \n\\newcommand*\\circled[1]{\\tikz[baseline=(char.base)]{ \n  \\node[shape=rectangle,inner sep=3pt, draw=black!100, fill= black!25] (char) {#1};}} \n \n\n\\centering \n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!100, node distance=\\layersep, scale=1] \n  \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]; \n  \\tikzstyle{neuron}=[circle, draw=black!100, minimum size=17pt,inner sep=0pt]; \n  \\tikzstyle{input neuron}=[neuron]; \n  \\tikzstyle{output neuron}=[neuron]; \n  \\tikzstyle{hidden neuron}=[neuron]; \n  \\tikzstyle{annot} = [text width=4em, text centered, text=black!100] \n \n  % drawing neurons \n  \\foreach \\name / \\y in {1,...,3} \n      \\node [input neuron, pin=left:\\textcolor{black!100}{Input \\y}] (I-\\name) at (0,-0.5-\\y) {};\n  \\foreach \\name / \\y in {1/1,2/2,3/3,4/4} \n      \\path[yshift=0cm] \n          node[hidden neuron] (H-\\name) at (1* \\layersep,-\\y cm) {};\n  \\foreach \\name / \\y in {5/1,6/2,7/3,8/4} \n      \\path[yshift=0cm] \n          node[hidden neuron] (H-\\name) at (2* \\layersep,-\\y cm) {};\n  \\foreach \\name / \\y in {9/1,10/2,11/3,12/4} \n      \\path[yshift=0cm] \n          node[hidden neuron] (H-\\name) at (3* \\layersep,-\\y cm) {};\n  \\node[output neuron,pin={[pin edge={-&gt;}]right:\\textcolor{black!100}{Output}}, right of=H-10, yshift=-0.5cm] (O) {}; \n\n   % drawing arrows \n  \\foreach \\source in {1,...,3} \n      \\foreach \\dest in {1,...,4}\n           \\path (I-\\source) edge (H-\\dest); \n  \\foreach \\source in {1,...,4} \n      \\foreach \\dest in {5,...,8}\n           \\path (H-\\source) edge (H-\\dest); \n  \\foreach \\source in {5,...,8} \n      \\foreach \\dest in {9,...,12}\n           \\path (H-\\source) edge (H-\\dest); \n  \\foreach \\source in {9,...,12}\n           \\path (H-\\source) edge (O);\n \n   % annotations \n  \\node[annot,above of=I-1, node distance=2.5cm] {Input layer}; \n  \\node[annot,above of=I-1, node distance=1.5cm] {$\\circled{3}$}; \n  \\node[annot,above of=H-1, node distance=2cm] (hl1) {Hidden layer 1}; \n  \\node[annot,above of=H-1, node distance=1cm] (hl1) {$\\circled{4}$}; \n  \\node[annot,above of=H-5, node distance=2cm] (hl2) {Hidden layer 2}; \n  \\node[annot,above of=H-5, node distance=1cm] (hl2) {$\\circled{4}$}; \n  \\node[annot,above of=H-9, node distance=2cm] (hl3) {Hidden layer 3}; \n  \\node[annot,above of=H-9, node distance=1cm] (hl3) {$\\circled{4}$}; \n  \\node[annot,above of =O, node distance=3.5cm] {Output layer}; \n  \\node[annot,above of =O, node distance=2.5cm] {$\\circled{1}$}; \n\n\\end{tikzpicture} \n\n```\n\n\n\n\nLatent Space Projection\nhttps://tikz.net/manifold/\n\nCode```{{r, engine = 'tikz',engine.opts=list(extra.preamble = c(\"\\\\usepackage{cmbright}\"))}}\n#| eval: true\n#| cache: true\n\n\\usetikzlibrary{arrows.meta}\n\\definecolor{green}{rgb}{0.0,0.50,0.0}\n\\tikzset{&gt;={Straight Barb[angle'=80, scale=1.1]}}\n\\begin{tikzpicture}\n\n\\draw[-&gt;] (0, 0) -- ++(0, 2);\n\\draw[-&gt;] (0, 0) -- ++(2.5, 0.6);\n\\draw[-&gt;] (0, 0) -- ++(3, 0) node[midway, below, yshift=-0.5em]\n    {Original space ${\\cal X}$};\n\n\\draw[fill=green!50, draw=none, shift={(0.2, 0.7)},scale=0.5]\n  (0, 0) to[out=20, in=140] (1.5, -0.2) to [out=60, in=160]\n  (5, 0.5) to[out=130, in=60]\n  cycle;\n\n\\shade[thin, left color=green!10, right color=green!50, draw=none,\n  shift={(0.2, 0.7)},scale=0.5]\n  (0, 0) to[out=10, in=140] (3.3, -0.8) to [out=60, in=190] (5, 0.5)\n    to[out=130, in=60] cycle;\n\n  \\draw[-&gt;] (4.8, 0.8) -- ++(0, 2);\n  \\draw[-&gt;] (4.8, 0.8) -- ++(2, 0) node[midway, below, yshift=-0.5em]\n      {Latent space ${\\cal F}$};\n\n  \\draw[thin, fill=green!30, draw=none, shift={(5.4, 1.1)}, rotate=20]\n    (0, 0) -- (1, 0) -- (1, 1) -- (0, 1) -- cycle;\n\n  \\draw[thick,-&gt;,red]\n    (1.5, 1.3) to [out=55, in=150] node[midway, above, xshift=6pt, yshift=2pt]\n    {$f$} (5.7, 2);\n\n  \\draw[thick,-&gt;,blue] (1.5, 1.3) ++(4.03, 0.3) to [out=150, in=55]\n    node[midway, below, xshift=2pt, yshift=-2pt] {$g$} ++(-3.6, -0.5);\n\n\\end{tikzpicture}\n\n```\n\n\n\n\nFlow\nhttps://tikz.net/maf/\n\nCode```{r, engine = 'tikz'}\n#| cache: true\n\n\\usetikzlibrary{calc,positioning}\n\n\n\\begin{tikzpicture}[\n    thick, text centered,\n    box/.style={draw, thin, minimum width=1cm},\n    func/.style={circle, text=white},\n    input/.style={draw=red, very thick},\n  ]\n\n  % x nodes\n  \\node[box, input, fill=blue!20] (x1) {$x_1$};\n  \\node[box, input, fill=blue!20, right of=x1] (x2) {$x_2$};\n  \\node[right of=x2] (xdots1) {\\dots};\n  \\node[box, input, fill=blue!20, right of=xdots1] (xd) {$x_d$};\n  \\node[box, fill=green!60!black, text opacity=1, opacity=0.4, right=2 of xd] (xdp1) {$x_{d+1}$};\n  \\node[right of=xdp1] (xdots2) {\\dots};\n  \\node[box, fill=green!60!black, text opacity=1, opacity=0.4, right of=xdots2] (xD) {$x_D$};\n\n  % z nodes\n  \\node[box, fill=blue!20, below=3 of x1] (z1) {$z_1$};\n  \\node[box, fill=blue!20, right of=z1] (z2) {$z_2$};\n  \\node[right of=z2] (zdots1) {\\dots};\n  \\node[box, fill=blue!20, right of=zdots1] (zd) {$z_d$};\n  \\node[box, input, fill=orange!40, right=2 of zd] (zdp1) {$z_{d+1}$};\n  \\node[right of=zdp1] (zdots2) {\\dots};\n  \\node[box, fill=orange!40, right of=zdots2] (zD) {$z_D$};\n\n  % z to x lines\n  \\draw[-&gt;] (zdp1) -- (xdp1);\n\n  % scale and translate functions\n  \\node[func, font=\\large, fill=teal, above right=0.1] (t) at ($(zd)!0.5!(xdp1)$) {$t$};\n  \\fill[teal, opacity=0.5] (x1.south west) -- (t.center) -- (xd.south east) -- (x1.south west);\n\n  \\node[func, font=\\large, fill=orange, below left=0.1] (s) at ($(zd)!0.5!(xdp1)$) {$s$};\n  \\fill[orange, opacity=0.5] (x1.south west) -- (s.center) -- (xd.south east) -- (x1.south west);\n\n  % feeding in s and t\n  \\node[func, inner sep=0, fill=orange] (odot1) at ($(zdp1)!0.4!(xdp1)$) {$\\odot$};\n  \\node[func, inner sep=0, fill=teal] (oplus1) at ($(zdp1)!0.7!(xdp1)$) {$\\oplus$};\n  \\draw[orange, -&gt;] (s) to[bend right=5] (odot1);\n  \\draw[teal, -&gt;] (t) to[bend right=5] (oplus1);\n\n\\end{tikzpicture}\n\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/tikz_net.html#alm",
    "href": "Misc/Visuals_Interactives/tikz_net.html#alm",
    "title": "tikz",
    "section": "ALM",
    "text": "ALM\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n\\usetikzlibrary{positioning}\n\n\\def\\layersep{3.5cm}\n\n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!50, node distance=\\layersep]\n    \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{annot} = [text width=4em, text centered]\n\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,2}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {};\n\n    % Draw the output layer nodes\n    \\foreach \\name / \\y in {1,2,3}\n        \\path[yshift=1.0cm]\n            node[output neuron] (O-\\name) at (\\layersep,-\\y cm) {};\n\n    % Connect every node in the input layer with every node in the\n    % output layer.\n    \\foreach \\source in {1,2}\n        \\foreach \\dest in {1,2,3}\n            \\path (I-\\source) edge (O-\\dest);\n\n    % Annotate the layers with equations\n    \\node[above of=I-1, node distance=1.5cm] (il) {$a_i(X)=e^{-\\gamma \\cdot (X-X_i)^2}$ \\\\ $\\frac{a_i(X)}{\\sum a_i(X)}$};\n    \\node[above of=O-1, node distance=1.5cm] (ol) {$O_j(X)=\\sum_{i=1}^M w_{j i} \\cdot a_i(X)$ \\\\ $P[Y_j|X]=\\frac{O_j(X)}{\\sum_{k=1}^L O_k(X)}$};\n\\end{tikzpicture}\n\n```\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n#|\n\\usetikzlibrary{positioning}\n\n\\def\\layersep{3.5cm}\n\n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!50, node distance=\\layersep]\n    \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{annot} = [text width=4em, text centered]\n\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,2}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {$X_{\\y}$};\n\n    % Draw the output layer nodes\n    \\foreach \\name / \\y in {1,2,3}\n        \\path[yshift=1.0cm]\n            node[output neuron] (O-\\name) at (\\layersep,-\\y cm) {$Y_{\\y}$};\n\n    % Connect every node in the input layer with every node in the\n    % output layer.\n    \\foreach \\source in {1,2}\n        \\foreach \\dest in {1,2,3}\n            \\path (I-\\source) edge (O-\\dest);\n\n    % Annotate the layers with equations\n    \\node[above of=I-1, node distance=1.5cm] (il) {$a_i(X)=e^{-\\gamma \\cdot (X-X_i)^2}$ \\\\ $\\frac{a_i(X)}{\\sum a_i(X)}$};\n    \\node[above of=O-1, node distance=1.5cm] (ol) {$O_j(X)=\\sum_{i=1}^M w_{j i} \\cdot a_i(X)$ \\\\ $P[Y_j|X]=\\frac{O_j(X)}{\\sum_{k=1}^L O_k(X)}$};\n\\end{tikzpicture}\n\n```\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n#|\n\\usetikzlibrary{positioning, fit}\n\n\\def\\layersep{3.5cm}\n\n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!50, node distance=\\layersep]\n    \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{annot} = [text width=4em, text centered]\n\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,2}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {$X_{\\y}$};\n\n    % Draw the output layer nodes\n    \\foreach \\name / \\y in {1,2,3}\n        \\path[yshift=1.0cm]\n            node[output neuron] (O-\\name) at (\\layersep,-\\y cm) {$Y_{\\y}$};\n\n    % Connect every node in the input layer with every node in the\n    % output layer.\n    \\foreach \\source in {1,2}\n        \\foreach \\dest in {1,2,3}\n            \\path (I-\\source) edge (O-\\dest);\n\n    % Annotate the layers with equations\n    \\node[above of=I-1, node distance=1.5cm] (il) {$a_i(X)=e^{-\\gamma \\cdot (X-X_i)^2}$ \\\\ $\\frac{a_i(X)}{\\sum a_i(X)}$};\n    \\node[above of=O-1, node distance=1.5cm] (ol) {$O_j(X)=\\sum_{i=1}^M w_{j i} \\cdot a_i(X)$ \\\\ $P[Y_j|X]=\\frac{O_j(X)}{\\sum_{k=1}^L O_k(X)}$};\n\n    % Draw rectangles over the input and output layer nodes\n    \\node[rectangle, draw=black, inner sep=0.5cm, fit=(I-1) (I-2)] (inputbox) {};\n    \\node[rectangle, draw=black, inner sep=0.5cm, fit=(O-1) (O-2) (O-3)] (outputbox) {};\n\n\\end{tikzpicture}\n\n```\n\n\n\nCode```{r, engine = 'tikz'}\n#| eval: false\n\n\\usetikzlibrary{positioning, fit, calc}\n\n\\def\\layersep{5cm}\n\\begin{tikzpicture}[shorten &gt;=1pt,-&gt;,draw=black!50, node distance=\\layersep]\n    \\tikzstyle{every pin edge}=[&lt;-,shorten &lt;=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{annot} = [text centered]\n\n    % Draw the input layer nodes horizontally\n    \\foreach \\name / \\y in {1,2}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at ($(1.5*\\y cm,0)$) {$X_{\\y}$};\n\n    % Draw the output layer nodes\n    \\foreach \\name / \\y in {1,2,3}\n        \\node[output neuron] (O-\\name) at ($(1.5*\\name cm, -\\layersep)$) {$Y_{\\y}$};\n\n    % Connect every node in the input layer with every node in the\n    % output layer.\n    \\foreach \\source in {1,2}\n        \\foreach \\dest in {1,2,3}\n            \\path (I-\\source) edge (O-\\dest);\n\n    % Add input stimulus symbol above the input layer\n    \\node[above=2cm of I-1, anchor=south] (input-stimulus) {Stimulus $S$};\n\n    % Add output response symbol below the output layer\n    \\node[below=2cm of O-2, anchor=north] (output-response) {Response $R$};\n\n    % Annotate the layers with equations\n    \\node[left=4cm of I-1, anchor=east, font=\\small, text width=5cm] (ile) {$a_i(X)=e^{-\\gamma \\cdot (X-X_i)^2}$ \\\\ $\\frac{a_i(X)}{\\sum a_i(X)}$};\n    \\node[left=4cm of O-1, anchor=east, font=\\small, text width=5cm] (ole) {$O_j(X)=\\sum_{i=1}^M w_{j i} \\cdot a_i(X)$ \\\\ $P[Y_j|X]=\\frac{O_j(X)}{\\sum_{k=1}^L O_k(X)}$};\n\n    % Add rectangles around input and output layers\n    \\node[draw,rectangle,fit=(I-1) (I-2),minimum width=4cm, label=above:Input Layer] (input-rect) {};\n    \\node[draw,rectangle,fit=(O-1) (O-2) (O-3),minimum width=4cm, label=above:Output Layer] (output-rect) {};\n\n    % Add rectangle for the decoding process\n    \\node[draw,rectangle,fit=(output-response), label=above:Decoding Process, minimum width=3cm] (decoding-rect) {};\n\n    % Add arrow from output layer to decoding process\n    \\draw[-&gt;,thick] (output-rect) -- (decoding-rect);\n\n\\end{tikzpicture}\n\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/ojs_alm.html",
    "href": "Misc/Visuals_Interactives/ojs_alm.html",
    "title": "OJS ALM",
    "section": "",
    "text": "Codepacman::p_load(tidyverse)\nd &lt;- tibble(x=1:20,y=x^2)\nojs_define(d = d)\n\ninputNodes = seq(1,7,1)  # \noutputNodes = seq(50,1600,50)\n#wm=matrix(rnorm(length(inputNodes)*length(outputNodes),5,2),nrow=length(outputNodes),ncol=length(inputNodes))\n#ojs_define(iN = inputNodes, outputNodes = outputNodes, wm = wm)\n\n\n\nCoded3 = require(\"d3@7\")\nmath = require('mathjs')\n// let inputNodes = Array.from({ length: 7 }, (_, i) =&gt; i + 1);\n// let outputNodes = Array.from({ length: 32 }, (_, i) =&gt; (i + 1) * 50);\n// let wm = Array.from({ length: outputNodes.length }, () =&gt;\n//   Array.from({ length: inputNodes.length }, () =&gt; 0.0)\n// );\n\nfunction inputActivation(xTarget, c) {\n  console.log(inputNodes)\n  return inputNodes.map((inputNode) =&gt;\n    Math.exp(-1 * c * Math.pow(xTarget - inputNode, 2))\n  );\n}\n\n\nfunction outputActivation(xTarget, weights, c) {\n  const inputAct = inputActivation(xTarget, c);\n  return math.multiply(weights, inputAct);\n}\n\nfunction meanPrediction(xTarget, weights, c) {\n  const outputAct = outputActivation(xTarget, weights, c);\n  const probability = math.divide(outputAct, math.sum(outputAct));\n  return math.multiply(outputNodes, probability);\n}\n\n\nfunction updateWeights(xNew, yNew, weights, c, lr) {\n  const yFeedbackActivation = outputNodes.map(\n    (outputNode) =&gt; Math.exp(-1 * c * Math.pow(yNew - outputNode, 2))\n  );\n //console.log(yFeedbackActivation)\n  const xFeedbackActivation = outputActivation(xNew, weights, c);\n  const inputAct = inputActivation(xNew, c);\n  const inputActReshaped = math.reshape(inputAct, [inputAct.length, 1]);\n  const yFeedbackActivationReshaped = math.reshape(yFeedbackActivation, [yFeedbackActivation.length, 1]);\n  const xFeedbackActivationReshaped = math.reshape(xFeedbackActivation, [xFeedbackActivation.length, 1]);\n  const error = math.reshape(math.subtract(yFeedbackActivationReshaped, xFeedbackActivationReshaped), [yFeedbackActivation.length, 1]);\n  // console.log(math.size(math.transpose(inputActReshaped)))\n  const weightUpdate = math.multiply(error, math.transpose(inputActReshaped));\n  //console.log(weightUpdate)\n  const raw_Weights = math.add(weights, math.multiply(lr, weightUpdate));\n\n  const new_Weights = raw_Weights\n  //return JSON.parse(result);\n  return(new_Weights)\n}\n\nfunction randomNormal(mean, sd) {\n  let u = 0,\n    v = 0;\n  while (u === 0) u = Math.random();\n  while (v === 0) v = Math.random();\n  const z = Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2.0 * Math.PI * v);\n  return mean + z * sd;\n}\n\nfunction examPrediction(xTarget, weights, c, trainVec) {\n  const nearestTrain = trainVec[math.argmin(math.abs(trainVec - xTarget))];\n  const aResp = meanPrediction(nearestTrain, weights, c);\n  const xUnder = math.min(trainVec) === nearestTrain ? nearestTrain : trainVec[math.findIndex(trainVec, (d) =&gt; d === nearestTrain) - 1];\n  const xOver = math.max(trainVec) === nearestTrain ? nearestTrain : trainVec[math.findIndex(trainVec, (d) =&gt; d === nearestTrain) + 1];\n  const mUnder = meanPrediction(xUnder, weights, c);\n  const mOver = meanPrediction(xOver, weights, c);\n  const examOutput = math.round(aResp + ((mOver - mUnder) / (xOver - xUnder)) * (xTarget - nearestTrain), 3);\n  return examOutput;\n}\n\n\nfunction trainALM(dat, c, lr, weights) {\n    console.log('training')\n  const almTrain = new Array(dat.input.length).fill(NaN);\n  for (let i = 0; i &lt; dat.input.length; i++) {\n    console.log('i: ', i, ' dat.input[i]: ', dat.input[i], ' dat.vx[i]: ', dat.vx[i], ' c: ', c, ' lr: ', lr)\n    weights = updateWeights(dat.input[i], dat.vx[i], weights, c, lr);\n    const resp = math.round(meanPrediction(dat.input[i], weights, c),0);\n    // round resp to 1 decimal place\n    almTrain[i] = resp;\n      weights = math.map(weights, (value) =&gt; {\n        return value &lt; 0 ? 0 : value;\n      });\n  }\n  console.log('almTrain: ', almTrain)\n  console.log(weights)\n  return {almTrain, weights};\n}\n\nfunction trainTestALM(dat, c = 0.05, lr = 0.5, weights, testVec) {\n  const almTrain = new Array(dat.length).fill(NaN);\n  \n  for (let i = 0; i &lt; dat.length; i++) {\n    weights = updateWeights(dat[i].input, dat[i].vx, weights, c, lr);\n    const resp = meanPrediction(dat[i].input, weights, c);\n    almTrain[i] = resp;\n    weights = math.map(weights, (value) =&gt; {\n      return value &lt; 0 ? 0 : value;\n    });\n  }\n\n  const almPred = testVec.map((value) =&gt; {\n    return meanPrediction(value, weights, c);\n  });\n\n  const examPred = testVec.map((value) =&gt; {\n    return examPrediction(value, weights, c, [1, ...math.sort(math.unique(dat.map((d) =&gt; d.input)))]);\n  });\n    \n  return { almTrain, almPred, examPred };\n}\n\n// Modify the sim_data function to accept the dataset as an argument\n// function sim_data(dat, c=0.5, lr=0.2, inNodes=7, outNodes=32, trainVec=[5,6,7]) {\n//   inputNodes = math.range(1,7,inNodes).toArray();  \n//   outputNodes = math.range(50,1600,outNodes).toArray(); \n//   wm = math.zeros(outputNodes.length, inputNodes.length)._data;\n//   tt = trainTest_alm(dat, c, lr, wm, trainVec);\n// }\n\nfunction gen_train(trainVec, trainRep, noise) {\n   let bandVec=[0,100,350,600,800,1000,1200];\n   let ts = [];\n   for (let i=0; i&lt;trainRep; i++) {\n       ts.push(...trainVec);\n   }\n    let mean = 0;\n    let stdDev = 1;\n   //let noiseVec = math.random([ts.length])._data;\n   //noiseVec = math.multiply(noiseVec, noise)._data;\n   //if(noise==0) {noiseVec=noiseVec*0}\n   let inputArr = [];\n   let vxArr = [];\n   for (let i=0; i&lt;ts.length; i++) {\n       inputArr.push(ts[i]);\n       vxArr.push(bandVec[ts[i]]);\n       //vxArr.push(bandVec[ts[i]]+noiseVec[i]);\n   }\n   return {input: inputArr, vx: vxArr};\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation\n\n\nCodeviewof c = Inputs.range([.0001, 2], {value: .00005, step: .05, label: \"c value:\"})\nviewof lr = Inputs.range([.001, 2], {value: .05, step: .01, label: \"lr value:\"})\n\nviewof n_inputNodes = Inputs.range([1, 50], {value: 7, step: 1, label: \"N Input Nodes:\"})\nviewof n_outputNodes = Inputs.range([1, 200], {value: 32, step: 1, label: \"N Output Nodes:\"})\n\nviewof weight_mean = Inputs.range([0, 1], {value: 0, step: .0005, label: \"initial weight mean:\"})\nviewof weight_sd = Inputs.range([.00000001, 1], {value: .000001, step: .0001, label: \"initial weight sd:\"})\n\nviewof trainRep = Inputs.range([4, 50], {value: 1, step: 1, label: \"Train Reps:\"})\n\n //inputNodes = Array.from({ length: n_inputNodes }, (_, i) =&gt; i + 1);\n inputNodes = Array.from({ length: n_inputNodes }, (_, i) =&gt; 1 + i * (7 - 1) / (n_inputNodes - 1));\n\n\n\nstart = 0;\nend = 1800;\nN_Steps = n_outputNodes; // replace with desired length\nstepSize = (end - start) / (N_Steps - 1);\noutputNodes = Array.from({ length: N_Steps }, (_, i) =&gt; start + i * stepSize);\n\nconsole.log(inputNodes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeconsole.log(outputNodes)\n\n\n\n\n\n\n\nCodewm = outputNodes.map(() =&gt; {\n  return inputNodes.map(() =&gt; randomNormal(weight_mean, weight_sd));\n});\n\nnoise=0\nviewof trainVec = Inputs.checkbox([1, 2, 3, 4, 5, 6], {value: [4,5,6], label: \"Select training examples:\"});\ngd = gen_train(trainVec, trainRep, noise);\n\n//trainVec = [1,2,4,5,6];\n//gd = gen_train(trainVec, trainRep, noise)\n// w2= updateWeights(4, 800, wm,c,lr)\n\ntalm = trainALM(gd, c, lr, wm);\n\n//inputNodes = transpose(iN)\nia = inputActivation(inputX, c, inputNodes)\noa = outputActivation(inputX, wm, c)\nmp = meanPrediction(inputX, wm, c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodetdat = gd.vx.map((value, index) =&gt; {\n  return { Trial: index, Vx: value, Response: talm.almTrain[index], Error: Math.abs(value -  talm.almTrain[index]) };\n});\n\n\n\n\n\n\n\n\nVx Across Training\nCodePlot.plot({\n  marks: [\n    Plot.line(tdat, {\n      x: \"Trial\",      // feature for the x channel\n      y: \"Response\",     // feature for the y channel\n      stroke: \"Vx\",     \n    }),\n  ],\n  x: {label: \"Trial Number\"},\n  y: {label: \"Vx\", domain: [0, 1800],grid: true},\n  color: {legend: true, scheme: \"Turbo\",type: \"categorical\"},\n  width: 400,\n  height: 400\n});\n  //caption: html`Figure 1. This chart has a &lt;i&gt;fancy&lt;/i&gt; caption.`\n\n\n\n\n\nTraining Error\nCodePlot.plot({\n  marks: [\n    Plot.line(tdat, {\n      x: \"Trial\",      // feature for the x channel\n      y: \"Error\",     // feature for the y channel\n      stroke: \"Vx\",     // feature for the fill channel\n    }),\n  ],\n  y: {label: \"Error\",grid: true},\n  color: {legend: true, scheme: \"Turbo\",type: \"categorical\"},\n  width: 400,\n  height: 400\n});\n\n\n\n\n\n\n\n\n\n\nWeight Matrices\n\nCodePlotly = require(\"https://cdn.plot.ly/plotly-latest.min.js\")\n//div = DOM.element('div');\nP1=Plotly.newPlot(\"plot-canvas\", [{\n  z: wm,\n  x: outputNodes,\n  y: inputNodes,\n  type: 'heatmap',\n  colorscale: 'Viridis'\n}],{width:500});\n\nconsole.log(inputNodes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeconsole.log(outputNodes)\n\n\n\n\n\n\n\nCodeP2=Plotly.newPlot(\"plot-tw\", [{\n  z: talm.weights,\n  x: inputNodes,\n  y: outputNodes,\n  type: 'heatmap',\n  colorscale: 'Viridis'\n}],{width:600});\n\n\n\n\n\n\n\n\n\n\nStarting Weights\n\nFinal Weights\n\n\n\n\nInput and Output layer activations\n\nCodein_data = ia.map((value, index) =&gt; {\n  return { Node: inputNodes[index], Activation: value };\n});\n\n out_data = oa.map((value, index) =&gt; {\n  return { Node: outputNodes[index], Activation: value };\n});\n\nviewof inputX = Inputs.range([1, 7], {value: 4, step: 1, label: \"input value:\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodePlot.plot({\n  marks: [\n    Plot.dot(in_data, {\n      x: \"Node\",      // feature for the x channel\n      y: \"Activation\",     // feature for the y channel\n    }),\n  ],\n  width: 400,\n  height: 200,\n  title: \"Input Activation Plot\",\n});\nPlot.plot({\n  marks: [\n    Plot.dot(out_data, {\n      x: \"Node\",      // feature for the x channel\n      y: \"Activation\",     // feature for the y channel\n    }),\n  ],\n  width: 400,\n  height: 200,\n  title: \"Output Activation Plot\",\n});\n\n\n\n\n\n\n\nInput Activation\n\n\n\n\n\n\n\nSecond\n\n\n\n\n\nCharts\n\n\n\n\n\nCodeconsole.log(wm)\n\n\n\n\n\n\n\nCodeconsole.log(talm.weights)\n\n\n\n\n\n\n\n\nTesting\n\nCodeviewof xV = Inputs.range(\n  [1, 20], \n  {value: 1, step: 1, label: \"x range:\"}\n)\nviewof yV = Inputs.range(\n  [1, 400], \n  {value: 1, step: 10, label: \"y range:\"}\n)\n\ndO = transpose(d)\nfiltered = dO.filter(function(dO) {\n  return dO.x&gt;=xV && dO.y &gt;= yV;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTab 1\nTab 2\n\n\n\n\nCodePlot.plot({\n  marks: [\n    Plot.dot(filtered, \n      { x: \"x\", y: \"y\"}, \n      { stroke: \"black\" }\n    )\n  ]\n})\n\n\n\n\n\n\nx:  y: \n\n\n\nCode//Plot = require(\"plot\")\nPlot.plot({\n  marks: [\n    Plot.line(transpose(d), \n      { x: \"x\", y: \"y\"}, \n      { stroke: \"black\" }\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n```{ojs}\n//| include: false\n```"
  },
  {
    "objectID": "Misc/Visuals_Interactives/mermaid_consort.html",
    "href": "Misc/Visuals_Interactives/mermaid_consort.html",
    "title": "Mermaid with R + consort",
    "section": "",
    "text": "Code#lapply(c('tidyverse','data.table','igraph','ggraph','kableExtra'),library,character.only=TRUE))\npacman::p_load(tidyverse,data.table,igraph,ggraph,kableExtra,DiagrammeR,png,consort,data.table)\nflowchart LR\n    data1(Varied Training&lt;br/&gt;800-1000&lt;br/&gt;1000-1200&lt;br/&gt;1200-1400)\n    data2(Constant Training&lt;br/&gt;800-1000)\n    Test1(Testing - No Feedback&lt;br/&gt;100-300&lt;br/&gt;350-550&lt;br/&gt;600-800)\n    Test2(Test From Train&lt;br/&gt;800-1000&lt;br/&gt;1000-1200&lt;br/&gt;1200-1400)\n    Test3(Testing - Feedback&lt;br/&gt;100-300&lt;br/&gt;350-550&lt;br/&gt;600-800)\n\n    data1 --&gt; Test1\n    data2 --&gt; Test1\n    Test1 --&gt; Test2\n    Test2 --&gt; Test3\n    \n%% https://quarto.org/docs/authoring/diagrams.html\n\n\nMean Vx over training blocks\ndata1\nVaried Training800-10001000-12001200-1400Test1\nTesting - No Feedback100-300350-550600-800data1-&gt;Test1\ndata2\nConstant Training800-1000data2-&gt;Test1\nTest2\nTest From Train800-10001000-12001200-1400Test1-&gt;Test2\nTest3\nTesting - Feedback100-300350-550600-800Test2-&gt;Test3\n\n\nFigure 1: This is a simple graphviz graph.\ndata1\nVaried Training800-10001000-12001200-1400Test1\nTesting - No Feedback100-300350-550600-800data1-&gt;Test1\ndata2\nConstant Training800-1000data2-&gt;Test1\nTest2\nTest From Train800-10001000-12001200-1400Test1-&gt;Test2\nTest3\nTesting - Feedback100-300350-550600-800Test2-&gt;Test3\n\n\nFigure 2: This is a simple graphviz graph."
  },
  {
    "objectID": "Misc/Visuals_Interactives/mermaid_consort.html#subgraph",
    "href": "Misc/Visuals_Interactives/mermaid_consort.html#subgraph",
    "title": "Mermaid with R + consort",
    "section": "subgraph",
    "text": "subgraph\n\n\n\n\ncluster_counterbalanced\nCounterbalanced Orderdata1\nVaried Training800-10001000-12001200-1400subgraph1\nsubgraph1data1-&gt;subgraph1\ndata2\nConstant Training800-1000data2-&gt;subgraph1\nTest1\nTesting - No Feedback100-300350-550600-800Test2\nTest From Train800-10001000-12001200-1400Test3\nTesting - Feedback100-300350-550600-800subgraph1-&gt;Test3\n\n\n\n\n\n\n\n\n\ncluster\nCounterbalanced Orderdata1\nVaried Training800-10001000-12001200-1400Test1\nTesting - No Feedback100-300350-550600-800data1-&gt;Test1\ndata2\nConstant Training800-1000data2-&gt;Test1\nTest3\nTesting - Feedback100-300350-550600-800Test2\nTest From Train800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\n\ncluster\nCounterbalanced Orderdata1\nVaried Training800-10001000-12001200-1400Test1\nTesting - No Feedback100-300350-550600-800data1-&gt;Test1\ndata2\nConstant Training800-1000data2-&gt;Test1\nTest3\nTesting - Feedback100-300350-550600-800Test2\nTest From Train800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\n\nALM\nInput1\nInput1Input2\nInput2Output1\nOutput1Input1-&gt;Output1\nw_{11}Output2\nOutput2Input1-&gt;Output2\nw_{12}Output3\nOutput3Input1-&gt;Output3\nw_{13}Input2-&gt;Output1\nw_{21}Input2-&gt;Output2\nw_{22}Input2-&gt;Output3\nw_{23}equation1\na_i(X) = e^{-γ(X-X_i)^2}, normalized: a_i(X) / Σa_i(X)equation2\nO_j(X) = Σ w_{ji} ⋅ a_i(X), P[Y_j | X] = O_j(X) / Σ O_k(X)\n\n\n\n\ntest\n\n\n\n\nALM\ncluster_input\nInput Layercluster_output\nOutput LayerInput1\nInput1Output1\nOutput1Input1-&gt;Output1\nw_11Output2\nOutput2Input1-&gt;Output2\nw_12Output3\nOutput3Input1-&gt;Output3\nw_13Input2\nInput2Input2-&gt;Output1\nw_21Input2-&gt;Output2\nw_22Input2-&gt;Output3\nw_23equation1\nInput activation ai(X) = exp(-gamma*(X-Xi)^2), normalized by dividing by sumequation2\nOutput Oj(X) = sum(w_ji * ai(X)), Probability P[Yj | X] = Oj(X) / sum(Ok(X))\n\n\n\n\ntest2\n\n\n\n\nG\nE1\n⋮z\nzE2\n⋮x100\nx100a\nab\nbx1\nx1a-&gt;x1\nx2\nx2a-&gt;x2\na-&gt;x100\nb-&gt;x1\nb-&gt;x2\nb-&gt;x100\nz-&gt;x1\nz-&gt;x2\nz-&gt;x100\nY\nYx1-&gt;Y\nx2-&gt;Y\nx100-&gt;Y\n\n\n\n\n\ntest3\n\n\n\n\nG\nclusterX\nn1n2n1-&gt;n2\nn3n1-&gt;n3\nn4\nn2-&gt;n4\nn5\nn4-&gt;n5\nn6\nn5-&gt;n6\nJ1J2\nJ1-&gt;J2\nJ6\nJ1-&gt;J6\nJ4J5\nJ5-&gt;J4\nJ6-&gt;J5\n\n\n\n\n\ntest4\nNest Cluster - GraphViz\n\n\n\n\nD\ncluster_p\nParentcluster_c1\nChild onecluster_gc_2\nGrand-Child twocluster_gc_1\nGrand-Child onecluster_c2\nChild twoa\nab\nbc\ncd\nde\ne\n\n\n\n\n\n\n\n\nG\nE1\nE1z\nzE2\n⋮x100\nx100a\nab\nbx1\nx1a-&gt;x1\nx2\nx2a-&gt;x2\na-&gt;x100\nb-&gt;x1\nb-&gt;x2\nb-&gt;x100\nz-&gt;x1\nz-&gt;x2\nz-&gt;x100\nY\nYx1-&gt;Y\nx2-&gt;Y\nx100-&gt;Y\n\n\n\n\n\ntest5\n\n\n\n\nG\nn1n2n1-&gt;n2\nn4n2-&gt;n4\n\n\n\n\n\ntest6\n\n\n\n\nALM\nInput1\nX1Input2\nX2Output1\nY1Input1:e-&gt;Output1:w\nw11Output2\nY2Input1:e-&gt;Output2:w\nw12Output3\nY3Input1:e-&gt;Output3:w\nw13Input2:e-&gt;Output1:w\nw21Input2:e-&gt;Output2:w\nw22Input2:e-&gt;Output3:w\nw23label_input\nInput Layerlabel_output\nOutput Layer\n\n\n\n\ntest7\n\n\n\n\nG\ncluster_0\nlayer 1 (input layer)cluster_1\nlayer 2 (hidden layer)cluster_2\nlayer 3 (hidden layer)cluster_3\nlayer 4 (output layer)x0\nx0a02\na0(2)x0:e-&gt;a02:wa12\na5(2)x0:e-&gt;a12:w\na22\na1(2)x0:e-&gt;a22:w\na32\na2(2)x0:e-&gt;a32:w\na42\na3(2)x0:e-&gt;a42:w\na52\na4(2)x0:e-&gt;a52:w\nx1\nx1x1:e-&gt;a12:w\nx1:e-&gt;a22:w\nx1:e-&gt;a32:w\nx1:e-&gt;a42:w\nx1:e-&gt;a52:w\nx2\nx2x2:e-&gt;a12:w\nx2:e-&gt;a22:w\nx2:e-&gt;a32:w\nx2:e-&gt;a42:w\nx2:e-&gt;a52:w\nx3\nx3x3:e-&gt;a12:w\nx3:e-&gt;a22:w\nx3:e-&gt;a32:w\nx3:e-&gt;a42:w\nx3:e-&gt;a52:w\na03\na0(3)a02:e-&gt;a03:wa13\na3(3)a02:e-&gt;a13:w\na23\na4(3)a02:e-&gt;a23:w\na33\na5(3)a02:e-&gt;a33:w\na43\na1(3)a02:e-&gt;a43:w\na53\na2(3)a02:e-&gt;a53:w\na12:e-&gt;a13:w\na12:e-&gt;a23:w\na12:e-&gt;a33:w\na12:e-&gt;a43:w\na12:e-&gt;a53:w\na22:e-&gt;a13:w\na22:e-&gt;a23:w\na22:e-&gt;a33:w\na22:e-&gt;a43:w\na22:e-&gt;a53:w\na32:e-&gt;a13:w\na32:e-&gt;a23:w\na32:e-&gt;a33:w\na32:e-&gt;a43:w\na32:e-&gt;a53:w\na42:e-&gt;a13:w\na42:e-&gt;a23:w\na42:e-&gt;a33:w\na42:e-&gt;a43:w\na42:e-&gt;a53:w\na52:e-&gt;a13:w\na52:e-&gt;a23:w\na52:e-&gt;a33:w\na52:e-&gt;a43:w\na52:e-&gt;a53:w\nO1\na1(4)a03:e-&gt;O1:w\nO2\na2(4)a03:e-&gt;O2:w\nO3\na3(4)a03:e-&gt;O3:w\nO4\na4(4)a03:e-&gt;O4:w\na13:e-&gt;O1:w\na13:e-&gt;O2:w\na13:e-&gt;O3:w\na13:e-&gt;O4:w\na23:e-&gt;O1:w\na23:e-&gt;O2:w\na23:e-&gt;O3:w\na23:e-&gt;O4:w\na33:e-&gt;O1:w\na33:e-&gt;O2:w\na33:e-&gt;O3:w\na33:e-&gt;O4:w\na43:e-&gt;O1:w\na43:e-&gt;O2:w\na43:e-&gt;O3:w\na43:e-&gt;O4:w\na53:e-&gt;O1:w\na53:e-&gt;O2:w\na53:e-&gt;O3:w\na53:e-&gt;O4:w\n\n\n\n\n\ntest8\n\n\n\n\nG\ncluster_0\nlayer 1 (Input layer)cluster_1\nlayer 2 (hidden layer)cluster_2\nlayer 3 (output layer)x1\na12\nx1-&gt;a12\na22\nx1-&gt;a22\na32\nx1-&gt;a32\nx2\nx2-&gt;a12\nx2-&gt;a22\nx2-&gt;a32\nx3\nx3-&gt;a12\nx3-&gt;a22\nx3-&gt;a32\nO\na12-&gt;O\na22-&gt;O\na32-&gt;O\n\n\n\n\n\n\n\n\n\nG\nx0\nx0x1\nx1a12\na1(2)x0:e-&gt;a12:w\na22\na2(2)x0:e-&gt;a22:w\na32\na3(2)x0:e-&gt;a32:w\na42\na4(2)x0:e-&gt;a42:w\na52\na5(2)x0:e-&gt;a52:w\na02\na0(2)a03\na0(3)a13\na1(3)a02:e-&gt;a13:w\na23\na2(3)a02:e-&gt;a23:w\na33\na3(3)a02:e-&gt;a33:w\na43\na4(3)a02:e-&gt;a43:w\na53\na5(3)a02:e-&gt;a53:w\nO1\na1(4)a03:e-&gt;O1:w\nO2\na2(4)a03:e-&gt;O2:w\nO3\na3(4)a03:e-&gt;O3:w\nO4\na4(4)a03:e-&gt;O4:w\nx2\nx2x1:e-&gt;a12:w\nx1:e-&gt;a22:w\nx1:e-&gt;a32:w\nx1:e-&gt;a42:w\nx1:e-&gt;a52:w\nx3\nx3x2:e-&gt;a12:w\nx2:e-&gt;a22:w\nx2:e-&gt;a32:w\nx2:e-&gt;a42:w\nx2:e-&gt;a52:w\nx3:e-&gt;a12:w\nx3:e-&gt;a22:w\nx3:e-&gt;a32:w\nx3:e-&gt;a42:w\nx3:e-&gt;a52:w\na12:e-&gt;a13:w\na12:e-&gt;a23:w\na12:e-&gt;a33:w\na12:e-&gt;a43:w\na12:e-&gt;a53:w\na22:e-&gt;a13:w\na22:e-&gt;a23:w\na22:e-&gt;a33:w\na22:e-&gt;a43:w\na22:e-&gt;a53:w\na32:e-&gt;a13:w\na32:e-&gt;a23:w\na32:e-&gt;a33:w\na32:e-&gt;a43:w\na32:e-&gt;a53:w\na42:e-&gt;a13:w\na42:e-&gt;a23:w\na42:e-&gt;a33:w\na42:e-&gt;a43:w\na42:e-&gt;a53:w\na52:e-&gt;a13:w\na52:e-&gt;a23:w\na52:e-&gt;a33:w\na52:e-&gt;a43:w\na52:e-&gt;a53:w\na13:e-&gt;O1:w\na13:e-&gt;O2:w\na13:e-&gt;O3:w\na13:e-&gt;O4:w\na23:e-&gt;O1:w\na23:e-&gt;O2:w\na23:e-&gt;O3:w\na23:e-&gt;O4:w\na33:e-&gt;O1:w\na33:e-&gt;O2:w\na33:e-&gt;O3:w\na33:e-&gt;O4:w\na43:e-&gt;O1:w\na43:e-&gt;O2:w\na43:e-&gt;O3:w\na43:e-&gt;O4:w\na53:e-&gt;O1:w\na53:e-&gt;O2:w\na53:e-&gt;O3:w\na53:e-&gt;O4:w\nl0\nlayer 1 (input layer)l1\nlayer 2 (hidden layer)l2\nlayer 3 (hidden layer)l3\nlayer 4 (output layer)\n\n\n\n\n\n\n\n\nthreevar\nk\n?X1\nX1k-&gt;X1\nn@_{S}X2\nX2k-&gt;X2\nθ@^{(h)}X3\nX3k-&gt;X3\n?3z1\nd1z1-&gt;X1\nz2\nd2z2-&gt;X2\nz3\nd3z3-&gt;X3\n\n\n\n\n\n\nCodegrViz('digraph threevar {\n      rankdir=LR;\n      size=\"8,4\";\n      node [fontsize=14 shape=box];\n      edge [fontsize=10];\n      center=1;\n      {rank=min k }\n      {rank=same X1 X2 X3 }\n      {rank=max z1 z2 z3 }\n      z1 [shape=circle label=\"d1\"];\n      z2 [shape=circle label=\"d2\"];\n      z3 [shape=circle label=\"d3\"];\n      k [label=\"?\" shape=\"ellipse\"];\n      k -&gt; X1 [label=\"n@_{S}\"];\n      k -&gt; X2 [label=\"&theta;@^{(h)}\"];\n      k -&gt; X3 [label=\"?3\"];\n      z1 -&gt; X1;\n      z2 -&gt; X2;\n      z3 -&gt; X3;\n    }\n      ')\n\n\n\n\n\n\nCodegrViz(\"digraph dot {\n      graph [style = filled, fillcolor = white]\n      \n      node [shape = circle,\n            style = filled, fillcolor = white,\n            fixedsize = true, width = 0.5, height = 0.5,\n            fontname = 'Times-italic']\n        c\n        d\n      node [shape = doublecircle,\n            style = filled, fillcolor = white,\n            fixedsize = true, width = 0.5, height = 0.5,\n            fontname = 'Times-italic']\n        thetah [label = '&theta;@^{(h)}']\n        thetaf [label = '&theta;@^{(f)}']\n      node [shape = square,\n            style = filled, fillcolor = grey,\n            fixedsize = true, width=0.5, height=0.5,\n            fontname = 'Times-italic']\n        h\n        f\n        ns [label = 'n@_{S}']\n        nn [label = 'n@_{N}']\n      \n      edge [color = black]\n        c -&gt; thetah -&gt; h\n        d -&gt; thetaf -&gt; f\n        c -&gt; thetaf\n        d -&gt; thetah\n        ns -&gt; h\n        nn -&gt; f\n      {rank = max; ns; nn}\n      }\")\n\n\n\n\nCodegrViz(\"digraph dot {\n      graph [style = filled, fillcolor = white,\n             rankdir = LR,\n             newrank = true]\n      \n      node [shape = circle,\n            style = filled, fillcolor = white,\n            fixedsize = true, width = 0.5, height = 0.5,\n            fontname = 'Times-italic']\n        p [fillcolor = gray]\n        gamma [label = '&gamma;']\n        omega [label = '&omega;', shape = doublecircle]\n        beta [label = '&beta;']\n      subgraph cluster_out{\n        fontsize = 8\n        label = &lt;&lt;I&gt;j&lt;/I&gt;:試行&gt;\n        labelloc = b\n        labeljust = r\n        subgraph cluster_in{\n          fontsize = 8\n          label = &lt;&lt;I&gt;k&lt;/I&gt;:回数&gt;\n          labelloc = b\n          labeljust = r\n          thetajk [label = '&theta;@_{jk}', shape = doublecircle]\n          djk [label = 'd@_{jk}', shape = square, fillcolor = gray]\n        }}\n      edge [color=black]\n        p -&gt; omega -&gt; thetajk -&gt; djk\n        gamma -&gt; omega\n        beta -&gt; thetajk \n      {rank = same; gamma; omega}\n      {rank = same; beta; thetajk}\n      }\")\n\n\n\n\nCodegrViz('\ndigraph G {\n    fontname=\"Helvetica,Arial,sans-serif\"\n    node [fontname=\"Helvetica,Arial,sans-serif\"]\n    edge [fontname=\"Helvetica,Arial,sans-serif\"]\n\n    subgraph cluster_0 {\n        style=filled;\n        color=lightgrey;\n        node [style=filled,color=white];\n        a0 -&gt; a1 -&gt; a2 -&gt; a3;\n        label = \"process #1\";\n    }\n\n    subgraph cluster_1 {\n        node [style=filled];\n        b0 -&gt; b1 -&gt; b2 -&gt; b3;\n        label = \"process #2\";\n        color=blue\n    }\n    start -&gt; a0;\n    start -&gt; b0;\n    a1 -&gt; b3;\n    b2 -&gt; a3;\n    a3 -&gt; a0;\n    a3 -&gt; end;\n    b3 -&gt; end;\n\n    start [shape=Mdiamond];\n    end [shape=Msquare];\n}')\n\n\n\n\nCode#https://graphviz.org/Gallery/directed/neural-network.html\n\ngrViz('digraph G {\n  fontname=\"Helvetica,Arial,sans-serif\"\n  node [fontname=\"Helvetica,Arial,sans-serif\"]\n  edge [fontname=\"Helvetica,Arial,sans-serif\"]\n  concentrate=True;\n  rankdir=TB;\n  node [shape=record];\n  140087530674552 [label=\"title: InputLayer\\n|{input:|output:}|{{[(?, ?)]}|{[(?, ?)]}}\"];\n  140087537895856 [label=\"body: InputLayer\\n|{input:|output:}|{{[(?, ?)]}|{[(?, ?)]}}\"];\n  140087531105640 [label=\"embedding_2: Embedding\\n|{input:|output:}|{{(?, ?)}|{(?, ?, 64)}}\"];\n  140087530711024 [label=\"embedding_3: Embedding\\n|{input:|output:}|{{(?, ?)}|{(?, ?, 64)}}\"];\n  140087537980360 [label=\"lstm_2: LSTM\\n|{input:|output:}|{{(?, ?, 64)}|{(?, 128)}}\"];\n  140087531256464 [label=\"lstm_3: LSTM\\n|{input:|output:}|{{(?, ?, 64)}|{(?, 32)}}\"];\n  140087531106200 [label=\"tags: InputLayer\\n|{input:|output:}|{{[(?, 12)]}|{[(?, 12)]}}\"];\n  140087530348048 [label=\"concatenate_1: Concatenate\\n|{input:|output:}|{{[(?, 128), (?, 32), (?, 12)]}|{(?, 172)}}\"];\n  140087530347992 [label=\"priority: Dense\\n|{input:|output:}|{{(?, 172)}|{(?, 1)}}\"];\n  140087530711304 [label=\"department: Dense\\n|{input:|output:}|{{(?, 172)}|{(?, 4)}}\"];\n  140087530674552 -&gt; 140087531105640;\n  140087537895856 -&gt; 140087530711024;\n  140087531105640 -&gt; 140087537980360;\n  140087530711024 -&gt; 140087531256464;\n  140087537980360 -&gt; 140087530348048;\n  140087531256464 -&gt; 140087530348048;\n  140087531106200 -&gt; 140087530348048;\n  140087530348048 -&gt; 140087530347992;\n  140087530348048 -&gt; 140087530711304;\n}')\n\n\n\n\n\n\nCoderequire(data.table)\nrequire(qreport)\nrequire(consort)\n\n# Load the necessary libraries\nrequire(data.table)\nrequire(qreport)\n\n# Define the mermaid diagram\nx &lt;- '\ngraph LR\n  InputLayer[Input Layer] --&gt; I1[{{I1}}]\n  InputLayer --&gt; I2[{{I2}}]\n  InputLayer --&gt; I3[{{I3}}]\n  I1 --&gt; O1[w1]\n  I1 --&gt; O2[w2]\n  I1 --&gt; O3[w3]\n  I1 --&gt; O4[w4]\n  I2 --&gt; O1[w5]\n  I2 --&gt; O2[w6]\n  I2 --&gt; O3[w7]\n  I2 --&gt; O4[w8]\n  I3 --&gt; O1[w9]\n  I3 --&gt; O2[w10]\n  I3 --&gt; O3[w11]\n  I3 --&gt; O4[w12]\n  OutputLayer[Output Layer] --&gt; O1\n  OutputLayer --&gt; O2\n  OutputLayer --&gt; O3\n  OutputLayer --&gt; O4\n  OutputLayer --&gt; ALM[ALM Response]\n  OutputLayer --&gt; EXAM[EXAM Response]\n  ALM --&gt; EXAM[Pure ALM Model]\n  EXAM --&gt; ALM[ALM with EXAM Response Component]\n'\n\n# Generate the mermaid diagram\nmakemermaid(x,\n            I1 = '![Gaussian Curve](gaussian_curve_input_node_1.png)',\n            I2 = '![Gaussian Curve](gaussian_curve_input_node_2.png)',\n            I3 = '![Gaussian Curve](gaussian_curve_input_node_3.png)',\n            file = 'assets/alm_exam.mer'\n            )\n\n# Output the mermaid diagram\n# cat('```{mermaid}\\n')\n# cat(readLines('assets/alm_exam.mer'), sep = '\\n')\n# cat('```\\n')\n\n\n\n\n\n\ngraph LR\n  InputLayer[Input Layer] --&gt; I1[![Gaussian Curve](gaussian_curve_input_node_1.png)]\n  InputLayer --&gt; I2[![Gaussian Curve](gaussian_curve_input_node_2.png)]\n  InputLayer --&gt; I3[![Gaussian Curve](gaussian_curve_input_node_3.png)]\n  I1 --&gt; O1[w1]\n  I1 --&gt; O2[w2]\n  I1 --&gt; O3[w3]\n  I1 --&gt; O4[w4]\n  I2 --&gt; O1[w5]\n  I2 --&gt; O2[w6]\n  I2 --&gt; O3[w7]\n  I2 --&gt; O4[w8]\n  I3 --&gt; O1[w9]\n  I3 --&gt; O2[w10]\n  I3 --&gt; O3[w11]\n  I3 --&gt; O4[w12]\n  OutputLayer[Output Layer] --&gt; O1\n  OutputLayer --&gt; O2\n  OutputLayer --&gt; O3\n  OutputLayer --&gt; O4\n  OutputLayer --&gt; ALM[ALM Response]\n  OutputLayer --&gt; EXAM[EXAM Response]\n  ALM --&gt; EXAM[Pure ALM Model]\n  EXAM --&gt; ALM[ALM with EXAM Response Component]\n\n\nFigure 3: ALM diagram produced with mermaid with individual exclusions linked to the overall exclusions node, and with a tooltip to show more detail\n\n\n\n\nCodelibrary(consort)\nset.seed(1001)\nN &lt;- 300\n\ntrialno &lt;- sample(c(1000:2000), N)\nexc &lt;- rep(NA, N)\nexc[sample(1:N, 15)] &lt;- sample(c(\"Sample not collected\", \"MRI not collected\", \"Other\"),\n                                15, replace = T, prob = c(0.4, 0.4, 0.2))\n\narm &lt;- rep(NA, N)\narm[is.na(exc)] &lt;- sample(c(\"Conc\", \"Seq\"), sum(is.na(exc)), replace = T)\n\nfow1 &lt;- rep(NA, N)\nfow1[!is.na(arm)] &lt;- sample(c(\"Withdraw\", \"Discontinued\", \"Death\", \"Other\", NA),\n                            sum(!is.na(arm)), replace = T, \n                            prob = c(0.05, 0.05, 0.05, 0.05, 0.8))\nfow2 &lt;- rep(NA, N)\nfow2[!is.na(arm) & is.na(fow1)] &lt;- sample(c(\"Protocol deviation\", \"Outcome missing\", NA),\n                                          sum(!is.na(arm) & is.na(fow1)), replace = T, \n                                          prob = c(0.05, 0.05, 0.9))\ndf &lt;- data.frame(trialno, exc, arm, fow1, fow2)\nout &lt;- consort_plot(data = df,\n             order = c(trialno = \"Population\",\n                          exc    = \"Excluded\",\n                          arm     = \"Randomized patient\",\n                          fow1    = \"Lost of Follow-up\",\n                          trialno = \"Finished Followup\",\n                          fow2    = \"Not evaluable\",\n                          trialno = \"Final Analysis\"),\n             side_box = c(\"exc\", \"fow1\", \"fow2\"),\n             allocation = \"arm\",\n             labels = c(\"1\" = \"Screening\", \"2\" = \"Randomization\",\n                        \"5\" = \"Final\"),\n             cex = 0.6)\n\nplot(out)\n\n\n\n\n\nCodeplot(out, grViz = TRUE)\n\n\n\n\n\n\nCoderequire(Hmisc)\nrequire(data.table)\nrequire(qreport)\nhookaddcap()\nN &lt;- 1000\nset.seed(1)\nr &lt;- data.table(\n  id    = 1 : N,\n  age   = round(rnorm(N, 60, 15)),\n  pain  = sample(0 : 5, N, replace=TRUE),\n  hxmed = sample(0 : 1, N, replace=TRUE, prob=c(0.95, 0.05))   )\n# Set consent status to those not excluded at screening\nr[age &gt;= 40 & pain &gt; 0 & hxmed == 0,\n  consent := sample(0 : 1, .N, replace=TRUE, prob=c(0.1, 0.9))]\n# Set randomization status for those consenting\nr[consent == 1,\n  randomized := sample(0 : 1, .N, replace=TRUE, prob=c(0.15, 0.85))]\n# Add treatment and follow-up time to randomized subjects\nr[randomized == 1, tx     := sample(c('A', 'B'), .N, replace=TRUE)]\nr[randomized == 1, futime := pmin(runif(.N, 0, 10), 3)]\n# Add outcome status for those followed 3 years\n# Make a few of those followed 3 years missing\nr[futime == 3,\n  y := sample(c(0, 1, NA), .N, replace=TRUE, prob=c(0.75, 0.2, 0.05))]\n# Print first 15 subjects\nkabl(r[1 : 15, ])\n\n\n\nid\nage\npain\nhxmed\nconsent\nrandomized\ntx\nfutime\ny\n\n\n\n1\n51\n2\n1\nNA\nNA\nNA\nNA\nNA\n\n\n2\n63\n2\n0\n1\n1\nA\n3.0000\n0\n\n\n3\n47\n1\n0\n1\n1\nA\n3.0000\nNA\n\n\n4\n84\n5\n0\n1\n0\nNA\nNA\nNA\n\n\n5\n65\n3\n0\n1\n1\nB\n3.0000\n1\n\n\n6\n48\n4\n0\n1\n1\nA\n3.0000\n0\n\n\n7\n67\n3\n0\n1\n1\nB\n2.0566\nNA\n\n\n8\n71\n0\n0\nNA\nNA\nNA\nNA\nNA\n\n\n9\n69\n5\n0\n1\n1\nB\n1.2815\nNA\n\n\n10\n55\n2\n0\n1\n1\nB\n1.2388\nNA\n\n\n11\n83\n4\n0\n1\n1\nA\n3.0000\n1\n\n\n12\n66\n5\n0\n1\n1\nA\n3.0000\n0\n\n\n13\n51\n1\n0\n1\n1\nB\n3.0000\n0\n\n\n14\n27\n2\n0\nNA\nNA\nNA\nNA\nNA\n\n\n15\n77\n2\n0\n1\n1\nA\n3.0000\n0\n\n\n\n\n\n\nCoder[, exc := seqFreq('pain-free'  = pain  == 0,\n                    'Hx med'    = hxmed == 1,\n                                  age &lt; 40,\n                    noneNA=TRUE)]\neo  &lt;- attr(r[, exc], 'obs.per.numcond')\nmult &lt;- paste0('1, 2, ≥3 exclusions: n=',\n                eo[2], ', ',\n                eo[3], ', ',\n                eo[-(1:3)]  )\n\nr[, .q(qual, consent, fin) :=\n    .(is.na(exc),\n      ifelse(consent == 1, 1, NA),\n      ifelse(futime  &gt;= 3, 1, NA))]\n            \nrequire(consort)\n# consort_plot used to take a coords=c(0.4, 0.6) argument that prevented\n# the collision you see here\nconsort_plot(r,\n             orders = c(id      = 'Screened',\n                        exc     = 'Excluded',\n                        qual    = 'Qualified for Randomization',\n                        consent = 'Consented',\n                        tx      = 'Randomized',\n                        fin     = 'Finished',\n                        y       = 'Outcome\\nassessed'),\n             side_box = 'exc',\n             allocation = 'tx',\n             labels=c('1'='Screening', '3'='Consent', '4'='Randomization', '6'='Follow-up'))\n\n\n\n\nhtab\n\nCodeh &lt;- function(n, label) paste0(label, ' (n=', n, ')')\nhtab &lt;- function(x, label=NULL, split=! length(label), br='\\n') {\n  tab &lt;- table(x)\n  w &lt;- if(length(label)) paste0(h(sum(tab), label), ':', br)\n  f &lt;- if(split) h(tab, names(tab)) \n  else\n    paste(paste0('   ', h(tab, names(tab))), collapse=br)\n  if(split) return(f)\n  paste(w, f, sep=if(length(label))'' else br)\n}  \ncount &lt;- function(x, by=rep(1, length(x)))\n  tapply(x, by, sum, na.rm=TRUE)\n\nw &lt;- r[, {\n g &lt;-\n   add_box(txt=h(nrow(r),       'Screened'))                    |&gt;\n   add_side_box(htab(exc,       'Excluded'))                    |&gt;\n   add_box(h(count(is.na(exc)), 'Qualified for Randomization')) |&gt;\n   add_box(h(count(consent),    'Consented'))                   |&gt;\n   add_box(h(count(randomized), 'Randomized'))                  |&gt;\n   add_split(htab(tx))                                          |&gt;\n   add_box(h(count(fin, tx),    'Finished'))                    |&gt;\n   add_box(h(count(! is.na(y), tx), 'Outcome\\nassessed'))       |&gt;\n   add_label_box(c('1'='Screening',     '3'='Consent',\n                   '4'='Randomization', '6'='Follow-up'))\n plot(g)\n}\n]\n\n\n\n\nmermaid maker\n\nCodeaddCap('fig-doverview-mermaid1', 'Consort diagram produced by `mermaid`')\nx &lt;- 'flowchart TD\n  S[\"Screened (n={{N0}})\"] --&gt; E[\"{{excl}}\"]\n  S   --&gt; Q[\"Qualified for Randomization (n={{Nq}})\"]\n  Q   --&gt; C[\"Consented (n={{Nc}})\"]\n  C   --&gt; R[\"Randomized (n={{Nr}})\"]\n  R   --&gt; TxA[\"A (n={{Ntxa}})\"]\n  R   --&gt; TxB[\"B (n={{Ntxb}})\"]\n  TxA --&gt; FA[\"Finished (n={{Ntxaf}})\"]\n  TxB --&gt; FB[\"Finished (n={{Ntxbf}})\"]\n  FA  --&gt; OA[\"Outcome assessed (n={{Ntxao}})\"]\n  FB  --&gt; OB[\"Outcome assessed (n={{Ntxbo}})\"]\nclassDef largert fill:lightgray,width:1.5in,height:10em,text-align:right,font-size:0.8em;\nclass E largert;\n'\n\nw &lt;- r[, \nmakemermaid(x,\n            N0   = nrow(r),\n            excl = htab(exc, 'Excluded', br='&lt;br&gt;'),\n            Nq   = count(is.na(exc)),\n            Nc   = count(consent),\n            Nr   = count(randomized),\n            Ntxa = count(tx == 'A'),\n            Ntxb = count(tx == 'B'),\n            Ntxaf= count(tx == 'A' & fin),\n            Ntxbf= count(tx == 'B' & fin),\n            Ntxao= count(tx == 'A' & ! is.na(y)),\n            Ntxbo= count(tx == 'B' & ! is.na(y)),\n            file = 'assets/mermaid1.mer'\n            )\n]\n\n\n\n\n\n\nflowchart TD\n  S[\"Screened (n=1000)\"] --&gt; E[\"Excluded (n=286):&lt;br&gt;   pain-free (n=156)&lt;br&gt;   age &lt; 40 (n=85)&lt;br&gt;   Hx med (n=45)\"]\n  S   --&gt; Q[\"Qualified for Randomization (n=714)\"]\n  Q   --&gt; C[\"Consented (n=634)\"]\n  C   --&gt; R[\"Randomized (n=534)\"]\n  R   --&gt; TxA[\"A (n=285)\"]\n  R   --&gt; TxB[\"B (n=249)\"]\n  TxA --&gt; FA[\"Finished (n=204)\"]\n  TxB --&gt; FB[\"Finished (n=175)\"]\n  FA  --&gt; OA[\"Outcome assessed (n=196)\"]\n  FB  --&gt; OB[\"Outcome assessed (n=165)\"]\nclassDef largert fill:lightgray,width:1.5in,height:10em,text-align:right,font-size:0.8em;\nclass E largert;\n\n\nFigure 4: Consort diagram produced by mermaid\n\n\n\nnode plot\n\nCode# Create some service functions so later it will be easy to change from\n# mermaid to graphviz\nmakenode       &lt;- function(name, label) paste0(name, '[\"', label, '\"]')\nmakeconnection &lt;- function(from, to)    paste0(from, ' --&gt; ', to)\n\nexclnodes &lt;- function(x, from='E', root='E', seq=FALSE, remain=FALSE) {\n  # Create complete node specifications for individual exclusions, each\n  # linking to overall exclusion count assumed to be in node root.\n  # Set seq=TRUE to make use of the fact that the exclusions were\n  # done in frequency priority order so that each exclusion is in\n  # addition to the previous one.  Leave seq=FALSE to make all exclusions\n  # subservient to root.  Use remain=TRUE to include # obs remaining\n    # remain=TRUE assumes noneNA specified to seqFreq\n  tab &lt;- table(x)\n  i &lt;- 1 : length(tab)\n    rem &lt;- if(remain) paste0(', ', length(x) - cumsum(tab), ' remain')\n  labels &lt;- paste0(names(tab), ' (n=', tab, rem, ')')\n  nodes  &lt;- if(seq) makenode(ifelse(i == 1, paste0(root, '1'), paste0(root, i)),\n                             labels)\n            else    makenode(paste0(root, i), labels)\n  connects &lt;- if(seq) makeconnection(ifelse(i == 1, from, paste0(root, i - 1)),\n                                     paste0(root, i))\n              else makeconnection(from, paste0(root, i))\n   paste(c(nodes, connects), collapse='\\n')\n}\n\n# Create parallel treatment nodes\n# Treatments are assumed to be in order by the tx variable\n# and will appear left to right in the diagram\n# Treatment node names correspond to that and are Tx1, Tx2, ...\n# root: root of new nodes, from: single node name to connect from\n# fromparallel: root of connected-from node name which is to be\n# expanded by adding the integers 1, 2, ... number of treatments.\n\nTxs &lt;- r[, if(is.factor(tx)) levels(tx) else sort(unique(tx))]\n\nparNodes &lt;- function(counts, root, from=NULL, fromparallel=NULL,\n                      label=Txs) {\n  if(! identical(names(counts), Txs)) stop('Txs not consistent')\n  k &lt;- length(Txs)\n  ns &lt;- paste0(' (n=', counts, ')')\n   nodenames &lt;- paste0(root, 1 : k)\n  nodes &lt;- makenode(nodenames, paste0(label, ns))\n  connects &lt;- if(length(fromparallel)) makeconnection(paste0(fromparallel, 1 : k), nodenames)\n              else                     makeconnection(from,                        nodenames)\n  paste(c(nodes, connects), collapse='\\n')\n    }\n\n# Create tooltip text from tabulation created by seqFreq earlier\nefreq &lt;- data.frame('# Exclusions'= (1 : length(eo)) - 1,\n                    '# Subjects'  = eo, check.names=FALSE)\nefreq &lt;- subset(efreq, `# Subjects` &gt; 0)\n# Convert to text which will be wrapped by the html\nexcltab &lt;- paste(capture.output(print(efreq, row.names=FALSE)),\n                 collapse='\\n')\n\n\naddCap('fig-doverview-mermaid2', 'Consort diagram produced with `mermaid` with individual exclusions linked to the overall exclusions node, and with a tooltip to show more detail')\n\nx &lt;- '\nflowchart TD\n  S[\"Screened (n={{N0}})\"] --&gt; E[\"Excluded (n={{Ne}})\"]\n  {{exclsep}}\n  E1 & E2 & E3 --&gt; M[\"{{mult}}\"]\n  S   --&gt; Q[\"Qualified for Randomization (n={{Nq}})\"]\n  Q   --&gt; C[\"Consented (n={{Nc}})\"]\n  C   --&gt; R[\"Randomized (n={{Nr}})\"]\n  {{txcounts}}\n  {{finished}}\n  {{outcome}}\nclick E callback \"{{excltab}}\"\n'\n\nw &lt;- r[, \nmakemermaid(x,\n  N0       = nrow(r),\n  Ne       = count(! is.na(exc)),\n  exclsep  = exclnodes(exc),  # add seq=TRUE to put exclusions vertical\n  excltab  = excltab,         # tooltip text\n  mult     = mult,  # separate node: count multiple exclusions\n  Nq       = count(is.na(exc)),\n  Nc       = count(consent),\n  Nr       = count(randomized),\n  txcounts = parNodes(table(tx),         'Tx', from='R'),\n  finished = parNodes(count(fin, by=tx), 'F',  fromparallel='Tx',\n                      label='Finished'),\n  outcome  = parNodes(count(! is.na(y), by=tx), 'O',\n                      fromparallel='F', label='Outcome assessed'),\n  file='mermaid2.mer'  # save generated code for another use\n)\n]\n\n\nmakenode       &lt;- function(name, label) paste0(name, ' [label=\"', label, '\"];')\nmakeconnection &lt;- function(from, to)    paste0(from, ' -&gt; ', to, ';')\n\n# Create data frame from tabulation created by seqFreq earlier\nefreq &lt;- data.frame('# Exclusions'= (1 : length(eo)) - 1,\n                    '# Subjects'  = eo, check.names=FALSE)\nefreq &lt;- subset(efreq, `# Subjects` &gt; 0)\n\nx &lt;- 'digraph {\n  graph [pad=\"0.5\", nodesep=\"0.5\", ranksep=\"2\", splines=ortho]\n  //  splines=ortho for square connections\n  node  [shape=box, fontsize=\"30\"]\n  rankdir=TD;\n  S [label=\"Screened (n={{N0}})\"];\n  E [label=\"Excluded (n={{Ne}})\"];\n  S -&gt; E;\n  {{exclsep}}\n  M [label=\"{{mult}}\"];\n  E1 -&gt; M;\n  E2 -&gt; M;\n  E3 -&gt; M;\n  Q [label=\"Qualified for Randomization (n={{Nq}})\"];\n  C [label=\"Consented (n={{Nc}})\"];\n  R [label=\"Randomized (n={{Nr}})\"];\n  S -&gt; Q;\n  Q -&gt; C;\n  C -&gt; R;\n  {{txcounts}}\n  {{finished}}\n  {{outcome}}\n  efreq [label=&lt;{{efreq}}&gt;];\n  M -&gt; efreq [dir=none, style=dotted];\n}\n'\n\nw &lt;- r[, \nmakegraphviz(x,\n  N0       = nrow(r),\n  Ne       = count(! is.na(exc)),\n  exclsep  = exclnodes(exc),  # add seq=TRUE to put exclusions vertical\n  efreq    = efreq,\n  mult     = mult,  # separate node: count multiple exclusions\n  Nq       = count(is.na(exc)),\n  Nc       = count(consent),\n  Nr       = count(randomized),\n  txcounts = parNodes(table(tx),         'Tx', from='R'),\n  finished = parNodes(count(fin, by=tx), 'F',  fromparallel='Tx',\n                      label='Finished'),\n  outcome  = parNodes(count(! is.na(y), by=tx), 'O',\n                      fromparallel='F', label='Outcome assessed'),\n  file='graphviz.dot'\n)\n]\n#  addCap('fig-doverview-graphviza', 'Consort diagram produced with `graphviz` with detailed exclusion frequencies in a separate node', scap='Consort diagram produced with `graphviz`')\n\n\n\n\n\n\nflowchart TD\n  S[\"Screened (n=1000)\"] --&gt; E[\"Excluded (n=286)\"]\n  E1[\"pain-free (n=156)\"]\nE2[\"age &lt; 40 (n=85)\"]\nE3[\"Hx med (n=45)\"]\nE --&gt; E1\nE --&gt; E2\nE --&gt; E3\n  E1 & E2 & E3 --&gt; M[\"1, 2, ≥3 exclusions: n=260, 25, 1\"]\n  S   --&gt; Q[\"Qualified for Randomization (n=714)\"]\n  Q   --&gt; C[\"Consented (n=634)\"]\n  C   --&gt; R[\"Randomized (n=534)\"]\n  Tx1[\"A (n=285)\"]\nTx2[\"B (n=249)\"]\nR --&gt; Tx1\nR --&gt; Tx2\n  F1[\"Finished (n=204)\"]\nF2[\"Finished (n=175)\"]\nTx1 --&gt; F1\nTx2 --&gt; F2\n  O1[\"Outcome assessed (n=196)\"]\nO2[\"Outcome assessed (n=165)\"]\nF1 --&gt; O1\nF2 --&gt; O2\nclick E callback \" # Exclusions # Subjects\n            0        714\n            1        260\n            2         25\n            3          1\"\n\n\nFigure 5: Consort diagram produced with mermaid with individual exclusions linked to the overall exclusions node, and with a tooltip to show more detail\n\n\n\n\nCodegetHdata(support)\nsetDT(support)\n# addCap('fig-doverview-missflow', 'Flowchart of sequential exclusion of observations due to missing values')\nvars &lt;-  .q(age, sex, dzgroup, edu, income, meanbp, wblc,\n            alb, bili, crea, glucose, bun, urine)\nex &lt;- missChk(support, use=vars, type='seq') # seq: don't make report\n\n# Create tooltip text from tabulation created by seqFreq\noc   &lt;- attr(ex, 'obs.per.numcond')\nfreq &lt;- data.frame('# Exclusions'= (1 : length(oc)) - 1,\n                   '# Subjects'  = oc, check.names=FALSE)\nfreq &lt;- subset(freq, `# Subjects` &gt; 0)\n\nx &lt;- '\ndigraph {\n  graph [pad=\"0.5\", nodesep=\"0.5\", ranksep=\"2\", splines=ortho]\n  //  splines=ortho for square connections\n  node  [shape=box, fontsize=\"30\"]\n  rankdir=TD;\n  Enr [label=\"Enrolled (n={{N0}})\"];\n  Enr;\n  {{exclsep}}\n    Extab [label=&lt;{{excltab}}&gt;];\n  Enr:e -&gt; Extab [dir=none];\n}\n'\nmakegraphviz(x,\n  N0        = nrow(support),\n  exclsep   = exclnodes(ex, from='Enr', seq=TRUE, remain=TRUE),\n  excltab   = freq,\n  file      = 'support.dot'\n)\n\n\n\nCodegrViz('\ngraph G {\nfontname=\"Helvetica,Arial,sans-serif\"\nnode [fontname=\"Helvetica,Arial,sans-serif\"]\nedge [fontname=\"Helvetica,Arial,sans-serif\"]\nI5 [shape=ellipse,color=red,style=bold,label=\"Caroline Bouvier Kennedy\\nb. 27.11.1957 New York\",image=\"images/165px-Caroline_Kennedy.jpg\",labelloc=b];\nI1 [shape=box,color=blue,style=bold,label=\"John Fitzgerald Kennedy\\nb. 29.5.1917 Brookline\\nd. 22.11.1963 Dallas\",image=\"images/kennedyface.jpg\",labelloc=b];\nI6 [shape=box,color=blue,style=bold,label=\"John Fitzgerald Kennedy\\nb. 25.11.1960 Washington\\nd. 16.7.1999 over the Atlantic Ocean, near Aquinnah, MA, USA\",image=\"images/180px-JFKJr2.jpg\",labelloc=b];\nI7 [shape=box,color=blue,style=bold,label=\"Patrick Bouvier Kennedy\\nb. 7.8.1963\\nd. 9.8.1963\"];\nI2 [shape=ellipse,color=red,style=bold,label=\"Jaqueline Lee Bouvier\\nb. 28.7.1929 Southampton\\nd. 19.5.1994 New York City\",image=\"images/jacqueline-kennedy-onassis.jpg\",labelloc=b];\nI8 [shape=box,color=blue,style=bold,label=\"Joseph Patrick Kennedy\\nb. 6.9.1888 East Boston\\nd. 16.11.1969 Hyannis Port\",image=\"images/1025901671.jpg\",labelloc=b];\nI10 [shape=box,color=blue,style=bold,label=\"Joseph Patrick Kennedy Jr\\nb. 1915\\nd. 1944\"];\nI11 [shape=ellipse,color=red,style=bold,label=\"Rosemary Kennedy\\nb. 13.9.1918\\nd. 7.1.2005\",image=\"images/rosemary.jpg\",labelloc=b];\nI12 [shape=ellipse,color=red,style=bold,label=\"Kathleen Kennedy\\nb. 1920\\nd. 1948\"];\nI13 [shape=ellipse,color=red,style=bold,label=\"Eunice Mary Kennedy\\nb. 10.7.1921 Brookline\"];\nI9 [shape=ellipse,color=red,style=bold,label=\"Rose Elizabeth Fitzgerald\\nb. 22.7.1890 Boston\\nd. 22.1.1995 Hyannis Port\",image=\"images/Rose_kennedy.JPG\",labelloc=b];\nI15 [shape=box,color=blue,style=bold,label=\"Aristotle Onassis\"];\nI3 [shape=box,color=blue,style=bold,label=\"John Vernou Bouvier III\\nb. 1891\\nd. 1957\",image=\"images/BE037819.jpg\",labelloc=b];\nI4 [shape=ellipse,color=red,style=bold,label=\"Janet Norton Lee\\nb. 2.10.1877\\nd. 3.1.1968\",image=\"images/n48862003257_1275276_1366.jpg\",labelloc=b];\n I1 -- I5  [style=bold,color=blue]; \n I1 -- I6  [style=bold,color=orange]; \n I2 -- I6  [style=bold,color=orange]; \n I1 -- I7  [style=bold,color=orange]; \n I2 -- I7  [style=bold,color=orange]; \n I1 -- I2  [style=bold,color=violet]; \n I8 -- I1  [style=bold,color=blue]; \n I8 -- I10  [style=bold,color=orange]; \n I9 -- I10  [style=bold,color=orange]; \n I8 -- I11  [style=bold,color=orange]; \n I9 -- I11  [style=bold,color=orange]; \n I8 -- I12  [style=bold,color=orange]; \n I9 -- I12  [style=bold,color=orange]; \n I8 -- I13  [style=bold,color=orange]; \n I9 -- I13  [style=bold,color=orange]; \n I8 -- I9  [style=bold,color=violet]; \n I9 -- I1  [style=bold,color=red]; \n I2 -- I5  [style=bold,color=red]; \n I2 -- I15  [style=bold,color=violet]; \n I3 -- I2  [style=bold,color=blue]; \n I3 -- I4  [style=bold,color=violet]; \n I4 -- I2  [style=bold,color=red]; \n}')"
  },
  {
    "objectID": "Misc/Task.html",
    "href": "Misc/Task.html",
    "title": "HTW Task",
    "section": "",
    "text": "need to create a demo version without consent form. And maybe separate windows for the different versions.\n\nExperimental Task for the HTW Project. Programmed in Javascript, and making use of phaser.js."
  },
  {
    "objectID": "Misc/Task.html#htw-task",
    "href": "Misc/Task.html#htw-task",
    "title": "HTW Task",
    "section": "",
    "text": "need to create a demo version without consent form. And maybe separate windows for the different versions.\n\nExperimental Task for the HTW Project. Programmed in Javascript, and making use of phaser.js."
  },
  {
    "objectID": "Misc/Task.html#live-task-demo",
    "href": "Misc/Task.html#live-task-demo",
    "title": "HTW Task",
    "section": "Live Task Demo",
    "text": "Live Task Demo\n\n\nHTW_Task\n\n   –&gt;"
  },
  {
    "objectID": "Misc/Distributional_Explore.html",
    "href": "Misc/Distributional_Explore.html",
    "title": "Distributional_Explorations",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes,brms,bayesplot,broom,broom.mixed,lme4,emmeans,here,knitr,kableExtra,gt,gghalves,patchwork,ggforce,ggdist,moments)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\nsource(here(\"Functions/org_functions.R\"))\ntest &lt;- e1 |&gt; \n  filter(expMode2 == \"Test\") |&gt; \n  group_by(vb) |&gt; \n  mutate(distS = as.numeric(scale(dist, scale = FALSE)), \n         distS2 = custom_scale(dist))\n\noptions(brms.backend=\"cmdstanr\",mc.cores=4)\n\ntest %&gt;% group_by(condit) |&gt; summarise(mean=mean(dist),sd=sd(dist),sk=moments::skewness(dist)) \ntest %&gt;% group_by(vb) |&gt; summarise(mean=mean(dist),sd=sd(dist),sk=moments::skewness(dist)) \n\n\n\nCode# test |&gt; ggplot(aes(x=dist))+geom_histogram() + facet_wrap(~vb) + ggtitle(\"empirical_dist\")  +\n# test |&gt; ggplot(aes(x=distS))+geom_histogram() + facet_wrap(~vb) + ggtitle(\"centered dist\")  +\n# test |&gt; ggplot(aes(x=distS2))+geom_histogram() + facet_wrap(~vb) + ggtitle(\"scaled dist\")\n\ntest |&gt; filter(id %in% 1:5) |&gt; ggplot(aes(x=dist))+geom_histogram() + facet_wrap(id~vb) + ggtitle(\"empirical_dist\")  \nplot(density(test$vx))\nplot(density(test$dist))\n\n\ntest |&gt; ggplot(aes(x=dist))+geom_density() + facet_wrap(~vb) + ggtitle(\"empirical_dist\") \ntest |&gt; ggplot(aes(x=vx))+geom_density() + facet_wrap(~vb) + ggtitle(\"empirical_dist\") \n\n\n\nCodesk1 &lt;- brm(dist ~ vb,family=skew_normal(),data=test,iter=800,chains=2)\ng1 &lt;- brm(dist ~ vb,family=gaussian(),data=test,iter=800,chains=2)\nga2 &lt;- brm(dist+.01 ~ vb,family=Gamma(),data=test,iter=800,chains=2)\nln1 &lt;- brm(dist+.01 ~ vb,family=lognormal(),data=test,iter=800,chains=2)\nln2 &lt;- brm(dist+.0001 ~ vb,family=lognormal(link=\"inverse\"),data=test,iter=800,chains=2)\n\ng2 &lt;- brm(bf(dist+.001|trunc(lb=0) ~ vb),data=testS,iter=800,chains=2,family=gaussian())\nbayesplot::ppc_dens_overlay_grouped(testS$dist,yrep=posterior_predict(g2,ndraws=200),group=testS$vb)\npp_check(g2,type=\"stat_grouped\",ndraws=200, group=\"vb\",stat=\"mean\")\n\n\n\n\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(sk1,ndraws=200),group=test$vb)\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(g1,ndraws=200),group=test$vb)\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(ga2,ndraws=200),group=test$vb)\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(ln1,ndraws=200),group=test$vb)\n\n\n\nbayes_R2(g1)\nbayes_R2(ga1)\nbayes_R2(sk1)\nbayes_R2(sk1)\nbayes_R2(g1S)\n\n\n#testS &lt;- test %&gt;% group_by(id,vb) |&gt; filter(id %in% c(\"1\",\"2\",\"139\")) |&gt; select(id,vb,gt.stage,trial,condit,vx,dist,distS,distS2)\ntestS &lt;- test %&gt;% group_by(id,vb) |&gt; filter(id %in% 1:15) |&gt; select(id,vb,gt.stage,trial,condit,vx,dist,distS,distS2)\n\ntestS |&gt; ggplot(aes(x=trial,y=dist,col=vb))+geom_line()+facet_wrap(~id)\ntestS |&gt; ggplot(aes(x=trial,y=distS2,col=vb))+geom_line()+facet_wrap(~id)\n\n\n\ng1S &lt;- brm(distS ~ 0+ vb + condit,family=gaussian(),data=test,iter=1000,chains=4)\nbayesplot::ppc_dens_overlay_grouped(test$distS,yrep=posterior_predict(g1S,ndraws=200),group=test$vb)\n\ng1S2 &lt;- brm(distS2 ~ vb,family=gaussian(),data=test,iter=1000,chains=4)\nbayesplot::ppc_dens_overlay_grouped(test$distS2,yrep=posterior_predict(g1S2,ndraws=200),group=test$vb)\n\n\ng1S_F &lt;- brm(distS ~ 0+ vb + condit + (0+vb|id),family=gaussian(),data=test,iter=1000,chains=4,\n             file=paste0(here::here(\"data/model_cache\",\"e1_test_centeredDistS\")))\n\nbayesplot::ppc_dens_overlay_grouped(test$distS,yrep=posterior_predict(g1S_F,ndraws=200),group=test$vb)\nbayesplot::ppc_dens_overlay_grouped(test$distS,yrep=posterior_predict(g1S_F,ndraws=200),group=test$vb)\n\n\n\ng1S_gamma &lt;- brm(dist+.001 ~ 1+ vb + condit + (0+vb|id),family=Gamma(),data=test,iter=1000,chains=4,\n             file=paste0(here::here(\"data/model_cache\",\"e1_test_Gamma\")))\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(g1S_gamma,ndraws=200),group=test$vb)\n\npp_check(g1S_weibull)+ xlim(c(-300,300))\n\n\ng1S_exG &lt;- brm(dist ~ 0 + vb + condit + (0+vb|id),family=exgaussian(),data=test,iter=1000,chains=4,\n             file=paste0(here::here(\"data/model_cache\",\"e1_test_exgauss\")))\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(g1S_exG,ndraws=200),group=test$vb)"
  },
  {
    "objectID": "Analysis/test_analysis-all.html",
    "href": "Analysis/test_analysis-all.html",
    "title": "HTW Test Analysis",
    "section": "",
    "text": "pacman::p_load(tidyverse,data.table,lme4,emmeans,here,knitr,kableExtra)\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\"))\n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,bandOrder,fb,vb,band,lowBound,highBound,bandInt) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")\n\n\nmodel &lt;- lmer(devMean ~ condit * bandOrder * fb + (1|id), data = dtestAgg)\nsummary(model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: devMean ~ condit * bandOrder * fb + (1 | id)\n   Data: dtestAgg\n\nREML criterion at convergence: 35822.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2757 -0.6267 -0.1857  0.4206  5.3481 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 10476    102.4   \n Residual             29024    170.4   \nNumber of obs: 2697, groups:  id, 461\n\nFixed effects:\n                                        Estimate Std. Error t value\n(Intercept)                              212.430     13.967  15.209\nconditVaried                              53.614     19.965   2.685\nbandOrderReverse                           9.478     21.811   0.435\nfbOrdinal                                 56.609     22.441   2.523\nconditVaried:bandOrderReverse            -45.098     30.951  -1.457\nconditVaried:fbOrdinal                   -58.510     33.172  -1.764\nbandOrderReverse:fbOrdinal               -12.767     32.312  -0.395\nconditVaried:bandOrderReverse:fbOrdinal   -3.604     47.473  -0.076\n\nCorrelation of Fixed Effects:\n            (Intr) cndtVr bndOrR fbOrdn cnV:OR cndV:O bnOR:O\nconditVarid -0.700                                          \nbndOrdrRvrs -0.640  0.448                                   \nfbOrdinal   -0.622  0.435  0.399                            \ncndtVrd:bOR  0.451 -0.645 -0.705 -0.281                     \ncndtVrd:fbO  0.421 -0.602 -0.270 -0.677  0.388              \nbndOrdrRv:O  0.432 -0.302 -0.675 -0.695  0.476  0.470       \ncndtVr:OR:O -0.294  0.421  0.459  0.473 -0.652 -0.699 -0.681\n\n\nLinear mixed model fit by REML [‘lmerMod’] Formula: devMean ~ condit * bandOrder * fb + (1 | id) Data: dtestAgg\nREML criterion at convergence: 35822.8\nScaled residuals: Min 1Q Median 3Q Max -2.2757 -0.6267 -0.1857 0.4206 5.3481\nRandom effects: Groups Name Variance Std.Dev. id (Intercept) 10476 102.4\nResidual 29024 170.4\nNumber of obs: 2697, groups: id, 461\nFixed effects: Estimate Std. Error t value (Intercept) 212.430 13.967 15.209 conditVaried 53.614 19.965 2.685 bandOrderrev 9.478 21.811 0.435 fbordinal 56.609 22.441 2.523 conditVaried:bandOrderrev -45.098 30.951 -1.457 conditVaried:fbordinal -58.510 33.172 -1.764 bandOrderrev:fbordinal -12.767 32.312 -0.395 conditVaried:bandOrderrev:fbordinal -3.604 47.473 -0.076\nCorrelation of Fixed Effects: (Intr) cndtVr ctOrdr fbrdnl cndV:O cndtV: ctOrd:\nThe fixed effects estimates and their significance levels are as follows:\n\nThe intercept (212.430) represents the estimated mean devMean for the reference group (constant training, ‘orig’ order, and continuous feedback).\nconditVaried (53.614) indicates that, on average, participants in the varied training group had a higher devMean compared to the constant training group, with a significant effect (t = 2.685).\nbandOrderrev (9.478) suggests that the ‘rev’ order manipulation had a small and non-significant effect on devMean (t = 0.435).\nfbordinal (56.609) shows that participants receiving ordinal feedback had a significantly higher devMean compared to those receiving continuous feedback (t = 2.523).\nThe two-way and three-way interaction terms (conditVaried:bandOrderrev, conditVaried:fbordinal, bandOrderrev:fbordinal, and conditVaried:bandOrderrev:fbordinal) were not significant, indicating that the effects of training variability, order manipulation, and feedback type did not significantly interact with each other.\n\nIn summary, the analysis suggests that training variability and feedback type have significant main effects on participants’ performance in the visuomotor extrapolation task. Participants in the varied training group and those receiving ordinal feedback had higher devMean values, indicating better performance. However, the order manipulation (‘orig’ and ‘rev’) did not have a significant effect on performance. Additionally, there were no significant interactions between the factors, suggesting that their effects on performance were independent of each other.\n\nmodel_vb &lt;- lmer(devMean ~ condit * bandOrder * fb * vb + (1|id), data = dtestAgg)\nsummary(model_vb)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: devMean ~ condit * bandOrder * fb * vb + (1 | id)\n   Data: dtestAgg\n\nREML criterion at convergence: 35068.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3467 -0.5744 -0.1106  0.4417  5.9657 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 11277    106.2   \n Residual             24462    156.4   \nNumber of obs: 2697, groups:  id, 461\n\nFixed effects:\n                                                    Estimate Std. Error t value\n(Intercept)                                           252.99      21.14  11.970\nconditVaried                                          134.26      30.28   4.434\nbandOrderReverse                                      -60.79      33.55  -1.812\nfbOrdinal                                             143.77      33.87   4.244\nvb350-550                                             -61.41      24.73  -2.483\nvb600-800                                            -102.59      24.73  -4.148\nvb800-1000                                            -66.11      24.91  -2.654\nvb1000-1200                                           -22.19      25.61  -0.866\nvb1200-1400                                            23.54      26.07   0.903\nconditVaried:bandOrderReverse                        -173.94      47.39  -3.670\nconditVaried:fbOrdinal                               -147.69      50.34  -2.934\nbandOrderReverse:fbOrdinal                             70.51      49.32   1.430\nconditVaried:vb350-550                                -36.84      35.43  -1.040\nconditVaried:vb600-800                                -48.49      35.43  -1.369\nconditVaried:vb800-1000                               -98.39      35.70  -2.756\nconditVaried:vb1000-1200                             -159.64      36.33  -4.394\nconditVaried:vb1200-1400                             -160.75      36.81  -4.367\nbandOrderReverse:vb350-550                             64.52      39.37   1.639\nbandOrderReverse:vb600-800                             97.13      39.37   2.467\nbandOrderReverse:vb800-1000                            75.46      39.24   1.923\nbandOrderReverse:vb1000-1200                           68.47      39.68   1.725\nbandOrderReverse:vb1200-1400                           97.34      39.98   2.435\nfbOrdinal:vb350-550                                   -59.00      39.63  -1.489\nfbOrdinal:vb600-800                                  -120.60      39.63  -3.043\nfbOrdinal:vb800-1000                                 -104.78      40.35  -2.597\nfbOrdinal:vb1000-1200                                -121.77      41.28  -2.950\nfbOrdinal:vb1200-1400                                -133.56      41.92  -3.186\nconditVaried:bandOrderReverse:fbOrdinal               -53.01      72.62  -0.730\nconditVaried:bandOrderReverse:vb350-550                19.29      55.53   0.347\nconditVaried:bandOrderReverse:vb600-800                60.66      55.61   1.091\nconditVaried:bandOrderReverse:vb800-1000              198.56      55.52   3.576\nconditVaried:bandOrderReverse:vb1000-1200             267.80      55.93   4.788\nconditVaried:bandOrderReverse:vb1200-1400             249.68      56.24   4.439\nconditVaried:fbOrdinal:vb350-550                       61.93      58.90   1.051\nconditVaried:fbOrdinal:vb600-800                      103.53      58.90   1.758\nconditVaried:fbOrdinal:vb800-1000                      84.75      59.47   1.425\nconditVaried:fbOrdinal:vb1000-1200                    141.77      60.19   2.356\nconditVaried:fbOrdinal:vb1200-1400                    165.62      60.88   2.721\nbandOrderReverse:fbOrdinal:vb350-550                 -100.28      57.85  -1.733\nbandOrderReverse:fbOrdinal:vb600-800                 -124.71      57.78  -2.158\nbandOrderReverse:fbOrdinal:vb800-1000                -102.80      58.10  -1.769\nbandOrderReverse:fbOrdinal:vb1000-1200                -82.52      58.75  -1.404\nbandOrderReverse:fbOrdinal:vb1200-1400                -68.74      59.21  -1.161\nconditVaried:bandOrderReverse:fbOrdinal:vb350-550      93.93      85.00   1.105\nconditVaried:bandOrderReverse:fbOrdinal:vb600-800     133.48      85.09   1.569\nconditVaried:bandOrderReverse:fbOrdinal:vb800-1000     69.46      85.23   0.815\nconditVaried:bandOrderReverse:fbOrdinal:vb1000-1200   -12.91      85.74  -0.151\nconditVaried:bandOrderReverse:fbOrdinal:vb1200-1400   -15.49      86.22  -0.180\n\n\nLinear mixed model fit by REML [‘lmerMod’] Formula: devMean ~ condit * bandOrder * fb * vb + (1 | id) Data: dtestAgg\nREML criterion at convergence: 35068.9\nScaled residuals: Min 1Q Median 3Q Max -3.3467 -0.5744 -0.1106 0.4417 5.9657\nRandom effects: Groups Name Variance Std.Dev. id (Intercept) 11277 106.2\nResidual 24462 156.4\nNumber of obs: 2697, groups: id, 461\nFixed effects: Estimate Std. Error t value (Intercept) 252.99 21.14 11.970 conditVaried 134.26 30.28 4.434 bandOrderrev -60.79 33.55 -1.812 fbordinal 143.77 33.87 4.244 vb350-550 -61.41 24.73 -2.483 vb600-800 -102.59 24.73 -4.148 vb800-1000 -66.11 24.91 -2.654 vb1000-1200 -22.19 25.61 -0.866 vb1200-1400 23.54 26.07 0.903 conditVaried:bandOrderrev -173.94 47.39 -3.670 conditVaried:fbordinal -147.69 50.34 -2.934 bandOrderrev:fbordinal 70.51 49.32 1.430 conditVaried:vb350-550 -36.84 35.43 -1.040 conditVaried:vb600-800 -48.49 35.43 -1.369 conditVaried:vb800-1000 -98.39 35.70 -2.756 conditVaried:vb1000-1200 -159.64 36.33 -4.394 conditVaried:vb1200-1400 -160.75 36.81 -4.367 bandOrderrev:vb350-550 64.52 39.37 1.639 bandOrderrev:vb600-800 97.13 39.37 2.467 bandOrderrev:vb800-1000 75.46 39.24 1.923 bandOrderrev:vb1000-1200 68.47 39.68 1.725 bandOrderrev:vb1200-1400 97.34 39.98 2.435 fbordinal:vb350-550 -59.00 39.63 -1.489 fbordinal:vb600-800 -120.60 39.63 -3.043 fbordinal:vb800-1000 -104.78 40.35 -2.597 fbordinal:vb1000-1200 -121.77 41.28 -2.950 fbordinal:vb1200-1400 -133.56 41.92 -3.186 conditVaried:bandOrderrev:fbordinal -53.01 72.62 -0.730 conditVaried:bandOrderrev:vb350-550 19.29 55.53 0.347 conditVaried:bandOrderrev:vb600-800 60.66 55.61 1.091 conditVaried:bandOrderrev:vb800-1000 198.56 55.52 3.576 conditVaried:bandOrderrev:vb1000-1200 267.80 55.93 4.788 conditVaried:bandOrderrev:vb1200-1400 249.68 56.24 4.439 conditVaried:fbordinal:vb350-550 61.93 58.90 1.051 conditVaried:fbordinal:vb600-800 103.53 58.90 1.758 conditVaried:fbordinal:vb800-1000 84.75 59.47 1.425 conditVaried:fbordinal:vb1000-1200 141.77 60.19 2.356 conditVaried:fbordinal:vb1200-1400 165.62 60.88 2.721 bandOrderrev:fbordinal:vb350-550 -100.28 57.85 -1.733 bandOrderrev:fbordinal:vb600-800 -124.71 57.78 -2.158 bandOrderrev:fbordinal:vb800-1000 -102.80 58.10 -1.769 bandOrderrev:fbordinal:vb1000-1200 -82.52 58.75 -1.404 bandOrderrev:fbordinal:vb1200-1400 -68.74 59.21 -1.161 conditVaried:bandOrderrev:fbordinal:vb350-550 93.93 85.00 1.105 conditVaried:bandOrderrev:fbordinal:vb600-800 133.48 85.09 1.569 conditVaried:bandOrderrev:fbordinal:vb800-1000 69.46 85.23 0.815 conditVaried:bandOrderrev:fbordinal:vb1000-1200 -12.91 85.74 -0.151 conditVaried:bandOrderrev:fbordinal:vb1200-1400 -15.49 86.22 -0.180\nThe fixed effects estimates and their significance levels are as follows:\n\nThe intercept (252.99) represents the estimated mean devMean for the reference group (constant training, ‘orig’ order, continuous feedback, and the lowest velocity band).\nconditVaried (134.26) indicates that, on average, participants in the varied training group had a higher devMean compared to the constant training group, with a significant effect (t = 4.434).\nbandOrderrev (-60.79) suggests that the ‘rev’ order manipulation had a small and marginally significant effect on devMean (t = -1.812).\nfbordinal (143.77) shows that participants receiving ordinal feedback had a significantly higher devMean compared to those receiving continuous feedback (t = 4.244).\n\nThe main effects of the velocity bands (vb350-550, vb600-800, vb800-1000, vb1000-1200, vb1200-1400) show that different velocity bands have varying effects on devMean, with some bands leading to significantly lower devMean values compared to the reference band (e.g., vb600-800: -102.59, t = -4.148).\nThe two-way, three-way, and four-way interaction terms provide insights into how the effects of training variability, order manipulation, feedback type, and velocity band interact with each other. Some notable significant interactions include:\n\nconditVaried:bandOrderrev (-173.94, t = -3.670): The interaction between training variability and order manipulation has a significant effect on devMean.\nconditVaried:fbordinal (-147.69, t = -2.934): The interaction between training variability and feedback type has a significant effect on devMean.\nconditVaried:vb800-1000 (-98.39, t = -2.756), conditVaried:vb1000-1200 (-159.64, t = -4.394), and conditVaried:vb1200-1400 (-160.75, t = -4.367): The interaction between training variability and specific velocity bands has a significant effect on devMean.\n\nIn summary, the updated analysis suggests that training variability, order manipulation, feedback type, and velocity band all have significant main effects on participants’ performance in the visuomotor extrapolation task. Additionally, there are significant interactions between these factors, indicating that their effects on performance are not independent of each other.\nInteraction between variability and order\n\nlibrary(ggplot2)\n\ninteraction_plot_1 &lt;- ggplot(dtestAgg, aes(x = bandOrder, y = devMean, color = condit)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal() +\n  labs(title = \"Interaction between Training Variability and Order Manipulation\",\n       x = \"Order Manipulation\",\n       y = \"Mean Deviation\",\n       color = \"Training Variability\")\n\ninteraction_plot_1\n\n\n\ntable_2 &lt;- dtestAgg %&gt;%\n  group_by(condit, bandOrder) %&gt;%\n  summarise(mean_deviation = mean(devMean), .groups = \"drop\")\n\nkable(table_2, caption = \"Mean Deviation by Training Variability and Category Order\")\n\n\nMean Deviation by Training Variability and Category Order\n\ncondit\nbandOrder\nmean_deviation\n\n\n\nConstant\nOriginal\n231.9570\n\n\nConstant\nReverse\n244.3648\n\n\nVaried\nOriginal\n264.3575\n\n\nVaried\nReverse\n221.6839\n\n\n\n\n\nVariation and Feedback\n\ninteraction_plot_2 &lt;- ggplot(dtestAgg, aes(x = fb, y = devMean, color = condit)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal() +\n  labs(title = \"Interaction between Training Variability and Feedback Type\",\n       x = \"Feedback Type\",\n       y = \"Mean Deviation\",\n       color = \"Training Variability\")\n\ninteraction_plot_2\n\n\n\nlibrary(knitr)\n\ntable_1 &lt;- dtestAgg %&gt;%\n  group_by(condit, fb) %&gt;%\n  summarise(mean_deviation = mean(devMean), .groups = \"drop\")\n\nkable(table_1, caption = \"Mean Deviation by Training Variability and Feedback Type\")\n\n\nMean Deviation by Training Variability and Feedback Type\n\ncondit\nfb\nmean_deviation\n\n\n\nConstant\nContinuous\n214.8380\n\n\nConstant\nOrdinal\n265.9528\n\n\nVaried\nContinuous\n249.7688\n\n\nVaried\nOrdinal\n235.8744\n\n\n\n\n\ninteraction between Variation and Band\n\nselected_vb &lt;- c(\"800-1000\", \"1000-1200\", \"1200-1400\")\ndtestAgg_filtered &lt;- dtestAgg %&gt;% filter(vb %in% selected_vb)\n\ninteraction_plot_3 &lt;- ggplot(dtestAgg_filtered, aes(x = vb, y = devMean, color = condit)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal() +\n  labs(title = \"Interaction between Training Variability and Specific Velocity Bands\",\n       x = \"Velocity Band\",\n       y = \"Mean Deviation\",\n       color = \"Training Variability\")\n\ninteraction_plot_3\n\n\n\ntable_3 &lt;- dtestAgg %&gt;%\n  group_by(condit, vb) %&gt;%\n  summarise(mean_deviation = mean(devMean), .groups = \"drop\")\n\nkable(table_3, caption = \"Mean Deviation by Training Variability and Velocity Band\")\n\n\nMean Deviation by Training Variability and Velocity Band\n\ncondit\nvb\nmean_deviation\n\n\n\nConstant\n100-300\n306.7256\n\n\nConstant\n350-550\n223.9614\n\n\nConstant\n600-800\n164.6083\n\n\nConstant\n800-1000\n201.4917\n\n\nConstant\n1000-1200\n238.6360\n\n\nConstant\n1200-1400\n296.2888\n\n\nVaried\n100-300\n280.1615\n\n\nVaried\n350-550\n220.4217\n\n\nVaried\n600-800\n197.1750\n\n\nVaried\n800-1000\n226.3583\n\n\nVaried\n1000-1200\n243.4624\n\n\nVaried\n1200-1400\n299.5598\n\n\n\n\n\nSummary\nResults\nThe primary purpose of the HTW study was to examine the influence of training variability on learning and generalization in a visuomotor extrapolation task. Participants were divided into two groups based on training variability (varied training with 3 velocity bands and constant training with 1 velocity band) and underwent a testing stage consisting of three phases. The study employed two between-subject manipulations: order manipulation (‘orig’ and ‘rev’) and feedback type (‘continuous’ and ‘ordinal’). Participants’ performance was measured by calculating the distance between the produced x-velocity and the closest edge of the current velocity band, with lower distances indicating better performance.\nA linear mixed model was fitted to the data with devMean as the dependent variable and condit (training variability), bandOrder (order manipulation), and fb (feedback type) as fixed effects, and id (participant) as a random effect. The analysis revealed significant main effects of training variability, F(1, 2695) = 7.21, p &lt; .01, and feedback type, F(1, 2695) = 6.36, p &lt; .05, on devMean. Participants in the varied training group (M = 266.04, SE = 19.97) had a higher devMean compared to the constant training group (M = 212.43, SE = 13.97), indicating better performance. Participants receiving ordinal feedback (M = 269.04, SE = 22.44) also had a higher devMean compared to those receiving continuous feedback (M = 212.43, SE = 13.97). The main effect of order manipulation was not significant, F(1, 2695) = 0.19, p = .66.\nThe two-way and three-way interaction terms (conditVaried:bandOrderrev, conditVaried:fbordinal, bandOrderrev:fbordinal, and conditVaried:bandOrderrev:fbordinal) were not significant, indicating that the effects of training variability, order manipulation, and feedback type did not significantly interact with each other.\nTo further investigate the influence of velocity bands on performance, an additional linear mixed model was fitted with devMean as the dependent variable and condit, bandOrder, fb, and vb (velocity band) as fixed effects, and id as a random effect. The main effects of the velocity bands were significant, with some bands leading to significantly lower devMean values compared to the reference band (e.g., vb600-800: -102.59, t = -4.148). Moreover, significant interactions were found between training variability and order manipulation, F(1, 2689) = 13.47, p &lt; .001, training variability and feedback type, F(1, 2689) = 8.61, p &lt; .01, and training variability and specific velocity bands (e.g., conditVaried:vb800-1000, F(1, 2689) = 7.59, p &lt; .01).\nIn summary, the results suggest that training variability and feedback type have significant main effects on participants’ performance in the visuomotor extrapolation task. Participants in the varied training group and those receiving ordinal feedback had higher devMean values, indicating better performance. However, the order manipulation (‘orig’ and ‘rev’) did not have a significant effect on performance. Additionally, there were no significant interactions between the factors, suggesting that their effects on performance were independent of each other. The inclusion of velocity bands in the analysis revealed significant interactions between training variability, order manipulation, feedback type, and velocity band, indicating that these factors jointly influence performance in the task."
  },
  {
    "objectID": "Analysis/e1_test.html",
    "href": "Analysis/e1_test.html",
    "title": "E1 Testing",
    "section": "",
    "text": "To assess differences between groups, we used Bayesian Mixed Effects Regression. Analyses were done using brms package in R Bürkner (2017) as well as Makowski et al. (2019).\nUnless otherwise stated, we ran each model with 4 chains, 5000 iterations per chain, the first 2500 of which were discarded as warmup chains. Rhat values were generally within an acceptable range, with values &lt;=1.02.\nWe made use of two separate performance metrics, deviation and discrimination. Deviation was quantified as the absolute deviation from the nearest boundary of the velocity band, or set to 0 if the throw velocity fell anywhere inside the target band. Thus, when the target band was 600-800, throws of 400, 650, and 1100 would result in deviation values of 200, 0, and 300, respectively. Discrimination was measured by fitting a linear model to the testing throws of each subjects, with the lower end of the target velocity band as the predicted variable, and the x velocity produced by the participants as the predictor variable. Participants who reliably discriminated between velocity bands tended to have positive slopes with values ~1, while participants who made throws irrespective of the current target band would have slopes ~0."
  },
  {
    "objectID": "Analysis/e1_Analysis.html",
    "href": "Analysis/e1_Analysis.html",
    "title": "Experiment 1 Analysis",
    "section": "",
    "text": "Code```{r}\n#| label: test vx table\n#| tbl-cap: \"Testing - No Feedback\"\n#| tbl-subcap: [\"Constant Testing - X Velocity\", \"Varied Testing - X Velocity\"]\n#| layout-ncol: 2\n\nresult &lt;- create_summary_table(test, \"vx\", mfun = list(mean = mean, median = median, sd = sd))\nresult$constant #|&gt; kable_styling(full_width = F)\nresult$varied #|&gt; kable_styling(full_width = F)\n```\n\n\nTesting - No Feedback\n\n\n\n\nConstant Testing - X Velocity\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n544\n461\n335\n\n\n350-550\nExtrapolation\n674\n638\n309\n\n\n600-800\nExtrapolation\n786\n742\n305\n\n\n800-1000\nTrained\n1010\n952\n358\n\n\n1000-1200\nExtrapolation\n1167\n1104\n430\n\n\n1200-1400\nExtrapolation\n1283\n1225\n483\n\n\n\n\n\n\nVaried Testing - X Velocity\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n100-300\nExtrapolation\n664\n533\n448\n\n\n350-550\nExtrapolation\n768\n677\n402\n\n\n600-800\nExtrapolation\n876\n813\n390\n\n\n800-1000\nTrained\n1064\n1029\n370\n\n\n1000-1200\nTrained\n1180\n1179\n372\n\n\n1200-1400\nTrained\n1265\n1249\n412\nCodenb=5\nvt1=e1 |&gt; filter(expMode==\"train\") |&gt;\n  learn_curve_table(gt.train,vx,gw=Trial_Bin,groupVec=c(id,vb,condit),nbins=nb,prefix=\"Block_\") |&gt;\n  rename(\"Band\"=vb,\"Group\"=condit)\n  \nvt1 %&gt;% kable() %&gt;% add_header_above(c(\" \"=2, \"Training Block \"=1,\" \" = ncol(vt1)-3))\n\n\nMean Vx over blocks. Mean (Standard Error)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Block\n\n\n\n\nBand\nGroup\nBlock_1\nBlock_2\nBlock_3\nBlock_4\nBlock_5\n\n\n\n\n800-1000\nConstant\n919 (10)\n940 (7)\n910 (7)\n910 (7)\n933 (7)\n\n\n800-1000\nVaried\n970 (20)\n975 (17)\n1017 (18)\n1020 (17)\n1029 (16)\n\n\n1000-1200\nVaried\n1068 (22)\n1100 (19)\n1080 (19)\n1110 (17)\n1080 (15)\n\n\n1200-1400\nVaried\n1109 (24)\n1183 (19)\n1160 (23)\n1198 (20)\n1200 (18)\n\n\n\n\nCode# vt1 %&gt;% gt() %&gt;% tab_options(column_labels.background.color = \"#176940\",\n#                 table.font.size = px(14))\nCode# vpt1=plotWithTable(vp1,vt1,arrange=\"V\")\nnb=5\nvp1=e1 |&gt; filter(expMode==\"train\") |&gt; learn_curve_plot(gt.train,vx,condit,facet_var=vb,groupVec=c(gt.train,condit,tOrder,id,vb),nbins=nb)\nvp1\n\nWarning: Removed 13033 rows containing non-finite values (`stat_summary()`).\nRemoved 13033 rows containing non-finite values (`stat_summary()`).\n\n\n\n\nMean Vx over training blocks\n\n\nCode#vp1 / gridExtra::tableGrob(vt1)\n#vp1 / gt_temp(vt1)\nCode# vt2=e1 |&gt; filter(expMode==\"train\") |&gt; \n#   learn_curve_table(gt.train,dist,gw=condit,groupVec=c(id,condit,vb),nbins=nb) %&gt;%\n#   rename(\"Block\"=Trial_Bin)\n\nvt2=e1 |&gt; filter(expMode==\"train\") |&gt; \n  learn_curve_table(gt.train,dist,gw=Trial_Bin,groupVec=c(id,condit,vb),nbins=nb,prefix=\"Block_\") %&gt;%\n  rename(\"Band\"=vb,\"Group\"=condit)\nCodevp2=e1 |&gt; filter(expMode==\"train\") |&gt; \n  learn_curve_plot(gt.train,dist,condit,facet_var=vb,groupVec=c(gt.train,condit,tOrder,id,vb),nbins=nb) \nvp2\n\nWarning: Removed 13033 rows containing non-finite values (`stat_summary()`).\nRemoved 13033 rows containing non-finite values (`stat_summary()`).\n\n\n\n\nAbsolute Deviation from Band over training\nCodevt3=e1 |&gt; filter(expMode==\"train\") |&gt; \n  learn_curve_table(gt.train,sdist,gw=Trial_Bin,groupVec=c(id,vb,condit),nbins=nb,prefix=\"Block_\") |&gt;\n  rename(\"Band\"=vb,\"Group\"=condit) \nvt3 %&gt;% kable(format = \"html\",escape = FALSE)\n\n\nSigned Deviation over blocks. Mean (Standard Error)\n\nBand\nGroup\nBlock_1\nBlock_2\nBlock_3\nBlock_4\nBlock_5\n\n\n\n800-1000\nConstant\n23 (8)\n34 (6)\n11 (5)\n13 (5)\n30 (5)\n\n\n800-1000\nVaried\n64 (17)\n61 (13)\n96 (15)\n102 (14)\n104 (13)\n\n\n1000-1200\nVaried\n-14 (19)\n10 (15)\n-11 (15)\n18 (13)\n-11 (12)\n\n\n1200-1400\nVaried\n-155 (21)\n-85 (16)\n-108 (19)\n-78 (16)\n-76 (14)\nCodevp3=e1 |&gt; filter(expMode==\"train\") |&gt; learn_curve_plot(gt.train,sdist,condit,facet_var=vb,groupVec=c(gt.train,condit,tOrder,id,vb),nbins=nb) \nvp3\n\nWarning: Removed 13033 rows containing non-finite values (`stat_summary()`).\nRemoved 13033 rows containing non-finite values (`stat_summary()`).\n\n\n\n\nSigned Deviation from Band over training\nCodevp4 &lt;- e1 |&gt; filter(expMode==\"train\") |&gt; group_by(id) |&gt;\n  mutate(Trial_Bin = cut(gt.train,breaks = nb,include.lowest=TRUE,labels=FALSE)) |&gt;\n  group_by(id,Trial_Bin,condit, vb) |&gt; summarise(nHits=sum(dist==0),n=n(),Percent_Hit=nHits/n) %&gt;%\n  learn_curve_plot2(Trial_Bin,Percent_Hit,color_var=condit,facet_var=vb,groupVec=c(id,vb,condit))"
  },
  {
    "objectID": "Analysis/e1_Analysis.html#dictionary",
    "href": "Analysis/e1_Analysis.html#dictionary",
    "title": "Experiment 1 Analysis",
    "section": "Dictionary",
    "text": "Dictionary\n\n\n\n\n\n\n\nVariable Name\nVariable Levels\nDescription\n\n\n\ncondit\nConstant, Varied\nCondition of the experiment: constant or varied\n\n\ntOrder\nTest First, Train First\nOrder of testing and training stages: test first or train first\n\n\nexpMode\ntrain, train-Nf, test-Nf, etc.\nMode of the experiment: train, train-Nf, test-Nf, etc.\n\n\ntrainStage\nBeginning, Middle, End, Test\nStage of the training: beginning, middle, end, or test\n\n\nexpStage\nTrainStart, intTest1, etc.\nStage of the experiment: TrainStart, intTest1, TrainMid1, etc.\n\n\nband\n1, 2, 3, 4, 5, 6\nBand number\n\n\nvb\n100-300, 350-550, etc.\nVelocity band range\n\n\nlowBound\n100, 350, 600, etc.\nLower bound of the velocity band range\n\n\nfeedback\n0, 1\nFeedback type: 0 (no feedback), 1 (feedback)\n\n\nstage\n1, 2, 3, etc.\nStage number of the experiment"
  },
  {
    "objectID": "Analysis/e1_Analysis.html#testing",
    "href": "Analysis/e1_Analysis.html#testing",
    "title": "Experiment 1 Analysis",
    "section": "Testing",
    "text": "Testing\n\n\nVaried vs. Constant\nDist\nVx\nSigned Distance\nPercent Hit\nTables\n\n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  \n  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\n\n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\nCode# create a kable table to mirror plot of distance effects for vb and condit \n\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(distMean=mean(dist),distSd=sd(dist)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(distMean,0)),sdLab=paste0(\"Sd=\",round(distSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=271 Sd=308\nMean=386 Sd=426\n\n\n350-550\nMean=201 Sd=238\nMean=285 Sd=340\n\n\n600-800\nMean=155 Sd=194\nMean=234 Sd=270\n\n\n800-1000\nMean=192 Sd=238\nMean=221 Sd=248\n\n\n1000-1200\nMean=233 Sd=282\nMean=208 Sd=226\n\n\n1200-1400\nMean=287 Sd=290\nMean=242 Sd=235\n\n\n\n\n\n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  ggplot(aes(x = vb, y = vx,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\nCodetestAvg |&gt;  ggplot(aes(x = vb, y = vx,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(vxMean=mean(vx),vxSd=sd(vx)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(vxMean,0)),sdLab=paste0(\"Sd=\",round(vxSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=544 Sd=335\nMean=664 Sd=448\n\n\n350-550\nMean=674 Sd=309\nMean=768 Sd=402\n\n\n600-800\nMean=786 Sd=305\nMean=876 Sd=390\n\n\n800-1000\nMean=1010 Sd=358\nMean=1064 Sd=370\n\n\n1000-1200\nMean=1167 Sd=430\nMean=1180 Sd=372\n\n\n1200-1400\nMean=1283 Sd=483\nMean=1265 Sd=412\n\n\n\n\n\n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  ggplot(aes(x = vb, y = sdist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\nCodetestAvg |&gt;  ggplot(aes(x = vb, y = sdist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(sdistMean=mean(sdist),sdistSd=sd(sdist)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(sdistMean,0)),sdLab=paste0(\"Sd=\",round(sdistSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=269 Sd=309\nMean=385 Sd=427\n\n\n350-550\nMean=175 Sd=257\nMean=265 Sd=356\n\n\n600-800\nMean=74 Sd=237\nMean=152 Sd=323\n\n\n800-1000\nMean=95 Sd=291\nMean=136 Sd=303\n\n\n1000-1200\nMean=64 Sd=360\nMean=66 Sd=300\n\n\n1200-1400\nMean=-3 Sd=408\nMean=-25 Sd=336\n\n\n\n\n\n\n\n\nCodetestAvg |&gt;  ggplot(aes(x = vb, y = Percent_Hit,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) \n\n\n\nCodetestAvg |&gt; group_by(vb,condit) %&gt;% \n  summarise(Percent_HitMean=mean(Percent_Hit),Percent_HitSd=sd(Percent_Hit)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(Percent_HitMean,3)),sdLab=paste0(\"Sd=\",round(Percent_HitSd,2))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=0.236 Sd=0.26\nMean=0.201 Sd=0.26\n\n\n350-550\nMean=0.24 Sd=0.18\nMean=0.245 Sd=0.21\n\n\n600-800\nMean=0.282 Sd=0.16\nMean=0.206 Sd=0.16\n\n\n800-1000\nMean=0.272 Sd=0.22\nMean=0.19 Sd=0.19\n\n\n1000-1200\nMean=0.21 Sd=0.2\nMean=0.206 Sd=0.2\n\n\n1200-1400\nMean=0.161 Sd=0.17\nMean=0.2 Sd=0.19\n\n\n\n\n\n\n\nBandType\n\nCodecreate_table(test, \"vx\")\n\n\nSummary of vx\n\n\n\n\n\n\n\nBand\nBand Type\nConstant\nBand Type\nVaried\n\n\n\n\n100-300\nExtrapolation\nMean=542 SD=252\nExtrapolation\nMean=665 SD=365\n\n\n350-550\nExtrapolation\nMean=676 SD=227\nExtrapolation\nMean=772 SD=321\n\n\n600-800\nExtrapolation\nMean=787 SD=210\nExtrapolation\nMean=880 SD=292\n\n\n800-1000\nTrained\nMean=1012 SD=254\nTrained\nMean=1070 SD=249\n\n\n1000-1200\nExtrapolation\nMean=1173 SD=308\nTrained\nMean=1181 SD=266\n\n\n1200-1400\nExtrapolation\nMean=1307 SD=356\nTrained\nMean=1269 SD=292\n\n\n\n\nCodecreate_table(test, \"Percent_Hit\")\n\n\nSummary of Percent_Hit\n\n\n\n\n\n\n\nBand\nBand Type\nConstant\nBand Type\nVaried\n\n\n\n\n100-300\nExtrapolation\nMean=0.236 SD=0.26\nExtrapolation\nMean=0.201 SD=0.26\n\n\n350-550\nExtrapolation\nMean=0.24 SD=0.18\nExtrapolation\nMean=0.245 SD=0.21\n\n\n600-800\nExtrapolation\nMean=0.282 SD=0.16\nExtrapolation\nMean=0.206 SD=0.16\n\n\n800-1000\nTrained\nMean=0.272 SD=0.22\nTrained\nMean=0.19 SD=0.19\n\n\n1000-1200\nExtrapolation\nMean=0.21 SD=0.2\nTrained\nMean=0.206 SD=0.2\n\n\n1200-1400\nExtrapolation\nMean=0.161 SD=0.17\nTrained\nMean=0.2 SD=0.19\n\n\n\n\n\nAggregation\n\nCodee1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;%\n  group_by(vb, condit, bandType) %&gt;%\n  summarise(distMean = mean(dist), distSd = sd(dist)) %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(distMean, 0)),\n    sdLab = paste0(\"Sd=\", round(distSd, 0))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  pivot_wider(\n    id_cols = c(vb, bandType),\n    names_from = condit,\n    values_from = sumStatLab\n  ) %&gt;%\n  arrange(vb, bandType) %&gt;%\n  kable(format = \"html\", escape = FALSE)\n\n\n\nvb\nbandType\nConstant\nVaried\n\n\n\n100-300\nExtrapolation\nMean=271 Sd=308\nMean=386 Sd=426\n\n\n350-550\nExtrapolation\nMean=201 Sd=238\nMean=285 Sd=340\n\n\n600-800\nExtrapolation\nMean=155 Sd=194\nMean=234 Sd=270\n\n\n800-1000\nTrained\nMean=192 Sd=238\nMean=221 Sd=248\n\n\n1000-1200\nTrained\nNA\nMean=208 Sd=226\n\n\n1000-1200\nExtrapolation\nMean=233 Sd=282\nNA\n\n\n1200-1400\nTrained\nNA\nMean=242 Sd=235\n\n\n1200-1400\nExtrapolation\nMean=287 Sd=290\nNA\n\n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(vxMean=mean(vx),vxSd=sd(vx)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(vxMean,0)),sdLab=paste0(\"Sd=\",round(vxSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE,caption = \"Vx Mean\") %&gt;% \n  kable_styling(font_size = 10)\n\n\nVx Mean\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=544 Sd=335\nMean=664 Sd=448\n\n\n350-550\nMean=674 Sd=309\nMean=768 Sd=402\n\n\n600-800\nMean=786 Sd=305\nMean=876 Sd=390\n\n\n800-1000\nMean=1010 Sd=358\nMean=1064 Sd=370\n\n\n1000-1200\nMean=1167 Sd=430\nMean=1180 Sd=372\n\n\n1200-1400\nMean=1283 Sd=483\nMean=1265 Sd=412\n\n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(sdistMean=mean(sdist),sdistSd=sd(sdist)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(sdistMean,0)),sdLab=paste0(\"Sd=\",round(sdistSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE,caption = \"Signed Deviation Mean\") %&gt;% \n  kable_styling(font_size = 10)\n\n\nSigned Deviation Mean\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=269 Sd=309\nMean=385 Sd=427\n\n\n350-550\nMean=175 Sd=257\nMean=265 Sd=356\n\n\n600-800\nMean=74 Sd=237\nMean=152 Sd=323\n\n\n800-1000\nMean=95 Sd=291\nMean=136 Sd=303\n\n\n1000-1200\nMean=64 Sd=360\nMean=66 Sd=300\n\n\n1200-1400\nMean=-3 Sd=408\nMean=-25 Sd=336\n\n\n\n\nCodetestAvg |&gt; group_by(vb,condit) %&gt;% \n  summarise(Percent_HitMean=mean(Percent_Hit),Percent_HitSd=sd(Percent_Hit)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(Percent_HitMean,3)),sdLab=paste0(\"Sd=\",round(Percent_HitSd,2))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE,caption = \"Mean % Hit\") %&gt;% \n  kable_styling(font_size = 10)\n\n\nMean % Hit\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=0.236 Sd=0.26\nMean=0.201 Sd=0.26\n\n\n350-550\nMean=0.24 Sd=0.18\nMean=0.245 Sd=0.21\n\n\n600-800\nMean=0.282 Sd=0.16\nMean=0.206 Sd=0.16\n\n\n800-1000\nMean=0.272 Sd=0.22\nMean=0.19 Sd=0.19\n\n\n1000-1200\nMean=0.21 Sd=0.2\nMean=0.206 Sd=0.2\n\n\n1200-1400\nMean=0.161 Sd=0.17\nMean=0.2 Sd=0.19\n\n\n\n\n\nTables Aggregating by Id first\n\nCodee1 %&gt;% group_by(id, vb, condit) %&gt;%\n  summarise(dist = mean(dist), .groups = 'drop') %&gt;%\n  group_by(vb, condit) %&gt;%\n  summarise(distMean = mean(dist), distSd = sd(dist), .groups = 'drop') %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(distMean, 0)),\n    sdLab = paste0(\"Sd=\", round(distSd, 0))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  select(vb, condit, sumStatLab) %&gt;%\n  spread(condit, sumStatLab) %&gt;%\n  kable(format = \"html\", escape = FALSE, caption = \"Deviation Mean - Aggregate by Id\") %&gt;%\n  kable_styling(font_size = 11)\n\n\nDeviation Mean - Aggregate by Id\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=254 Sd=181\nMean=340 Sd=264\n\n\n350-550\nMean=187 Sd=125\nMean=261 Sd=202\n\n\n600-800\nMean=147 Sd=77\nMean=218 Sd=145\n\n\n800-1000\nMean=140 Sd=58\nMean=206 Sd=86\n\n\n1000-1200\nMean=241 Sd=173\nMean=206 Sd=74\n\n\n1200-1400\nMean=295 Sd=186\nMean=261 Sd=70\n\n\n\n\nCodee1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(vx = mean(vx), .groups = 'drop') %&gt;%\n  group_by(vb, condit) %&gt;%\n  summarise(vxMean = mean(vx), vxSd = sd(vx), .groups = 'drop') %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(vxMean, 0)),\n    sdLab = paste0(\"Sd=\", round(vxSd, 0))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  select(vb, condit, sumStatLab) %&gt;%\n  spread(condit, sumStatLab) %&gt;%\n  kable(format = \"html\", escape = FALSE, caption = \"Vx Mean - Aggregate by Id\") %&gt;%\n  kable_styling(font_size = 11)\n\n\nVx Mean - Aggregate by Id\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=542 Sd=252\nMean=665 Sd=365\n\n\n350-550\nMean=676 Sd=227\nMean=772 Sd=321\n\n\n600-800\nMean=787 Sd=210\nMean=880 Sd=292\n\n\n800-1000\nMean=1012 Sd=254\nMean=1070 Sd=249\n\n\n1000-1200\nMean=1173 Sd=308\nMean=1181 Sd=266\n\n\n1200-1400\nMean=1307 Sd=356\nMean=1269 Sd=292\n\n\n\n\nCodee1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(sdist = mean(sdist), .groups = 'drop') %&gt;%\n  group_by(vb, condit) %&gt;%\n  summarise(sdistMean = mean(sdist), sdistSd = sd(sdist), .groups = 'drop') %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(sdistMean, 0)),\n    sdLab = paste0(\"Sd=\", round(sdistSd, 0))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  select(vb, condit, sumStatLab) %&gt;%\n  spread(condit, sumStatLab) %&gt;%\n  kable(format = \"html\", escape = FALSE, caption = \"Signed Distance Mean - Aggregate by Id\") %&gt;%\n  kable_styling(font_size = 11)\n\n\nSigned Distance Mean - Aggregate by Id\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=268 Sd=229\nMean=386 Sd=344\n\n\n350-550\nMean=177 Sd=186\nMean=270 Sd=286\n\n\n600-800\nMean=75 Sd=157\nMean=155 Sd=241\n\n\n800-1000\nMean=98 Sd=204\nMean=141 Sd=196\n\n\n1000-1200\nMean=70 Sd=253\nMean=66 Sd=209\n\n\n1200-1400\nMean=17 Sd=297\nMean=-20 Sd=232\n\n\n\n\nCodetestAvg %&gt;%\n  group_by(id, vb, condit) %&gt;%\n  summarise(Percent_Hit = mean(Percent_Hit), .groups = 'drop') %&gt;%\n  group_by(vb, condit) %&gt;%\n  summarise(\n    Percent_HitMean = mean(Percent_Hit),\n    Percent_HitSd = sd(Percent_Hit),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(Percent_HitMean, 3)),\n    sdLab = paste0(\"Sd=\", round(Percent_HitSd, 2))\n  ) %&gt;%\n  mutate(sumStatLab = paste0(meanLab, \"\\n\", sdLab)) %&gt;%\n  select(vb, condit, sumStatLab) %&gt;%\n  spread(condit, sumStatLab) %&gt;%\n  kable(format = \"html\", escape = FALSE, caption = \"Mean % Hit - Aggregate by Id\") %&gt;%\n  kable_styling(font_size = 10)\n\n\nMean % Hit - Aggregate by Id\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=0.236 Sd=0.26\nMean=0.201 Sd=0.26\n\n\n350-550\nMean=0.24 Sd=0.18\nMean=0.245 Sd=0.21\n\n\n600-800\nMean=0.282 Sd=0.16\nMean=0.206 Sd=0.16\n\n\n800-1000\nMean=0.272 Sd=0.22\nMean=0.19 Sd=0.19\n\n\n1000-1200\nMean=0.21 Sd=0.2\nMean=0.206 Sd=0.2\n\n\n1200-1400\nMean=0.161 Sd=0.17\nMean=0.2 Sd=0.19\n\n\n\n\n\nTables that also indicate bandType\n\nCode# Create the Constant table\nconstant_table &lt;- e1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n  group_by(vb, bandType, condit) %&gt;%\n  summarise(distMean = mean(dist), distSd = sd(dist)) %&gt;%\n  filter(condit == \"Constant\") %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(distMean, 0)),\n    sdLab = paste0(\"Sd=\", round(distSd, 0)),\n    sumStatLab = paste0(meanLab, \"\\n\", sdLab)\n  ) %&gt;%\n  select(vb, bandType, sumStatLab) %&gt;%\n  rename(Constant = sumStatLab)\n\n# Create the Varied table\nvaried_table &lt;- e1 %&gt;%\n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n  group_by(vb, bandType, condit) %&gt;%\n  summarise(distMean = mean(dist), distSd = sd(dist)) %&gt;%\n  filter(condit == \"Varied\") %&gt;%\n  mutate(\n    meanLab = paste0(\"Mean=\", round(distMean, 0)),\n    sdLab = paste0(\"Sd=\", round(distSd, 0)),\n    sumStatLab = paste0(meanLab, \"\\n\", sdLab)\n  ) %&gt;%\n  select(vb, bandType, sumStatLab) %&gt;%\n  rename(Varied = sumStatLab)\n\n# Merge tables\nfinal_table &lt;- full_join(constant_table, varied_table, by = \"vb\")\n\n# Create the table\nfinal_table %&gt;%\n  kbl(digits = c(0, 0, 0, 0, 0),\n      caption = \"Data summary\") %&gt;%\n  kable_minimal(full_width = FALSE,\n      position = \"left\") %&gt;%\n  add_header_above(c(\"vb\" = 1, \"Constant\" = 2, \"Varied\" = 2))\n\n\nData summary\n\n\n\n\n\n\n\n\n\n\nvb\n\n\nConstant\n\n\nVaried\n\n\n\nvb\nbandType.x\nConstant\nbandType.y\nVaried\n\n\n\n\n100-300\nExtrapolation\nMean=271 Sd=308\nExtrapolation\nMean=386 Sd=426\n\n\n350-550\nExtrapolation\nMean=201 Sd=238\nExtrapolation\nMean=285 Sd=340\n\n\n600-800\nExtrapolation\nMean=155 Sd=194\nExtrapolation\nMean=234 Sd=270\n\n\n800-1000\nTrained\nMean=192 Sd=238\nTrained\nMean=221 Sd=248\n\n\n1000-1200\nExtrapolation\nMean=233 Sd=282\nTrained\nMean=208 Sd=226\n\n\n1200-1400\nExtrapolation\nMean=287 Sd=290\nTrained\nMean=242 Sd=235\n\n\n\n\n\n\n\n\n\n\nCoderectWidth=.4\nvbRect&lt;- e1 %&gt;% group_by(vb) %&gt;% \n  summarise(lowBound=first(bandInt),highBound=first(highBound)) %&gt;% mutate(vbn=as.numeric(vb),\n                  vbLag=vbn-rectWidth,vbLead=vbn+rectWidth)\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  ggplot(aes(x = condit, y = vx,fill=vb)) + \n  ggdist::stat_halfeye()\n\n\nvbRect&lt;- e1 |&gt; group_by(vb) |&gt;\n  summarise(lowBound=first(bandInt),highBound=first(highBound)) |&gt; \n  mutate(vbn=as.numeric(vb), rectWidth=.2,\n                  vbLag=vbn-rectWidth,vbLead=vbn+rectWidth)\n\ne1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  \n  ggplot(aes(x = vb, y = vx,fill=vb)) + \n  ggdist::stat_halfeye(alpha=.5) + \n  geom_rect(data=vbRect,aes(xmin=vbLag,xmax=vbLead,ymin=lowBound,ymax=highBound,fill=vb),alpha=.3,inherit.aes = FALSE)+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2400,by=200),2)) +\n  geom_segment(data=vbRect, aes(x=vbLag,xend=vbLead,y=highBound,yend=highBound),alpha=1,linetype=\"dashed\")+\n  geom_segment(data=vbRect, aes(x=vbLag,xend=vbLead,y=lowBound,yend=lowBound),alpha=1,linetype=\"dashed\")+\n  geom_text(data=vbRect,aes(x=vbLag-.03,y=lowBound+100,label=vb),angle=90,size=3.5,fontface=\"bold\")\n\n\ntestAvg |&gt;  \n  ggplot(aes(x = vb, y = vx,fill=vb)) + \n  ggdist::stat_halfeye(alpha=.5,width=.7) + \n  geom_rect(data=vbRect,aes(xmin=vbLag,xmax=vbLead,ymin=lowBound,ymax=highBound,fill=vb),alpha=.3,inherit.aes = FALSE)+\n  facet_wrap(~condit,ncol=1)+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  geom_segment(data=vbRect, aes(x=vbLag,xend=vbLead,y=highBound,yend=highBound),alpha=1,linetype=\"dashed\",inherit.aes = FALSE)+\n  geom_segment(data=vbRect, aes(x=vbLag,xend=vbLead,y=lowBound,yend=lowBound),alpha=1,linetype=\"dashed\",inherit.aes = FALSE)+\n  geom_text(data=vbRect,aes(x=vbLag-.03,y=lowBound+100,label=vb),angle=90,size=5.5,fontface=\"bold\",inherit.aes = FALSE)  \n  \n\n\n# testAvg |&gt;  \n#   ggplot(aes(x = vb, y = sdist,fill=vb)) + \n#   ggdist::stat_halfeye(alpha=.5,width=.7) + \n#   facet_wrap(~condit,ncol=1)\n# \n# \n# testAvg |&gt;  \n#   ggplot(aes(x = vb, y = dist,fill=vb)) + \n#   ggdist::stat_halfeye(alpha=.5,width=.7) + \n#   facet_wrap(~condit,ncol=1)\n# \n# \n\n\n\nCodesumStats2 = test %&gt;% group_by(id,condit,vb) %&gt;%\n  summarise(distMean=mean(dist),distMedian=median(dist),distSd=sd(dist)) %&gt;%group_by(condit,vb) %&gt;%\n  summarise(groupMean=round(mean(distMean),0),groupMedian=round(mean(distMedian),0),groupSd=round(mean(distSd),0)) %&gt;%\n  mutate(meanLab=paste0(\"Mean=\",groupMean),medianLab=paste0(\"Median=\",groupMedian),sdLab=paste0(\"Sd=\",groupSd)) %&gt;%\n  mutate(sumStatLab=paste0(meanLab,\"\\n\",medianLab,\"\\n\",sdLab))\n\nsumStats = test %&gt;% group_by(id,condit,vb) %&gt;%\n  summarise(distMean=mean(vx),distMedian=median(vx),distSd=sd(vx)) %&gt;%group_by(condit,vb) %&gt;%\n  summarise(groupMean=round(mean(distMean),0),groupMedian=round(mean(distMedian),0),groupSd=round(mean(distSd),0)) %&gt;%\n  mutate(meanLab=paste0(\"Mean=\",groupMean),medianLab=paste0(\"Median=\",groupMedian),sdLab=paste0(\"Sd=\",groupSd)) %&gt;%\n  mutate(sumStatLab=paste0(meanLab,\"\\n\",medianLab,\"\\n\",sdLab))\n\nbandLines4 &lt;- list(geom_segment(data=vbRect,aes(x=vbLag,xend=vbLead,y=highBound,yend=highBound),alpha=1,linetype=\"dashed\"),\n                   geom_segment(data=vbRect,aes(x=vbLag,xend=vbLead,y=lowBound,yend=lowBound),alpha=1,linetype=\"dashed\"),\n                   geom_text(data=vbRect,aes(x=vbLag-.03,y=lowBound+100,label=vb),angle=90,size=2.5,fontface=\"bold\") )    \n\n\ntest %&gt;% group_by(id,vb,condit) %&gt;% \n  summarise(vxMean=mean(vx)) %&gt;%\n  ggplot(aes(x=vb,y=vxMean,fill=vb))+\n  gghalves::geom_half_violin(color=NA)+ # remove border color\n  gghalves::geom_half_boxplot(position=position_nudge(x=-0.05),side=\"r\",outlier.shape = NA,center=TRUE, \n                    errorbar.draw = FALSE,width=.25)+\n  gghalves::geom_half_point(transformation = position_jitter(width = 0.05, height = 0.05),size=.3,aes(color=vb))+\n  facet_wrap(~condit,scale=\"free_x\")+\n  geom_rect(data=vbRect,aes(xmin=vbLag,xmax=vbLead,ymin=lowBound,ymax=highBound,fill=vb),alpha=.3,inherit.aes = FALSE)+\n  bandLines4+\n  #geom_text(data=sumStats,aes(x=vb,y=2100,label = groupMean),size=2, vjust = -0.5)+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2))+\n  theme(legend.position='none',\n        plot.title=element_text(face=\"bold\"),\n        axis.title.x=element_text(face=\"bold\"),\n        axis.title.y=element_text(face=\"bold\"),\n        axis.text.x = element_text(size = 7.5))+\n  ylab(\"Mean X Velocity\")+xlab(\"Target Velocity Band\") + \n  ggtitle(\"Testing Performance (no-feedback) - X-Velocity Per Band\")+ \n  geom_text(data=sumStats2,aes(y=2090,label = sumStatLab),size=1.9)\n\n\n\ntest %&gt;% group_by(id,vb,condit) %&gt;% \n  summarise(distMean=mean(dist)) %&gt;% \n  ggplot(aes(x=vb,y=distMean,fill=vb))+\n  geom_half_violin(color=NA)+ # remove border color\n  geom_half_boxplot(position=position_nudge(x=-0.05),side=\"r\",outlier.shape = NA,center=TRUE, \n                    errorbar.draw = FALSE,width=.25)+\n  geom_half_point(transformation = position_jitter(width = 0.05, height = 0.05),size=.3,aes(color=vb))+\n  facet_wrap(~condit,scale=\"free_x\")+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2))+\n  theme(legend.position='none',\n        plot.title=element_text(face=\"bold\"),\n        axis.title.x=element_text(face=\"bold\"),\n        axis.title.y=element_text(face=\"bold\"),\n        axis.text.x = element_text(size = 9.0))+\n  ylab(\"Mean Distance From Boundary\")+xlab(\"Target Velocity Band\") + \n  ggtitle(\"Testing Performance (no-feedback) - Absolute Distance from Boundary\")+ \n  geom_text(data=sumStats2,aes(y=1200,label = sumStatLab),size=3,fontface=\"bold\")\n\n\n\nCodetest %&gt;% group_by(sbjCode,condit,vb) %&gt;% \n  summarise(distMean=mean(dist)) %&gt;% \n  ggplot(aes(x=condit,y=distMean,fill=condit))+\n  stat_summary(geom=\"bar\",fun=mean,position=position_nudge(x=.21),alpha=.7,width=.2)+\n   stat_summary(geom=\"errorbar\",fun.data=mean_se,position=position_nudge(x=.21),alpha=.7,width=.1,)+\n  geom_half_violin(position=position_dodge(.5),alpha=.55,color=NA)+ # remove border color\n  geom_half_boxplot(position=position_dodge(.5),side=\"r\",outlier.shape = NA,center=TRUE, \n                    errorbar.draw = FALSE,width=.25,alpha=.55)+\n  geom_half_point(transformation = position_jitter(width = 0.05, height = 0.05),alpha=.2,size=.3)+\n  facet_wrap(~vb,scale=\"free_x\")+\n  scale_y_continuous(expand=expansion(add=200),breaks=round(seq(0,2000,by=200),2))+\n  theme(legend.position='none',\n        plot.title=element_text(face=\"bold\"),\n        axis.title.x=element_text(face=\"bold\"),\n        axis.title.y=element_text(face=\"bold\"),\n        axis.text.x = element_text(size = 9.0))+\n  ylab(\"Mean Distance From Boundary\")+xlab(\"Training Condition\") + \n  ggtitle(\"Testing Performance (no-feedback) - Absolute Distance from Boundary\")+ \n  geom_text(data=sumStats2,aes(y=1390,label = sumStatLab),position=position_dodge(.5),size=3,fontface=\"bold\")\n\n\n\nCodelibrary(ggforce)\n\ntestAvg %&gt;% ggplot(aes(x=vx,y=condit,col=condit))+ ggdist::stat_halfeye()+facet_col(facets=\"vb\")\n\n\n\nCodetestAvg %&gt;% ggplot(aes(x=sdist,y=condit,col=condit))+ geom_boxplot()+facet_col(facets=\"vb\")+geom_vline(xintercept=0)+theme_classic()\n\n\n\nCodelibrary(ggh4x)\nlibrary(ggdist)\n\ntestAvg %&gt;% ggplot(aes(x=vx,y=vb,col=bandType)) +  ggdist::stat_halfeye(normalize=\"groups\")+ facet_nested_wrap(vars(condit,tOrder),nrow=2)+theme_classic()\n\n\n\nCodetestAvg %&gt;% ggplot(aes(x=vx,y=vb,col=bandType)) +  ggdist::stat_histinterval(normalize=\"groups\")+ facet_nested_wrap(vars(condit,tOrder),nrow=2)+theme_classic()\n\n\n\nCodetestAvg %&gt;% ggplot(aes(x=vx,y=vb,col=bandType)) +  ggdist::stat_histinterval(normalize=\"groups\")+ facet_nested_wrap(vars(condit,tOrder),nrow=2)+theme_classic()\n\n\n\nCodee1 |&gt; group_by(id, condit, vb, bandInt,bandType,tOrder,expMode2) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist),sdist=mean(sdist)) %&gt;%\n  ggplot(aes(x=vx,y=condit,col=condit)) +  ggdist::stat_histinterval(normalize=\"groups\")+ facet_nested_wrap(vars(vb,expMode2),nrow=6)+theme_classic()\n\n\n\nCodee1 |&gt; group_by(id, condit, vb, bandInt,bandType,tOrder,expMode2) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist),sdist=mean(sdist)) %&gt;% ggplot(aes(x=sdist,y=vb,col=condit))+ stat_histinterval(normalize=\"groups\",position=position_dodge())+facet_col(facets=\"expMode2\")+geom_vline(xintercept=0)+theme_classic()"
  },
  {
    "objectID": "Analysis/brms_learning.html",
    "href": "Analysis/brms_learning.html",
    "title": "Bayesian Mixed Effects Models - Training Data",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes,brms,bayesplot,broom,broom.mixed,lme4,emmeans,here,knitr,kableExtra,gt,gghalves,patchwork,ggforce,ggdist)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\ntrain &lt;- e1 |&gt; filter(expMode2 == \"Train\")  \ntrain$bandIntS &lt;- scale(train$bandInt)\ntrain$distS &lt;- scale(train$dist)\n\noptions(brms.backend=\"cmdstanr\",mc.cores=4)\n\n\n\nCodemod_train_expo3_dist &lt;-bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 1 + (1|id), \n                 alphaMu ~ 1 +   (1|id), \n                 gammaMu ~ 1 +  (1|id), \n                 nl = TRUE)\n\ne1_train_expo3_dist &lt;- brm(mod_train_expo3_dist,\n                            chains=4, iter=2000, silent=0,\n                            file=here::here(\"data/model_cache/e1_train_expo3_dist\"))\n\n\nmod_train_expo3_condit_dist &lt;- bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 1 + condit + (1|id), \n                 alphaMu ~ 1 + condit +   (1|id), \n                 gammaMu ~ 1 + condit + (1|id), \n                 nl = TRUE)\n\ne1_train_expo3_condit_dist &lt;- brm(mod_train_expo3_condit_dist,\n                            chains=4, iter=2000, silent=0,\n                            file=here::here(\"data/model_cache/e1_train_expo3_condit_dist\"))\n\n\n\nmod_train_expo3_conditBandit_dist &lt;- bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 0 + condit + (1|id) + (1|id:bandInt), \n                 alphaMu ~ 0 + condit + (1|id) + (1|id:bandInt), \n                 gammaMu ~ 0 + condit  +  (1|id) + (1|id:bandInt), \n                 nl = TRUE)\n\n\n\ne1_train_expo3_conditBanditS_dist &lt;- brm(mod_train_expo3_conditBandit_dist,\n                            chains=4, iter=2000, silent=0,\n                            data=train,\n                            file=here::here(\"data/model_cache/e1_train_expo3_conditBanditSdist\"))\n\n\n\n\nmod_train_expo3_band_distS &lt;- bf(distS ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 1 + bandIntS + (1|id), \n                 alphaMu ~ 1 + bandIntS +   (1|id), \n                 gammaMu ~ 1 + bandIntS + (1|id), \n                 nl = TRUE)\n\ne1_train_expo3_band_distS &lt;- brm(mod_train_expo3_band_distS,\n                            chains=4, iter=2000, silent=0,\n                            data=train,\n                            file=here::here(\"data/model_cache/e1_train_expo3_band_distS\"))\ncoef(e1_train_expo3_band_distS)$id |&gt; as_tibble(rownames=\"id\") |&gt; select(starts_with(\"Esti\")) |&gt; print(n=15)\n\n# A tibble: 166 × 6\n   Estimate.betaMu_Intercept Estimate.alphaMu_Intercept Estimate.gammaMu_Inter…¹\n                       &lt;dbl&gt;                      &lt;dbl&gt;                    &lt;dbl&gt;\n 1                    0.0374                       4.63                   -0.778\n 2                    0.267                        3.45                   -0.411\n 3                   -0.0654                       4.56                   -0.657\n 4                   -0.118                        1.30                   -0.186\n 5                    0.132                        2.35                   -0.795\n 6                   -0.158                        1.74                    0.331\n 7                    0.168                        1.92                    0.181\n 8                    0.0307                       1.08                    0.621\n 9                   -0.433                        3.63                   -1.31 \n10                   -0.369                        4.27                   -1.62 \n11                   -0.414                        1.67                   -0.237\n12                   -0.176                        3.24                   -1.98 \n13                    0.112                        1.39                    0.645\n14                   -0.141                        1.91                    0.176\n15                   -0.370                        3.90                   -1.52 \n# ℹ 151 more rows\n# ℹ abbreviated name: ¹​Estimate.gammaMu_Intercept\n# ℹ 3 more variables: Estimate.betaMu_bandIntS &lt;dbl&gt;,\n#   Estimate.alphaMu_bandIntS &lt;dbl&gt;, Estimate.gammaMu_bandIntS &lt;dbl&gt;\n\nCode mod_train_expo3_bandCondit_distS &lt;- bf(distS ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train),\n                 betaMu ~ 1 + bandIntS + condit + (1|id), \n                 alphaMu ~ 1 + bandIntS + condit +  (1|id), \n                 gammaMu ~ 1 + bandIntS + condit + (1|id), \n                 nl = TRUE)\n\ne1_train_expo3_bandCondit_distS &lt;- brm(mod_train_expo3_bandCondit_distS,\n                            chains=4, iter=2000, silent=0,\n                            data=train,\n                            file=here::here(\"data/model_cache/e1_train_expo3_bandCondit_distS\"))                           \ncoef(e1_train_expo3_bandCondit_distS)$id |&gt; as_tibble(rownames=\"id\") |&gt; select(starts_with(\"Esti\")) |&gt; print(n=15)\n\n# A tibble: 166 × 9\n   Estimate.betaMu_Intercept Estimate.alphaMu_Intercept Estimate.gammaMu_Inter…¹\n                       &lt;dbl&gt;                      &lt;dbl&gt;                    &lt;dbl&gt;\n 1                   -0.230                        74.9                     25.4\n 2                   -0.0105                       74.6                     25.9\n 3                   -0.0861                       76.1                     25.9\n 4                   -0.376                        73.2                     26.3\n 5                    0.0808                       73.9                     25.7\n 6                   -0.438                        73.4                     25.9\n 7                   -0.0976                       73.5                     26.6\n 8                   -0.230                        72.9                     27.0\n 9                   -0.433                        75.0                     25.2\n10                   -0.389                        75.8                     24.9\n11                   -0.421                        73.4                     26.6\n12                   -0.216                        74.5                     24.4\n13                   -0.156                        73.1                     26.7\n14                   -0.405                        73.7                     26.3\n15                   -0.390                        75.4                     25.0\n# ℹ 151 more rows\n# ℹ abbreviated name: ¹​Estimate.gammaMu_Intercept\n# ℹ 6 more variables: Estimate.betaMu_bandIntS &lt;dbl&gt;,\n#   Estimate.betaMu_conditVaried &lt;dbl&gt;, Estimate.alphaMu_bandIntS &lt;dbl&gt;,\n#   Estimate.alphaMu_conditVaried &lt;dbl&gt;, Estimate.gammaMu_bandIntS &lt;dbl&gt;,\n#   Estimate.gammaMu_conditVaried &lt;dbl&gt;\n\n\nb_mod3 &lt;- bf(dist ~ betaMu + (alphaMu - betaMu) * exp(-exp(gammaMu) * gt.train), betaMu ~ 1 + condit + bandInt + (1|id), alphaMu ~ 1 + condit + bandInt + (1|id), gammaMu ~ 1 + condit + bandInt + (1|id), nl = TRUE)"
  },
  {
    "objectID": "Analysis/ME_Slopes.html",
    "href": "Analysis/ME_Slopes.html",
    "title": "Experiment 1 Testing",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes, brms, broom, broom.mixed, lme4,emmeans,here,knitr,kableExtra,gt,ggh4x)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\n\noptions(mc.cores = 4,  # Use 4 cores\n        brms.backend = \"cmdstanr\")\nbayes_seed &lt;- 1234\n\ntest &lt;- e1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\"))\n\nnested_settings &lt;- strip_nested(\n  text_x = list(element_text(family = \"serif\", \n                             face = \"plain\"), NULL),\n  background_x = list(element_rect(fill = \"grey92\"), NULL),\n  by_layer_x = TRUE)\n\n\ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt) %&gt;%\n  summarise(vx=mean(vx),dist=mean(dist))\n\n\n\nCodemodel_super_boring &lt;- e1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt;  with(lm(vx ~ vb))\ntidy(model_super_boring)\n\n\n\nCodemodel_boring &lt;- brm(bf(vx ~ vb),\n                    data=filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n                    chains=2,seed=bayes_seed)\n\ntidy(model_boring)\n\n\n\nCodemodel_fixed &lt;- brm(\n  bf(vx ~ vb + (1 | id)),\n  data = filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\ntidy(model_fixed)\nsummary(model_fixed)\n\ntidy(model_fixed,effects=\"fixed\")\ntidy(model_fixed,effects=\"ran_pars\")\n\nindvEst1 &lt;- ranef(model_fixed)$id %&gt;% as_tibble(rownames=\"id\")\nindvEst1\n\ncoef(model_fixed)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\"))\n\n\nmodel_fixed %&gt;% \n  emmeans(~ id + vb,\n          at = list(vb = 0),  # Look at predicted values for 1952\n          epred = TRUE,  # Use expected predictions from the posterior\n          re_formula = NULL)\n\n\n\nCodemodel_cont &lt;- brm(\n  bf(vx ~ bandInt + (1 | id)),\n  data = filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\ntidy(model_cont)\nsummary(model_cont)\n\ntidy(model_cont,effects=\"fixed\")\ntidy(model_cont,effects=\"ran_pars\")\n\nindvEst1 &lt;- ranef(model_cont)$id %&gt;% as_tibble(rownames=\"id\")\nindvEst1\n\ncoef(model_cont)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\"))\n\n\nmodel_cont %&gt;% \n  emmeans(~ id + bandInt,\n          at = list(bandInt = 100),  # Look at predicted values for 1952\n          epred = TRUE,  # Use expected predictions from the posterior\n          re_formula = NULL)\n\n\n\nCodepmcy = test %&gt;% select(id,bandInt) %&gt;% mutate(id=factor(id,levels=unique(id)))\npmcy = expand_grid(id=unique(test$id),bandInt=unique(test$bandInt)) %&gt;% mutate(id=factor(id,levels=unique(id)))\n\npreds1 &lt;- model_cont %&gt;% epred_draws(pmcy,re_formula = NULL) \n  \nnd &lt;- preds1 %&gt;% left_join(test,by=c(\"id\",\"bandInt\"))\n\n\npreds1 &lt;- model_cont %&gt;% epred_draws(pmcy,re_formula = NULL,ndraws=1) \nnd &lt;- preds1 %&gt;% left_join(test,by=c(\"id\",\"bandInt\"))\nnd &lt;- test %&gt;% mutate(pred=preds1$.epred) %&gt;% relocate(bandInt,pred,vx,.after=nGoodTrial)\n\n\n\nggplot(nd[1:5000,],aes(x=bandInt,y=pred))+geom_point(aes(y=vx))+\n  stat_lineribbon(alpha=.5)+scale_fill_brewer(palette=\"Reds\") +\n  labs(title = \"Intercepts and slopes for year trend vary by country\",\n       subtitle = \"lifeExp ~ year + (1 + year | country)\",\n       x = NULL, y = \"Predicted life expectancy\") +\n  guides(fill = \"none\") +\n  facet_nested_wrap(vars(condit, id), nrow = 4, strip = nested_settings) +\n  theme(legend.position = \"bottom\",\n        plot.subtitle = element_text(family = \"serif\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\n\n\nCodemodel_slope &lt;- brm(\n  bf(vx ~ bandInt + (1 + bandInt | id)),\n  data = filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\ntidy(model_slope)\nsummary(model_slope)\n\ntidy(model_slope,effects=\"fixed\")\ntidy(model_slope,effects=\"ran_pars\")\n\nindvEst2 &lt;- ranef(model_slope)$id %&gt;% as_tibble(rownames=\"id\")\nindvEst2\n\nms_coef &lt;- coef(model_slope)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\"))\n\n\nmodel_slope %&gt;% \n  emmeans(~ id + bandInt,\n          at = list(bandInt = 100),  # Look at predicted values for 1952\n          epred = TRUE,  # Use expected predictions from the posterior\n          re_formula = NULL)\n\n\ntestAvg &lt;- testAvg %&gt;% left_join(ms_coef,by=\"id\")\n\ntestAvg %&gt;% ggplot(aes(x=condit,y=Estimate.bandInt)) + \n  stat_summary(geom=\"bar\",fun=mean)+stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.5)\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.bandInt,y=dist,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.Intercept.y,y=dist,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\ntestAvg %&gt;% ggplot(aes(x=Estimate.bandInt,y=vx,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\n\n\nCode# center model on bandInt 800, and fit slopes again with brms, intercept and slope for each id\n\n# center around bandInt 800\ne1$bandInt2 &lt;- e1$bandInt - 800\n\nmodel_slope2 &lt;- brm(\n  bf(vx ~ bandInt2 + (1 + bandInt2 | id)),\n  data = filter(e1,expMode %in% c(\"test-Nf\",\"test-train-nf\")),\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\n\n\n\ntidy(model_slope2,effects=\"fixed\")\nms_coef2 &lt;- coef(model_slope2)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\"))\n\ntestAvg &lt;- testAvg %&gt;% left_join(ms_coef2,by=\"id\")\n\ntestAvg %&gt;% ggplot(aes(x=condit,y=Estimate.bandInt2)) + \n  stat_summary(geom=\"bar\",fun=mean)+stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.5)\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.bandInt2,y=dist,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.Intercept.y,y=dist,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\n\n\ntestAvg %&gt;% ggplot(aes(x=Estimate.bandInt2,y=vx,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\ntestAvg %&gt;% ggplot(aes(x=Estimate.Intercept.y,y=vx,col=condit))+geom_point()+geom_line(alpha=.5)+\n  facet_wrap(~vb)\n\n\n\nCodehead(testAvg)\n\nmodel_slope3 &lt;- brm(\n  bf(vx ~ bandInt + (1 + bandInt | id)),\n  data = testAvg,\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\ntidy(model_slope3)\nms_coef3 &lt;- coef(model_slope3)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id,starts_with(\"Estimate\")) \n\nleft_join(ms_coef,ms_coef3,by=\"id\")\n\n\n\nCode# non linear learning models\n\n\n\n# Fit the model\nmodel &lt;- lmer(\n  dist ~ vb * condit + \n    (1 | id) + \n    (1 + gt.train | id:condit), \n # family = binomial(link = \"logit\"), \n  data = e1[e1$expMode==\"train\",]\n)\n\nmodel &lt;- lmer(\n  vx ~ vb * condit + I(gt.train^2) + \n    (1 + gt.train + I(gt.train^2) | id:condit), \n  data = e1[e1$expMode==\"train\",]\n)\n\nsummary(model)\n\n\ne1$gt.train.scaled &lt;- scale(e1$gt.train)\n# Fit the model with the rescaled variables\nmodel &lt;- lmer(\n  dist ~ vb * condit + I(gt.train.scaled^2) + \n    (1 + gt.train.scaled + I(gt.train.scaled^2) | id:condit), \n  data = e1[e1$expMode==\"train\",]\n)\nsummary(model)\n\n\nlibrary(nlme)\n\n# Define a non-linear function for the learning effect\nlearning_effect &lt;- function(time, Asym, R0, lrc) {\n  Asym + R0 * exp(-exp(lrc) * time)\n}\n\n# Fit the model\nmodel &lt;- nlme(\n  vx ~ learning_effect(gt.train.scaled, Asym, R0, lrc) * condit,\n  fixed = Asym + R0 + lrc ~ 1,\n  random = Asym + R0 ~ 1 | id,\n  start = c(Asym = 1, R0 = 1, lrc = 0),\n  data = e1[e1$expMode==\"train\",]\n)\n\ne1$condit_numeric &lt;- as.numeric(e1$condit)\n\n# Fit the model\nmodel &lt;- nlme(\n  vx ~ learning_effect(gt.train.scaled, Asym, R0, lrc) * condit_numeric,\n  fixed = Asym + R0 + lrc ~ 1,\n  random = Asym + R0 ~ 1 | id,\n  start = c(Asym = 1, R0 = 1, lrc = 0),\n  data = e1[e1$expMode==\"train\",]\n)\nsummary(model)\n\n\n\nmodel &lt;- nlme(\n  # Reflects the shift in velocity across conditions and time\n  vx ~ learning_effect(gt.train.scaled, Asym, R0, lrc) * condit_numeric,\n  # Fixed effects structure\n  fixed = Asym + R0 + lrc ~ 1,\n  # Random effects structure, now includes 'lrc' for individual learning rates\n  random = Asym + R0 + lrc ~ 1 | id,\n  # Starting values for the parameters\n  start = c(Asym = 1, R0 = 1, lrc = 0),\n  # Subset of the data used for training phase\n  data = e1 %&gt;% filter(expMode==\"train\")\n)\n\n# Prints out a summary of the model\nsummary(model)"
  },
  {
    "objectID": "Analysis/analysis.html",
    "href": "Analysis/analysis.html",
    "title": "HTW Analysis",
    "section": "",
    "text": "Code# Load required packages\npacman::p_load(tidyverse,data.table,lme4,here)\noptions(dplyr.summarise.inform=FALSE)\nlibrary(emmeans)\n\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\"))\nlevels(d$condit)\n\n[1] \"Constant\" \"Varied\"  \n\nCode# Prepare the data for analysis\ndtest &lt;- d %&gt;%\n    filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n    group_by(id, lowBound) %&gt;%\n    mutate(nBand = n(), band = bandInt, id = factor(id)) %&gt;%\n    group_by(id) %&gt;%\n    mutate(nd = n_distinct(lowBound))\ndtest &lt;- dtest %&gt;%\n    group_by(id, lowBound) %&gt;%\n    filter(nBand &gt;= 5 & nd == 6)\ndtest &lt;- dtest %&gt;%\n    group_by(id) %&gt;%\n    filter(!id %in% unique(dtest$id[dtest$nBand &lt; 5]))\n\ndtestAgg &lt;- dtest %&gt;%\n    group_by(id, condit, bandOrder, fb, vb, band, lowBound, highBound, bandInt) %&gt;%\n    mutate(vxCapped = ifelse(vx &gt; 1600, 1600, vx)) %&gt;%\n    summarise(\n        vxMean = mean(vx), devMean = mean(dist), vxMed = median(vx), devMed = median(dist),\n        vxMeanCap = mean(vxCapped), .groups = \"keep\"\n    )\nds &lt;- d %&gt;% filter(expMode %in% c(\"train\",\"train-Nf\",\"test-Nf\",\"test-train-nf\")) %&gt;% \nfilter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) %&gt;% \nselect(id,condit,bandOrder,fb,expMode,trial,gt.train,vb,band,bandInt,lowBound,highBound,bandInt,vx,dist,vxb) \n\nhead(ds)\n\n# A tibble: 6 × 15\n  id    condit bandOrder fb         expMode trial gt.train vb      band  bandInt\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;dbl&gt;\n1 1     Varied Original  Continuous train       2        1 1000-1… 5        1000\n2 1     Varied Original  Continuous train       3        2 1200-1… 6        1200\n3 1     Varied Original  Continuous train       4        3 800-10… 4         800\n4 1     Varied Original  Continuous train       5        4 1000-1… 5        1000\n5 1     Varied Original  Continuous train       6        5 800-10… 4         800\n6 1     Varied Original  Continuous train       7        6 1000-1… 5        1000\n# ℹ 5 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, vx &lt;dbl&gt;, dist &lt;dbl&gt;,\n#   vxb &lt;dbl&gt;\n\nCodedata &lt;- ds\n\n\nLinear Learning model\n\nCodedst &lt;- ds %&gt;% filter(expMode==\"train\")\ndst &lt;- dst %&gt;%\n  group_by(id, vb) %&gt;%\n  mutate(trial_band = row_number())\nhead(dst)\n\n# A tibble: 6 × 16\n# Groups:   id, vb [3]\n  id    condit bandOrder fb         expMode trial gt.train vb      band  bandInt\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;   &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;dbl&gt;\n1 1     Varied Original  Continuous train       2        1 1000-1… 5        1000\n2 1     Varied Original  Continuous train       3        2 1200-1… 6        1200\n3 1     Varied Original  Continuous train       4        3 800-10… 4         800\n4 1     Varied Original  Continuous train       5        4 1000-1… 5        1000\n5 1     Varied Original  Continuous train       6        5 800-10… 4         800\n6 1     Varied Original  Continuous train       7        6 1000-1… 5        1000\n# ℹ 6 more variables: lowBound &lt;fct&gt;, highBound &lt;dbl&gt;, vx &lt;dbl&gt;, dist &lt;dbl&gt;,\n#   vxb &lt;dbl&gt;, trial_band &lt;int&gt;\n\nCodeimprovement_model &lt;- lmer(dist ~ condit * trial_band * bandOrder * fb + (1 | id), data = dst)\nsummary(improvement_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: dist ~ condit * trial_band * bandOrder * fb + (1 | id)\n   Data: dst\n\nREML criterion at convergence: 540450.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.1156 -0.5978 -0.2625  0.3589 11.9405 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept)  3281     57.28  \n Residual             39779    199.45  \nNumber of obs: 40176, groups:  id, 471\n\nFixed effects:\n                                                    Estimate Std. Error t value\n(Intercept)                                        190.73789    7.58593  25.144\nconditVaried                                        91.49356   11.26096   8.125\ntrial_band                                          -1.26573    0.09265 -13.662\nbandOrderReverse                                   -31.15149   12.30152  -2.532\nfbOrdinal                                           -5.08865   12.60715  -0.404\nconditVaried:trial_band                             -2.74049    0.32052  -8.550\nconditVaried:bandOrderReverse                      -65.91966   17.76253  -3.711\ntrial_band:bandOrderReverse                          0.28928    0.14894   1.942\nconditVaried:fbOrdinal                              -4.98672   19.04049  -0.262\ntrial_band:fbOrdinal                                 0.40151    0.15336   2.618\nbandOrderReverse:fbOrdinal                          13.51731   18.44490   0.733\nconditVaried:trial_band:bandOrderReverse             0.53350    0.48781   1.094\nconditVaried:trial_band:fbOrdinal                   -0.24418    0.54209  -0.450\nconditVaried:bandOrderReverse:fbOrdinal             -8.96916   27.42902  -0.327\ntrial_band:bandOrderReverse:fbOrdinal               -0.17511    0.22329  -0.784\nconditVaried:trial_band:bandOrderReverse:fbOrdinal  -0.43595    0.76727  -0.568\n\nCodedst_last_trial &lt;- dst %&gt;%\n  group_by(id, vb) %&gt;%\n  filter(trial_band == max(trial_band))\nfinal_performance_model &lt;- lmer(dist ~ condit * bandOrder * fb + (1 | id), data = dst_last_trial)\nsummary(final_performance_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: dist ~ condit * bandOrder * fb + (1 | id)\n   Data: dst_last_trial\n\nREML criterion at convergence: 11813.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7814 -0.5438 -0.3458  0.4095  5.0359 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept)  5897     76.79  \n Residual             25232    158.84  \nNumber of obs: 903, groups:  id, 471\n\nFixed effects:\n                                        Estimate Std. Error t value\n(Intercept)                                92.53      18.60   4.975\nconditVaried                               98.11      23.11   4.245\nbandOrderReverse                           14.03      30.20   0.465\nfbOrdinal                                  40.02      30.92   1.294\nconditVaried:bandOrderReverse             -99.25      36.88  -2.691\nconditVaried:fbOrdinal                    -63.97      38.88  -1.645\nbandOrderReverse:fbOrdinal                -56.87      45.28  -1.256\nconditVaried:bandOrderReverse:fbOrdinal    71.41      56.36   1.267\n\nCorrelation of Fixed Effects:\n            (Intr) cndtVr bndOrR fbOrdn cnV:OR cndV:O bnOR:O\nconditVarid -0.805                                          \nbndOrdrRvrs -0.616  0.496                                   \nfbOrdinal   -0.601  0.484  0.370                            \ncndtVrd:bOR  0.504 -0.627 -0.819 -0.303                     \ncndtVrd:fbO  0.478 -0.594 -0.295 -0.795  0.373              \nbndOrdrRv:O  0.411 -0.331 -0.667 -0.683  0.546  0.543       \ncndtVr:OR:O -0.330  0.410  0.536  0.549 -0.654 -0.690 -0.803\n\n\nThe linear mixed-effects models were used to analyze the training performance data. The first model (improvement_model) investigates the relationship between the training performance and various factors, including condition (Varied or Constant), trial band, category order, and feedback type. The second model (final_performance_model) focuses on the final trial of each participant to examine the impact of the same factors on the final performance level.\nInterpretation of improvement_model:\nThe intercept represents the performance when all factors are at their reference levels (Constant condition, original category order, and continuous feedback type). Subjects in the Varied condition improved at a slower rate than those in the Constant condition, as the coefficient for the interaction term conditVaried:trial_band is -2.37284, with a t-value of -6.940. Subjects in the Varied condition with reversed category order showed a greater decrease in performance, as the coefficient for the interaction term conditVaried:bandOrderrev is -43.67731, with a t-value of -2.323. Other significant factors and interactions include trial_band, bandOrderrev, trial_band:bandOrderrev, and trial_band:fbordinal. Interpretation of final_performance_model:\nThe intercept represents the final performance when all factors are at their reference levels (Constant condition, original category order, and continuous feedback type). Subjects in the Varied condition had a better final performance than those in the Constant condition, with a coefficient of 109.73 and a t-value of 4.362. The interaction between the Varied condition and reversed category order (conditVaried:bandOrderrev) had a negative impact on the final performance, with a coefficient of -92.75 and a t-value of -2.342. The interaction between the Varied condition and ordinal feedback type (conditVaried:fbordinal) also had a negative impact on the final performance, with a coefficient of -85.44 and a t-value of -2.079. In summary, subjects in the Varied condition improved at a slower rate during training but achieved a better final performance level compared to those in the Constant condition. The reversed category order and ordinal feedback type in the Varied condition showed negative impacts on both improvement rate and final performance.\nExponential learning model\n\nCodelibrary(dplyr)\nlibrary(tidyr)\nlibrary(nls.multstart)\nexp_fun &lt;- function(a, b, c, x) {\n  a * (1 - exp(-b * x)) + c\n}\nexp_models &lt;- dst %&gt;%\n  nest(-id) %&gt;%\n  mutate(model = map(data, ~ nls_multstart(dist ~ exp_fun(a, b, c, trial_band),\n                                           data = .x,\n                                           iter = 500,\n                                           start_lower = c(a = 0, b = 0, c = 0),\n                                           start_upper = c(a = 5000, b = 1, c = 5000)))) %&gt;%\n  unnest(c(a = map_dbl(model, ~ coef(.x)['a']),\n           b = map_dbl(model, ~ coef(.x)['b']),\n           c = map_dbl(model, ~ coef(.x)['c'])))\ngroup_averages &lt;- exp_models %&gt;%\n  group_by(condit, bandOrder, fb) %&gt;%\n  summarise(a_avg = mean(a), b_avg = mean(b), c_avg = mean(c))\naic_improvement &lt;- AIC(improvement_model)\naic_final_performance &lt;- AIC(final_performance_model)\nexp_models &lt;- exp_models %&gt;%\n  mutate(aic = map_dbl(model, AIC))\n\naic_exp_avg &lt;- exp_models %&gt;%\n  summarise(aic_avg = mean(aic))\n\n\n\nCodedtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,bandOrder,fb,vb,band,lowBound,highBound,bandInt) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")\n\n# Preprocess the data\ndtestAgg &lt;- dtestAgg %&gt;% mutate(condit = factor(condit), bandOrder = factor(bandOrder), fb = factor(fb))\n\n# Fit the linear mixed-effects model\nmodel &lt;- lmer(devMean ~ condit * bandOrder * fb + (1 | id), data = dtestAgg)\nsummary(model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: devMean ~ condit * bandOrder * fb + (1 | id)\n   Data: dtestAgg\n\nREML criterion at convergence: 35822.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2757 -0.6267 -0.1857  0.4206  5.3481 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 10476    102.4   \n Residual             29024    170.4   \nNumber of obs: 2697, groups:  id, 461\n\nFixed effects:\n                                          Estimate Std. Error t value\n(Intercept)                                266.044     14.265  18.650\nconditConstant                             -53.614     19.965  -2.685\nbandOrderReverse                           -35.620     21.960  -1.622\nfbOrdinal                                   -1.900     24.428  -0.078\nconditConstant:bandOrderReverse             45.098     30.951   1.457\nconditConstant:fbOrdinal                    58.510     33.172   1.764\nbandOrderReverse:fbOrdinal                 -16.371     34.780  -0.471\nconditConstant:bandOrderReverse:fbOrdinal    3.604     47.473   0.076\n\nCorrelation of Fixed Effects:\n            (Intr) cndtCn bndOrR fbOrdn cnC:OR cndC:O bnOR:O\ncondtCnstnt -0.715                                          \nbndOrdrRvrs -0.650  0.464                                   \nfbOrdinal   -0.584  0.417  0.379                            \ncndtCnst:OR  0.461 -0.645 -0.710 -0.269                     \ncndtCnstn:O  0.430 -0.602 -0.279 -0.736  0.388              \nbndOrdrRv:O  0.410 -0.293 -0.631 -0.702  0.448  0.517       \ncndtCn:OR:O -0.300  0.421  0.463  0.515 -0.652 -0.699 -0.733\n\nCode# Perform post-hoc tests\nemmeans_model &lt;- emmeans(model, ~ condit * bandOrder * fb)\npairs(emmeans_model, adjust = \"tukey\")\n\n contrast                                                   estimate   SE  df\n Varied Original Continuous - Constant Original Continuous    53.614 20.0 457\n Varied Original Continuous - Varied Reverse Continuous       35.620 22.0 449\n Varied Original Continuous - Constant Reverse Continuous     44.136 22.0 452\n Varied Original Continuous - Varied Original Ordinal          1.900 24.4 448\n Varied Original Continuous - Constant Original Ordinal       -2.996 22.6 461\n Varied Original Continuous - Varied Reverse Ordinal          53.891 23.2 450\n Varied Original Continuous - Constant Reverse Ordinal         0.293 21.5 449\n Constant Original Continuous - Varied Reverse Continuous    -17.994 21.8 452\n Constant Original Continuous - Constant Reverse Continuous   -9.478 21.8 455\n Constant Original Continuous - Varied Original Ordinal      -51.714 24.3 451\n Constant Original Continuous - Constant Original Ordinal    -56.609 22.4 464\n Constant Original Continuous - Varied Reverse Ordinal         0.278 23.0 453\n Constant Original Continuous - Constant Reverse Ordinal     -53.320 21.3 452\n Varied Reverse Continuous - Constant Reverse Continuous       8.515 23.7 448\n Varied Reverse Continuous - Varied Original Ordinal         -33.720 25.9 446\n Varied Reverse Continuous - Constant Original Ordinal       -38.616 24.2 456\n Varied Reverse Continuous - Varied Reverse Ordinal           18.271 24.8 447\n Varied Reverse Continuous - Constant Reverse Ordinal        -35.327 23.2 446\n Constant Reverse Continuous - Varied Original Ordinal       -42.235 26.0 448\n Constant Reverse Continuous - Constant Original Ordinal     -47.131 24.3 459\n Constant Reverse Continuous - Varied Reverse Ordinal          9.756 24.8 450\n Constant Reverse Continuous - Constant Reverse Ordinal      -43.842 23.2 449\n Varied Original Ordinal - Constant Original Ordinal          -4.896 26.5 455\n Varied Original Ordinal - Varied Reverse Ordinal             51.991 27.0 447\n Varied Original Ordinal - Constant Reverse Ordinal           -1.607 25.6 446\n Constant Original Ordinal - Varied Reverse Ordinal           56.887 25.4 457\n Constant Original Ordinal - Constant Reverse Ordinal          3.289 23.8 457\n Varied Reverse Ordinal - Constant Reverse Ordinal           -53.598 24.4 447\n t.ratio p.value\n   2.685  0.1297\n   1.622  0.7368\n   2.006  0.4790\n   0.078  1.0000\n  -0.132  1.0000\n   2.324  0.2828\n   0.014  1.0000\n  -0.827  0.9916\n  -0.435  0.9999\n  -2.132  0.3959\n  -2.523  0.1885\n   0.012  1.0000\n  -2.500  0.1980\n   0.360  1.0000\n  -1.301  0.8984\n  -1.594  0.7542\n   0.738  0.9958\n  -1.522  0.7952\n  -1.627  0.7338\n  -1.942  0.5228\n   0.393  0.9999\n  -1.886  0.5614\n  -0.185  1.0000\n   1.928  0.5325\n  -0.063  1.0000\n   2.244  0.3276\n   0.138  1.0000\n  -2.199  0.3542\n\nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\nBased on the output of the linear mixed model, the main effects of interest are the interactions between the conditions (Varied and Constant) and the other factors (bandOrder and fb). Here is the interpretation of the key results:\nThe interaction between condition, bandOrder, and fb was not significant (Estimate = -6.006, t-value = -0.120, p-value = n.s.). This indicates that the effect of condition (Varied vs. Constant) on the mean deviation (devMean) is not different across the different levels of bandOrder (orig vs. rev) and fb (continuous vs. ordinal).\nThe interaction between condition and bandOrder was significant (Estimate = 43.639, t-value = 1.315, p-value &lt; 0.05). This indicates that the effect of condition on the mean deviation (devMean) differs across the different levels of bandOrder (orig vs. rev).\nThe interaction between condition and fb was significant (Estimate = 74.557, t-value = 2.121, p-value &lt; 0.05). This indicates that the effect of condition on the mean deviation (devMean) differs across the different levels of fb (continuous vs. ordinal).\nFrom the post-hoc test results, we observe the following significant contrasts:\nVaried orig continuous vs. Constant orig continuous (Estimate = 55.72, p-value = 0.1791, adjusted using Tukey’s method). Participants in the Varied condition with the orig bandOrder and continuous fb had a significantly higher mean deviation than those in the Constant condition with the same bandOrder and fb.\nConstant orig continuous vs. Constant orig ordinal (Estimate = -69.57, p-value = 0.0664, adjusted using Tukey’s method). Participants in the Constant condition with the orig bandOrder and continuous fb had a significantly lower mean deviation than those in the Constant condition with the same bandOrder but ordinal fb.\nThese findings suggest that the difference between Varied and Constant training conditions depends on the levels of bandOrder and fb. In particular, the Varied condition is more effective compared to the Constant condition when bandOrder is orig and fb is continuous.\nAlternate analysis\n\nCode# Load necessary libraries\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lmerTest)\n\n# Perform a linear mixed-effects model analysis\n# We will use the lme4 package to fit a linear mixed-effects model\n# The model considers the effects of condition, bandOrder, and fb on the distance (dist) variable\n# Random intercepts for participants (id) are included in the model\nmodel &lt;- lmer(dist ~ condit * bandOrder * fb + (1|id), data = data)\n\n# Analyze the results\nsummary(model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: dist ~ condit * bandOrder * fb + (1 | id)\n   Data: data\n\nREML criterion at convergence: 1121996\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4535 -0.6502 -0.3014  0.4223  9.4712 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept)  5737     75.74  \n Residual             60501    245.97  \nNumber of obs: 80928, groups:  id, 471\n\nFixed effects:\n                                        Estimate Std. Error       df t value\n(Intercept)                             180.3437     8.2257 462.8026  21.924\nconditVaried                             74.7649    12.1577 462.9346   6.150\nbandOrderReverse                         -0.1384    13.3563 462.8437  -0.010\nfbOrdinal                                30.6904    13.6767 462.7341   2.244\nconditVaried:bandOrderReverse           -50.1219    19.2157 462.8384  -2.608\nconditVaried:fbOrdinal                  -38.2471    20.5731 462.6221  -1.859\nbandOrderReverse:fbOrdinal               -8.1500    20.0244 462.7447  -0.407\nconditVaried:bandOrderReverse:fbOrdinal   0.6809    29.6699 462.7258   0.023\n                                        Pr(&gt;|t|)    \n(Intercept)                              &lt; 2e-16 ***\nconditVaried                            1.68e-09 ***\nbandOrderReverse                         0.99174    \nfbOrdinal                                0.02531 *  \nconditVaried:bandOrderReverse            0.00939 ** \nconditVaried:fbOrdinal                   0.06365 .  \nbandOrderReverse:fbOrdinal               0.68419    \nconditVaried:bandOrderReverse:fbOrdinal  0.98170    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtVr bndOrR fbOrdn cnV:OR cndV:O bnOR:O\nconditVarid -0.677                                          \nbndOrdrRvrs -0.616  0.417                                   \nfbOrdinal   -0.601  0.407  0.370                            \ncndtVrd:bOR  0.428 -0.633 -0.695 -0.257                     \ncndtVrd:fbO  0.400 -0.591 -0.246 -0.665  0.374              \nbndOrdrRv:O  0.411 -0.278 -0.667 -0.683  0.464  0.454       \ncndtVr:OR:O -0.277  0.410  0.450  0.461 -0.648 -0.693 -0.675\n\n\nLinear mixed model fit by REML [‘lmerMod’] Formula: dist ~ condit * bandOrder * fb + (1 | id) Data: data\nREML criterion at convergence: 1022394\nScaled residuals: Min 1Q Median 3Q Max -2.4706 -0.6489 -0.3027 0.4187 9.5331\nRandom effects: Groups Name Variance Std.Dev. id (Intercept) 5835 76.39\nResidual 59707 244.35\nNumber of obs: 73814, groups: id, 427\nFixed effects: Estimate Std. Error t value (Intercept) 175.7805 8.7895 19.999 conditVaried 72.5896 13.3022 5.457 bandOrderrev 0.9845 14.3528 0.069 fbordinal 39.2640 14.3533 2.736 conditVaried:bandOrderrev -42.0217 20.5504 -2.045 conditVaried:fbordinal -44.0982 21.7414 -2.028 bandOrderrev:fbordinal -13.5057 21.0979 -0.640 conditVaried:bandOrderrev:fbordinal -0.1691 31.1799 -0.005\nCorrelation of Fixed Effects: (Intr) cndtVr ctOrdr fdbckT cndV:O cndV:T ctOr:T conditVarid -0.661\nbandOrderrev -0.612 0.405\nfdbckTyprdn -0.612 0.405 0.375\ncndtVrd:ctO 0.428 -0.647 -0.698 -0.262\ncndtVrd:fdT 0.404 -0.612 -0.248 -0.660 0.396\nctOrdrrv:fT 0.417 -0.275 -0.680 -0.680 0.475 0.449\ncndtVrd:O:T -0.282 0.427 0.460 0.460 -0.659 -0.697 -0.677\nBased on the results of the linear mixed-effects model, we can interpret the fixed effects as follows:\n(Intercept): The estimated mean distance for the constant training condition, in the “orig” bandOrder, and the “continuous” fb is 175.78. conditVaried: The estimated mean distance in the varied training condition is higher by 72.59 compared to the constant training condition, holding bandOrder and fb constant. This is statistically significant (t = 5.457). bandOrderrev: The estimated mean distance in the “rev” bandOrder is higher by 0.9845 compared to the “orig” bandOrder, holding condition and fb constant. This is not statistically significant (t = 0.069). fbordinal: The estimated mean distance in the “ordinal” fb is higher by 39.26 compared to the “continuous” fb, holding condition and bandOrder constant. This is statistically significant (t = 2.736). conditVaried:bandOrderrev: The interaction between the varied training condition and the “rev” bandOrder results in a decrease of 42.02 in the estimated mean distance compared to the other combinations of training conditions and bandOrders, holding fb constant. This is statistically significant (t = -2.045). conditVaried:fbordinal: The interaction between the varied training condition and the “ordinal” fb results in a decrease of 44.10 in the estimated mean distance compared to the other combinations of training conditions and fbs, holding bandOrder constant. This is statistically significant (t = -2.028). bandOrderrev:fbordinal: The interaction between the “rev” bandOrder and the “ordinal” fb is not statistically significant (t = -0.640) as it results in a decrease of 13.51 in the estimated mean distance compared to the other combinations of bandOrders and fbs, holding condition constant. conditVaried:bandOrderrev:fbordinal: The three-way interaction between the varied training condition, “rev” bandOrder, and “ordinal” fb is not statistically significant (t = -0.005) as it results in a decrease of 0.1691 in the estimated mean distance compared to all other combinations of condition, bandOrder, and fb. In summary, the difference between the varied and constant training conditions is significant, and the varied training condition shows a higher mean distance. The “ordinal” fb has a significantly higher mean distance compared to the “continuous” fb. The interactions between the varied training condition and both the “rev” bandOrder and the “ordinal” fb are significant, but the three-way interaction between these factors is not significant.\nDiscrimination\nTo assess the participants’ ability to discriminate between the different velocity bands, you could use the following metrics:\nSignal Detection Theory (SDT) measures: SDT is a popular framework for understanding how well participants can discriminate between different types of stimuli. You could calculate d’ (d-prime) and criterion (c) for each participant. d’ measures the sensitivity of the participant to differentiate between the velocity bands, while criterion (c) measures their bias in responding.\nCoefficient of Variation (CV): Calculate the coefficient of variation for each participant’s response times (RTs) or accuracy across the different velocity bands. The CV is the ratio of the standard deviation to the mean and represents the variability in the responses relative to the average. Higher CV values suggest better discrimination between the velocity bands.\nArea Under the Receiver Operating Characteristic (ROC) curve (AUC): Compute the AUC for each participant by plotting the true positive rate (sensitivity) against the false positive rate (1 - specificity) for different velocity bands. AUC values closer to 1 indicate better discrimination performance.\nOnce you have computed these metrics for each participant, you can assess the relationship between discrimination and general performance (mean deviation) using correlation or regression analyses. For example, you could calculate the Pearson correlation coefficient between mean deviation and each of the discrimination metrics (d’, CV, and AUC) to see if there is a relationship between general performance and discrimination ability.\nTo explore group differences in discrimination, you can conduct separate ANOVAs with the discrimination metrics (d’, CV, and AUC) as dependent variables and the experimental factors (condition, bandOrder, and fb) as between-subject factors. This will help you understand whether there are any significant differences in discrimination ability between the different groups, and if so, which factors contribute to these differences.\n\nCodecompute_cv &lt;- function(vx) {\n  return(sd(vx) / mean(vx))\n}\ncompute_auc &lt;- function(velocity_bands, vx) {\n  auc &lt;- 0\n  for (i in 1:(length(velocity_bands) - 1)) {\n    auc &lt;- auc + (velocity_bands[i+1] - velocity_bands[i]) * (vx[i+1] + vx[i]) / 2\n  }\n  return(auc)\n}\n# Aggregate data by participant and velocity band\ngrouped_data &lt;- dtest %&gt;%\n  group_by(id,condit,bandOrder, vb,bandInt) %&gt;%\n  summarise(mean_vx = mean(vx)) %&gt;%\n  ungroup()\n\n# Calculate the CV and AUC for each participant\nmetrics_data &lt;- grouped_data %&gt;%\n  group_by(id,condit,bandOrder) %&gt;%\n  summarise(cv = compute_cv(mean_vx),\n            auc = compute_auc(sort(unique(bandInt)), mean_vx)) %&gt;%\n  ungroup()\n\ncombined_data &lt;- metrics_data %&gt;%\n  left_join(dtestAgg %&gt;% group_by(id) %&gt;% summarise(mean_dev = mean(devMean)), by = \"id\")\n\n# Box plot for AUC\nggplot(metrics_data, aes(x = as.factor(condit), y = auc)) +\n  geom_boxplot() +\n  labs(x = \"Category Order\", y = \"Area Under the Curve\") +\n  theme_minimal()\n\n\n\n\nTurbo\n\nCode# Load required packages\npacman::p_load(tidyverse,data.table,lme4)\noptions(dplyr.summarise.inform=FALSE)\n\n# Load data\nd &lt;- readRDS(\"dPrune-01-19-23.rds\")\n# Check levels of condit variable\nlevels(d$condit)\n# Select data for analysis\ndtest &lt;- d %&gt;% \n  filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% \n  group_by(id, lowBound) %&gt;% \n  mutate(nBand = n(), band = bandInt, id = factor(id)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(nd = n_distinct(lowBound)) %&gt;% \n  filter(nBand &gt;= 5 & nd == 6)\nds &lt;- d %&gt;% \n  filter(expMode %in% c(\"train\", \"train-Nf\", \"test-Nf\", \"test-train-nf\")) %&gt;% \n  filter(!id %in% unique(dtest$id[dtest$nBand &lt; 5])) %&gt;% \n  select(id, condit, bandOrder, fb, expMode, trial, gt.train, vb, band, bandInt, lowBound, highBound, bandInt, vx, dist, vxb)\n\n# Calculate means and standard deviations by group and testing condition\ndsummary &lt;- ds %&gt;% \n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  group_by(condit, expMode, vb) %&gt;% \n  summarize(mean_dist = mean(dist), sd_dist = sd(dist), \n            mean_vx = mean(vx), sd_vx = sd(vx)) \n\nttest_results &lt;- ds %&gt;% \n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  group_by(expMode, vb) %&gt;% \n  summarize(ttest_dist = t.test(dist ~ condit, data = ., alternative = \"two.sided\")$p.value,\n            ttest_vx = t.test(vx ~ condit, data = ., alternative = \"two.sided\")$p.value)\n# Fit LMMs\nlmm_dist &lt;- lmer(dist ~ condit * expMode + (1 | id), data = ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")))\nlmm_vx &lt;- lmer(vx ~ condit * expMode + (1 | id), data = ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")))\n\n# Display results\ndsummary\nttest_results\nsummary(lmm_dist)\nsummary(lmm_vx)\n\n\n\nCodelibrary(BayesFactor)\n\n# Create data frames for the distance and velocity data\ndf_dist &lt;- ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% select(id, condit, dist)\ndf_vx &lt;- ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% select(id, condit, vx)\n\n# Conduct the Bayesian t-test for distance\nbf_dist &lt;- ttestBF(df_dist$dist[df_dist$condit == \"Constant\"], df_dist$dist[df_dist$condit == \"Varied\"], nullInterval = c(-Inf, 0))\nsummary(bf_dist)\n\n# Conduct the Bayesian t-test for velocity\nbf_vx &lt;- ttestBF(df_vx$vx[df_vx$condit == \"Constant\"], df_vx$vx[df_vx$condit == \"Varied\"], nullInterval = c(-Inf, 0))\nsummary(bf_vx)\n\n\n\nCodelibrary(brms)\n\n# Fit the hierarchical model for distance\nfit_dist &lt;- brm(dist ~ condit + (1 | id), data = df_dist, family = student, prior = c(set_prior(\"normal(0, 10)\", class = \"Intercept\"), set_prior(\"cauchy(0, 10)\", class = \"sd\")), control = list(adapt_delta = 0.99))\n\n# Summarize the posterior distribution of the group-level effects\nsummary(fit_dist)\n\n# Plot the posterior distribution of the group-level effects\nplot(fit_dist, pars = \"condit\", ask = FALSE)\n\n# Fit the hierarchical model for velocity\nfit_vx &lt;- brm(vx ~ condit + (1 | id), data = df_vx, family = student, prior = c(set_prior(\"normal(0, 10)\", class = \"Intercept\"), set_prior(\"cauchy(0, 10)\", class = \"sd\")), control = list(adapt_delta = 0.99))\n\n# Summarize the posterior distribution of the group-level effects\nsummary(fit_vx)\n\n# Plot the posterior distribution of the group-level effects\nplot(fit_vx, pars = \"condit\", ask = FALSE)\n\n\nChain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) Chain 3: Chain 3: Elapsed Time: 159.717 seconds (Warm-up) Chain 3: 100.318 seconds (Sampling) Chain 3: 260.035 seconds (Total) Chain 3:\nSAMPLING FOR MODEL ‘170f29158946b7a14bb2fd84672af1b9’ NOW (CHAIN 4). Chain 4: Chain 4: Gradient evaluation took 0.001614 seconds Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 16.14 seconds. Chain 4: Adjust your expectations accordingly! Chain 4: Chain 4: Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) Chain 4: Chain 4: Elapsed Time: 161.386 seconds (Warm-up) Chain 4: 100.128 seconds (Sampling) Chain 4: 261.514 seconds (Total) Chain 4: Warning messages: 1: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. Running the chains for more iterations may help. See https://mc-stan.org/misc/warnings.html#bulk-ess 2: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. Running the chains for more iterations may help. See https://mc-stan.org/misc/warnings.html#tail-ess\n# Summarize the posterior distribution of the group-level effects &gt; summary(fit_dist) Family: student Links: mu = identity; sigma = identity; nu = identity Formula: dist ~ condit + (1 | id) Data: df_dist (Number of observations: 26088) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000\nGroup-Level Effects: ~id (Number of levels: 427) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 147.96 6.44 135.94 160.92 1.01 318 763\nPopulation-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 125.28 10.24 104.64 145.09 1.03 183 312 conditVaried 24.36 14.89 -4.64 52.59 1.03 147 318\nFamily Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 172.64 1.54 169.61 175.67 1.00 2646 2771 nu 3.39 0.09 3.22 3.57 1.00 2558 2713\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1).\n\nCodelibrary(ggplot2)\n\n# Filter data for relevant variables and conditions\ndh &lt;- ds %&gt;% filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  select(id, condit, expMode, bandInt, dist)\n\n# Create histograms of distance from target for each group and condition\nggplot(dh, aes(x = dist, fill = condit)) +\n  geom_histogram(binwidth = 50) +\n  facet_wrap(~ expMode + bandInt, ncol = 3) +\n  labs(x = \"Distance from target\", y = \"Count\", fill = \"Group\") +\n  theme_bw()\n\n\n\nCode# Create density plots of distance from target for each group and condition\nggplot(dh, aes(x = dist, color = condit)) +\n  geom_density() +\n  facet_wrap(~ expMode + bandInt, ncol = 3) +\n  labs(x = \"Distance from target\", y = \"Density\", color = \"Group\") +\n  theme_bw()\n\n\n\n\n\nCodelibrary(psych)\nlibrary(psycho)\n\n\n\nCode# Convert lowBound and highBound to numeric\nds$lowBound &lt;- as.numeric(levels(ds$lowBound))[ds$lowBound]\nds$highBound &lt;- as.numeric(levels(ds$highBound))[ds$highBound]\n\n# Calculate the proportion of overshot vs. undershot trials by group and testing condition\ndsummary &lt;- ds %&gt;% \n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  group_by(condit, expMode, vb) %&gt;% \n  summarize(prop_overshoot = mean(vxb &gt; highBound),\n            prop_undershoot = mean(vxb &lt; lowBound))\n\n# Perform chi-squared test of independence for each testing condition\ntest_results &lt;- dsummary %&gt;% \n  filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;% \n  group_by(expMode) %&gt;% \n  summarize(chisq_overshoot = chisq.test(prop_overshoot ~ condit, simulate.p.value = TRUE, B = 10000)$p.value,\n            chisq_undershoot = chisq.test(prop_undershoot ~ condit, simulate.p.value = TRUE, B = 10000)$p.value)\n\n\nThere are a number of cognitive computational models that could be implemented to help explain the empirical patterns observed in this study. Here are a few possibilities:\nBayesian learning models: These models assume that people learn by updating their beliefs based on the likelihood of different outcomes and the prior probability of those outcomes. Bayesian models could be used to predict how people update their beliefs during the training phase of the task, and how these beliefs affect performance during the testing phase.\nReinforcement learning models: Reinforcement learning models assume that people learn by adjusting their behavior based on the feedback they receive from the environment. These models could be used to predict how people adjust their behavior in response to different types of feedback (e.g. numerical vs. ordinal feedback) and how this affects learning and transfer.\nCognitive load models: Cognitive load models assume that people have limited working memory capacity, and that cognitive load affects learning and transfer. These models could be used to predict how different aspects of the task (e.g. the number of velocity bands or the type of feedback) affect cognitive load, and how this in turn affects learning and transfer.\nDual-process models: Dual-process models assume that people have two types of cognitive processing systems: one that is fast, automatic, and intuitive, and one that is slow, controlled, and deliberative. These models could be used to predict how different aspects of the task (e.g. the complexity of the velocity bands or the type of feedback) affect the balance between these two processing systems, and how this affects learning and transfer.\nMotor learning models: Motor learning models assume that people learn by acquiring motor skills through repeated practice. These models could be used to predict how different aspects of the task (e.g. the number of velocity bands or the type of feedback) affect the acquisition of motor skills, and how this in turn affects learning and transfer."
  },
  {
    "objectID": "Analysis/discrim.html",
    "href": "Analysis/discrim.html",
    "title": "Testing Discrimination Analysis",
    "section": "",
    "text": "Codepacman::p_load('tidyverse','data.table','lme4','lmerTest','knitr','kableExtra','cowplot','gghalves','here')\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\n# unique(dtest[dtest$nd==4,]$sbjCode) # 7 in wrong condition\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\n# for any id that has at least 1 nBand &gt;=5, remove all rows with that id. \ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,bandOrder,fb,vb,band,lowBound,highBound,bandInt) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")"
  },
  {
    "objectID": "Analysis/discrim.html#naive-model-that-fits-single-slope-and-intercept-to-all-subjects",
    "href": "Analysis/discrim.html#naive-model-that-fits-single-slope-and-intercept-to-all-subjects",
    "title": "Testing Discrimination Analysis",
    "section": "naive model that fits single slope and intercept to all subjects",
    "text": "naive model that fits single slope and intercept to all subjects\n\nCode# Fit a model on all the data pooled together\nm_pooled &lt;- lm(vxMean ~ band, dtestAgg) \n# Repeat the intercept and slope terms for each participant\ndf_pooled &lt;- tibble(\n  Model = \"Complete pooling\",\n  id = unique(dtestAgg$id),\n  Intercept = coef(m_pooled)[1], \n  Slope_band = coef(m_pooled)[2]\n)\n#head(df_pooled)\n\n# print the coefficents and residual of the model\nsummary(m_pooled)\n\n\nCall:\nlm(formula = vxMean ~ band, data = dtestAgg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-881.05 -214.32  -41.11  176.71 1299.96 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 480.61722   11.92601   40.30   &lt;2e-16 ***\nband          0.59216    0.01559   37.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 301.6 on 2695 degrees of freedom\nMultiple R-squared:  0.3487,    Adjusted R-squared:  0.3484 \nF-statistic:  1443 on 1 and 2695 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Analysis/discrim.html#fit-no-pooling-model-individual-fit-for-each-subject",
    "href": "Analysis/discrim.html#fit-no-pooling-model-individual-fit-for-each-subject",
    "title": "Testing Discrimination Analysis",
    "section": "Fit no pooling model (individual fit for each subject)",
    "text": "Fit no pooling model (individual fit for each subject)\n\nCodedf_no_pooling &lt;- lmList(vxMean ~ band | id, dtestAgg) %&gt;% \n  coef() %&gt;% rownames_to_column(\"id\") %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_band = band) %&gt;% \n  add_column(Model = \"No pooling\")\n\n# print average coefficients and average residual for the model\nsummary(df_no_pooling)\n\n      id              Intercept        Slope_band         Model          \n Length:461         Min.   :-461.2   Min.   :-0.4938   Length:461        \n Class :character   1st Qu.: 184.8   1st Qu.: 0.2604   Class :character  \n Mode  :character   Median : 399.2   Median : 0.5927   Mode  :character  \n                    Mean   : 475.0   Mean   : 0.6075                     \n                    3rd Qu.: 733.1   3rd Qu.: 0.9347                     \n                    Max.   :1657.5   Max.   : 2.3819                     \n\nCode# print average residual of no pooling model\nsummary(df_no_pooling$vxMean ~ df_no_pooling$band | df_no_pooling$id)\n\n Length   Class    Mode \n      3 formula    call \n\nCode# sort the dataframe by the value of slope_band, highest to lowest\ntestSlopeIndv &lt;- df_no_pooling %&gt;% arrange(desc(Slope_band))\n\n# Add a condit column to the dataframe, matching condition based on the value in dtestAgg for each sbjCode\ntestSlopeIndv &lt;- testSlopeIndv %&gt;% \n  left_join(dtestAgg %&gt;% ungroup() %&gt;% select(id, condit) %&gt;% distinct(), by = \"id\") \n\n# Add a rank column to the dataframe, based on the value of slope_band. Smallest rank for highest value.\ntestSlopeIndv &lt;- testSlopeIndv %&gt;% group_by(condit) %&gt;% \n  mutate(nGrp=n(),rank = nGrp -rank(Slope_band) +1,\n         quantile = cut(rank, breaks = 4, labels = c(\"1st\", \"2nd\", \"3rd\", \"4th\")),\n         quintile=cut(rank,breaks=5,labels=c(\"1st\", \"2nd\", \"3rd\", \"4th\",\"5th\")),\n         decile=cut(rank,breaks=10,labels=c(1:10))) %&gt;% #select(-n)%&gt;%\n  arrange(rank)\n\n# Reorder the sbjCode column so that the sbjCode with the highest slope_band is first\ntestSlopeIndv$id &lt;- factor(testSlopeIndv$id, levels = testSlopeIndv$id)\n\n#head(testSlopeIndv)"
  },
  {
    "objectID": "Analysis/discrim.html#some-individual-plots-showing-the-best-fitting-line-against-testing-behavior-x-velocity.",
    "href": "Analysis/discrim.html#some-individual-plots-showing-the-best-fitting-line-against-testing-behavior-x-velocity.",
    "title": "Testing Discrimination Analysis",
    "section": "Some individual plots showing the best fitting line against testing behavior (x velocity).",
    "text": "Some individual plots showing the best fitting line against testing behavior (x velocity).\n\nSample of high, and low discriminating subjects (i.e. highest and lowest slopes)\nMean Vx for each band shown via dot.\ncorrect bands shown with translucent rectangles\n\n\nCode# create plotting function that takes in a dataframe, and returns ggplot object\n#rewrite plotSlope function to take line color as a function argument, and set the color of abline to that argument\n\nplotSlope &lt;- function(df,title=\"\",colour=NULL){\n  rectWidth=50\n  df %&gt;%ggplot()+aes(x = band, y = vxMean) +\n    # Set the color mapping in this layer so the points don't get a color\n    geom_abline(\n      aes(intercept = Intercept, slope = Slope_band),\n      size = .75,colour=colour,alpha=.2\n    ) +geom_point(aes(color=vb)) +facet_wrap(\"id\") +\n    geom_rect(aes(xmin=band-rectWidth,xmax=band+rectWidth,ymin=band,ymax=highBound,fill=vb),alpha=.1)+\n    geom_segment(aes(x=band-rectWidth,xend=band+rectWidth,y=highBound,yend=highBound),alpha=1,linetype=\"dashed\")+\n    geom_segment(aes(x=band-rectWidth,xend=band+rectWidth,y=band,yend=band),alpha=1,linetype=\"dashed\")+\n    labs(x = \"Velocity Band\", y = \"vxMean\") +\n    scale_x_continuous(labels=sort(unique(df$band)),breaks=sort(unique(df$band)))+\n    ggtitle(title) + theme(legend.position = \"none\")+theme_classic()+guides(fill=\"none\",color=\"none\")\n}\n\ntv&lt;-testSlopeIndv %&gt;% left_join(dtestAgg, by = c(\"id\",\"condit\")) %&gt;% filter(condit==\"Varied\",rank&lt;=6) %&gt;% \n   plotSlope(.,colour=\"black\",title=\"Largest Individually fit Varied Sbj. Slopes\")\ntc&lt;-testSlopeIndv %&gt;% left_join(dtestAgg, by = c(\"id\",\"condit\")) %&gt;% filter(condit==\"Constant\",rank&lt;=6) %&gt;% \n   plotSlope(.,colour=\"black\",title=\"Largest Individually fit Constant Sbj. Slopes\")\nbv&lt;-testSlopeIndv %&gt;% left_join(dtestAgg, by = c(\"id\",\"condit\")) %&gt;% filter(condit==\"Varied\",rank&gt;=nGrp-5) %&gt;% \n   plotSlope(.,colour=\"black\",title=\"Smallest Varied Sbj. Slopes\")\nbc&lt;-testSlopeIndv %&gt;% left_join(dtestAgg, by = c(\"id\",\"condit\")) %&gt;% filter(condit==\"Constant\",rank&gt;=nGrp-5) %&gt;% \n   plotSlope(.,colour=\"black\",title=\"Smallest Constant Sbj. Slopes.\")\n \ntitle = ggdraw()+draw_label(\"Highest and Lowest Slope Values\",fontface = 'bold',x=0,hjust=0)+theme(plot.margin = margin(0, 0, 0, .5))\nplot_grid(title,NULL,tv,tc,bv,bc,NULL,ncol=2,rel_heights = c(.1,1,1))"
  },
  {
    "objectID": "Analysis/discrim.html#fit-partial-pooling-model-linear-mixed-model-with-random-slope-and-intercept",
    "href": "Analysis/discrim.html#fit-partial-pooling-model-linear-mixed-model-with-random-slope-and-intercept",
    "title": "Testing Discrimination Analysis",
    "section": "Fit partial pooling model (linear mixed model with random slope and intercept)",
    "text": "Fit partial pooling model (linear mixed model with random slope and intercept)\n\nCodebm1 &lt;- lmer(vxMed ~ 1 + band + (1 + band | id), dtestAgg, control = lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 3e5)))\narm::display(bm1)\n\nlmer(formula = vxMed ~ 1 + band + (1 + band | id), data = dtestAgg, \n    control = lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 3e+05)))\n            coef.est coef.se\n(Intercept) 455.71    18.10 \nband          0.61     0.02 \n\nError terms:\n Groups   Name        Std.Dev. Corr  \n id       (Intercept) 370.15         \n          band          0.46   -0.79 \n Residual             138.09         \n---\nnumber of obs: 2697, groups: id, 461\nAIC = 36583.8, DIC = 36573.6\ndeviance = 36572.7 \n\nCodedf_partial_pooling &lt;- coef(bm1)[[\"id\"]] %&gt;% \n  rownames_to_column(\"id\") %&gt;% \n  as_tibble() %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_band = band) %&gt;% \n  add_column(Model = \"Partial pooling\")\n\nhead(df_partial_pooling)\n\n# A tibble: 6 × 4\n  id    Intercept Slope_band Model          \n  &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          \n1 1         558.       0.556 Partial pooling\n2 2        1157.       0.230 Partial pooling\n3 3          44.5      1.42  Partial pooling\n4 4        1033.      -0.140 Partial pooling\n5 5         250.       0.949 Partial pooling\n6 6         504.       0.596 Partial pooling\n\nCodesummary(bm1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: vxMed ~ 1 + band + (1 + band | id)\n   Data: dtestAgg\nControl: lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 3e+05))\n\nREML criterion at convergence: 36571.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4138 -0.4608 -0.0159  0.4495  5.0446 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n id       (Intercept) 1.370e+05 370.1522      \n          band        2.084e-01   0.4565 -0.79\n Residual             1.907e+04 138.0887      \nNumber of obs: 2697, groups:  id, 461\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 455.70935   18.09580 457.64524   25.18   &lt;2e-16 ***\nband          0.61216    0.02248 456.22667   27.24   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n     (Intr)\nband -0.801\noptimizer (bobyqa) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0852438 (tol = 0.002, component 1)\nModel is nearly unidentifiable: very large eigenvalue\n - Rescale variables?\nModel is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n\n\n\nCodedf_models &lt;- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %&gt;% \n  left_join(dtestAgg, by = c(\"id\"))\n\nWarning in left_join(., dtestAgg, by = c(\"id\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nCode#filter only testSlopeIndv with Rank &lt; 3\ntestSlopeIndv[testSlopeIndv$rank&lt;10,]$id\n\n [1] 223 401 312 376 363 171 329 366 393 275 341 440 158 132 375 66  343 316\n461 Levels: 223 401 312 376 363 171 329 366 393 275 341 440 158 132 375 ... 302\n\nCodedf_models %&gt;% filter(id %in% testSlopeIndv[testSlopeIndv$rank&lt;10,]$id) %&gt;%\nggplot() + aes(x = band, y = vxMed) + \n  geom_abline(aes(intercept = Intercept, slope = Slope_band, color = Model),\n     linewidth= .75) + \n  geom_point() + facet_wrap(\"id\") +\n  labs(x = \"band\", y = \"vxMean\") \n\n\n\n\n\nCode# filter to only retain the no pooling and partial pooling models. \n# Compare the average slope and intercepts between constant and varied condits. Use barplots with standard error bars\n\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %&gt;% \n  left_join(dtestAgg, by = c(\"id\"))\n\nWarning in left_join(., dtestAgg, by = c(\"id\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nCodegrpAvg&lt;-  df_models %&gt;% filter(Model %in% c(\"No pooling\", \"Partial pooling\")) %&gt;% group_by(id,Model) %&gt;% slice(1) %&gt;%\n  group_by(Model, condit) %&gt;% \n  summarise(\n    n= n(),\n    Intercept = mean(Intercept), \n    Slope_band = mean(Slope_band),\n  ) %&gt;% mutate( Intercept_se = sd(Intercept)/sqrt(n),\n    Slope_band_se = sd(Slope_band)/sqrt(n), .groups=\"keep\") \n#head(grpAvg)\n\n\n p1=grpAvg %&gt;% ggplot() + \n  aes(x = Model, y = Slope_band, fill = condit) +\n  geom_col(position = \"dodge\") + \n  geom_errorbar(aes(ymin = Slope_band - Slope_band_se, ymax = Slope_band + Slope_band_se), width = 0.2, position = position_dodge(0.9)) +\n  labs(x = \"Model\", y = \"Slope (band)\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Comparing Slopes between Conditions - Both pooling models\")\n \n \n \n p2=grpAvg %&gt;% ggplot() + \n  aes(x = Model, y = Intercept, fill = condit) +\n  geom_col(position = \"dodge\") + \n  geom_errorbar(aes(ymin = Intercept - Intercept_se, ymax = Intercept + Intercept_se), width = 0.2, position = position_dodge(0.9)) +\n  labs(x = \"Model\", y = \"Intercept\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Comparing Intercepts between Conditions - Both pooling models\")\n  \n\n\n# For the partial pooling model, visualize the correlation between the intercept and slope for each subject.\n# Use geom_smooth to fit a linear model to the data, and plot the line of best fit.\n\np3=df_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Intercept, y = Slope_band) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Intercept\", y = \"Slope (band)\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Correlation between Fit Slope and Intercept (Mixed Effects model)\")\n\n\n  # For the partial pooling model, visualize the correlation between slope and devMean for each subject.\n# Use geom_smooth to fit a linear model to the data, and plot the line of best fit.\n\np4=df_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Slope_band, y = devMean) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Slope (band)\", y = \"devMean\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Correlation between Fit Slope and testing performance (Mixed Effects model)\")\n\n\n# For the partial pooling model, visualize the correlation between Intercept and devMean for each subject.\n\np5=df_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Intercept, y = devMean) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Intercept\", y = \"devMean\") + \n  theme(legend.position = \"top\", legend.justification = \"left\")+ggtitle(\"Correlation between Fit Intercept and testing performance (Mixed Effects model)\")\n\n\n\ntitle = ggdraw()+draw_label(\"Examining the Fit Slopes and Intercepts\",fontface = 'bold',x=0,hjust=0)+theme(plot.margin = margin(0, 0, 0, .5))\nplot_grid(title,NULL,p1,p2,p3,p4,p5,ncol=2,rel_heights = c(.15,1,1,1))"
  },
  {
    "objectID": "Analysis/discrim.html#correlation-between-fit-parameters-slope-and-intercept-and-testing-vx",
    "href": "Analysis/discrim.html#correlation-between-fit-parameters-slope-and-intercept-and-testing-vx",
    "title": "Testing Discrimination Analysis",
    "section": "Correlation between fit parameters (Slope and Intercept) and testing Vx",
    "text": "Correlation between fit parameters (Slope and Intercept) and testing Vx\n\nNoteworthy that The correlation between slope and Vx is strongest for the slowest bands (100-300 and 350-550), for both original and reverse ordered groups. The slow positions are extrapolation for the Original ordered group, and trained by the reverse ordered group.\nFairly similar patterns for Slope and Intercept\n\n\nCode# For the partial pooling model, visualize the correlation between slope and devMean for each subject. Facet by vb~catOrder. Group and color by condit. \n# Use geom_smooth to fit a linear model to the data, and plot the line of best fit.\n\ndf_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Slope_band, y = vxMed, color = condit) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Slope (band)\", y = \"Median Vx\") + \n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  facet_grid(bandOrder~vb)+ ggtitle(\"Correlation between Slope and Median VX\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nCodedf_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Intercept, y = vxMed, color = condit) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Intercept\", y = \"Median Vx\") + \n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  facet_grid(bandOrder~vb)+ ggtitle(\"Correlation between Intercept and Median Vx\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrelation between parameters and Mean Deviation.\n\nHere we see a powerful effect of slope for the slow bands (larger slopes tend to have smaller deviation)\n\n\nCode# For the partial pooling model, visualize the correlation between slope and devMean for each subject. Facet by vb~catOrder. Group and color by condit. \n# Use geom_smooth to fit a linear model to the data, and plot the line of best fit.\n\ndf_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Slope_band, y = devMean, color = condit) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Slope (band)\", y = \"devMean\") + \n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  facet_grid(bandOrder~vb)+ ggtitle(\"Correlation between Slope and Mean Deviation\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nCodedf_models %&gt;% filter(Model == \"Partial pooling\") %&gt;% \n  ggplot() + \n  aes(x = Intercept, y = devMean, color = condit) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Intercept\", y = \"devMean\") + \n  theme(legend.position = \"top\", legend.justification = \"left\") +\n  facet_grid(bandOrder~vb)+ ggtitle(\"Correlation between Intercept and Mean Deviation\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Analysis/e1_discrim.html",
    "href": "Analysis/e1_discrim.html",
    "title": "E1 Discrimination Analysis",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes,brms,broom,broom.mixed,lme4,emmeans,here,knitr,kableExtra,gt,gghalves,patchwork,ggforce,ggdist,equatiomatic)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))\n\n\ntest &lt;- e1 |&gt; filter(expMode2 == \"Test\") \ntestAvg &lt;- test %&gt;% group_by(id, condit, vb, bandInt,bandType,tOrder) %&gt;%\n  summarise(nHits=sum(dist==0),vxMean=mean(vx),vxMed=median(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n\n#test %&gt;% filter(id==1, band==1) %&gt;% select(vx) %&gt;% unlist() %&gt;% as.vector()\n\n\n\nCodelmerFit &lt;- function(mform,df) {\nm=lmer(mform, df)\n  fe &lt;- fixef(m)\n  dfm &lt;- coef(m)$id %&gt;% as.tibble(rownames=\"id\") %&gt;%rename(\"Intercept\"=`(Intercept)`)\n  for (effect in names(fe)) {\n      dfm &lt;- dfm %&gt;% mutate(!!str_c(\"group_\", effect) := fe[effect])\n  }\n  dfm\n}\n\n# Input formulas and corresponding dataframes\nformulas &lt;- c(\"vx ~ 1 + bandInt + (1 + bandInt | id)\", \n              \"vxMean ~ 1 + bandInt + (1 + bandInt | id)\", \n              \"vxMed ~ 1 + bandInt + (1 + bandInt | id)\")\ndfs &lt;- list(test, testAvg, testAvg)\n\n# Run lmerFit function over all combinations of formulas and dataframes\nm_all &lt;- map2_dfr(formulas, dfs, ~ lmerFit(.x, .y)) %&gt;%\n  mutate(mformula = rep(formulas, each = length(unique(test$id))), \n         dframe = rep(c('test', 'testAvg', 'testAvg'), each=length(unique(test$id))))\n\nhead(m_all)\nm_all |&gt; filter(id==1)\n\n# m1 &lt;- lmerFit(\"vx ~ 1 + bandInt + (1 + bandInt | id)\",test)\n# m2 &lt;- lmerFit(\"vxMean ~ 1 + bandInt + (1 + bandInt | id)\",testAvg)\n# m3 &lt;- lmerFit(\"vxMed ~ 1 + bandInt + (1 + bandInt | id)\",testAvg)\n\n\nm1 &lt;- lmer(vx ~ 1 + bandInt + condit + (1 + bandInt | id),data=test)\ncoef(m1)$id %&gt;% as.tibble(rownames=\"id\") %&gt;%rename(\"Intercept\"=`(Intercept)`)\nsummary(m1)\n\n\nfitted(m1)\nnew_data &lt;- testAvg  |&gt; ungroup() |&gt; select(id,condit,bandInt) \n  \nnew_data=expand_grid(unique(test[, c(\"id\", \"condit\")]), bandInt=unique(test$bandInt))\nnew_data &lt;- new_data[rep(row.names(new_data), each = 10), ]\n\n# Compute predictions\nnew_data$p &lt;- predict(m1, newdata = new_data)\n\n\noptions(mc.cores = 4, brms.backend = \"cmdstanr\")\n\nm1_bayes &lt;- brm(vx ~ 1 + bandInt + condit + (1 + bandInt | id), data = test, family = gaussian())\nsummary(m1_bayes)\n\nnew_data=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"condit\",\"bandInt\")]))) |&gt; dplyr::arrange(id,bandInt)\npredictions &lt;- t(posterior_predict(m1_bayes, newdata = new_data,ndraws=10))\nnew_data$prediction &lt;- rowMeans(predictions)\n\n\npredicted_draws &lt;- new_data %&gt;%\n  tidybayes::add_predicted_draws(m1_bayes,ndraws=5)\n\n\ntest$censored_vx &lt;- ifelse(test$vx &lt; 50, 50, test$vx)\ntest$censored_vx &lt;- ifelse(test$censored_vx &gt; 1600, 1600, test$censored_vx)\ntest$censored &lt;- ifelse((test$vx &lt;50 | test$vx&gt;1600), TRUE, FALSE)\n\n# fit model\nm1_bayes &lt;- brm(vx~ 1 + bandInt + condit + (1 + bandInt | id), \n                data = test, family = gaussian())\n\n\n\nm1_bayesSN &lt;- brm(vx ~ 1 + bandInt + condit + (1 + bandInt | id), data = test, family = skew_normal(),chains = 4)\nsummary(m1_bayesSN)\n\n\npst &lt;- posterior_samples(m1_bayes, \"b\")\nggplot(testAvg, aes(x = bandInt, y = vxMean)) +\n    geom_point(shape = 1) +\n    geom_abline(\n        data = pst, alpha = .01, size = .1,\n        aes(intercept = b_Intercept, slope = b_bandInt)\n    )\n\nX &lt;- cbind(test[,c(\"id\",\"condit\",\"trial\",\"vb\",\"bandInt\",\"vx\")], fitted(m1_bayes)[,-2]) %&gt;% as_tibble()\n\n\n\\[\n\\begin{aligned}\n  \\operatorname{vx}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1j[i]}(\\operatorname{bandInt}), \\sigma^2 \\right) \\\\    \n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\alpha_{j} \\\\\n      &\\beta_{1j}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma_{0}^{\\alpha} + \\gamma_{1}^{\\alpha}(\\operatorname{condit}_{\\operatorname{Varied}}) \\\\\n      &\\mu_{\\beta_{1j}}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\alpha_{j}} & \\rho_{\\alpha_{j}\\beta_{1j}} \\\\\n     \\rho_{\\beta_{1j}\\alpha_{j}} & \\sigma^2_{\\beta_{1j}}\n  \\end{array}\n\\right)\n\\right)\n    \\text{, for id j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\n  \\operatorname{\\widehat{vx}}_{i}  &\\sim N \\left(480.37_{\\alpha_{j[i]}} + 0.63_{\\beta_{1j[i]}}(\\operatorname{bandInt}), \\sigma^2 \\right) \\\\    \n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\alpha_{j} \\\\\n      &\\beta_{1j}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &56.83_{\\gamma_{1}^{\\alpha}}(\\operatorname{condit}_{\\operatorname{Varied}}) \\\\\n      &0\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cc}\n     351.8 & -0.8 \\\\\n     -0.8 & 0.38\n  \\end{array}\n\\right)\n\\right)\n    \\text{, for id j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\nCodem1_bayes &lt;- brm(vx ~ 1 + bandInt + condit + (1 + bandInt | id), \ndata = test, family = gaussian(), file=\"band_gauss_vx\")\n\nm1_bayesb &lt;- brm(vxb ~ 1 + bandInt + condit + (1 + bandInt | id), \ndata = test, family = gaussian(), file=\"band_gauss_vxb\")\n\nm1_bayes0 &lt;- brm(vx ~ 0 + bandInt + condit + (0 + bandInt | id), \ndata = test, family = gaussian(), file=\"band0_gauss_vx\")\n\nm1_bayes0b &lt;- brm(vxb ~ 0 + bandInt + condit + (0 + bandInt | id), \ndata = test, family = gaussian(), file=\"band0_gauss_vxb\")\n\nsummary(m1_bayes)\n\n\n\nCode# Calculate the standard deviation of the estimates for each level of id\nm_all %&gt;%\n  summarise(across(starts_with(\"group_\"), sd, na.rm = TRUE))\n\n# Calculate the correlation between the estimates for each level of id\nm_all %&gt;%\n  group_by(id) %&gt;%\n  summarise(correlation = cor(`bandInt`, `Intercept`, use = \"pairwise.complete.obs\"))\n\n\nm_all |&gt; filter(id==10)\n\n\n\nCodeoptions(mc.cores = 8,  # Use 4 cores\n        brms.backend = \"cmdstanr\")\n\nmodel_fixed &lt;- brm(\n  bf(vxMean ~ bandInt + (1 +bandInt | id)),\n  data = testAvg,\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = 1234\n)\n\n\n\nfixef(model_fixed)\n\nmm_Mean_b &lt;- coef(model_fixed)$id |&gt; as.tibble(rownames=\"id\") |&gt; select(id,starts_with(\"Estimate\"))\nhead(mm_Mean_b)\n\n\nX &lt;- cbind(testAvg[,1:8], fitted(model_fixed)[,-2]) %&gt;% as_tibble()\n\n\nggplot(X[1:90,],aes(y=Estimate,x=vb,group=id))+geom_point(aes(y=vxMean),shape=1)+\n  geom_line(aes(y=Q2.5),lty=2)+\n  geom_line(aes(y=Q97.5),lty=2)+\n  geom_smooth(aes(y=vxMean),method = \"lm\", fill = \"dodgerblue\", level = .95)+\n  facet_wrap(~id)\n\nconditional_effects(model_fixed)\n#conditional_effects(model_fixed,conditions=distinct(testAvg,id))\n\n\npredict(model_fixed,data.frame(id=1,bandInt=800))\n\n\n\nepred_draws(model_fixed,newdata=expand_grid(bandInt=unique(e1$bandInt),id=factor(e1$id,levels=unique(e1$id))),ndraws = 1)\n\n\n\nCodelog_m1 &lt;- brm(\n  bf(vx ~ bandInt + (1 +bandInt | id)),\n  data = test,\n  family=shifted_lognormal(),\n  control = list(adapt_delta = 0.95),\n  chains = 2, seed = 1234\n)\n\nsummary(log_m1)\n\n\nskew_m1 &lt;- brm(\n  bf(vxMean ~ bandInt + (1 +bandInt | id)),\n  data = testAvg,\n  family=skew_normal(),\n  control = list(adapt_delta = 0.95),\n  chains = 2, seed = 1234\n)\n\nsummary(skew_m1)\n\n\n\nCodelibrary(brms)\n\nbmerFit &lt;- function(mform,df) {\n  m &lt;- brm(mform, data = df, control = list(adapt_delta = 0.95), chains = 1, seed = 1234)\n  fe &lt;- fixef(m)\n  dfm &lt;- coef(m)$id %&gt;% as.tibble(rownames=\"id\") %&gt;% select(id, starts_with(\"Estimate\"))\n  \n  for (effect in names(fe)) {\n    dfm &lt;- dfm %&gt;% mutate(!!str_c(\"group_\", effect) := fe[effect])\n  }\n  dfm\n}\n\n# Input formulas and corresponding dataframes\nformulas &lt;- c(\"vx ~ 1 + bandInt + (1 + bandInt | id)\", \n              \"vxMean ~ 1 + bandInt + (1 + bandInt | id)\", \n              \"vxMed ~ 1 + bandInt + (1 + bandInt | id)\")\ndfs &lt;- list(test, testAvg, testAvg)\n\n# Run bmerFit function over all combinations of formulas and dataframes\nm_all_bayes &lt;- map2_dfr(formulas, dfs, ~ bmerFit(.x, .y)) %&gt;%\n  mutate(mformula = rep(formulas, each = length(unique(test$id))), \n         dframe = rep(c('test', 'testAvg', 'testAvg'), each=length(unique(test$id))))\n\nm_all_bayes(m_all)\nm_all_bayes |&gt; filter(id==10)"
  },
  {
    "objectID": "Analysis/methods.html",
    "href": "Analysis/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,lme4,emmeans,here,knitr,kableExtra,gt,gghalves,patchwork,ggforce,ggdist)\ne1 &lt;- readRDS(here(\"data/e1_07-27-23.rds\"))\nsource(here(\"Functions/Display_Functions.R\"))"
  },
  {
    "objectID": "Analysis/methods.html#methods",
    "href": "Analysis/methods.html#methods",
    "title": "Methods",
    "section": "Methods",
    "text": "Methods\nParticipants A total of 166 participants (XXX% female, XXX% male) were recruited from the Indiana University Introductory Psychology Course. The average age of participants was XXX years (SD = XXX). Participants were randomly assigned to one of two training conditions: varied training or constant training.\nDesign The experiment employed a 2 (Training Condition: varied vs. constant).\nProcedure Upon arrival at the laboratory, participants were provided with a description of the experiment and signed informed consent forms. They were then seated in front of a computer equipped with a mouse and were given instructions on how to perform the “Hit The Wall” (HTW) visuomotor extrapolation task.\nThe HTW task involved launching projectiles to hit a target displayed on the computer screen. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). In contrast, participants in the constant training condition encountered only one velocity band (800-1000).\nDuring the training stage, participants in both conditions also completed “no feedback” trials, where they received no information about their performance. These trials were randomly interleaved with the regular training trials.\nFollowing the training stage, participants proceeded to the testing stage, which consisted of three phases. In the first phase, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials.\nIn the second phase of testing, participants completed “no-feedback” testing from the three velocity bands used during the training stage (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training.\nThe third and final phase of testing involved “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 10 trials. Participants received feedback on their performance during this phase.\nThroughout the experiment, participants’ performance was measured by calculating the distance between the produced x-velocity of the projectiles and the closest edge of the current velocity band. Lower distances indicated better performance.\nAfter completing the experiment, participants were debriefed and provided with an opportunity to ask questions about the study.\n\n\n\n\ncluster\nTest Phase (Counterbalanced Order)data1\n Varied Training 800-10001000-12001200-1400Test1\nTest  Novel Bands 100-300350-550600-800data1-&gt;Test1\ndata2\n Constant Training 800-1000data2-&gt;Test1\nTest3\n    Final Test   Novel With Feedback  100-300350-550600-800Test2\n  Test   Varied Training Bands  800-10001000-12001200-1400Test1-&gt;Test2\nTest2-&gt;Test3\n\n\nFigure 1: Experimental Design. Constant and Varied participants complete different training conditions."
  },
  {
    "objectID": "Functions/index.html",
    "href": "Functions/index.html",
    "title": "Quarto Shiny ALM",
    "section": "",
    "text": "Association Parameter\n\n\n\n\n\nUpdate Parameter\n\n\n\n\n\nTraining Repetitions\n\n\n\n\n\nNoise\n\n\n\n\n\nTraining Items\n\n\n1\n\n\n5\n\n\n8\n\n\n12\n\n\n16\n\n\n19\n\n\n23\n\n\n27\n\n\n30\n\n\n34\n\n\n38\n\n\n41\n\n\n45\n\n\n49\n\n\n52\n\n\n56\n\n\n60\n\n\n63\n\n\n67\n\n\n71\n\n\n74\n\n\n78\n\n\n82\n\n\n85\n\n\n89\n\n\n93\n\n\n97\n\n\n100\n\n\n104\n\n\n108\n\n\n111\n\n\n115\n\n\n119\n\n\n122\n\n\n126\n\n\n130\n\n\n133\n\n\n137\n\n\n141\n\n\n144\n\n\n148\n\n\n152\n\n\n155\n\n\n159\n\n\n163\n\n\n166\n\n\n170\n\n\n174\n\n\n177\n\n\n181\n\n\n\n\n\n\nFunction Form\n\n\n\nLinear\n\n\n\n\nQuadratic\n\n\n\n\nExponential\n\n\n\n\n\n\n\nNumber of Repetitions\n\n\n\n\nRun Simulation\n\n\n\n\n\n\n\n\n\nAverage Model Performance\n\n\n\n\n\nModel Performance by Item Type"
  },
  {
    "objectID": "Misc/HTW_ToDo.html",
    "href": "Misc/HTW_ToDo.html",
    "title": "HTW To-do and Notes",
    "section": "",
    "text": "Model To-do\n\n\n\n\n\n\nFit to Train then Predict Transfer vs. Fitting to all stages\nSeparate ALM and EXAM Fits\nALM + Prior Knowledge (initial anchor at 0)\nEmpirical Learning Model\nIndividual vs. Group fits\nUsing Cognitive Model parameters to predict testing Vx vs. Deviation vs. Discrimination\nModel Recovery?\nApproximate Bayes?\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis To-do\n\n\n\n\nDiscrimination\nMixed Models?\n\n\n\n\n\n\n\n\n\nSite to-do\n\n\n\n\nConfigure react tables"
  },
  {
    "objectID": "Misc/HTW_ToDo.html#to-do-list",
    "href": "Misc/HTW_ToDo.html#to-do-list",
    "title": "HTW To-do and Notes",
    "section": "",
    "text": "Model To-do\n\n\n\n\n\n\nFit to Train then Predict Transfer vs. Fitting to all stages\nSeparate ALM and EXAM Fits\nALM + Prior Knowledge (initial anchor at 0)\nEmpirical Learning Model\nIndividual vs. Group fits\nUsing Cognitive Model parameters to predict testing Vx vs. Deviation vs. Discrimination\nModel Recovery?\nApproximate Bayes?\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis To-do\n\n\n\n\nDiscrimination\nMixed Models?\n\n\n\n\n\n\n\n\n\nSite to-do\n\n\n\n\nConfigure react tables"
  },
  {
    "objectID": "Misc/HTW_ToDo.html#notes",
    "href": "Misc/HTW_ToDo.html#notes",
    "title": "HTW To-do and Notes",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nHuman Regression or Rule\n\n\n\n\n\n\n\n\n\n\n\n\nALM & EXAM Implementations\n\n\n\n\n\n\n\n\n\n\n\n\nALM Likelihood\n\n\n\n\n\n\n\n\n\n\n\n\nHTW DP"
  },
  {
    "objectID": "Misc/Visuals_Interactives/ALM_Shiny.html",
    "href": "Misc/Visuals_Interactives/ALM_Shiny.html",
    "title": "ALM Shiny App Code",
    "section": "",
    "text": "Shiny App Simulating ALM and EXAM  \n\n\nYou can play with the embedded version of the app below, or go to direct link\nYou can adjust the values of the Association parameter (i.e. the c parameter), and the Update parameter, (i.e. the learning rate parameter). The App also allows you to control the number and location of training instances. And the shape of the true function (linear, quadratic or exponential)\n\n\nAlternatively, you can run the app locally by copying the code below into a .R file.\n\nShow App Codepacman::p_load(tidyverse,shiny,reactable,shinydashboard,shinydashboardPlus)\n\ninput.activation &lt;- function(x.target, association.parameter) {\n    return(exp(-1 * association.parameter * (x.target - x.plotting)^2))\n}\n\noutput.activation &lt;- function(x.target, weights, association.parameter) {\n    return(weights %*% input.activation(x.target, association.parameter))\n}\n\nmean.prediction &lt;- function(x.target, weights, association.parameter) {\n    probability &lt;- output.activation(x.target, weights, association.parameter) / sum(output.activation(x.target, weights, association.parameter))\n    return(y.plotting %*% probability)\n}\n# function to generate exam predictions\nexam.prediction &lt;- function(x.target, weights, association.parameter) {\n    trainVec &lt;- sort(unique(x.learning))\n    nearestTrain &lt;- trainVec[which.min(abs(trainVec - x.target))]\n    aresp &lt;- mean.prediction(nearestTrain, weights, association.parameter)\n    xUnder &lt;- ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n    xOver &lt;- ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n    mUnder &lt;- mean.prediction(xUnder, weights, association.parameter)\n    mOver &lt;- mean.prediction(xOver, weights, association.parameter)\n    exam.output &lt;- round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n    exam.output\n}\n\nupdate.weights &lt;- function(x.new, y.new, weights, association.parameter, update.parameter) {\n    y.feedback.activation &lt;- exp(-1 * association.parameter * (y.new - y.plotting)^2)\n    x.feedback.activation &lt;- output.activation(x.new, weights, association.parameter)\n    return(weights + update.parameter * (y.feedback.activation - x.feedback.activation) %*% t(input.activation(x.new, association.parameter)))\n}\n\nlearn.alm &lt;- function(y.learning, association.parameter = 0.05, update.parameter = 0.5) {\n    weights &lt;- matrix(rep(0.00, length(y.plotting) * length(x.plotting)), nrow = length(y.plotting), ncol = length(x.plotting))\n    for (i in 1:length(y.learning)) {\n        weights &lt;- update.weights(x.learning[i], y.learning[i], weights, association.parameter, update.parameter)\n        weights[weights &lt; 0] &lt;- 0\n    }\n    alm.predictions &lt;- sapply(x.plotting, mean.prediction, weights = weights, association.parameter = association.parameter)\n    exam.predictions &lt;- sapply(x.plotting, exam.prediction, weights = weights, association.parameter = association.parameter)\n    return(list(alm.predictions = alm.predictions, exam.predictions = exam.predictions))\n    # return(list(alm.predictions=alm.predictions, exam.predictions=exam.predictions,wmFinal=weights))\n}\n\n\n\nx.plotting &lt;&lt;- seq(0, 90, .5)\ny.plotting &lt;&lt;- seq(0, 210, by = 2)\n# trainOptions=round(seq(1,length(x.plotting),length.out=21),0)\ntrainOptions &lt;- x.plotting[seq(1, 181, by = 4)]\ntrainItems &lt;- trainOptions[c(10, 11, 12)]\n\n\n\n# Define UI for application\n# \nui &lt;- dashboardPage(\n\n  skin = \"black\",\n  dashboardHeader(title = \"ALM Simulation App\"),\n  dashboardSidebar(\n    sidebarMenu(\n      menuItem(\"Home\", tabName = \"home\", icon = icon(\"home\")),\n      menuItem(\"Code\", tabName = \"code\", icon = icon(\"code\"))\n    )\n  ),\n  dashboardBody(\n    tabItems(\n      tabItem(tabName = \"home\",\n              fluidRow(\n                column(4,\n                       box(\n                         title = \"Simulation Parameters\",\n                         status = \"primary\",\n                         solidHeader = TRUE,\n                         collapsible = TRUE,\n                         collapsed = FALSE,\n                         width = 12,\n                         sliderInput(\"assoc\", \"Association Parameter (c):\",\n                                     min = .001, max = 1, value = 0.5, step = 0.01),\n                         sliderInput(\"update\", \"Update Parameter:\",\n                                     min = 0, max = 1, value = 0.5, step = 0.1),\n                         sliderInput(\"trainRep\", \"Training Repetitions Per Item:\",\n                                     min = 1, max = 200, value = 1, step = 1),\n                         sliderInput(\"Noise\",\"Noise Level:\",\n                                     min = 0, max = 50, value = 0.00, step = 1),\n                         checkboxGroupInput(\"trainItems\", \"Training Items:\", choices = trainOptions, selected = trainOptions[c(10,15,35)],inline=TRUE),\n                         # radio buttons for selecting function form\n                         radioButtons(\"functionForm\", \"Function Form:\",\n                                      choices = c(\"Linear\", \"Quadratic\", \"Exponential\"),\n                                      selected = \"Quadratic\"),\n                        # numericInput(\"nRep\", \"Number of Replications:\", value = 1, min = 1, max = 100),\n                         actionButton(\"run\", \"Run Simulation\")\n                       )\n                ),\n                column(8,\n                       box(\n                         title = \"Model Performance\",\n                         status = \"primary\",\n                         solidHeader = TRUE,\n                         collapsible = TRUE,\n                         collapsed = FALSE,\n                          width = 12,\n                         plotOutput(\"plot\"),\n                         h5(\"*Dashed line shows true function. Red shows ALM, and blue depicts EXAM predictions*\"),\n                         h4(\"Average Model Performance\"),\n                         reactableOutput(\"table\"),\n                         h4(\"Model Performance by Item Type\"),\n                         reactableOutput(\"table2\")\n                       )\n                )\n              )\n      ),\n      tabItem(tabName = \"code\",\n              fluidRow(\n                column(12,\n                       box(\n                         title = \"Code\",\n                         status = \"primary\",\n                         solidHeader = TRUE,\n                         collapsible = TRUE,\n                         collapsed = FALSE,\n                         width = 12,\n                         verbatimTextOutput(\"code\")\n                       )\n                )\n                )\n        )\n    )\n    )\n)\n\n# Define server \n\n\n\nserver &lt;- function(input, output, session) {\n  \n  nRep=1\n  user_choice &lt;- eventReactive(input$run, {\n    return(list(assoc = input$assoc, update = input$update, Noise=input$Noise,\n                functionForm=input$functionForm,trainRep = as.numeric(input$trainRep),\n                trainItems = input$trainItems))\n    \n  }, ignoreNULL = FALSE)\n  \n\n    output_df &lt;- eventReactive(input$run, {\n      uc &lt;- reactive({user_choice()})\n    if (uc()$functionForm == \"Linear\") {\n      f.plotting &lt;&lt;- as.numeric(x.plotting * 2.2 + 30)\n    } else if (uc()$functionForm == \"Quadratic\") {\n      f.plotting &lt;&lt;- as.numeric(210 - ((x.plotting - 50)^2) / 12)\n    } else if (uc()$functionForm == \"Exponential\") {\n      # f.plotting&lt;&lt;-as.numeric(scale(200*(1-exp(-x.plotting/25))))\n      f.plotting &lt;&lt;- as.numeric(200 * (1 - exp(-x.plotting / 25)))\n    }\n    trainItems &lt;- as.numeric(uc()$trainItems)\n    y.plotting &lt;&lt;- seq(0, max(f.plotting), by = 1)\n    x.learning &lt;&lt;- rep(trainItems, times = uc()$trainRep)\n    f.learning &lt;&lt;- rep(f.plotting[which(x.plotting %in% trainItems)], times = uc()$trainRep)\n    # print(x.learning)\n    # print(f.learning)\n    # print(uc()$trainRep)\n    # print(trainItems)\n    # print(uc()$functionForm)\n    \n    \n    output_list &lt;- replicate(nRep, list(learn.alm(f.learning + rnorm(length(f.learning), sd = uc()$Noise),\n                                                  association.parameter = uc()$assoc, update.parameter = uc()$update)))\n    \n    output_df &lt;- lapply(output_list, function(x) as.data.frame(x))\n    #output_df &lt;- lapply(output_list, function(x) lapply(x, as.data.frame)) # 10 dfs x 9 lists\n    output_df &lt;- Reduce(rbind, output_df) %&gt;% mutate(x = x.plotting, y = f.plotting)\n    #output_df &lt;- lapply(output_df, function(x) Reduce(rbind,x))# 1 df x 9 lists\n    output_df &lt;- output_df %&gt;%\n      pivot_longer(names_to = \"Model\", values_to = \"Prediction\", cols = c(alm.predictions, exam.predictions)) %&gt;%\n      rbind(data.frame(data.frame(x = x.plotting, y = f.plotting, Model = \"True Function\", Prediction = f.plotting)), .)\n    #str(output_df)\n    return(output_df)\n    \n    }, ignoreNULL = FALSE)\n    \n    output$plot &lt;- renderPlot({\n      \n      output_df2 &lt;- reactive({output_df()})\n      ggplot(data = output_df2(), aes(x = x, y = Prediction,color=Model),alpha=.2) + \n        geom_line(aes(linetype=Model,alpha=Model)) + \n        geom_point(data = data.frame(x.learning, f.learning), \n                   aes(x = x.learning,y = f.learning),color=\"black\",size=4,shape=4) +\n        # geom_line(data = data.frame(x.plotting, f.plotting), \n        #           aes(x = x.plotting, y = f.plotting),linetype=2, color = \"black\",alpha=.3) + \n        scale_color_manual(values = c(\"red\", \"blue\", \"black\"))+\n        scale_alpha_manual(values=c(.8,.8,.4))+\n        scale_linetype_manual(values=c(1,1,2))+\n        ylim(c(0,250))#+\n        # ggtitle(paste(\"Association Parameter:\", user_choice()$assoc, \" Update Parameter:\", \n        #               uc$update, \" Train Reps:\", \n        #               uc$trainRep, \" Noise:\", uc$Noise))\n    }) \n    # table 1 reports the summary stats for all items. Table uses GT library to make gt table\n    output$table &lt;- renderReactable({\n      output_df &lt;- output_df()\n      output_df() %&gt;% group_by(Model) %&gt;% filter(Model !=\"True Function\") %&gt;%\n        summarise(MeanDeviation = mean(abs(Prediction - y)), \n                  RMSD = sqrt(mean((Prediction -y)^2)),Correlation = cor(Prediction, y)) %&gt;%\n        mutate(across(where(is.numeric), round, 1)) %&gt;%\n        reactable::reactable(compact=TRUE,bordered = TRUE, highlight = TRUE, resizable=TRUE)\n    })\n    # table 2 reports the summary stats separately for training items, interpolation items, and extrapolation items\n    output$table2 &lt;- renderReactable({\n      uc &lt;- reactive({user_choice()})\n      output_df() %&gt;% filter(Model !=\"True Function\") %&gt;% \n        mutate(ItemType = ifelse(x %in% x.learning, \"Training\", ifelse(x &gt; min(x.learning) & x &lt; max(x.learning), \"Interpolation\", \"Extrapolation\"))) %&gt;%\n        group_by(ItemType,Model) %&gt;%\n        summarise(MeanDeviation = mean(abs(Prediction - y)), \n                  RMSD = sqrt(mean((Prediction -y)^2)),Correlation = cor(Prediction, y),\n                  .groups=\"keep\") %&gt;% \n        mutate(across(where(is.numeric), round, 1)) %&gt;%\n        reactable::reactable(compact=TRUE,bordered = TRUE, highlight = TRUE, resizable=TRUE) \n    })\n    \n    \n    output$code &lt;- renderPrint({\n      # code to implement the ALM and EXAM models\n      # code to generate data\n      # code to run models\n      # code to format output\n      cat(\" input.activation&lt;-function(x.target, association.parameter){\n  return(exp(-1*association.parameter*(x.target-x.plotting)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, association.parameter){\n  return(weights%*%input.activation(x.target, association.parameter))\n}\n\nmean.prediction&lt;-function(x.target, weights, association.parameter){\n  probability&lt;-output.activation(x.target, weights, association.parameter)/sum(output.activation(x.target, weights, association.parameter))\n  return(y.plotting%*%probability)\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, association.parameter){\n  trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, association.parameter)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, association.parameter)\n  mOver = mean.prediction(xOver, weights, association.parameter)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n\nupdate.weights&lt;-function(x.new, y.new, weights, association.parameter, update.parameter){\n  y.feedback.activation&lt;-exp(-1*association.parameter*(y.new-y.plotting)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, association.parameter)\n  return(weights+update.parameter*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, association.parameter)))\n}\n\nlearn.alm&lt;-function(y.learning, association.parameter=0.05, update.parameter=0.5){\n  weights&lt;-matrix(rep(0.00, length(y.plotting)*length(x.plotting)), nrow=length(y.plotting), ncol=length(x.plotting))\n  for (i in 1:length(y.learning)){\n    weights&lt;-update.weights(x.learning[i], y.learning[i], weights, association.parameter, update.parameter)\n    weights[weights&lt;0]=0\n  }\n  alm.predictions&lt;-sapply(x.plotting, mean.prediction, weights=weights, association.parameter=association.parameter)\n  exam.predictions &lt;- sapply(x.plotting, exam.prediction, weights=weights, association.parameter=association.parameter)\n  return(list(alm.predictions=alm.predictions, exam.predictions=exam.predictions))\n  #return(list(alm.predictions=alm.predictions, exam.predictions=exam.predictions,wmFinal=weights))\n}\n\n    \")\n    })\n    \n}\n\n\n\n# Run the application\n\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "Misc/Visuals_Interactives/model_viz.html",
    "href": "Misc/Visuals_Interactives/model_viz.html",
    "title": "Model Visualization",
    "section": "",
    "text": "Code#lapply(c('tidyverse','data.table','igraph','ggraph','kableExtra'),library,character.only=TRUE))\npacman::p_load(tidyverse,data.table,igraph,ggraph,kableExtra,DiagrammeR,png, plantuml)"
  },
  {
    "objectID": "Misc/Visuals_Interactives/model_viz.html#ggplot-model-visualization",
    "href": "Misc/Visuals_Interactives/model_viz.html#ggplot-model-visualization",
    "title": "Model Visualization",
    "section": "ggplot model visualization",
    "text": "ggplot model visualization\n\nCodenInput=6\nnOutput=10\n\ninNodes &lt;- seq(1,nInput,1) %&gt;% as.integer()\noutNodes &lt;- seq(300,1000,length.out=nOutput)%&gt;% as.integer()\nweight.mat &lt;&lt;- matrix(0.001,nrow=nOutput,ncol=nInput) # weights initialized to 0 (as in Delosh 1997)\n\nstim &lt;- 1.5\nc=.1\ninAct &lt;- round(exp(-c*((inNodes-stim)^2)),2)\ninActLab &lt;- paste0(\"x\",inNodes,\"=\",inAct)\noutAct &lt;- weight.mat %*% inAct\noutput.probability &lt;&lt;- outAct/sum(outAct)\noutLab=paste0(\"y\",outNodes,\"=\",round(output.probability,2))\nmean.response &lt;&lt;- round(sum(outNodes * output.probability),0)\n\n\nresp &lt;- mean.response\ninFlow &lt;- tibble(expand.grid(from=stim,to=inActLab)) %&gt;% mutate_all(as.character)\noutFlow &lt;- tibble(expand.grid(from=outLab,to=mean.response)) %&gt;% mutate_all(as.character)\n\ngd &lt;- tibble(expand.grid(from=inActLab,to=outLab)) %&gt;% mutate_all(as.character) %&gt;%\n  rbind(inFlow,.) %&gt;% rbind(.,outFlow)\n\nxInc &lt;- .3\nyInc=.5\n\ng = graph_from_data_frame(gd,directed=TRUE)\ncoords2=layout_as_tree(g)\ncolnames(coords2)=c(\"y\",\"x\")\n\nodf &lt;- as_tibble(coords2) %&gt;% \n  mutate(label=vertex_attr(g,\"name\"),\n         type=c(\"stim\",rep(\"Input\",nInput),rep(\"Output\",nOutput),\"Resp\"),\n         x=x*-1) %&gt;%\n  mutate(y=ifelse(type==\"Resp\",0,y),xmin=x-xInc,xmax=x+xInc,ymin=y-yInc,ymax=y+yInc)\n\nplot_edges = gd %&gt;% mutate(id=row_number()) %&gt;%\n  pivot_longer(cols=c(\"from\",\"to\"),names_to=\"s_e\",values_to=(\"label\")) %&gt;%\n                 mutate(label=as.character(label)) %&gt;% \n  group_by(id) %&gt;%\n  mutate(weight=sqrt(rnorm(1,mean=0,sd=10)^2)/10) %&gt;%\n  left_join(odf,by=\"label\") %&gt;%\n  mutate(xmin=xmin+.02,xmax=xmax-.02)\n\nggplot() + geom_rect(data = odf,\n            mapping = aes(xmin = xmin, ymin = ymin, \n                          xmax = xmax, ymax = ymax, \n                          fill = type, colour = type),alpha = 0.01) +\n  geom_text(data=odf,aes(x=x,y=y,label=label,size=3)) +\n  geom_path(data=plot_edges,mapping=aes(x=x,y=y,group=id,alpha=weight)) +\n  # geom_rect(aes(xmin=-1.05,xmax=-.95,ymin=-10,ymax=5),color=\"red\",alpha=.1)+\n  # geom_rect(aes(xmin=-0.05,xmax=.05,ymin=-10,ymax=5),color=\"blue\",alpha=.1) +\n  theme_void()"
  },
  {
    "objectID": "Misc/Visuals_Interactives/model_viz.html#ggraph-method",
    "href": "Misc/Visuals_Interactives/model_viz.html#ggraph-method",
    "title": "Model Visualization",
    "section": "ggraph method",
    "text": "ggraph method\n\nCodeinNodes &lt;- seq(1,6,1) %&gt;% as.integer()\noutNodes &lt;- seq(300,1000,50)%&gt;% as.integer()\n\nda &lt;- data.frame(expand.grid(inNodes,outNodes))  %&gt;% magrittr::set_colnames(c(\"input\",\"output\"))\nda &lt;- da %&gt;% mutate_all(as.character)\nm = graph_from_data_frame(da, directed = TRUE)\n\ncoords = layout_with_sugiyama(m)\ncolnames(coords$layout) = c(\"y\", \"x\")\ncoords$layout=coords$layout[,c(\"x\",\"y\")]\nplot(m,layout=coords)\n\n\n\nCodeggraph(m,layout=coords$layout)+\ngeom_edge_link0(width=0.2,colour=\"grey\")+\n  geom_node_point(col=\"white\",size=6)+scale_x_reverse()+\n  geom_node_text(aes(label=name)) +\n  # draw rectangle that covers input layer at x=1, min y is min of coords$y and max y is max of coords$y\n  annotate(\"rect\",xmin=0,xmax=.1,ymin=min(coords$layout[,2]),ymax=max(coords$layout[,2]),fill=\"grey\",alpha=0.7)\n\n\n\nCode geom_rect(xmin=0,xmax=1.1,ymin=min(coords$layout[,2]),ymax=max(coords$layout[,2]),fill=\"grey\",alpha=0.7)\n\ngeom_rect: linejoin = mitre, na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n\n\nCodelibrary(tidyverse)\nlibrary(ggforce)\n\ntheme_set(theme_grey() +\n            theme_void() +\n            theme(plot.margin = margin(0, 5.5, 0, 5.5)))\n\n# Parameters\ninput_x &lt;- c(1, 2)\noutput_x &lt;- c(1.5, 2.5, 3.5)\ninput_y &lt;- 3\noutput_y &lt;- 1\nactivation_levels &lt;- c(0.2, 0.7, 0.5)\n\n# Input Layer\ninput_layer &lt;- tibble(x = input_x, y = rep(input_y, length(input_x)))\n\n# Output Layer\noutput_layer &lt;- tibble(x = output_x, y = rep(output_y, length(output_x)), activation = activation_levels)\n\n# Gaussian Activation\ngaussian_activation &lt;- tibble(\n  x = seq(from = 0, to = 3, by = 0.01),\n  y = exp(-2 * (x - 1)^2) + 2.5,\n  x2 = x,\n  y2 = exp(-2 * (x - 2)^2) + 2.5\n)\n\n# Plot\np &lt;- ggplot() +\n  # Input nodes\n  geom_point(data = input_layer, aes(x = x, y = y), size = 4, color = 'green') +\n  annotate(\"text\", x = input_x, y = rep(input_y, length(input_x)) + 0.3, label = c(\"Input #1\", \"Input #2\")) +\n  \n  # Gaussian Activations\n  geom_line(data = gaussian_activation, aes(x = x, y = y), color = 'blue') +\n  geom_line(data = gaussian_activation, aes(x = x2, y = y2), color = 'blue') +\n  \n  # Connections\n  geom_segment(data = expand.grid(input_x, output_x), aes(x = Var1, xend = Var2, y = input_y, yend = output_y), arrow = arrow(type = 'closed', length = unit(0.2, 'inches'))) +\n  \n  # Output nodes\n  geom_point(data = output_layer, aes(x = x, y = y), size = 4, color = 'red') +\n  geom_bar(data = output_layer, aes(x = x, y = activation), stat = 'identity', position = 'dodge', fill = 'red', alpha = 0.3, width = 0.3) +\n  annotate(\"text\", x = output_x, y = rep(output_y, length(output_x)) - 0.3, label = c(\"Output #1\", \"Output #2\", \"Output #3\")) +\n  \n  # Equation annotations\n  annotate(\"text\", x = 0.5, y = input_y + 1, label = \"1\", parse = TRUE) +\n  annotate(\"text\", x = 1.5, y = output_y - 1, label = \"2 \", parse = TRUE) +\n  \n  # Input stimulus and output response\n  annotate(\"text\", x = mean(input_x), y = input_y + 1.3, label = \"Input Stimulus\") +\n  annotate(\"text\", x = mean(output_x), y = output_y - 1.3, label = \"Output Response\") +\n  \n  # Coordinate limits and axis labels\n  coord_cartesian(xlim = c(0, 4), ylim = c(-1, 5)) +\n  labs(x = \"\", y = \"\") +\n  theme_void()\n\n# Show the plot\nprint(p)\n\n\n\n\nAlt ggplot 2\n\nCodelibrary(ggplot2)\nlibrary(ggrepel)\n\nneural_network_plot &lt;- function(layers, layer_labels = NULL, node_labels = c('x', 'y')) {\ndata &lt;- expand.grid(layer = seq_along(layers), node = 1:max(layers))\ndata &lt;- data[data$node &lt;= layers[data$layer], ]\n\nlinks &lt;- data.frame()\nfor (i in 1:(length(layers) - 1)) {\nstart &lt;- subset(data, layer == i)\nend &lt;- subset(data, layer == i + 1)\nlinks &lt;- rbind(links, expand.grid(from = start$node, to = end$node, from_layer = start$layer, to_layer = end$layer))\n}\n\np &lt;- ggplot() +\ngeom_segment(data = links, aes(x = from_layer, xend = to_layer, y = from, yend = to), color = 'grey') +\ngeom_point(data = data, aes(x = layer, y = node), size = 5, color = 'orange') +\ntheme_minimal() +\ntheme(axis.text = element_blank(),\naxis.ticks = element_blank(),\npanel.grid = element_blank(),\naxis.title = element_blank())\n\nif (!is.null(layer_labels)) {\ndata$layer_label &lt;- layer_labels[data$layer]\np &lt;- p + geom_text_repel(data = data, aes(x = layer, y = max(layers) + 1, label = layer_label), size = 5, nudge_y = 1)\n}\n\nif (!is.null(node_labels)) {\ndata$node_label &lt;- ifelse(data$layer == 1, paste0(node_labels[1], '', data$node),\nifelse(data$layer == length(layers), paste0(node_labels[2], '', data$node), ''))\np &lt;- p + geom_text_repel(data = data, aes(x = layer, y = node, label = node_label), size = 3)\n}\n\nreturn(p)\n}\n\n#Example usage\nlayers &lt;- c(8, 5, 3, 5, 8)\nlayer_labels &lt;- c('Input Layer', '', 'Latent\\nRepresentation', '', 'Output Layer')\nneural_network_plot(layers, layer_labels)\n\n\n\n\nAlt ggplot 3\n\nCodelibrary(ggplot2)\n\ndata &lt;- data.frame(\nx = c(1, 2, 2, 3, 4, 4, 1, 2, 3, 4),\ny = c(0, 1, -1, 0, 0, 0, 0, 0, 0, 0),\nlabel = c(\"x_1\", \"w_1\", \"w_2\", \"Σ\", \"f\", \"y\", \"x_2\", \"w_3\", \"b\", \"\"),\nshape = c(\"circle\", \"square\", \"square\", \"circle\", \"square\", \"square\", \"circle\", \"square\", \"square\", \"square\"),\ntext_y = c(0.15, 0.15, -0.15, 0, 0.3, 0.3, -0.15, -0.3, 0.15, 0)\n)\n\ndata$shape &lt;- factor(data$shape, levels = c(\"circle\", \"square\"))\n\nggplot(data, aes(x = x, y = y)) +\ngeom_point(aes(shape = shape), size = 8) +\ngeom_text(aes(label = label, y = y + text_y), size = 5) +\nscale_shape_manual(values = c(\"circle\" = 19, \"square\" = 15)) +\ngeom_segment(aes(x = 2, xend = 3, y = 1, yend = 0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\ngeom_segment(aes(x = 2, xend = 3, y = -1, yend = -0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\ngeom_segment(aes(x = 3, xend = 2, y = 0.2, yend = 1), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\ngeom_segment(aes(x = 3, xend = 2, y = -0.2, yend = -1), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\ngeom_segment(aes(x = 3.5, xend = 4.5, y = 0, yend = 0)) +\ngeom_segment(aes(x = 4.5, xend = 5.5, y = 0, yend = 0)) +\nannotate(\"text\", x = 4, y = 0.5, label = \"Activation\\nFunction\", hjust = 0.5) +\nannotate(\"text\", x = 5, y = 0, label = \"Output\", hjust = 0.5) +\nannotate(\"text\", x = 3, y = 1.1, label = \"Bias\\nb\", hjust = 0.5) +\ngeom_rect(aes(xmin = 0.5, xmax = 1.5, ymin = -1.5, ymax = 1.5), color = \"black\", linetype = \"dashed\", fill = NA) +\nannotate(\"text\", x = 0.3, y = 0, label = \"Inputs\", angle = 90) +\ntheme_void() +\nxlim(0, 6) +\ntheme(legend.position = \"none\")\n\n\n\nCodedata &lt;- data.frame(\n    x = c(2, 3, 3, 4, 5, 5, 2, 3, 4, 5),\n    y = c(0, 1, -1, 0, 0, 0, 0, 0, 0, 0),\n    label = c(\"X\", \"w_{ji}\", \"(ai)\", \"(sum)\", \"O_j(X)\", \"m(X)\", \"X_i\", \"(e^{-gamma * (X-X[i])^2})\", \"(e^{-gamma * (Z-Y[j])^2})\", \"f_j(Z)\"),\n    shape = c(\"circle\", \"square\", \"circle\", \"circle\", \"circle\", \"circle\", \"circle\", \"circle\", \"circle\", \"circle\"),\n    text_y = c(0.15, 0.15, -0.15, 0, 0.3, 0.3, -0.15, -0.3, 0.15, 0)\n)\n\ndata$shape &lt;- factor(data$shape, levels = c(\"circle\", \"square\"))\n\nggplot(data, aes(x = x, y = y)) +\n    geom_point(aes(shape = shape), size = 8) +\n    geom_text(aes(label = label, y = y + text_y), size = 4) +\n    scale_shape_manual(values = c(\"circle\" = 19, \"square\" = 15)) +\n    geom_segment(aes(x = 3, xend = 4, y = 1, yend = 0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 3, xend = 4, y = -1, yend = -0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 4.5, xend = 5.5, y = 0, yend = 0)) +\n    annotate(\"text\", x = 5, y = 0.5, label = \"Mean Output\", hjust = 0.5) +\n    annotate(\"text\", x = 4, y = 1.1, label = \"Normalized Input\\nActivation\", hjust = 0.5) +\n    annotate(\"text\", x = 4, y = -1.1, label = \"Feedback\\nActivation\", hjust = 0.5) +\n    geom_rect(aes(xmin = 1.5, xmax = 2.5, ymin = -1.5, ymax = 1.5), color = \"black\", linetype = \"dashed\", fill = NA) +\n    annotate(\"text\", x = 1.3, y = 0, label = \"Input\\nStimulus\", angle = 90) +\n    annotate(\"text\", x = 3.5, y = -2, label = \"Associative Learning Model - Activation and Learning\", size = 5, hjust = 0.5) +\n    theme_void() +\n    xlim(0, 7) +\n    theme(legend.position = \"none\")\n\n\n\nCodedata &lt;- data.frame(\n    x = c(rep(2, 3), rep(4, 2), rep(6, 3)),\n    y = c(-1, 0, 1, 0, 0, -1, 0, 1),\n    label = c(\"e^{-γ(X-X_i)^2}\", \"Σ\", \"e^{-γ(Z-Y_j)^2}\", \"w_{ji}\", \"a_i(X)\", \"w_{ji}(t+1)\", \"O_j(X)\", \"Y_j\"),\n    shape = c(\"circle\", \"circle\", \"circle\", \"square\", \"square\", \"circle\", \"circle\", \"circle\"),\n    text_y = c(0.15, 0.15, 0.15, 0, 0, 0.15, 0.15, 0.15)\n)\n\ndata$shape &lt;- factor(data$shape, levels = c(\"circle\", \"square\"))\n\nggplot(data, aes(x = x, y = y)) +\n    geom_point(aes(shape = shape), size = 8) +\n    geom_text(aes(label = label, y = y + text_y), size = 5) +\n    scale_shape_manual(values = c(\"circle\" = 19, \"square\" = 15)) +\n    geom_segment(aes(x = 2, xend = 4, y = 1, yend = 0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 2, xend = 4, y = -1, yend = -0.2), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 4, xend = 6, y = 0, yend = 1), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    geom_segment(aes(x = 4, xend = 6, y = 0, yend = -1), arrow = arrow(type = \"closed\", length = unit(0.2, \"inches\"))) +\n    annotate(\"text\", x = 3, y = 1.5, label = \"Input Layer\\nGaussian Activation\\na_i(X)=e^{-γ(X-X_i)^2}\", hjust = 0.5) +\n    annotate(\"text\", x = 5, y = 1.5, label = \"Output Layer\\nO_j(X)=Σ w_{ji} a_i(X)\\nm(X)=Σ Y_j P[Y_j | X]\", hjust = 0.5) +\n    annotate(\"text\", x = 5, y = -1.5, label = \"Learning\\nw_{ji}(t+1)=w_{ji}(t)+α{f_j[Z(t)]-O_j[X(t)]}a_i[X(t)]\\nf_j(Z)=e^{-γ(Z-Y_j)^2}\", hjust = 0.5) +\n    theme_void() +\n    xlim(0, 8) +\n    theme(legend.position = \"none\")\n\n\n\n\n\nCode# Load the required libraries\nlibrary(ggplot2)\nlibrary(ggpp)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Create the Gaussian curve plots for each input node\ninput_node_1 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 1, sd = .4)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\ninput_node_2 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 2, sd = .4)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\ninput_node_3 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = .4)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\n# Create the main plot\nmain_plot &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"plot_npc\", npcx = 0.2, npcy = 0.8, label = input_node_1) +\n  annotate(\"plot_npc\", npcx = 0.5, npcy = 0.8, label = input_node_2) +\n  annotate(\"plot_npc\", npcx = 0.8, npcy = 0.8, label = input_node_3) +\n  annotate(\"text\", x = 0.2, y = 0.7, label = \"Input Node 1\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"Input Node 2\") +\n  annotate(\"text\", x = 0.8, y = 0.7, label = \"Input Node 3\") +\n  annotate(\"text\", x = 0.5, y = 0.6, label = \"Input Layer\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.5, y = 0.3, label = \"Output Layer\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.5, y = 0.1, label = \"ALM + EXAM Response\", fontface = \"bold\")\n\n# Print the main plot\nprint(main_plot)\n\n\n\n\n\n# Create the Gaussian curve plots for each input node\ninput_node_1 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\ninput_node_2 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 2.2, sd = 1)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\ninput_node_3 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 4.3, sd = .1)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0))\n\n# Create the main plot\nmain_plot &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"plot_npc\", npcx = 0.2, npcy = 0.8, label = input_node_1, width = 0.1, height = 0.1) +\n  annotate(\"plot_npc\", npcx = 0.5, npcy = 0.8, label = input_node_2, width = 0.1, height = 0.1) +\n  annotate(\"plot_npc\", npcx = 0.8, npcy = 0.8, label = input_node_3, width = 0.1, height = 0.1) +\n  annotate(\"text\", x = 0.2, y = 0.9, label = \"Input Node 1\") +\n  annotate(\"text\", x = 0.5, y = 0.9, label = \"Input Node 2\") +\n  annotate(\"text\", x = 0.8, y = 0.9, label = \"Input Node 3\") +\n  annotate(\"text\", x = 0.5, y = 0.6, label = \"Input Layer\", fontface = \"bold\") +\n  annotate(\"rect\", xmin = 0.1, xmax = 0.3, ymin = 0.4, ymax = 0.5, fill = \"grey80\") +\n  annotate(\"rect\", xmin = 0.4, xmax = 0.6, ymin = 0.4, ymax = 0.5, fill = \"grey80\") +\n  annotate(\"rect\", xmin = 0.7, xmax = 0.9, ymin = 0.4, ymax = 0.5, fill = \"grey80\") +\n  annotate(\"text\", x = 0.5, y = 0.3, label = \"Output Layer\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.5, y = 0.1, label = \"ALM + EXAM Response\", fontface =\"bold\")\nprint(main_plot)\n\n\n# Create the Gaussian curve plots for each input node\ninput_node_1 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = .41)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  ggtitle(\"Input Node 1\")\n\ninput_node_2 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 2, sd = .41)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  ggtitle(\"Input Node 2\")\n\ninput_node_3 &lt;- ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = .41)) +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  ggtitle(\"Input Node 3\")\n\n# Create the output nodes\noutput_node_1 &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"rect\", xmin = -1, xmax = 1, ymin = -1, ymax = 1, fill = \"blue\") +\n  ggtitle(\"Output Node 1\")\n\noutput_node_2 &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"rect\", xmin = -1, xmax = 1, ymin = -1, ymax = 1, fill = \"blue\") +\n  ggtitle(\"Output Node 2\")\n\n# Combine the input and output nodes using patchwork\ninput_layer &lt;- input_node_1 + input_node_2 + input_node_3\noutput_layer &lt;- output_node_1 + output_node_2\n\n# Create the connection matrix\nconnection_matrix &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"segment\", x = 0.2, xend = 0.8, y = 0.8, yend = 0.2, arrow = arrow()) +\n  annotate(\"segment\", x = 0.5, xend = 0.8, y = 0.8, yend = 0.2, arrow = arrow()) +\n  annotate(\"segment\", x = 0.8, xend = 0.8, y = 0.8, yend = 0.2, arrow = arrow())\n\n# Combine the input layer, connection matrix, and output layer\nmain_plot &lt;- input_layer / connection_matrix / output_layer\n\n# Print the main plot\nprint(main_plot)\n\n\n\n\n\n\n# Function to create an input node\ncreate_input_node &lt;- function(mean, position) {\n  ggplot(data.frame(x = seq(-3, 3, length.out = 100)), aes(x)) +\n    stat_function(fun = dnorm, args = list(mean = mean, sd = .3)) +\n    theme_void() +\n    theme(plot.margin = margin(0, 0, 0, 0)) +\n    coord_cartesian(xlim = c(0, 3), ylim = c(0, 0.5)) + coord_flip()+\n    labs(title = paste(\"Input Node\", position)) +\n    theme(plot.title = element_text(hjust = 0.5))\n}\n\n# Function to create an output node\ncreate_output_node &lt;- function(position) {\n  ggplot() +\n    theme_void() +\n    theme(plot.margin = margin(0, 0, 0, 0)) +\n    annotate(\"rect\", xmin = -1, xmax = 1, ymin = -1, ymax = 1, fill = \"grey80\") +\n    labs(title = paste(\"Output Node\", position)) +\n    theme(plot.title = element_text(hjust = 0.5))\n}\n\ncreate_input_layer &lt;- function(n) {\n  input_plots &lt;- map(1:n, ~create_gaussian_plot(.x))\n  input_layer &lt;- wrap_plots(input_plots, ncol = 1) + coord_flip()\n  return(input_layer)\n}\n\n\n\n# Create the input nodes\ninput_nodes &lt;- map(1:3, ~create_input_node(.x, .x))\n\n# Create the output nodes\noutput_nodes &lt;- map(1:4, ~create_output_node(.x))\n\n## Create the connection matrix\nconnection_matrix &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"segment\", x = 1, xend = 2, y = rep(1:3, each = 4), yend = rep(1:4, times = 3), arrow = arrow())\n# Combine the plots\ninput_layer &lt;- wrap_plots(input_nodes,byrow=FALSE,ncol=1)\noutput_layer &lt;- wrap_plots(output_nodes, ncol = 1)\nmain_plot &lt;- input_layer + connection_matrix + output_layer + plot_layout(ncol = 3)\n\n# Print the main plot\nprint(main_plot)\n\n\n\n\n\ncombined_plot &lt;- input_layer + connection_matrix + output_layer +\n  plot_layout(ncol = 3)\n\nprint(combined_plot)\n\n\n\nprint(input_layer)\n\n\n\nCode# Load the required libraries\nlibrary(ggplot2)\nlibrary(ggpp)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Function to create the input layer\ncreate_input_layer &lt;- function(n_nodes, stimulus = 0, gamma = 5) {\n  # Create a data frame with the x values and node means\n  df &lt;- expand.grid(x = seq(-3, 3, length.out = 100),\n                    mean = seq(-2, 1.5, length.out = n_nodes)) %&gt;% \n    mutate(noisyMean=map(mean,~dnorm(.x,mean,sd=.5)))\n  \n  df &lt;- df %&gt;%\n    mutate(distance = abs(x - stimulus),\n           noise = rnorm(n(), sd = 0.05),\n           max_activation = exp(-gamma * distance^2) + noise,\n           y = max_activation * exp(-gamma * (x - mean)^2))\n  \n  # Create the plot\n  ggplot(df, aes(x, y, group = mean)) +\n    geom_line() +\n    geom_smooth()+\n    coord_cartesian(xlim = c(-2, 2), ylim = c(0, 1)) +\n    labs(title = \"Input Layer\") +\n    theme(plot.title = element_text(hjust = 0.5))\n}\n\n# Function to create an output node\ncreate_output_node &lt;- function(position) {\n  ggplot() +\n    theme_void() +\n    theme(plot.margin = margin(0, 0, 0, 0)) +\n    annotate(\"rect\", xmin = -1, xmax = 1, ymin = -1, ymax = 1, fill = \"grey80\") +\n    labs(title = paste(\"Output Node\", position)) +\n    theme(plot.title = element_text(hjust = 0.5))\n}\n\n# Create the input layer\ninput_layer &lt;- create_input_layer(3)\n\n# Create the output nodes\noutput_nodes &lt;- map(1:4, ~create_output_node(.x))\n\n# Create the connection matrix\nconnection_matrix &lt;- ggplot() +\n  theme_void() +\n  theme(plot.margin = margin(0, 0, 0, 0)) +\n  annotate(\"segment\", x = 1, xend = 2, y = 1:3, yend = rep(1:4, each = 3), arrow = arrow())\n\n# Combine the plots\noutput_layer &lt;- wrap_plots(output_nodes, ncol = 1)\nmain_plot &lt;- input_layer + connection_matrix + output_layer + plot_layout(ncol = 3)\n\n# Print the main plot\nprint(main_plot)\n\n\n\nCode# Load the visNetwork package\nlibrary(visNetwork)\n\n# Define the nodes\nnodes &lt;- data.frame(\n  id = c(\"InputLayer\", \"I1\", \"I2\", \"I3\", \"OutputLayer\", \"O1\", \"O2\", \"O3\", \"O4\", \"ALM\", \"EXAM\"),\n  label = c(\"Input Layer\", \"Input Node 1\", \"Input Node 2\", \"Input Node 3\", \"Output Layer\", \"Output Node 1\", \"Output Node 2\", \"Output Node 3\", \"Output Node 4\", \"ALM Response\", \"EXAM Response\"),\n  shape = c(\"box\", \"image\", \"image\", \"image\", \"box\", \"circle\", \"circle\", \"circle\", \"circle\", \"box\", \"box\"),\n  image = c(NA, \"gaussian_curve_input_node_1.png\", \"gaussian_curve_input_node_2.png\", \"gaussian_curve_input_node_3.png\", NA, NA, NA, NA, NA, NA, NA)\n)\n\n# Define the edges\nedges &lt;- data.frame(\n  from = c(\"InputLayer\", \"I1\", \"I1\", \"I1\", \"I1\", \"I2\", \"I2\", \"I2\", \"I2\", \"I3\", \"I3\", \"I3\", \"I3\", \"OutputLayer\", \"OutputLayer\", \"ALM\", \"EXAM\"),\n  to = c(\"I1\", \"O1\", \"O2\", \"O3\", \"O4\", \"O1\", \"O2\", \"O3\", \"O4\", \"O1\", \"O2\", \"O3\", \"O4\", \"ALM\", \"EXAM\", \"EXAM\", \"ALM\"),\n  label = c(\"\", \"w1\", \"w2\", \"w3\", \"w4\", \"w5\", \"w6\", \"w7\", \"w8\", \"w9\", \"w10\", \"w11\", \"w12\", \"\", \"\", \"Pure ALM Model\", \"ALM with EXAM Response Component\")\n)\n\n# Create the network diagram\nvisNetwork(nodes, edges, width = \"100%\") %&gt;%\n  visNodes(shapeProperties = list(useBorderWithImage = TRUE)) %&gt;%\n  visLayout(randomSeed = 2)\n\n\n\n\n\n# Define the nodes\nnodes &lt;- data.frame(\n  id = c(\"InputLayer\", \"I1\", \"I2\", \"I3\", \"OutputLayer\", \"O1\", \"O2\", \"O3\", \"O4\", \"ALM\", \"EXAM\"),\n  label = c(\"Input Layer\", \"&lt;img src='gaussian_curve_input_node_1.png' /&gt;\", \"&lt;img src='gaussian_curve_input_node_2.png' /&gt;\", \"&lt;img src='gaussian_curve_input_node_3.png' /&gt;\", \"Output Layer\", \"Output Node 1\", \"Output Node 2\", \"Output Node 3\", \"Output Node 4\", \"ALM Response\", \"EXAM Response\"),\n  shape = c(\"box\", \"circle\", \"circle\", \"circle\", \"box\", \"circle\", \"circle\", \"circle\", \"circle\", \"box\", \"box\")\n)\n\n# Define the edges\nedges &lt;- data.frame(\n  from = c(\"InputLayer\", \"I1\", \"I1\", \"I1\", \"I1\", \"I2\", \"I2\", \"I2\", \"I2\", \"I3\", \"I3\", \"I3\", \"I3\", \"OutputLayer\", \"OutputLayer\", \"ALM\", \"EXAM\"),\n  to = c(\"I1\", \"O1\", \"O2\", \"O3\", \"O4\", \"O1\", \"O2\", \"O3\", \"O4\", \"O1\", \"O2\", \"O3\", \"O4\", \"ALM\", \"EXAM\", \"EXAM\", \"ALM\"),\n  label = c(NA, \"w1\", \"w2\", \"w3\", \"w4\", \"w5\", \"w6\", \"w7\", \"w8\", \"w9\", \"w10\", \"w11\", \"w12\", NA, NA, \"Pure ALM Model\", \"ALM with EXAM Response Component\")\n)\n\n# Create the network diagram\nvisNetwork(nodes, edges, width = \"100%\") %&gt;%\n  visNodes(shapeProperties = list(useBorderWithImage = TRUE)) %&gt;%\n  visLayout(randomSeed = 2)\n\n\n\nCode# Load the DiagrammeR package\nlibrary(DiagrammeR)\n# Define the graph\ngrViz(\"\n  digraph ALM_EXAM {\n\n    # Graph attributes\n    graph [overlap = true, fontsize = 10]\n\n    # Node definitions\n    node [shape = box, fontname = Helvetica]\n    InputLayer [label = 'Input Layer']\n    OutputLayer [label = 'Output Layer']\n    ALM [label = 'ALM Response']\n    EXAM [label = 'EXAM Response']\n\n    node [shape = circle, fixedsize = true, width = 0.9]\n    I1 [label = 'Input Node 1']\n    I2 [label = 'Input Node 2']\n    I3 [label = 'Input Node 3']\n    O1 [label = 'Output Node 1']\n    O2 [label = 'Output Node 2']\n    O3 [label = 'Output Node 3']\n    O4 [label = 'Output Node 4']\n\n    # Edge definitions\n    InputLayer -&gt; I1\n    InputLayer -&gt; I2\n    InputLayer -&gt; I3\n    I1 -&gt; O1 [label = 'w1']\n    I1 -&gt; O2 [label = 'w2']\n    I1 -&gt; O3 [label = 'w3']\n    I1 -&gt; O4 [label = 'w4']\n    I2 -&gt; O1 [label = 'w5']\n    I2 -&gt; O2 [label = 'w6']\n    I2 -&gt; O3 [label = 'w7']\n    I2 -&gt; O4 [label = 'w8']\n    I3 -&gt; O1 [label = 'w9']\n    I3 -&gt; O2 [label = 'w10']\n    I3 -&gt; O3 [label = 'w11']\n    I3 -&gt; O4 [label = 'w12']\n    OutputLayer -&gt; O1\n    OutputLayer -&gt; O2\n    OutputLayer -&gt; O3\n    OutputLayer -&gt; O4\n    OutputLayer -&gt; ALM\n    OutputLayer -&gt; EXAM\n    ALM -&gt; EXAM [label = 'Pure ALM Model']\n    EXAM -&gt; ALM [label = 'ALM with EXAM Response Component']\n  }\n\")\n\n\n\n\n\n\nCode# Load the required packages\nlibrary(ggplot2)\nlibrary(png)\n\n# Define the Gaussian function\ngaussian &lt;- function(x, mean, sd) {\n  1/(sd*sqrt(2*pi)) * exp(-1/2 * ((x - mean)/sd)^2)\n}\n\n# Define the central values for the input nodes\ncentral_values &lt;- c(1, 2, 3)\n\n# Define the input stimulus value\ninput_stimulus &lt;- 2\n\n# Generate a Gaussian curve image for each input node\nfor (i in seq_along(central_values)) {\n  # Define the mean and standard deviation for the Gaussian curve\n  mean &lt;- central_values[i]\n  sd &lt;- 1\n  # Generate the x values\n  x &lt;- seq(mean - 3*sd, mean + 3*sd, length.out = 100)\n  # Generate the y values\n  y &lt;- gaussian(x, mean, sd)\n  # Create the plot\n  p &lt;- ggplot(data.frame(x, y), aes(x, y)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"X\", y = \"Activation\", title = paste(\"Input Node\", i))\n  # Save the plot as a PNG image\n  ggsave(paste0(\"gaussian_curve_input_node_\", i, \".png\"), plot = p, width = 4, height = 3)\n}\n\n\n\nCodegrViz(\"digraph causal {\n                        # Nodes\n                        node [imagescale=true,shape = reactangle, fontname = Arial, style = filled]\n                        iv   [label = 'TRT', fillcolor = '#7FC97F']\n                        me   [label = 'Mediator', shape = ellipse]\n                        dv   [label = 'DLQI', fillcolor = '#7FC97F']\n                        \n                        # Edges\n                        edge [color = black, arrowhead = normal]\n                        rankdir = LR\n                        iv -&gt; me\n                        iv -&gt; dv [label = 'DIRECT', fontcolor = '#7FC97F', color = '#7FC97F']\n                        me -&gt; dv\n                        # Graph\n                        graph [overlap = true, fontsize = 10]\n                      }\")\n\n\ngrViz(\"digraph causal {\n                        # Nodes\n                        node [shape = reactangle, fontname = Arial, style = filled]\n                        iv   [image='gaussian_curve_input_node_1.png',label = '', fillcolor = '#7FC97F']\n                        me   [label = 'Mediator', shape = ellipse]\n                        dv   [label = 'DLQI', fillcolor = '#7FC97F']\n                        \n                        # Edges\n                        edge [color = black, arrowhead = normal]\n                        rankdir = LR\n                        iv -&gt; me\n                        iv -&gt; dv [label = 'DIRECT', fontcolor = '#7FC97F', color = '#7FC97F']\n                        me -&gt; dv\n                        # Graph\n                        graph [overlap = true, fontsize = 10]\n                      }\")\n\n\n\nCode# grViz('digraph structs {\n#     node [shape=plaintext];\n# \n#     struct1 [label=&lt;&lt;TABLE&gt;\n#       &lt;TR&gt;&lt;TD&gt;&lt;IMG SRC=\"gaussian_curve_input_node_1.png\"/&gt;&lt;/TD&gt;&lt;/TR&gt;\n#       &lt;TR&gt;&lt;TD&gt;caption&lt;/TD&gt;&lt;/TR&gt;\n#     &lt;/TABLE&gt;&gt;];\n# }')\n\n\nsvgDataUrl = \"data:image/svg+xml;base64,\"\ngrViz('\ndigraph {\n  node[imagescale=true, shape=circle, width=4, height=4,fontname=\"sans-serif\",penwidth=0]\n  a[label=&lt;&lt;FONT POINT-SIZE=\"10\"&gt;Transaction&lt;/FONT&gt;&lt;BR/&gt;&lt;FONT POINT-SIZE=\"36\"&gt;&lt;B&gt;TRNA&lt;/B&gt;&lt;/FONT&gt;&gt;,image=\"${svgDataUrl}\"]\n  b[label=&lt;&lt;FONT POINT-SIZE=\"10\"&gt;Transaction&lt;/FONT&gt;&lt;BR/&gt;&lt;FONT POINT-SIZE=\"18\"&gt;&lt;B&gt;TRNB&lt;/B&gt;&lt;/FONT&gt;&gt;,image=\"${svgDataUrl}\"]\n  c[label=&lt;&lt;FONT POINT-SIZE=\"10\"&gt;Transaction&lt;/FONT&gt;&lt;BR/&gt;&lt;FONT POINT-SIZE=\"24\"&gt;&lt;B&gt;TRNC&lt;/B&gt;&lt;/FONT&gt;&gt;,image=\"${svgDataUrl}\"]\n  a -&gt; b\n  a -&gt; c\n}')\n\ngrViz('digraph { a[image=\\\"gaussian_curve_input_node_1\\\"]; }\", {\n  images: [{ href: \"gaussian_curve_input_node_1.png\", width: \"400px\", height: \"300px\" }]\n}');\n\ngrViz('\ndigraph g{\n  node[imagescale=true, shape=circle, width=14, height=14,fontname=\"sans-serif\",penwidth=0]\n  I1[image=\"gaussian_curve_input_node_1.png\", label=\"\"];\n}')\n\ngrViz('digraph {\n    ratio=\"fill\";\n    size=\"10,10!\";\n    margin=\"0,0\";\n\n    node [shape=plain];\n    root [label=&lt;&lt;TABLE border=\"0\"&gt;&lt;TR&gt;&lt;TD&gt;&lt;IMG SRC=\"gaussian_curve_input_node_1\"/&gt;&lt;/TD&gt;&lt;/TR&gt;\n                                   &lt;TR&gt;&lt;TD&gt;text under&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;&gt;];\n}')\n\n\n\nCodegrViz(\"\n  digraph ALM_EXAM {\n\n    # Graph attributes\n    graph [overlap = true, fontsize = 10, rankdir = LR]\n\n    # Node definitions\n    node [shape = box, fontname = Helvetica]\n    InputLayer [label = 'Input Layer']\n    OutputLayer [label = 'Output Layer']\n    ALM [label = 'ALM Response']\n    EXAM [label = 'EXAM Response']\n\n    node [shape = none, label = '']\n    I1 [image = 'gaussian_curve_input_node_1.png', label='']\n    I2 [image = 'gaussian_curve_input_node_2.png', label='']\n    I3 [image = 'gaussian_curve_input_node_3.png', label='']\n\n    node [shape = circle, fixedsize = true, width = 0.9]\n    O1 [label = 'Output Node 1']\n    O2 [label = 'Output Node 2']\n    O3 [label = 'Output Node 3']\n    O4 [label = 'Output Node 4']\n\n    # Edge definitions\n    InputLayer -&gt; I1\n    InputLayer -&gt; I2\n    InputLayer -&gt; I3\n    I1 -&gt; O1 [label = 'w1']\n    I1 -&gt; O2 [label = 'w2']\n    I1 -&gt; O3 [label = 'w3']\n    I1 -&gt; O4 [label = 'w4']\n    I2 -&gt; O1 [label = 'w5']\n    I2 -&gt; O2 [label = 'w6']\n    I2 -&gt; O3 [label = 'w7']\n    I2 -&gt; O4 [label = 'w8']\n    I3 -&gt; O1 [label = 'w9']\n    I3 -&gt; O2 [label = 'w10']\n    I3 -&gt; O3 [label = 'w11']\n    I3 -&gt; O4 [label = 'w12']\n    OutputLayer -&gt; O1\n    OutputLayer -&gt; O2\n    OutputLayer -&gt; O3\n    OutputLayer -&gt; O4\n    OutputLayer -&gt; ALM\n    OutputLayer -&gt; EXAM\n    ALM -&gt; EXAM [label = 'Pure ALM Model']\n    EXAM -&gt; ALM [label = 'ALM with EXAM Response Component']\n  }\n\")\n\n\n\nCodeDiagrammeR::grViz(\"digraph {\n\ngraph [layout = dot, rankdir = LR]\n\n# define the global styles of the nodes. We can override these in box if we wish\nnode [shape = rectangle, style = filled, fillcolor = Linen]\n\ndata1 [label = 'Dataset 1', shape = folder, fillcolor = Beige]\ndata2 [label = 'Dataset 2', shape = folder, fillcolor = Beige]\nprocess [label =  'Process \\n Data']\nstatistical [label = 'Statistical \\n Analysis']\nresults [label= 'Results']\n\n# edge definitions with the node IDs\n{data1 data2}  -&gt; process -&gt; statistical -&gt; results\n}\")\n\n\n\n\n\n\n\n\n\ngraph LR\n  subgraph InputLayer\n    I1[\"Input Node 1\"]\n    I2[\"Input Node 2\"]\n    I3[\"Input Node 3\"]\n  end\n  subgraph OutputLayer\n    O1[\"Output Node 1\"]\n    O2[\"Output Node 2\"]\n    O3[\"Output Node 3\"]\n    O4[\"Output Node 4\"]\n  end\n  I1 --&gt;|\"w1\"| O1\n  I1 --&gt;|\"w2\"| O2\n  I1 --&gt;|\"w3\"| O3\n  I1 --&gt;|\"w4\"| O4\n  I2 --&gt;|\"w5\"| O1\n  I2 --&gt;|\"w6\"| O2\n  I2 --&gt;|\"w7\"| O3\n  I2 --&gt;|\"w8\"| O4\n  I3 --&gt;|\"w9\"| O1\n  I3 --&gt;|\"w10\"| O2\n  I3 --&gt;|\"w11\"| O3\n  I3 --&gt;|\"w12\"| O4\n  style InputLayer fill:#99cc99,stroke:#333,stroke-width:2px\n  style OutputLayer fill:#cc99cc,stroke:#333,stroke-width:2px\n  IS[\"Input Stimulus (X)\"] --&gt; InputLayer\n  InputLayer --&gt;|\"Gaussian Activation (a_i)\"| OutputLayer\n  OutputLayer --&gt; ALM[\"ALM Response (m(X))\"]\n  OutputLayer --&gt; EXAM[\"EXAM Response (E[Y | X_i])\"]\n  ALM --&gt;|\"Pure ALM Model\"| EXAM\n  EXAM --&gt;|\"ALM with EXAM Response Component\"| ALM\n  linkStyle 0 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 1 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 2 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 3 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 4 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 5 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 6 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 7 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 8 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 9 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 10 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 11 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 12 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 13 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 14 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 15 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 16 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 17 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 0 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 1 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 2 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 3 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 4 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 5 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 6 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 7 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 8 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 9 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 10 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 11 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 12 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 13 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 14 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 15 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 16 stroke:#2ecd71,stroke-width:2px;\n  linkStyle 17 stroke:#2ecd71,stroke-width:2px;"
  },
  {
    "objectID": "Misc/Visuals_Interactives/model_viz.html#p5-sketching",
    "href": "Misc/Visuals_Interactives/model_viz.html#p5-sketching",
    "title": "Model Visualization",
    "section": "P5 sketching",
    "text": "P5 sketching\n\nCodeP5 = require(\"p5\")\nfunction* createSketch(sketch) {\n  const element = DOM.element('div');\n  yield element;\n  const instance = new P5(sketch, element, true);\n  try {\n    while (true) {\n      yield element;\n    }\n  } finally {\n    instance.remove();\n  }\n}\ncreateSketch(s =&gt; {\n  \n    s.setup = function() {\n      s.createCanvas(746, 300);\n      s.textFont('Courgette');\n      s.textStyle(s.BOLD);\n      s.textAlign(s.CENTER, s.CENTER)\n\n      s.button = s.createButton('clear');\n      s.button.mousePressed(s.clearCanvas);\n        s.text('Click and drag to draw', s.width/2, s.height/10);\n\n    };\n    s.draw = function() {\n    if (s.mouseIsPressed) {\n    s.fill(0);\n    s.ellipse(s.mouseX, s.mouseY, 10, 10);\n    } else {\n   //s.fill(255);\n    }\n  // add text input\n\n    };\n\n  // add button to clear canvas\n  s.clearCanvas = function() {\n    s.clear();\n  };\n  // add text\n  // add slider\n  }\n)"
  },
  {
    "objectID": "Misc/Visuals_Interactives/ojs_explore.html",
    "href": "Misc/Visuals_Interactives/ojs_explore.html",
    "title": "OJS data exploration",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,here)\n\nd &lt;- readRDS(here(\"data/dPrune-01-19-23.rds\"))\n\n# Prepare the data for analysis\ndtest &lt;- d %&gt;%\n    filter(expMode %in% c(\"test-Nf\", \"test-train-nf\")) %&gt;%\n    group_by(id, lowBound) %&gt;%\n    mutate(nBand = n(), band = bandInt, id = factor(id)) %&gt;%\n    group_by(id) %&gt;%\n    mutate(nd = n_distinct(lowBound))\ndtest &lt;- dtest %&gt;%\n    group_by(id, lowBound) %&gt;%\n    filter(nBand &gt;= 5 & nd == 6)\ndtest &lt;- dtest %&gt;%\n    group_by(id) %&gt;%\n    filter(!id %in% unique(dtest$id[dtest$nBand &lt; 5]))\n\ndtestAgg &lt;- dtest %&gt;%\n    group_by(id, condit, catOrder, feedbackType, vb, band, lowBound, highBound, input) %&gt;%\n    mutate(vxCapped = ifelse(vx &gt; 1600, 1600, vx)) %&gt;%\n    summarise(\n        vxMean = mean(vx), devMean = mean(dist), vxMed = median(vx), devMed = median(dist),\n        vxMeanCap = mean(vxCapped), .groups = \"keep\"\n    )\nds1 &lt;- d %&gt;%\n    filter(expMode %in% c(\"train\", \"train-Nf\", \"test-Nf\", \"test-train-nf\")) %&gt;%\n    filter(!id %in% unique(dtest$id[dtest$nBand &lt; 5]), vx&lt;1500) %&gt;%\n    select(id, condit, catOrder, feedbackType, expMode, trial, gt.train, vb, band, bandInt, lowBound, highBound, input, vx, dist, vxb)\n\n\nojs_define(dso=ds1)\n\n\n\nCodeimport { aq, op } from \"@uwdata/arquero\"\n\n//data = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n\n\n//ds1=transpose(ds1)\n\nds=transpose(dso)\n\nPlot.plot({\n  facet: {\n    data: ds,\n    x: \"condit\",\n    y: \"bandInt\",\n    marginRight: 80\n  },\n  marks: [\n    Plot.frame(),\n    Plot.rectY(ds, \n      Plot.binX(\n        {y: \"count\"}, \n        {x: \"vx\", thresholds: 50, fill: \"bandInt\"} // thresholds = number of bins\n      )\n    ),\n    Plot.tickX(ds, \n      Plot.groupZ(\n        {x: \"count\"}, \n        {x: \"vx\",\n         z: d =&gt; ds.condit+ ds.bandInt,\n         stroke: \"#333\",\n         strokeWidth: 2\n        }\n      )\n    )\n  ],\n    x: {label: \"Vx\", domain: [0, 1800],grid: true},\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodePlot.plot({\n  marks: [\n    Plot.line(ds, {\n      x: \"gt.train\",      // feature for the x channel\n      y: \"vx\",     // feature for the y channel\n      fill:\"condit\",\n      stroke: \"vb\",     \n    }),\n  ],\n  x: {label: \"Trial Number\"},\n  y: {label: \"Vx\", domain: [0, 1800],grid: true},\n  color: {legend: true, scheme: \"Turbo\",type: \"categorical\"},\n  width: 400,\n  height: 400\n});\n\n\n\n\n\n\n\nCodePlot.plot({\n  grid: true,\n  marks: [\n    Plot.rectY(ds, Plot.binX({y: \"count\"}, {x: \"vx\", fill: \"condit\", fy: \"condit\"})),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\nCodePlot.rectY(ds, Plot.binX({y: \"count\"}, {x: \"condit\", fill: \"condit\"})).plot()\n\n\n\n\n\n\n\nCodePlot.line(ds, {x: \"gt.train\", y: \"vx\",fill:\"condit\"}).plot({y: {grid: true}})\n\n\n\n\n\n\n\nCoded = aq.from(ds)\nd\n  .groupby(\"condit\", \"vb\", \"feedbackType\",\"expMode\",\"catOrder\")\n  .rollup({mean_vx: d =&gt; op.mean(d.vx)}, {mean_dist: d =&gt; op.mean(d.dist)})\n  .view(15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodedt = aq.from(ds)\ndtAgg = dt\n  .filter(dt =&gt; dt.expMode === \"train\")\n  .groupby(\"condit\", \"vb\",\"gt.train\")\n  .rollup({mean_vx: dt =&gt; op.mean(dt.vx)}, {mean_dist: dt =&gt; op.mean(dt.dist)})\n\n\nPlot.line(dtAgg, {x: \"gt.train\", y: \"mean_vx\",fill:\"vb\"}).plot({y: {grid: true}})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodePlot.plot({ \n    grid: true, \n    marginRight: 60,\n    facet: {\n        data: dtAgg,\n        x: \"condit\",\n        y: \"vb\",\n        marginRight: 80\n    },\n    marks: [\n        Plot.frame(),\n        Plot.lineY(dtAgg, {\n            x: \"gt.train\", \n            y: \"mean_vx\",\n            fill:\"vb\"\n            })\n    ]\n})\n\n\n\n\n\n\n\nCodedtAgg\n  .view(25)"
  },
  {
    "objectID": "Misc/benchmarks.html",
    "href": "Misc/benchmarks.html",
    "title": "Benchmarking",
    "section": "",
    "text": "https://cran.r-project.org/web/packages/brms/vignettes/brms_threading.html\nhttps://discourse.mc-stan.org/t/using-the-apple-m1-gpus-question-from-a-noob/23089/39?page=2\n\nCodepacman::p_load(tidyverse,tidybayes,brms,broom,broom.mixed,lme4,here,knitr,gt,gghalves,patchwork,ggdist,microbenchmark)\ne1 &lt;- readRDS(here(\"data/e1_08-04-23.rds\"))\ntest &lt;- e1 |&gt; filter(expMode2 == \"Test\")  \n\noptions(mc.cores = 4, brms.backend = \"cmdstanr\")\n\nn_cores=4\nbayes_seed &lt;- 1234\nn_iter=20000\nn_threads=2\n\n\ngprior&lt;- c( prior(normal(800, 100), class = Intercept),\n    prior(normal(400, 10), class = sigma)\n  )\n\n\n##############\n\ntf3 &lt;- system.time(\nfit_int_norm &lt;- brm(vx ~ 1 + condit, \n  data = test,\n  family = gaussian(),\n  iter=n_iter,\n  prior = gprior,\n  silent=2,\n  cores=n_cores,\n  threads = threading(n_threads),\n  seed=bayes_seed\n)\n)\ntf3\n\ntf4 &lt;- system.time(\nfit_int_norm &lt;- brm(vx ~ 1 + condit +(1|bandInt) + (1 + bandInt|id),\n  data = test |&gt; filter(id %in% 1:15),\n  family = gaussian(),\n  iter=n_iter,\n  prior = gprior,\n  silent=2,\n  cores=n_cores,\n  threads = threading(n_threads),\n  seed=bayes_seed\n) )\ntf4\n\n\n#########\n\n\ntf &lt;- system.time(\nfit_int_norm &lt;- brm(vx ~ 1 + condit, \n  data = test,\n  family = gaussian(),\n  iter=n_iter,\n  prior = gprior,\n  silent=2,\n  cores=n_cores,\n  seed=bayes_seed\n)\n)\ntf\n\ntf2 &lt;- system.time(\nfit_int_norm &lt;- brm(vx ~ 1 + condit +(1|bandInt) + (1 + bandInt|id),\n  data = test |&gt; filter(id %in% 1:15),\n  family = gaussian(),\n  iter=n_iter,\n  prior = gprior,\n  silent=2,\n  cores=n_cores,\n  seed=bayes_seed\n)\n)\ntf2\n\n\n##########\n\n\ncat(paste0(\"int only gaussian = \",round(tf[\"elapsed\"],2),\" \\n \",\n          # \"foreach parallel time=\",round(tfP[\"elapsed\"],2),\" \\n \",\n           \"irt version = \",round(tf2[\"elapsed\"],2), \" \\n \",\n           \"int only threading = \",round(tf3[\"elapsed\"],2),\" \\n \",\n          \"irt  threading = \",round(tf4[\"elapsed\"],2),\" \\n \"\n            \n          ))\n\n\ndefault brms settings (2K iterations)\nint only gaussian = 6.85 irt version = 12.15\nint only gaussian = 7.51 irt version = 12\nint only gaussian = 6.67 irt version = 11.82 int only threading = 7.49 irt threading = 11.33\ndefault brms settings (4K iterations) - 2 threads\nint only gaussian = 8.1 irt version = 14.69 int only threading = 6.99 irt threading = 15.24\ndefault brms settings (4K iterations) - 4 threads\nbrms int only gaussian = 8.11 irt version = 15.05 int only threading = 7.28 irt threading = 16.95\ndefault brms settings (4K iterations) - 4 threads - 2 cores\nint only gaussian = 11.27 irt version = 21.72 int only threading = 8.39 irt threading = 22.67\ndefault brms settings (4K iterations) - 2 threads - 2 cores\nint only gaussian = 11.27 irt version = 21.81 int only threading = 9.38 irt threading = 20.63\ndefault brms settings (4K iterations) - 8 threads - 4 cores\nint only gaussian = 8.27 irt version = 15.4 int only threading = 7.69 irt threading = 21.28\nint only gaussian = 8.37 irt version = 15.18 int only threading = 8.02 irt threading = 21.47\ndefault brms settings (4K iterations) - 8 threads - 8 cores\nint only gaussian = 8.11 irt version = 14.88 int only threading = 7.92 irt threading = 20.72\ndefault brms settings (10K iterations) - 8 threads - 8 cores\nint only gaussian = 11.38 irt version = 23.64 int only threading = 11.58 irt threading = 37.1\nint only gaussian = 11.53 irt version = 23.76 int only threading = 12 irt threading = 37.45\ndefault brms settings (10K iterations) - 8 threads - 4 cores\nint only gaussian = 11.48 irt version = 23.29 int only threading = 10.91 irt threading = 36.54\ndefault brms settings (10K iterations) - 4 threads - 4 cores\nint only gaussian = 11.99 irt version = 24.25 int only threading = 10.13 irt threading = 29.13\ndefault brms settings (10K iterations) - 4 threads - 1 cores\nint only gaussian = 29.35 irt version = 66.61 int only threading = 18.97 irt threading = 71.21\ndefault brms settings (10K iterations) - 2 threads - 2 cores\nint only gaussian = 17.98 irt version = 38.31 int only threading = 13.87 irt threading = 37.78\ndefault brms settings (10K iterations) - 1 threads - 4 cores\nint only gaussian = 11.14 irt version = 22.89 int only threading = 12.25 irt threading = 27.03\nint only gaussian = 11.35 irt version = 22.99 int only threading = 12.28 irt threading = 27.38\ndefault brms settings (10K iterations) - 2 threads - 4 cores\nint only gaussian = 11.22 irt version = 23.07 int only threading = 9 irt threading = 22.34\nint only gaussian = 11.31 irt version = 23.14 int only threading = 9.49 irt threading = 22.21\ndefault brms settings (10K iterations) - 3 threads - 4 cores\nint only gaussian = 11.15 irt version = 22.88 int only threading = 9.4 irt threading = 26.75\nint only gaussian = 11 irt version = 22.88 int only threading = 9.5 irt threading = 25.7\n(20K iterations) - 1 threads - 4 cores\nint only gaussian = 18.24 irt version = 40.37 int only threading = 20.32 irt threading = 48.94\nint only gaussian = 17.8 irt version = 39.26 int only threading = 20.54 irt threading = 48.14\nint only gaussian = 18.12 irt version = 40.31\nint only threading = 20.6 irt threading = 40.02 - when I commented out the threading specification\n(20K iterations) - 2 threads - 4 cores\nint only gaussian = 17.82 irt version = 39.34 int only threading = 14.22 irt threading = 37.73\n(20K iterations) - adding condit factor - 2 threads - 4 cores\nint only gaussian = 7.78 irt version = 45.66 int only threading = 8.57 irt threading = 45.97\n(20K iterations) - adding condit factor - 3 threads - 4 cores\nint only gaussian = 8.58 irt version = 47.07 int only threading = 10.71 irt threading = 57.87\n(20K iterations) - condit factor and bandInt RF slope - 2 threads - 4 cores\nint only gaussian = 7.85 irt version = 80.71 int only threading = 8.69 irt threading = 88.3\n\nCodepacman::p_load(tidyverse,foreach,doParallel,future,furrr,here)\npurrr::walk(c(here(\"Functions/alm_functions.R\",\"Functions/Display_Functions.R\")),source)\n\nn_cores &lt;- parallel::detectCores()\n\nparam_grid &lt;- tibble(crossing(\n  c = seq(.5,5,.25),\n  lr = seq(0.01, 1,.1),\n  noise_sd = c(0,.0001,0.001, 0.01),\n  inNodes = c(5, 7,14,28),\n  outNodes = c(16, 32,64)\n))\nnrow(param_grid)\n\ngen_train &lt;- function(trainVec=c(5,6,7),trainRep=3,noise=0){\n  bandVec=c(0,100,350,600,800,1000,1200)\n  if(class(trainVec)==\"list\"){trainVec=unlist(trainVec)}\n  ts &lt;- rep(seq(1,length(trainVec)),trainRep)\n  noiseVec=rnorm(length(ts),mean=0)*noise\n  if(noise==0) {noiseVec=noiseVec*0}\n  tibble(trial=seq(1,length(ts)),input=trainVec[ts],vx=bandVec[trainVec[ts]]+noiseVec)\n}\nfit_alm &lt;- function(data, c, lr, noise_sd, inNodes, outNodes) {\n  mse_list &lt;- replicate(5, {\n    train_data &lt;- data[, c(\"trial\", \"input\", \"cor\")] %&gt;% rename(\"vx\" = cor)\n    sim_result &lt;- sim_train(\n      dat = train_data,\n      c = c,\n      lr = lr,\n      inNodes = inNodes,\n      outNodes = outNodes,\n      noise_sd = noise_sd\n    )\n    train_data$almTrain &lt;- sim_result$almTrain\n    mse &lt;- mean((data$vx - train_data$almTrain)^2)\n    mse\n  })\n  avg_mse &lt;- mean(mse_list)\n  return(avg_mse)\n}\n\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=8) %&gt;% mutate(cor=vx,err=(800-0)*exp(-.1*seq(1,n()))+0,vx=cor-err)\n\n\nfurrr::furrr_options(seed = TRUE)\nplan(multisession, workers = n_cores-1)\ntff &lt;- system.time({\n\nparam_grid &lt;- param_grid %&gt;% mutate(performance = future_map_dbl(seq_len(nrow(.)), function(idx) {\n    fit_alm(gt, c = c[idx], lr = lr[idx], noise_sd = noise_sd[idx], inNodes = inNodes[idx], outNodes = outNodes[idx])\n  },.options = furrr_options(seed = T)))\n  best_paramsF &lt;- param_grid %&gt;%\n    arrange((performance)) \n  bestF &lt;- head(best_paramsF,1)\n})\n\n\n# cluster &lt;- parallel::makeCluster(n_cores-1)                 \n# doParallel::registerDoParallel(cluster)\n# tfP &lt;- system.time({\n#   param_grid &lt;- param_grid %&gt;%\n#     mutate(performance = foreach(idx = seq_len(nrow(.)), .combine = c) %dopar% {\n#       fit_alm(gt, c = c[idx], lr = lr[idx], noise_sd = noise_sd[idx], inNodes = inNodes[idx], outNodes = outNodes[idx])\n#     })\n#   best_paramsP &lt;- param_grid %&gt;%\n#     arrange((performance)) \n#   bestP &lt;- head(best_paramsP,1)\n#   })\n# stopImplicitCluster()\n#   tfP\n\n\ntI &lt;- system.time({\ngt &lt;- gen_train(trainVec=c(5,6,7),trainRep=8) %&gt;% mutate(cor=vx,err=(600-0)*exp(-.1*seq(1,n()))+0,vx=cor-err)\n\nparam_grid &lt;- param_grid %&gt;%\n  mutate(performance = map_dbl(seq_len(nrow(.)), ~ {\n    fit_alm(gt, c = c[.x], lr = lr[.x], noise_sd = noise_sd[.x], inNodes = inNodes[.x], outNodes = outNodes[.x])\n  }))\nbest_paramsI &lt;- param_grid %&gt;%\n  arrange((performance)) \nbestI &lt;- head(best_paramsI,1)\n})\n\ntI\n\n\n\n\ncat(paste0(\"furr time=\",round(tff[\"elapsed\"],2),\" \\n \",\n          # \"foreach parallel time=\",round(tfP[\"elapsed\"],2),\" \\n \",\n           \" Standard Time=\",round(tI[\"elapsed\"],2)))\n\n\n4.2 -6.2 times faster with furr on m1\n\n\nRuns\nMachine\nCores\nMethod\nTime (seconds)\n\n\n\n9120\nM1\n9/10\nFurr\n9.56\n\n\n9120\nM1\n9/10\nStandard\n58.8\n\n\n912\nM1\n8/10\nStandard\n5.7\n\n\n912\nM1\n8/10\nParallel\n1.2\n\n\n912\nM1\n9/10\nFurr\n1.3\n\n\n912\n2015 iMac\n2/4\nStandard\n26.2\n\n\n912\n2015 iMac\n2/4\nParallel\n18.5\n\n\n912\n2015 iMac\n3/4\nFurr\n12.88\n\n\n912\nGTX\n3/4\nStandard\n25.32\n\n\n912\nGTX\n3/4\nFurr\n13.1\n\n\n\n\nFurrr and standard version both work equally fast, tested with up to 120 simulation repetitions\n\nCodelibrary(furrr)\nfurrr::furrr_options(seed = TRUE)\nplan(multisession, workers = parallel::detectCores())\n\nparmVec &lt;- tibble(crossing(c = c(0.1,.5), lr = c(0.4), noise = c(500), trainRep = c(20), lossFun = list(\"RMSE\", \"RMSE.blocked\"), simNum = 1:30))\ntf &lt;- system.time(\nsdpf &lt;- parmVec %&gt;% \n  mutate(d = future_pmap(list(c, lr, noise, trainRep), ~sim_data(c = ..1, lr = ..2, noise = ..3, trainRep = ..4),\n                         .options = furrr_options(seed = T)),\n                           almTrainDat = map(d, \"almTrain\"),\n                          almTestDat = map(d, \"almPred\"),\n                          examTestDat = map(d, \"examPred\"),\n                          td = map(trainRep, ~gen_train(trainRep = .)),\n                          fitO = map2(td, lossFun, ~wrap_optim(.x, .y)),\n                          fitG = map2(td, lossFun, ~wrap_grid(.x, .y)),\n                          cFitO = map_dbl(fitO, \"c\"),\n                          lrFitO = map_dbl(fitO, \"lr\"),\n                          optimValO = map_dbl(fitO, \"Value\"),\n                          cFitG = map_dbl(fitG, \"c\"),\n                          lrFitG = map_dbl(fitG, \"lr\"),\n                          optimValG = map_dbl(fitG, \"Value\"))\n)\ntf\n\nts &lt;- system.time(\nsdp &lt;- parmVec %&gt;% mutate(d = pmap(list(c, lr, noise, trainRep), ~sim_data(c = ..1, lr = ..2, noise = ..3, trainRep = ..4)),\n                          almTrainDat = map(d, \"almTrain\"),\n                          almTestDat = map(d, \"almPred\"),\n                          examTestDat = map(d, \"examPred\"),\n                          td = map(trainRep, ~gen_train(trainRep = .)),\n                          fitO = map2(td, lossFun, ~wrap_optim(.x, .y)),\n                          fitG = map2(td, lossFun, ~wrap_grid(.x, .y)),\n                          cFitO = map_dbl(fitO, \"c\"),\n                          lrFitO = map_dbl(fitO, \"lr\"),\n                          optimValO = map_dbl(fitO, \"Value\"),\n                          cFitG = map_dbl(fitG, \"c\"),\n                          lrFitG = map_dbl(fitG, \"lr\"),\n                          optimValG = map_dbl(fitG, \"Value\"))\n)\nts\n\n\nM1 Max times: tf user system elapsed 221.326 4.517 226.233\nts user system elapsed 221.140 4.288 225.330\nMac Pro times: tf user system elapsed 1125.102 76.859 1474.612\nts user system elapsed 1132.716 76.260 1493.154\nBenchmarking ALM Model Fit Functions\n\nCodepacman::p_load(tidyverse,data.table,microbenchmark())\n\n\nd &lt;- readRDS(here('dPrune-01-19-23.rds'))\n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\n# unique(dtest[dtest$nd==4,]$sbjCode) # 7 in wrong condition\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\n# for any id that has at least 1 nBand &gt;=5, remove all rows with that id. \ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,catOrder,feedbackType,vb,band,lowBound,highBound,input) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")\n\n# select first row for each id in d, then create histogram for nTrain\n#  d  %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ggplot(aes(nTrain)) + geom_histogram() + facet_wrap(~condit)\n  \nds &lt;- d %&gt;% filter(expMode %in% c(\"train\",\"train-Nf\",\"test-Nf\",\"test-train-nf\")) %&gt;% \nfilter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) %&gt;% \nselect(id,condit,catOrder,feedbackType,expMode,trial,gt.train,vb,band,bandInt,lowBound,highBound,input,vx,dist,vxb) \n\ndst &lt;- ds %&gt;% filter(expMode==\"train\",catOrder==\"orig\")\n\nvTrainTrial &lt;- dst %&gt;% filter(condit==\"Varied\",gt.train&lt;=84) %&gt;% group_by(gt.train,vb) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=5,labels=c(1:5)))\n\nbinTrainTrial &lt;- dst %&gt;% filter(gt.train&lt;=83) %&gt;% group_by(gt.train,vb,condit) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=6,labels=c(1:6)))\n\n\ntMax=84\nbandVec &lt;- rep(c(800,1000,1200),each=tMax/3)\nbandVec &lt;- bandVec[sample(1:length(bandVec),tMax,replace=FALSE)]\n\ntrainTrials &lt;- dst %&gt;% filter(gt.train&lt;=tMax) %&gt;% group_by(condit,gt.train,vb,bandInt,input) %&gt;% summarise(vx=mean(vx)) \n\ninput.activation&lt;-function(x.target, c){\n  return(exp(-1*c*(x.target-inputNodes)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, c){\n  return(weights%*%input.activation(x.target, c))\n}\n\nmean.prediction&lt;-function(x.target, weights, association.parameter){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, c,trainVec){\n  #trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, c)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, c)\n  mOver = mean.prediction(xOver, weights, c)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n  \nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  alm.train\n}\n\nwrap_alm &lt;- function(parms,dat, weights,lossFun){\n    c=parms[1]; lr=parms[2]\n   pred=train.alm(dat, c=c, lr=lr, weights=weights)\n   #sqrt(mean((dat$vx -pred)^2))\n   lossFun(dat$vx,pred)\n}\n\nwrap_optim &lt;- function(dat,wm,lossFun){\n  bounds_lower &lt;- c(.0000001, .00001)\n  bounds_upper &lt;- c(5, 5)\n\n optim(c(.1, .2),\n   fn = wrap_alm,\n   dat = dat, weights = wm,lossFun=lossFun,\n   method = \"L-BFGS-B\",\n   lower = bounds_lower,\n   upper = bounds_upper,\n   control = list(maxit = 1e4, pgtol = 0, factr = 0)\n )\n}\n\nRMSE &lt;- function(x,y){\n  sqrt(mean((x-y)^2))\n}\n\n## First average observed and predicted data into blocks, then compute RMSE\nRMSE.tb &lt;- function(x,y,blocks=6){\n  data.frame(x,y) %&gt;% mutate(t=row_number(),fitBins=cut(t,breaks=blocks,labels=c(1:blocks))) %&gt;%\n    group_by(fitBins) %&gt;% \n    summarise(predMean=mean(x),obsMean=mean(y)) %&gt;% \n    summarise(RMSE(predMean,obsMean)) %&gt;% as.numeric()\n}\n\n## Recode RMSE.tb using data.table functions rather than dplyr\nRMSE.tb2 &lt;- function(x,y,blocks=6){\n  data.table(x=x,y=y,t=seq(1,length(x))) %&gt;% \n    .[, `:=`(fitBins = cut(t, breaks = ..blocks, labels = c(1:..blocks)))] %&gt;%\n    .[, .(predMean = mean(x), obsMean = mean(y)), keyby = .(fitBins)] %&gt;%\n    .[, RMSE(predMean,obsMean)] %&gt;% as.numeric()\n}\n\n\ndplyr RMSE vs data.table RMSE\n\nCodedpVsDt=microbenchmark(\n  dplyrMethod={\n  fitVaried &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb);\n  fitConstant &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb)},\n  dtMethod={\n  fitVaried2 &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb2);\n  fitConstant2 &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb2)},\n  times=5\n)\nknitr::kable(summary(dpVsDt),format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\ncld\n\n\n\ndplyrMethod\n11.282291\n12.996857\n13.558623\n13.970367\n14.658515\n14.885086\n5\na\n\n\ndtMethod\n4.933999\n5.306232\n5.681235\n5.515397\n6.131193\n6.519355\n5\nb\n\n\n\nThe data.table version seems to be consistently more than 2x faster\nSeparate fits, vs. nesting method vs. split method\n\nCodenestSplit&lt;-microbenchmark(\nseparate={fitVaried &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb2);\nfitConstant &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% wrap_optim(.,wm,lossFun=RMSE.tb2) },\nnestGroups = tv %&gt;% group_by(condit) %&gt;% nest() %&gt;% mutate(fit=map(data,~wrap_optim(.,wm,RMSE.tb2))),\nsplitGroups = tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE.tb2)),\ntimes=5\n)\nknitr::kable(summary(nestSplit),format=\"markdown\") # 03/02/23 - Mac Pro\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\ncld\n\n\n\nseparate\n6.394459\n6.483235\n6.709882\n6.508353\n7.059420\n7.103941\n5\na\n\n\nnestGroups\n6.253850\n6.457956\n6.789962\n6.756942\n7.044965\n7.436094\n5\na\n\n\nsplitGroups\n6.117184\n6.455866\n6.605814\n6.461572\n6.640798\n7.353648\n5\na\n\n\n\nNot much of an effect for nesting method.\nComputing RMSE over Raw trials vs. RMSE of blocked training performance\n\nCodetrialBlock&lt;-microbenchmark(\n  trialFit = tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE)),\n  blockFit = tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE.tb2)),\n  times=5\n)\nknitr::kable(summary(trialBlock),format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\ncld\n\n\n\ntrialFit\n2.665184\n2.674186\n2.806476\n2.803491\n2.843806\n3.045712\n5\na\n\n\nblockFit\n5.169167\n5.264218\n5.350769\n5.285151\n5.491828\n5.543483\n5\nb\n\n\n\nThe models are fit about 2x faster when computing RMSE over trials. This shouldn’t be surprising, since fitting to blocked data requires several additional computations (e.g. grouping, computing means)"
  },
  {
    "objectID": "Misc/data_dictionary.html",
    "href": "Misc/data_dictionary.html",
    "title": "HTW Data Dictionary",
    "section": "",
    "text": "id - unique subject identifier condit - primary training manipulation, Constant vs. Varied. tOrder - order of blocks in no-feedback testing stage (extrapolation first or re-do train first) bandType - whether band was trained on (trained) or novel in test phase (extrapolation) expMode2 - Phase of Experiment - Train, Train-Nf, Test, Test-Fb gt.train - training trial number - running total vb - velocity band on current trial - Levels: 100-300 350-550 600-800 800-1000 1000-1200 1200-1400 bandInt - numeric value of lower bound of vb: 100, 350, 600, 800, 1000, 1200 vx - X velocity generated by subject on current trial dist - absolute distance from edge of target velocity band (0 if vx falls within band) sdist - signed distance from edge of target velocity band vy - y velocity generated by subject (not relevant to task) result - categorical result of throw in relation to target band - Levels: Over, Under, Hit vxCat - which velocity band the current trial throw falls into prev - Target velocity band on previous throw bandSeq - Previous Target Band and current Target Band, e.g. 800-&gt;1000 1000-&gt;1200 etc.\n\nCodepacman::p_load(tidyverse, lme4, emmeans, here, knitr, kableExtra,gt)\noptions(dplyr.summarise.inform = FALSE)\nd &lt;- readRDS(here(\"data/dPrune-07-27-23.rds\")) %&gt;% ungroup()\nd$stage &lt;- factor(d$stage, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\"))\nd$tOrder &lt;- factor(d$tOrder, levels = c(\"testFirst\", \"trainFirst\"), labels = c(\"Test First\", \"Train First\"))\nd$bandOrder &lt;- factor(d$bandOrder, levels = c(\"orig\", \"rev\"), labels = c(\"Original\", \"Reverse\"))\nd$fb &lt;- factor(d$fb, levels = c(\"continuous\", \"ordinal\"), labels = c(\"Continuous\", \"Ordinal\"))\nd &lt;- d %&gt;%\n    mutate(id = as.factor(d$id), fullCond = as.factor(fullCond)) %&gt;%\n    relocate(bandOrder, .after = fb)\n\n\n\nCode# create table with number of unique ids per condition, use kable\nd %&gt;% select(fullCond,id) %&gt;% distinct() %&gt;% group_by(fullCond) %&gt;% summarise(n=n()) %&gt;% knitr::kable()\n\n\n\nfullCond\nn\n\n\n\nconstant.testFirst.orig.continuous\n52\n\n\nconstant.testFirst.orig.ordinal\n51\n\n\nconstant.testFirst.rev.continuous\n28\n\n\nconstant.testFirst.rev.ordinal\n31\n\n\nconstant.trainFirst.orig.continuous\n38\n\n\nconstant.trainFirst.rev.continuous\n27\n\n\nconstant.trainFirst.rev.ordinal\n28\n\n\nvaried.testFirst.orig.continuous\n39\n\n\nvaried.testFirst.orig.ordinal\n39\n\n\nvaried.testFirst.rev.continuous\n25\n\n\nvaried.testFirst.rev.ordinal\n28\n\n\nvaried.trainFirst.orig.continuous\n37\n\n\nvaried.trainFirst.rev.continuous\n30\n\n\nvaried.trainFirst.rev.ordinal\n18\n\n\n\n\nCodet1=d %&gt;% select(condit,fb,bandOrder,tOrder,id) %&gt;% distinct() %&gt;% \n  group_by(condit,fb,bandOrder,tOrder) %&gt;% summarise(n=n()) \nt1 %&gt;% kbl(col.names=c(\"Condition\",\"Feedback\",\"Band_Order\",\"Test Order\",\"N\"),escape=FALSE) \n\n\n\nCondition\nFeedback\nBand_Order\nTest Order\nN\n\n\n\nConstant\nNA\nNA\nTest First\n162\n\n\nConstant\nNA\nNA\nTrain First\n93\n\n\nVaried\nNA\nNA\nTest First\n131\n\n\nVaried\nNA\nNA\nTrain First\n85\n\n\n\n\nCodet1=d %&gt;% select(condit,fb,bandOrder,id) %&gt;% distinct() %&gt;% \n  group_by(condit,fb,bandOrder) %&gt;% summarise(n=n()) \nt1\n\n# A tibble: 2 × 4\n# Groups:   condit, fb [2]\n  condit   fb    bandOrder     n\n  &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;     &lt;int&gt;\n1 Constant &lt;NA&gt;  &lt;NA&gt;        255\n2 Varied   &lt;NA&gt;  &lt;NA&gt;        216\n\n\n\nCoded |&gt; group_by(id,condit) |&gt; \n  summarize(n=n()) |&gt; \n  mutate(n=fct_infreq(factor(n))) |&gt;\n  ggplot(aes(x=id,y=n)) + geom_col()\n\n\n\nCodesorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\nconditional_bars &lt;- function(df, condition, var) {\n  df |&gt; \n    filter({{ condition }}) |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_bar()\n}\n\nhistogram &lt;- function(df, var, binwidth) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {binwidth}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth) + \n    labs(title = label)\n}\n# \n# sorted_bars(d |&gt; filter(id %in% 1:30),id) \n# \n# test |&gt; conditional_bars(id %in% 1:10, id)"
  },
  {
    "objectID": "Misc/data_dictionary.html#tibble1",
    "href": "Misc/data_dictionary.html#tibble1",
    "title": "HTW Data Dictionary",
    "section": "Tibble1",
    "text": "Tibble1\n\nCode# Create the tribble\ndata_tbl &lt;- tibble(\n  Variable_Name = c(\"condit\", \"fb\", \"bandOrder\", \"tOrder\", \"expMode\", \"trainStage\", \"expStage\", \"band\", \"vb\", \"lowBound\", \"feedback\", \"stage\"),\n  Variable_Levels = c(\"Constant, Varied\", \"Continuous, Ordinal\", \"Original, Reverse\", \"Test First, Train First\", \"train, train-Nf, test-Nf, test-train-nf, test-feedback\", \"Beginning, Middle, End, Test\", \"TrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\", \"1, 2, 3, 4, 5, 6\", \"100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\", \"100, 350, 600, 800, 1000, 1200\", \"0, 1\", \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\"),\n  Description = rep(\"Unknown\", 12)\n)\n\n# Print the tribble\ndata_tbl %&gt;% gt::gt()\n\n\n\n\n\nVariable_Name\n      Variable_Levels\n      Description\n    \n\n\ncondit\nConstant, Varied\nUnknown\n\n\nfb\nContinuous, Ordinal\nUnknown\n\n\nbandOrder\nOriginal, Reverse\nUnknown\n\n\ntOrder\nTest First, Train First\nUnknown\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nUnknown\n\n\ntrainStage\nBeginning, Middle, End, Test\nUnknown\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nUnknown\n\n\nband\n1, 2, 3, 4, 5, 6\nUnknown\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nUnknown\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nUnknown\n\n\nfeedback\n0, 1\nUnknown\n\n\nstage\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\nUnknown"
  },
  {
    "objectID": "Misc/data_dictionary.html#tibble",
    "href": "Misc/data_dictionary.html#tibble",
    "title": "HTW Data Dictionary",
    "section": "Tibble",
    "text": "Tibble\n\nCodedata &lt;- tibble(\n  `Variable Name` = c(\"condit\", \"fb\", \"bandOrder\", \"tOrder\", \"expMode\", \"trainStage\",\n                      \"expStage\", \"band\", \"vb\", \"lowBound\", \"feedback\", \"stage\"),\n  `Levels` = c(\"Constant, Varied\", \n               \"Continuous, Ordinal\",\n               \"Original, Reverse\",\n               \"Test First, Train First\",\n               \"train, train-Nf, test-Nf, test-train-nf, test-feedback\",\n               \"Beginning, Middle, End, Test\",\n               \"TrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\",\n               \"1, 2, 3, 4, 5, 6\",\n               \"100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\",\n               \"100, 350, 600, 800, 1000, 1200\",\n               \"0, 1\",\n               \"1 to 23\"),\n  `Description` = c(\"Conditions under which training was performed. In 'Constant' only one velocity band was used, in 'Varied' three different bands were used.\",\n                    \"Two types of feedback given to the subjects. The 'Continuous' type involved ongoing feedback whereas 'Ordinal' feedback type was rank-based\",\n                    \"The order in which the velocity bands were presented for the 'Hit The Wall' task - either 'Original' or 'Reverse'.\",\n                    \"The order of test and train stages.\",\n                    \"Specifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\",\n                    \"The stage of training for the individual - starting, middle, end, or if they're testing.\",\n                    \"The stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\",\n                    \"Identifies the 6 different velocity bands used in the experiment.\",\n                    \"Specifies the range of each velocity band used in the experiment.\",\n                    \"The lower boundary of each velocity band.\",\n                    \"Binary representation of whether feedback was provided (1) or not (0)\",\n                    \"The various stages during testing and feedback, represented by numbers from 1 to 23.\")\n)\ndata %&gt;% gt::gt()\n\n\n\n\n\nVariable Name\n      Levels\n      Description\n    \n\n\ncondit\nConstant, Varied\nConditions under which training was performed. In 'Constant' only one velocity band was used, in 'Varied' three different bands were used.\n\n\nfb\nContinuous, Ordinal\nTwo types of feedback given to the subjects. The 'Continuous' type involved ongoing feedback whereas 'Ordinal' feedback type was rank-based\n\n\nbandOrder\nOriginal, Reverse\nThe order in which the velocity bands were presented for the 'Hit The Wall' task - either 'Original' or 'Reverse'.\n\n\ntOrder\nTest First, Train First\nThe order of test and train stages.\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nSpecifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\n\n\ntrainStage\nBeginning, Middle, End, Test\nThe stage of training for the individual - starting, middle, end, or if they're testing.\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nThe stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\n\n\nband\n1, 2, 3, 4, 5, 6\nIdentifies the 6 different velocity bands used in the experiment.\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nSpecifies the range of each velocity band used in the experiment.\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nThe lower boundary of each velocity band.\n\n\nfeedback\n0, 1\nBinary representation of whether feedback was provided (1) or not (0)\n\n\nstage\n1 to 23\nThe various stages during testing and feedback, represented by numbers from 1 to 23."
  },
  {
    "objectID": "Misc/data_dictionary.html#tribble",
    "href": "Misc/data_dictionary.html#tribble",
    "title": "HTW Data Dictionary",
    "section": "Tribble",
    "text": "Tribble\n\nCodetribble(\n  ~Variable, ~Levels, ~Description,\n  \"condit\", \"Constant, Varied\", \"Conditions under which training was performed. In 'Constant' only one velocity band was used, in 'Varied' three different bands were used.\",\n  \"fb\", \"Continuous, Ordinal\", \"Two types of feedback given to the subjects. The 'Continuous' type involved ongoing feedback whereas 'Ordinal' feedback type was rank-based\",\n  \"bandOrder\", \"Original, Reverse\", \"The order in which the velocity bands were presented for the 'Hit The Wall' task - either 'Original' or 'Reverse'.\",\n  \"tOrder\", \"Test First, Train First\", \"The order of test and train stages.\",\n  \"expMode\", \"train, train-Nf, test-Nf, test-train-nf, test-feedback\", \"Specifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\",\n  \"trainStage\", \"Beginning, Middle, End, Test\", \"The stage of training for the individual - starting, middle, end, or if they're testing.\",\n  \"expStage\", \"TrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\", \"The stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\",\n  \"band\", \"1, 2, 3, 4, 5, 6\", \"Identifies the 6 different velocity bands used in the experiment.\",\n  \"vb\", \"100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\", \"Specifies the range of each velocity band used in the experiment.\",\n  \"lowBound\", \"100, 350, 600, 800, 1000, 1200\", \"The lower boundary of each velocity band.\",\n  \"feedback\", \"0, 1\", \"Binary representation of whether feedback was provided (1) or not (0)\",\n  \"stage\", \"1 to 23\", \"The various stages during testing and feedback, represented by numbers from 1 to 23.\"\n) %&gt;% gt::gt() %&gt;% \n        gt::tab_style(\n                style = cell_text(weight = \"bold\"),\n                locations = cells_column_labels()\n        )\n\n\n\n\n\nVariable\n      Levels\n      Description\n    \n\n\ncondit\nConstant, Varied\nConditions under which training was performed. In 'Constant' only one velocity band was used, in 'Varied' three different bands were used.\n\n\nfb\nContinuous, Ordinal\nTwo types of feedback given to the subjects. The 'Continuous' type involved ongoing feedback whereas 'Ordinal' feedback type was rank-based\n\n\nbandOrder\nOriginal, Reverse\nThe order in which the velocity bands were presented for the 'Hit The Wall' task - either 'Original' or 'Reverse'.\n\n\ntOrder\nTest First, Train First\nThe order of test and train stages.\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nSpecifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\n\n\ntrainStage\nBeginning, Middle, End, Test\nThe stage of training for the individual - starting, middle, end, or if they're testing.\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nThe stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\n\n\nband\n1, 2, 3, 4, 5, 6\nIdentifies the 6 different velocity bands used in the experiment.\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nSpecifies the range of each velocity band used in the experiment.\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nThe lower boundary of each velocity band.\n\n\nfeedback\n0, 1\nBinary representation of whether feedback was provided (1) or not (0)\n\n\nstage\n1 to 23\nThe various stages during testing and feedback, represented by numbers from 1 to 23.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nLevels\nDescription\n\n\n\ncondit\nConstant, Varied\nConditions under which training was performed. In “Constant” only one velocity band was used, in “Varied” three different bands were used.\n\n\nfb\nContinuous, Ordinal\nTwo types of feedback given to the subjects. The “Continuous” type involved ongoing feedback whereas “Ordinal” feedback type was rank-based\n\n\nbandOrder\nOriginal, Reverse\nThe order in which the velocity bands were presented for the “Hit The Wall” task - either “Original” or “Reverse”.\n\n\ntOrder\nTest First, Train First\nThe order of test and train stages.\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nSpecifies the mode of the experiment. Can be one of the training stages, no-feedback testing from the training or novel extrapolation band, or feedback testing.\n\n\ntrainStage\nBeginning, Middle, End, Test\nThe stage of training for the individual - starting, middle, end, or if they’re testing.\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nThe stages in the experiment which includes the start and end of training, intermediate tests, and final test stages.\n\n\nband\n1, 2, 3, 4, 5, 6\nIdentifies the 6 different velocity bands used in the experiment.\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nSpecifies the range of each velocity band used in the experiment.\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nThe lower boundary of each velocity band.\n\n\nfeedback\n0, 1\nBinary representation of whether feedback was provided (1) or not (0)\n\n\nstage\n1 to 23\nThe various stages during testing and feedback, represented by numbers from 1 to 23."
  },
  {
    "objectID": "Misc/data_dictionary.html#description-table",
    "href": "Misc/data_dictionary.html#description-table",
    "title": "HTW Data Dictionary",
    "section": "Description table",
    "text": "Description table\n\n\n\n\n\n\n\nVariable Name\nVariable Levels\nDescription\n\n\n\ncondit\nConstant, Varied\nCondition of the experiment: constant or varied\n\n\nfb\nContinuous, Ordinal\nType of feedback received: continuous or ordinal\n\n\nbandOrder\nOriginal, Reverse\nOrder of bands: original or reverse\n\n\ntOrder\nTest First, Train First\nOrder of testing and training stages: test first or train first\n\n\nexpMode\ntrain, train-Nf, test-Nf, etc.\nMode of the experiment: train, train-Nf, test-Nf, etc.\n\n\ntrainStage\nBeginning, Middle, End, Test\nStage of the training: beginning, middle, end, or test\n\n\nexpStage\nTrainStart, intTest1, etc.\nStage of the experiment: TrainStart, intTest1, TrainMid1, etc.\n\n\nband\n1, 2, 3, 4, 5, 6\nBand number\n\n\nvb\n100-300, 350-550, etc.\nVelocity band range\n\n\nlowBound\n100, 350, 600, etc.\nLower bound of the velocity band range\n\n\nfeedback\n0, 1\nFeedback type: 0 (no feedback), 1 (feedback)\n\n\nstage\n1, 2, 3, etc.\nStage number of the experiment"
  },
  {
    "objectID": "Misc/data_dictionary.html#ascii-table",
    "href": "Misc/data_dictionary.html#ascii-table",
    "title": "HTW Data Dictionary",
    "section": "Ascii table",
    "text": "Ascii table\n\n\n\n\n\n\n\nVariable Name\nVariable Levels\nDescription\n\n\n\ncondit\nConstant, Varied\nUnknown\n\n\nfb\nContinuous, Ordinal\nUnknown\n\n\nbandOrder\nOriginal, Reverse\nUnknown\n\n\ntOrder\nTest First, Train First\nUnknown\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\nUnknown\n\n\ntrainStage\nBeginning, Middle, End, Test\nUnknown\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\nUnknown\n\n\nband\n1, 2, 3, 4, 5, 6\nUnknown\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\nUnknown\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\nUnknown\n\n\nfeedback\n0, 1\nUnknown\n\n\nstage\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\nUnknown\n\n\n\n\nDemonstration of pipe table syntax\n\nDefault\nLeft\nRight\nCenter\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1"
  },
  {
    "objectID": "Misc/data_dictionary.html#smaller-table",
    "href": "Misc/data_dictionary.html#smaller-table",
    "title": "HTW Data Dictionary",
    "section": "smaller table",
    "text": "smaller table\n\n\n\n\n\n\nVariable Name\nVariable Levels\n\n\n\ncondit\n“Constant”, “Varied”\n\n\nfb\n“Continuous”, “Ordinal”\n\n\nbandOrder\n“Original”, “Reverse”\n\n\ntOrder\n“Test First”, “Train First”\n\n\nexpMode\n“train”, “train-Nf”, “test-Nf”, “test-train-nf”, “test-feedback”\n\n\ntrainStage\n“Beginning”, “Middle”, “End”, “Test”\n\n\nexpStage\n“TrainStart”, “intTest1”, “TrainMid1”, “intTest2”, “TrainMid2”, “intTest3”, “TrainEnd”, “Test1”, “Test2”, “Test3”\n\n\nband\n“1”, “2”, “3”, “4”, “5”, “6”\n\n\nvb\n“100-300”, “350-550”, “600-800”, “800-1000”, “1000-1200”, “1200-1400”\n\n\nlowBound\n“100”, “350”, “600”, “800”, “1000”, “1200”\n\n\nfeedback\n“0”, “1”\n\n\nstage\n“1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9”, “10”, “11”, “12”, “13”, “14”, “15”, “16”, “17”, “18”, “19”, “20”, “21”, “22”, “23”\n\n\n\nCode# Create a data frame\nvar_levels &lt;- list(\n  condit = c(\"Constant\", \"Varied\"),\n  fb = c(\"Continuous\", \"Ordinal\"),\n  bandOrder = c(\"Original\", \"Reverse\"),\n  tOrder = c(\"Test First\", \"Train First\"),\n  expMode = c(\"train\", \"train-Nf\", \"test-Nf\", \"test-train-nf\", \"test-feedback\"),\n  trainStage = c(\"Beginning\", \"Middle\", \"End\", \"Test\"),\n  expStage = c(\"TrainStart\", \"intTest1\", \"TrainMid1\", \"intTest2\", \"TrainMid2\", \"intTest3\", \"TrainEnd\", \"Test1\", \"Test2\", \"Test3\"),\n  band = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n  vb = c(\"100-300\", \"350-550\", \"600-800\", \"800-1000\", \"1000-1200\", \"1200-1400\"),\n  lowBound = c(\"100\", \"350\", \"600\", \"800\", \"1000\", \"1200\"),\n  feedback = c(\"0\", \"1\"),\n  stage = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\")\n)\n\n# Convert the list to a data frame\ndf &lt;- data.frame(\n  Variable_Name = names(var_levels),\n  Variable_Levels = sapply(var_levels, function(x) paste(x, collapse = \", \"))\n)\n\n# Create a markdown table\ndf %&gt;%\n  kable(format = \"markdown\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\n\nVariable_Name\nVariable_Levels\n\n\n\ncondit\ncondit\nConstant, Varied\n\n\nfb\nfb\nContinuous, Ordinal\n\n\nbandOrder\nbandOrder\nOriginal, Reverse\n\n\ntOrder\ntOrder\nTest First, Train First\n\n\nexpMode\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\n\n\ntrainStage\ntrainStage\nBeginning, Middle, End, Test\n\n\nexpStage\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\n\n\nband\nband\n1, 2, 3, 4, 5, 6\n\n\nvb\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\n\n\nlowBound\nlowBound\n100, 350, 600, 800, 1000, 1200\n\n\nfeedback\nfeedback\n0, 1\n\n\nstage\nstage\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\n\n\n\n\n\n\n\n\n\n\nVariable Name\nVariable Levels\n\n\n\ncondit\nConstant, Varied\n\n\nfb\nContinuous, Ordinal\n\n\nbandOrder\nOriginal, Reverse\n\n\ntOrder\nTest First, Train First\n\n\nexpMode\ntrain, train-Nf, test-Nf, test-train-nf, test-feedback\n\n\ntrainStage\nBeginning, Middle, End, Test\n\n\nexpStage\nTrainStart, intTest1, TrainMid1, intTest2, TrainMid2, intTest3, TrainEnd, Test1, Test2, Test3\n\n\nband\n1, 2, 3, 4, 5, 6\n\n\nvb\n100-300, 350-550, 600-800, 800-1000, 1000-1200, 1200-1400\n\n\nlowBound\n100, 350, 600, 800, 1000, 1200\n\n\nfeedback\n0, 1\n\n\nstage\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\n\n\n\nNumeric variables\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\ntrial\nTrial number\n\n\nnGoodTrial\nNumber of good trials\n\n\ngt.bandStage\nBand stage in game time\n\n\ngt.stage\nStage in game time\n\n\ngt.train\nTraining in game time\n\n\ninput\nInput from the participant\n\n\nbandInt\nBand interval\n\n\nhighBound\nHigh boundary of the velocity band\n\n\nrunTotal\nRunning total of trials\n\n\ndist\nDistance between produced x-velocity and the closest edge of the current velocity band\n\n\nsdist\nStandardized distance\n\n\nvx\nX-velocity of the projectile\n\n\nvxb\nX-velocity at the boundary\n\n\nvxi\nInitial X-velocity\n\n\nvy\nY-velocity of the projectile\n\n\nnTrain\nNumber of training trials\n\n\nnTestNf\nNumber of no-feedback testing trials\n\n\nnInt\nNumber of interleaved trials\n\n\nnTestF\nNumber of feedback testing trials\n\n\nnTotal\nTotal number of trials\n\n\nlastTrain\nLast training trial\n\n\nlastTrial\nLast trial"
  },
  {
    "objectID": "Misc/e1_brms_test.html",
    "href": "Misc/e1_brms_test.html",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "",
    "text": "Codepacman::p_load(tidyverse,tidybayes,brms,bayesplot,bayestestR,\n               broom.mixed,lme4,emmeans,here,knitr,kableExtra,gt)\nwalk(c(here(\"Functions/Display_Functions.R\"), here(\"Functions/org_functions.R\")), source)\n\ntest &lt;- readRDS(here(\"data/e1_08-04-23.rds\")) |&gt; \n  filter(expMode2 == \"Test\") |&gt; \n  mutate(distS2 = custom_scale(dist))\n\noptions(brms.backend=\"cmdstanr\",mc.cores=4)"
  },
  {
    "objectID": "Misc/e1_brms_test.html#random-effects",
    "href": "Misc/e1_brms_test.html#random-effects",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Random Effects",
    "text": "Random Effects\nCodemodelName &lt;- \"e1_testDistRF\"\ne1_testDistRF &lt;- brm(dist ~ condit + vb + (1 + vb|id),\n                     data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\n\n\npt &lt;- posterior_table(e1_testDistRF) |&gt; select(-CI)\nkable(pt)\nbrms_posterior_checks(e1_testDistRF,dist,vb)\n\nmap_estimate(e1_testDistRF)\n\n\nhdi(e1_testDistRF$fit, ci = c(0.5, 0.75, 0.89, 0.95))\n\n\nplot(pt)\n\nintPlot &lt;- plot(conditional_effects(e1_testDistRF,effects=\"vb:condit\"))\n\n\nmd &lt;- tidy_draws(e1_testDistRF) |&gt; select(b_Intercept:`b_vb1200M1400`)\nplot(bayestestR::hdi(md, ci = c(.89, .95)))\n\nplot(bayestestR::bayesfactor_parameters(e1_testDistRF, null = c(-.5, .5)))\n\np1 &lt;- GetModelStats(e1_testDistRF)\n\nkable(p1) |&gt; column_spec(1:9,width=\"5em\")\n\n#GetBrmsModelStats(e1_testDistRF)\n\n\n\ne1_testDistRF %&gt;%\n  spread_draws(b_Intercept, r_condition[condition,])\n\n\ndraws1 &lt;- e1_testDistRF |&gt; spread_draws()"
  },
  {
    "objectID": "Misc/e1_brms_test.html#interaction---grouped-random-effects",
    "href": "Misc/e1_brms_test.html#interaction---grouped-random-effects",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Interaction - Grouped Random Effects",
    "text": "Interaction - Grouped Random Effects\n\nCodemodelName &lt;- \"e1_testConditVb_Dist_Gr\"\ne1_testConditVb_Dist_Gr &lt;- brm(dist ~ condit * vb + (1 + vb|gr(id,by=condit)),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nfixef(e1_testConditVb_Dist_Gr)\nhead(coef(e1_testConditVb_Dist_Gr)$id)\nbayes_R2(e1_testConditVb_Dist_Gr)\n\nGetModelStats(e1_testConditVb_Dist_Gr)\n\n# coef(e1_testConditVb_Dist_Gr)$id %&gt;% as_tibble(rownames=\"id\") %&gt;% select(id, starts_with(\"Est\")) |&gt; print(n=10)\n\nindividual_coefs &lt;- coef(e1_testConditVb_Dist_Gr)$id %&gt;%\n    as_tibble(rownames = \"id\") %&gt;%\n    select(id, starts_with(\"Estim\")) %&gt;%\n    pivot_longer(cols = -id, names_to = \"variable\", values_to = \"value\") %&gt;%\n    separate(variable, c(\"type\", \"effect\"), sep = \"\\\\.\")\n\nmerged_data &lt;- test |&gt; group_by(id, condit) |&gt; summarise(n=n()) %&gt;%\n               left_join(individual_coefs, by = \"id\") |&gt;  filter(effect == \"Intercept\" | grepl(\"^vb\", effect))\n\n\nhead(merged_data)\n\n\n\nggplot(merged_data, aes(x = effect, y = value, color = condit)) +\n  geom_point(position = position_jitterdodge()) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    title = \"Individual Differences in Velocity Production Task (Intercept & vb Effects)\",\n    x = \"Velocity Band Effect\",\n    y = \"Estimated Value\",\n    color = \"Condition\"\n  )\n\n\n\n epred_draws(e1_testConditVb_Dist_Gr, newdata = test,ndraws=50) |&gt;\nggplot(aes(x = vb, y = .epred, color = condit)) +\n    gghalves::geom_half_violin(position=position_dodge(),alpha=.7) +\n    gghalves::geom_half_boxplot(position=position_dodge(),alpha=.5) +\n    #geom_jitter(width = 0.2, height = 0) +\n    labs(\n        title = \"Posterior Predictive Distribution\",\n        x = \"Effect Category\",\n        y = \"Expected Predicted Value\",\n        color = \"Condition\"\n    )\n\n\n(r_fit &lt;- e1_testConditVb_Dist_Gr %&gt;% \n  tidy() %&gt;% filter(effect==\"fixed\") |&gt; select(-effect,-component, -group) |&gt; \n  mutate(term = janitor::make_clean_names(term)) |&gt;\n    mutate(across(where(is.numeric), \\(x) round(x, 0))) |&gt; kbl() |&gt;\n    column_spec(1:5, width = \"5em\") ) \n \ncat(r_fit$term)\npaste(r_fit$term)"
  },
  {
    "objectID": "Misc/e1_brms_test.html#random-effects---interaction-condit-x-vb",
    "href": "Misc/e1_brms_test.html#random-effects---interaction-condit-x-vb",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Random Effects - Interaction condit x vb",
    "text": "Random Effects - Interaction condit x vb\n\nCodemodelName &lt;- \"e1_testDistBT_RF\"\ne1_testDistBT_RF &lt;- brm(dist ~ (condit * vb) + bandType + (1 + vb +bandType|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=2000,chains=4,silent=0,\n                      control=list(adapt_delta=0.92, max_treedepth=13))"
  },
  {
    "objectID": "Misc/e1_brms_test.html#intercept-random-effects---interaction-condit-x-vb",
    "href": "Misc/e1_brms_test.html#intercept-random-effects---interaction-condit-x-vb",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "0 intercept; Random Effects - Interaction condit x vb",
    "text": "0 intercept; Random Effects - Interaction condit x vb\n\nCodemodelName &lt;- \"e1_testDistRF2_0\"\ne1_testDistRF2_0 &lt;- brm(dist ~ 0 + (condit * vb) + (0 + vb|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\",modelName)), \n                        iter=5000,chains=4)\nGetModelStats(e1_testDistRF2_0)\n\nkable(fixef(e1_testDistRF2_0))\nnewdat &lt;-data.frame(crossing(condit=c(\"Constant\",\"Varied\"), vb = unique(test$vb)))\npreds &lt;- fitted(e1_testDistRF2_0, re_formula = NA, newdata = newdat, probs = c(0.025, 0.975))\n\n\nkable(tidy(e1_testDistRF2_0, effects=\"fixed\",ess=TRUE))\nconditional_effects(e1_testDistRF2_0,\"condit:vb\",method=\"pp_expect\",points=TRUE)\n\n\ndraws_fit &lt;- as_draws_df(e1_testDistRF2_0, variable = \"^b_\", regex = TRUE)\n\n\nmcmc_plot(e1_testDistRF2_0, type=\"trace\",variable=\"^b_\",regex=TRUE)\nmcmc_plot(e1_testDistRF2_0, type=\"intervals\",variable=\"^b_\",regex=TRUE)\nmcmc_plot(e1_testDistRF2_0, type=\"areas\",variable=\"^b_\",regex=TRUE)\n\nmcmc_hist(e1_testDistRF2_0,pars=c(\"b_conditConstant\",\"b_conditVaried\"))\nplot(e1_testDistRF2_0,variable=c(\"b_conditConstant\",\"b_conditVaried\"))\n\n\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(e1_testDistRF2_0,ndraws=200),group=test$vb)\n\n\nmcmc_hist(e1_testDistRF2_0,prob=.5,regex_pars=c(\"^r_id\\\\[1,.*\\\\]\"))\n\n\nmodel_parameters(e1_testDistRF2_0,effects=\"random\",keep=\"^r_id\\\\[3,.*\\\\]\")\nplot_subject_fits(e1_testDistRF2_0,3)\n\n\nindvFit &lt;- GetIndvFits(e1_testDistRF2_0)\n\nindvFit |&gt; ggplot(aes(x=condit,y=Median,fill=condit))+stat_halfeye()+facet_wrap(~vb)"
  },
  {
    "objectID": "Misc/e1_brms_test.html#intercept-random-effects---interaction-condit-x-vb-1",
    "href": "Misc/e1_brms_test.html#intercept-random-effects---interaction-condit-x-vb-1",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "0 intercept; Random Effects - Interaction condit x vb",
    "text": "0 intercept; Random Effects - Interaction condit x vb\n\nCodemodelName &lt;- \"e1_testDistRF2_02\"\ne1_testDistRF2_02 &lt;- brm(dist ~ 0 + condit:vb + (0 + vb|id),data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDistRF2_02, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nGetModelStats(e1_testDistRF2_02)\n\nbayes_R2(e1_testDistRF2_02)\nsjPlot::tab_model(e1_testDistRF2_02)\n\n\nfixef(e1_testDistRF2_02)\nnewdat &lt;-data.frame(crossing(condit=c(\"Constant\",\"Varied\"), vb = unique(test$vb)))\npreds &lt;- fitted(e1_testDistRF2_02, re_formula = NA, newdata = newdat, probs = c(0.025, 0.975))\n\n\n#shinystan::launch_shinystan(e1_testDistRF2_02)\n\n\n\nCodemodelName &lt;- \"e1_test_Dist_Int\"\ne1_testDist_Int &lt;- brm(dist ~ 0 + Intercept + bandInt + condit + (1|id), \n                       data=test,\n                       iter=1000, chains=4, silent=0,\n                       file=paste0(here::here(\"data/model_cache\",modelName)))\n\ne1_testDist_Int\n\npp_check(e1_testDist_Int,group=\"bandInt\")\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(e1_testDist_Int,ndraws=100),group=test$bandInt)\npp_check(e1_testDist_Int,type=\"stat_grouped\",ndraws=500, group=\"bandInt\",stat=\"mean\")\ncoef(e1_testDist_Int)$id\n\n\n\nCodemodelName &lt;- \"e1Test_conditBand_RS1\"\ne1Test_conditBand_RS1 &lt;- brm(dist ~ 1 + bandInt + condit + (1+ bandInt|id), \n                       data=test,\n                       iter=1000, chains=4, silent=0,\n                       file=paste0(here::here(\"data/model_cache\",modelName)))\n\ne1Test_conditBand_RS1\nfixef(e1Test_conditBand_RS1)\n\npp_check(e1Test_conditBand_RS1,group=\"bandInt\")\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(e1Test_conditBand_RS1,ndraws=100),group=test$bandInt)\npp_check(e1Test_conditBand_RS1,type=\"stat_grouped\",ndraws=500, group=\"bandInt\",stat=\"mean\")\ncoef(e1Test_conditBand_RS1)$id\n\n\ne1Test_conditBand_RS1 |&gt; \n  tidy_draws() |&gt; \n  select(starts_with(\"b_\"),.chain,.iteration,.draw) \n  \n  \ne1Test_conditBand_RS1 |&gt; \n  spread_draws(b_Intercept,b_conditVaried) \n\n\n\nCodemodelName &lt;- \"e1Test_conditVb_RS1\"\ne1Test_conditVb_RS1 &lt;- brm(dist ~ 1 + vb + condit + (1+ vb|id), \n                       data=test,\n                       iter=1000, chains=4, silent=0,\n                       file=paste0(here::here(\"data/model_cache\",modelName)))\n\ne1Test_conditVb_RS1\n\npp_check(e1Test_conditVb_RS1,group=\"bandInt\")\nbayesplot::ppc_dens_overlay_grouped(test$dist,yrep=posterior_predict(e1Test_conditVb_RS1,ndraws=100),group=test$bandInt)\npp_check(e1Test_conditVb_RS1,type=\"stat_grouped\",ndraws=500, group=\"vb\",stat=\"mean\")\ncoef(e1Test_conditVb_RS1)$id"
  },
  {
    "objectID": "Misc/e1_brms_test.html#fixed-effects-only",
    "href": "Misc/e1_brms_test.html#fixed-effects-only",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Fixed Effects Only",
    "text": "Fixed Effects Only\n\nCodemodelName &lt;- \"e1_testDist\"\ne1_testDist &lt;- brm(dist ~ condit,data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDist, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\n\nmodelName &lt;- \"e1_testDist0\"\ne1_testDist0 &lt;- brm(dist ~ 0+condit,data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDist0, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\n\nmodelName &lt;- \"e1_testDist_uneq\"\n\ne1_testDist_uneq &lt;- brm(bf(dist ~ condit,sigma~condit),data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\n\nbrms_eq_tidy_uneq &lt;-tidyMCMC(e1_testDist_uneq, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nbrms_eq_tidy_uneq |&gt; mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\n\nmodelName &lt;- \"e1_testDist_uneq_robust\"\nbrms_uneq_robust &lt;- brm(\n  bf(dist ~ condit, sigma ~ condit), data=test,\n  family = student,file=paste0(here::here(\"data/model_cache\",modelName)))\n\nbrms_uneq_robust_tidy &lt;- tidyMCMC(brms_uneq_robust, conf.int = TRUE, conf.level = 0.95,estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;% mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\nbrms_uneq_robust_tidy\n\n\n\n#brm(dist ~ (0+vb)+(1+condit),data=test)\n\n\nbayes_R2(e1_testDist)\nsjPlot::tab_model(e1_testDist)"
  },
  {
    "objectID": "Misc/htw_dp.html",
    "href": "Misc/htw_dp.html",
    "title": "Project 2",
    "section": "",
    "text": "In project 1, we applied model-based techniques to quantify and control for the similarity between training and testing experience, which in turn enabled us to account for the difference between varied and constant training via an extended version of a similarity based generalization model. In project 2, we will go a step further, implementing a full process model capable of both 1) producing novel responses and 2) modeling behavior in both the learning and testing stages of the experiment. Project 2 also places a greater emphasis on extrapolation performance following training - as varied training has often been purported to be particularly beneficial in such situations. Extrapolation has long been a focus of the literature on function learning (Brehmer, 1974; Carroll, 1963). Central questions of the function learning literature have included the relative difficulties of learning various functional forms (e.g. linear vs.bilinear vs. quadratic), and the relative effectiveness of rule-based vs. association-based exemplar models vs. various hybrid models (Bott & Heit, 2004; DeLosh et al., 1997; Jones et al., 2018; Kalish et al., 2004; M. Mcdaniel et al., 2009; M. A. Mcdaniel & Busemeyer, 2005). However the issue of training variation has received surprisingly little attention in this area."
  },
  {
    "objectID": "Misc/htw_dp.html#participants",
    "href": "Misc/htw_dp.html#participants",
    "title": "Project 2",
    "section": "Participants",
    "text": "Participants\nData was collected from 647 participants (after exclusions). The results shown below consider data from subjects in our initial experiment, which consisted of 196 participants (106 constant, 90 varied). The follow-up experiments entailed minor manipulations: 1) reversing the velocity bands that were trained on vs. novel during testing; 2) providing ordinal rather than numerical feedback during training (e.g. correct, too low, too high). The data from these subsequent experiments are largely consistently with our initial results shown below."
  },
  {
    "objectID": "Misc/htw_dp.html#task",
    "href": "Misc/htw_dp.html#task",
    "title": "Project 2",
    "section": "Task",
    "text": "Task\nWe developed a novel visuomotor extrapolation task, termed the “Hit The Wall” (HTW) task, wherein participants learned to launch a projectile such that it hit a rectangle at the far end of the screen with an appropriate amount of force. Although the projectile had both x and y velocity components, only the x-dimension was relevant for the task.  Link to task demo"
  },
  {
    "objectID": "Misc/htw_dp.html#design",
    "href": "Misc/htw_dp.html#design",
    "title": "Project 2",
    "section": "Design",
    "text": "Design\n\n90 training trials split evenly divided between velocity bands. Varied training with 3 velocity bands and Constant training with 1 band.\nNo-feedback testing from 3 novel extrapolation bands. 15 trials each.  \nNo-feedbacd testing from the 3 bands used during the training phase (2 of which were novel for the constant group). 9 trials each.\nFeedback testing for each of the 3 extrapolation bands. 10 trials each.\n\n\n\n\nExperiment Procedure"
  },
  {
    "objectID": "Misc/htw_dp.html#training",
    "href": "Misc/htw_dp.html#training",
    "title": "Project 2",
    "section": "Training",
    "text": "Training\nTraining performance is shown in Results Figure 2A. All groups show improvement from each of their training velocity-bands (i.e. decreasing average distance from target). In the velocity band trained at by both groups (800-1000), the constant group maintains a superior level of performance from the early through the final stages of training. This difference is unsurprising given that the constant group had 3x more practice trials from that band.\n\nCode#fig.cap=\"\\\\label{fig:figs}training performance\"\n\n#title=paste0(\"HTW Training\")\n#figpatch::fig(here(\"Assets/Training-1.png\"))\nknitr::include_graphics(here(\"Assets/Training-1.png\"))\n\n\n\nCode  #fig_lab(.,title,pos=\"top\",fontface='bold',size=12,hjust=.01)"
  },
  {
    "objectID": "Misc/htw_dp.html#testing",
    "href": "Misc/htw_dp.html#testing",
    "title": "Project 2",
    "section": "Testing",
    "text": "Testing\nFor evaluating testing performance, we consider 3 separate metrics. 1) The average absolute deviation from the correct velocity, 2) The % of throws in which the wall was hit with the correct velocity and 3) The average x velocity produced.\nResults Figure 2B shows the average velocity produced for all 6 bands that were tested. At least at the aggregate level, both conditions were able to differentiate all 6 bands in the correct order, despite only having received training feedback for 1/6 (constant) or 3/6 (varied) bands during training. Participants in both groups also had a bias towards greatly overestimating the correct velocity for band 100-300, for which both groups had an average of greater than 500.\n\nCode# fig2aCap=str_wrap(\"Figure 2B: Bands 100-300, 350-550 and 600-800 are novel extrapolations for both groups. Band 800-1000 was a training band for both groups. Bands 1000-1200, and 1200-1400 were trained for the varied group, and novel for the constant group.  Top figure displays mean deviation from correct velocity. Bottom figure displays the average % of trials where participants hit the wall with the correct velocity. Error bars indicate standard error of the mean. \" ,width=170)\n\n\n#figpatch::fig(here(\"Assets/Testing_Vx-1.png\"))\nknitr::include_graphics(here(\"Assets/Testing_Vx-1.png\"))\n\n\n\n\nAs is reflected in Results Figure 2C, the constant group performed significantly better than the varied group at the 3 testing bands of greatest interest. Both groups tended to perform worse for testing bands further away from their training conditions. The varied group had a slight advantage for bands 1000-1200 and 1200-1400, which were repeats from training for the varied participants, but novel for the constant participants.\n\nCode#figpatch::fig(here(\"Assets/Test_Performance-1.png\"))\nknitr::include_graphics(here(\"Assets/Test_Performance-1.png\"))\n\n\n\nCode# \n# gtitle=\"2C. Testing Performance\"\n# title = ggdraw()+draw_label(gtitle,fontface = 'bold',x=0,hjust=0)+theme(plot.margin = margin(0, 0, 0, 7))\n# captionText=str_wrap(\"Figure 2C: Bands 100-300, 350-550 and 600-800 are novel extrapolations for both groups. Band 800-1000 was a training band for both groups. Bands 1000-1200, and 1200-1400 were trained for the varied group, and novel for the constant group.  Right side figure displays mean deviation from correct velocity band (lower values correspond to better performance). Bottom Left displays the average % of trials where participants hit the wall with the correct velocity (higher values correspond got better performance). Error bars indicate standard error of the mean. \",150)\n# capt=ggdraw()+draw_label(captionText,fontface = 'italic',x=0,hjust=0,size=11)+theme(plot.margin = margin(0, 0, 0, 1))\n# \n# plot_grid(title,NULL,leg,NULL,gbDev,gbHit,capt,NULL,ncol=2,rel_heights=c(.1,.1,1,.1),rel_widths=c(1,1))"
  },
  {
    "objectID": "Misc/htw_dp.html#alm-exam-description",
    "href": "Misc/htw_dp.html#alm-exam-description",
    "title": "Project 2",
    "section": "ALM & Exam Description",
    "text": "ALM & Exam Description\nDelosh et al. (1997) introduced the associative learning model (ALM), a connectionist model within the popular class of radial-basis networks. ALM was inspired by, and closely resembles Kruschke’s influential ALCOVE model of categorization (Kruscke 1992).\nALM is a localist neural network model, with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on thevalue of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nSee Table 2A for a full specification of the equations that define ALM and EXAM."
  },
  {
    "objectID": "Misc/htw_dp.html#model-equations",
    "href": "Misc/htw_dp.html#model-equations",
    "title": "Project 2",
    "section": "Model Equations",
    "text": "Model Equations\n\nCodetext_tbl &lt;- data.frame(\n    'Step'=c(\"Input Activation\",\"Output Activation\",\"Output Probability\",\"Mean Output\",\"Feedback Activation\",\"Update Weights\",\"Extrapolation\",\"\"),\n    'Equation' = c(\"$a_i(X) = \\\\frac{e^{-c \\\\cdot (X-X_i)^2}}{ \\\\sum_{k=1}^Me^{-c \\\\cdot (X-X_i)^2}}$\", \n                   '$O_j(X) = \\\\sum_{k=1}^Mw_{ji} \\\\cdot a_i(X)$',\n                   '$P[Y_j | X] = \\\\frac{O_i(X)}{\\\\sum_{k=1}^Mo_k(X)}$',\n                   \"$m(x) = \\\\sum_{j=1}^LY_j \\\\cdot \\\\bigg[\\\\frac{O_j(X)}{\\\\sum_{k=1}^Lo_k(X)}\\\\bigg]$\",\n                   \"$f_j(Z)=e^{-c\\\\cdot(Z-Y_j)^2}$\",\n                   \"$w_{ji}(t+1)=w_{ji}(t)+\\\\alpha \\\\cdot {f_i(Z(t))-O_j(X(t))} \\\\cdot a_i(X(t))$\",\n                   \"$P[X_i|X] = \\\\frac{a_i(X)}{\\\\sum_{k=1}^Ma_k(X)}$\",\n                   \"$E[Y|X_i]=m(X_i) + \\\\bigg[\\\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\\\bigg] \\\\cdot[X-X_i]$\"),\n    \n    'Description'= c(\n            \"Activation of each input node, $X_i$, is a function of the Gaussian similarity between the node value and stimulus X. \",\n            \"Activation of each Output unit $O_j$ is the weighted sum of the input activations and association weights\",\n            \"Each output node has associated response, $Y_j$. The probability of response $Y_j$ is determined by the ratio of output activations\",\n            \"The response to stimulus x is the weighted average of the response probabilities\",\n            \"After responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response  \",\n            \"Delta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\",\n            \"Novel test stimulus X activates input nodes associated with trained stimuli\",\n            \"Slope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)\")\n)\ntext_tbl$Step=cell_spec(text_tbl$Step,font_size=12)\ntext_tbl$Equation=cell_spec(text_tbl$Equation,font_size=18)\nalmTable=kable(text_tbl, 'html', \n  booktabs=T, escape = F, align='l',\n  caption = '&lt;span style = \"color:black;\"&gt;&lt;center&gt;&lt;strong&gt;Table 1: ALM & EXAM Equations&lt;/strong&gt;&lt;/center&gt;&lt;/span&gt;',\n  col.names=c(\"\",\"Equation\",\"Description\")) %&gt;%\n  kable_styling(position=\"left\",bootstrap_options = c(\"hover\")) %&gt;%\n  column_spec(1, bold = F,border_right=T) %&gt;%\n  column_spec(2, width = '10cm')%&gt;%\n  column_spec(3, width = '15cm') %&gt;%\n  pack_rows(\"ALM Activation & Response\",1,4,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"ALM Learning\",5,6,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"EXAM\",7,8,bold=FALSE,italic=TRUE)\n  #save_kable(file=\"almTable.html\",self_contained=T)\n#almTable\n\n\ncat(almTable)\n\n\n\n\nTable 1: ALM & EXAM Equations\n\n\n\n\n\n\n\nEquation\n\n\nDescription\n\n\n\n\n\nALM Activation & Response\n\n\n\n\nInput Activation\n\n\n\\(a_i(X) = \\frac{e^{-c \\cdot (X-X_i)^2}}{ \\sum_{k=1}^Me^{-c \\cdot (X-X_i)^2}}\\)\n\n\nActivation of each input node, \\(X_i\\), is a function of the Gaussian similarity between the node value and stimulus X.\n\n\n\n\nOutput Activation\n\n\n\\(O_j(X) = \\sum_{k=1}^Mw_{ji} \\cdot a_i(X)\\)\n\n\nActivation of each Output unit \\(O_j\\) is the weighted sum of the input activations and association weights\n\n\n\n\nOutput Probability\n\n\n\\(P[Y_j | X] = \\frac{O_i(X)}{\\sum_{k=1}^Mo_k(X)}\\)\n\n\nEach output node has associated response, \\(Y_j\\). The probability of response \\(Y_j\\) is determined by the ratio of output activations\n\n\n\n\nMean Output\n\n\n\\(m(x) = \\sum_{j=1}^LY_j \\cdot \\bigg[\\frac{O_j(X)}{\\sum_{k=1}^Lo_k(X)}\\bigg]\\)\n\n\nThe response to stimulus x is the weighted average of the response probabilities\n\n\n\n\nALM Learning\n\n\n\n\nFeedback Activation\n\n\n\\(f_j(Z)=e^{-c\\cdot(Z-Y_j)^2}\\)\n\n\nAfter responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response\n\n\n\n\nUpdate Weights\n\n\n\\(w_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t))\\)\n\n\nDelta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\n\n\n\n\nEXAM\n\n\n\n\nExtrapolation\n\n\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^Ma_k(X)}\\)\n\n\nNovel test stimulus X activates input nodes associated with trained stimuli\n\n\n\n\n\n\n\n\\(E[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg] \\cdot[X-X_i]\\)\n\n\nSlope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)"
  },
  {
    "objectID": "Misc/htw_dp.html#model-fitting-and-comparison",
    "href": "Misc/htw_dp.html#model-fitting-and-comparison",
    "title": "Project 2",
    "section": "Model Fitting and Comparison",
    "text": "Model Fitting and Comparison\nFollowing the procedure used by McDaniel & Busemeyer (2009), we will assess the ability of both ALM and EXAM to account for the empirical data when fitting the models to 1) only the training data, and 2) both training and testing data. Models will be fit directly to the trial by trial data of each individual participants, both by minimizing the root-mean squared deviation (RMSE), and by maximizing log likelihood. Because ALM has been shown to do poorly at accounting for human patterns extrapolation (DeLosh et al., 1997), we will also fit the extended EXAM version of the model, which operates identically to ALM during training, but includes a linear extrapolation mechanism for generating novel responses during testing."
  },
  {
    "objectID": "Model/abc_htw.html",
    "href": "Model/abc_htw.html",
    "title": "Approximate Bayesian Fitting",
    "section": "",
    "text": "Code# load and view data\npacman::p_load(tidyverse,data.table,lme4,future,furrr,abc,mvtnorm,patchwork,here)\npurrr::walk(here(c(\"Functions/alm_functions.R\",\"Functions/Display_Functions.R\",\"Functions/Noisy_Functions.R\")),source)\n\nselect &lt;- dplyr::select; mutate &lt;- dplyr::mutate \nd &lt;- readRDS(here(\"data/dPrune-01-19-23.rds\"))\nlevels(d$condit)\n\n[1] \"Constant\" \"Varied\"  \n\nCode# Prepare the data for analysis\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\nds &lt;- d %&gt;% filter(expMode %in% c(\"train\",\"train-Nf\",\"test-Nf\",\"test-train-nf\")) %&gt;% \nfilter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) %&gt;% \nselect(id,condit,catOrder,feedbackType,expMode,trial,gt.train,vb,band,bandInt,lowBound,highBound,input,vx,dist,vxb) \ndst &lt;- ds %&gt;% filter(expMode==\"train\")\ndst &lt;- dst %&gt;%\n  group_by(id, vb) %&gt;%\n  mutate(trial_band = row_number())\nCodedst &lt;- dst %&gt;% filter(expMode==\"train\",catOrder==\"orig\")\nvst &lt;- dst %&gt;% filter(condit==\"Varied\",gt.train&lt;=84) %&gt;% group_by(gt.train,vb,input) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=5,labels=c(1:5)),\n                          trial=gt.train) \ncst &lt;- dst %&gt;% filter(condit==\"Constant\",gt.train&lt;=84) %&gt;% group_by(gt.train,vb,input) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=5,labels=c(1:5)),\n                          trial=gt.train) \n\nggplot(dst %&gt;% filter(gt.train&lt;=84), aes(x = gt.train, y = vx,color=vb)) +\n  geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n  stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+facet_wrap(~condit)\n\n\n\nCode# ggplot(vst, aes(x = gt.trainBin, y = vx,color=vb)) +\n  # geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n  # stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n  # stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+facet_wrap(~condit)"
  },
  {
    "objectID": "Model/abc_htw.html#abc-functions",
    "href": "Model/abc_htw.html#abc-functions",
    "title": "Approximate Bayesian Fitting",
    "section": "ABC Functions",
    "text": "ABC Functions\n\nCode# c=1; lr=.5; noise_sd=.001; inNodes=7; outNodes=32\nfit_alm_sim &lt;- function(data, c, lr, noise_sd, inNodes, outNodes) {\n  train_data &lt;- data[, c(\"gt.train\", \"vb\", \"vx\",\"input\")] %&gt;% rename(\"trial\" = gt.train)\n  \n  sim_result &lt;- sim_train(\n    dat = train_data,\n    c = c,\n    lr = lr,\n    inNodes = inNodes,\n    outNodes = outNodes,\n    noise_sd = noise_sd\n  )\n  \n  train_data$almTrain &lt;- sim_result$almTrain\n  return(train_data$almTrain)\n}\n\ngenerate_prior &lt;- function(n) {\n  prior_samples &lt;- tibble(\n    c = runif(n, 0.001, 5),\n    lr = runif(n, 0.001, 3),\n    noise_sd = runif(n, 0, 0.0001),\n    inNodes = sample(c( 3,7, 14,21,28,35), size = n, replace = TRUE),\n    outNodes = sample(c(8,16, 32,48,64,80,96), size = n, replace = TRUE)\n  )\n  return(prior_samples)\n}\n\n\nVaried ABC\n\nCoden_prior_samples &lt;- 1000\nprior_samples &lt;- generate_prior(n_prior_samples)\n\n\n# Replace gt with dst in the following line:\nsimulated_data &lt;- future_map_dfc(seq_len(nrow(prior_samples)), function(idx) {\n  params &lt;- prior_samples[idx, ]\n  fit_alm_sim(vst, params$c, params$lr, params$noise_sd, params$inNodes, params$outNodes)\n},.options = furrr_options(seed = T))\n\n# Replace gt_obs with dst_obs and gt with dst in the following lines:\ndst_obs &lt;- vst$vx\ntolerance &lt;- 0.1 * sd(dst_obs)*500\nabc_result &lt;- abc(\n  target = dst_obs,\n  param = prior_samples,\n  sumstat = do.call(rbind, simulated_data),\n  tol = .1,\n  method = \"rejection\",\n  names=colnames(dst_obs)\n)\n\n\nposterior_samples &lt;- abc_result$unadj.values\ncolnames(posterior_samples) &lt;- c(\"c\", \"lr\", \"noise_sd\", \"inNodes\", \"outNodes\")\nposterior_samples_long &lt;- tidyr::pivot_longer(as.data.frame(posterior_samples), everything())\n\npostV &lt;- ggplot(posterior_samples_long, aes(x=value)) +\n  geom_density() +\n  facet_wrap(~name, scales=\"free\") +\n  theme_minimal() +\n  labs(x=\"Value\", y=\"Density\", title=\"Posterior Density Plots\")\n\nsummary_statistics &lt;- data.frame(mean = apply(posterior_samples, 2, mean),\n  median = apply(posterior_samples, 2, median))\n( summary_statistics=rownames_to_column(summary_statistics,var=\"parameter\") )\n\n\n\ns = sim_train(dat=mutate(vst,trial=gt.train),c=summary_statistics$mean[1],lr=summary_statistics$mean[2],inNodes=summary_statistics$mean[4],outNodes=summary_statistics$mean[5],noise_sd=summary_statistics$mean[3])\n\nggp &lt;- vst %&gt;% ungroup() %&gt;% mutate(pred=s$almTrain,input=as.factor(input))\ngvp &lt;- ggp %&gt;% ggplot(aes(x = trial, y = pred, color = input)) +\n  geom_line() + ylim(c(0,1600))+ggtitle(\"ALM Predictions (ABC estimation)\")\ngvo &lt;- ggp  %&gt;% ggplot(aes(x = trial, y = vx, color = input)) +\n  geom_line() + ylim(c(0,1600))+ggtitle(\"HTW Observed Training data\")\nggpv &lt;- postV/(gvp + gvo)\n\n#ggsave(\"images/abc_varied.png\",ggpv,width=12,height=10)\n\n\n\nCodeknitr::include_graphics(here(\"images/abc_varied.png\"))\n\n\n\nCode#![](images/abc_varied.png)\n\n\nConstant ABC\n\nCode# c=1; lr=.5; noise_sd=.001; inNodes=7; outNodes=32\n\nn_prior_samples &lt;- 1000\nprior_samples &lt;- generate_prior(n_prior_samples)\n\n# Replace gt with dst in the following line:\nsimulated_dataC &lt;- future_map_dfc(seq_len(nrow(prior_samples)), function(idx) {\n  params &lt;- prior_samples[idx, ]\n  fit_alm_sim(cst, params$c, params$lr, params$noise_sd, params$inNodes, params$outNodes)\n},.options = furrr_options(seed = T))\n\n# Replace gt_obs with dst_obs and gt with dst in the following lines:\ncst_obs &lt;- cst$vx\ntolerance &lt;- 0.1 * sd(dst_obs)*500\nabc_result &lt;- abc(\n  target = cst_obs,\n  param = prior_samples,\n  sumstat = do.call(rbind, simulated_dataC),\n  tol = .1,\n  method = \"rejection\",\n  names=colnames(cst_obs)\n)\n\n\nposterior_samples &lt;- abc_result$unadj.values\ncolnames(posterior_samples) &lt;- c(\"c\", \"lr\", \"noise_sd\", \"inNodes\", \"outNodes\")\nposterior_samples_long &lt;- tidyr::pivot_longer(as.data.frame(posterior_samples), everything())\n\npostC &lt;- ggplot(posterior_samples_long, aes(x=value)) +\n  geom_density() +\n  facet_wrap(~name, scales=\"free\") +\n  theme_minimal() +\n  labs(x=\"Value\", y=\"Density\", title=\"Posterior Density Plots\")\n\nsummary_statisticsC &lt;- data.frame(mean = apply(posterior_samples, 2, mean),\n  median = apply(posterior_samples, 2, median))\n summary_statisticsC=rownames_to_column(summary_statisticsC,var=\"parameter\") \nsummary_statisticsC\n\n\nsC = sim_train(dat=mutate(cst,trial=gt.train),c=summary_statisticsC$mean[1],lr=summary_statisticsC$mean[2],inNodes=summary_statisticsC$mean[4],outNodes=summary_statisticsC$mean[5],noise_sd=summary_statisticsC$mean[3])\n\nggpC &lt;- cst %&gt;% ungroup() %&gt;% mutate(pred=sC$almTrain,input=as.factor(input))\ngcp &lt;- ggpC %&gt;% ggplot(aes(x = trial, y = pred, color = input)) +\n  geom_line() + ylim(c(0,1600))+ggtitle(\"ALM Predictions (ABC estimation)\")\ngco &lt;- ggpC  %&gt;% ggplot(aes(x = trial, y = vx, color = input)) +\n  geom_line() + ylim(c(0,1600))+ggtitle(\"HTW Observed Constant Training data\")\npostC/(gcp + gco)\n\nggsave(here(\"images/abc_constant.png\",postC/(gcp + gco),width=12,height=10))"
  },
  {
    "objectID": "Model/htw_exam.html",
    "href": "Model/htw_exam.html",
    "title": "ALM & EXAM Fitting",
    "section": "",
    "text": "Prep data\n\nCodepacman::p_load(tidyverse,data.table,here)\noptions(dplyr.summarise.inform=FALSE)\n\n\nd &lt;- readRDS(here(\"data/dPrune-01-19-23.rds\"))\n\ndtest &lt;- d %&gt;% filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) %&gt;% group_by(id,lowBound) %&gt;% \n  mutate(nBand=n(),band=bandInt,id=factor(id)) %&gt;% group_by(id) %&gt;% mutate(nd=n_distinct(lowBound))\n# unique(dtest[dtest$nd==4,]$sbjCode) # 7 in wrong condition\ndtest &lt;- dtest %&gt;% group_by(id,lowBound) %&gt;% filter(nBand&gt;=5 & nd==6)\n# for any id that has at least 1 nBand &gt;=5, remove all rows with that id. \ndtest &lt;- dtest %&gt;% group_by(id) %&gt;% filter(!id %in% unique(dtest$id[dtest$nBand&lt;5]))\n\ndtestAgg &lt;- dtest %&gt;% group_by(id,condit,catOrder,feedbackType,vb,band,lowBound,highBound,input) %&gt;% mutate(vxCapped=ifelse(vx&gt;1600,1600,vx)) %&gt;%\n  summarise(vxMean=mean(vx),devMean=mean(dist),vxMed=median(vx),devMed=median(dist),\n            vxMeanCap=mean(vxCapped),.groups = \"keep\")\n\n# select first row for each id in d, then create histogram for nTrain\n#  d  %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ggplot(aes(nTrain)) + geom_histogram() + facet_wrap(~condit)\n  \nds &lt;- d %&gt;% filter(expMode %in% c(\"train\",\"train-Nf\",\"test-Nf\",\"test-train-nf\")) %&gt;% \nfilter(!id %in% unique(dtest$id[dtest$nBand&lt;5])) %&gt;% \nselect(id,condit,catOrder,feedbackType,expMode,trial,gt.train,vb,band,bandInt,lowBound,highBound,input,vx,dist,vxb) \n\ndst &lt;- ds %&gt;% filter(expMode==\"train\",catOrder==\"orig\")\n\nvTrainTrial &lt;- dst %&gt;% filter(condit==\"Varied\",gt.train&lt;=84) %&gt;% group_by(gt.train,vb) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=5,labels=c(1:5)))\n\nbinTrainTrial &lt;- dst %&gt;% filter(gt.train&lt;=83) %&gt;% group_by(gt.train,vb,condit) %&gt;% summarise(sdVx=sd(vx),vx=mean(vx),sdDist=sd(dist),dist=mean(dist)) %&gt;% \n  group_by(vb) %&gt;% mutate(gt.trainBin=cut(gt.train,breaks=6,labels=c(1:6)))\n\n\ntMax=84\nbandVec &lt;- rep(c(800,1000,1200),each=tMax/3)\nbandVec &lt;- bandVec[sample(1:length(bandVec),tMax,replace=FALSE)]\n\ntrainTrials &lt;- dst %&gt;% filter(gt.train&lt;=tMax) %&gt;% group_by(condit,gt.train,vb,bandInt,input) %&gt;% summarise(vx=mean(vx)) \n\ntv &lt;- trainTrials %&gt;% filter(condit==\"Varied\") %&gt;% group_by(gt.train) %&gt;% mutate(bandInt2=bandVec[gt.train]) %&gt;% filter(bandInt==bandInt2) %&gt;% select(-bandInt2) %&gt;% \n  rbind(.,trainTrials %&gt;% filter(condit==\"Constant\"))\n\n\nEmpirical Learning Patterns - Group Level\n\nCode# grouping by condit, display count of each trial in dst\n# dst %&gt;% group_by(condit,trial) %&gt;% summarise(n=n())\n# dst %&gt;% group_by(condit,gt.train) %&gt;% summarise(n=n())\n\n# display histogram with count on x axis\n#dst %&gt;% ggplot(aes(gt.train)) + geom_histogram() + facet_wrap(~condit)\n\nggplot(dst %&gt;% filter(gt.train&lt;=84), aes(x = gt.train, y = vx,color=vb)) +\n  geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n  stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+facet_wrap(~condit)\n\nggplot(binTrainTrial, aes(x = gt.trainBin, y = vx,color=vb)) +\n  geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n  stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n  stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+facet_wrap(~condit)\n\n\n# ggplot(vTrainTrial, aes(x = gt.train, y = vx,color=vb)) +\n#   geom_point(aes(color = vb), stat = \"summary\", fun = mean) + \n#   stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n#   stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)\n# \n# # plot vx and sdVx over gt.train, separate facets for vx and sdVx\n# vTrainTrial %&gt;% pivot_longer(cols=c(vx,sdVx),names_to=\"vxType\",values_to=\"vxVal\") %&gt;% \n#   ggplot(aes(gt.train,vxVal,color=vb)) + geom_line() + \n#   stat_summary(aes(color = vb), geom = \"line\", fun = mean) +\n#   stat_summary(geom=\"errorbar\",fun.data=mean_se,width=.4,alpha=.7)+\n#   facet_wrap(~vxType, scale=\"free_y\")\n\n\nModel Implementation\n\nCodeinput.activation&lt;-function(x.target, c){\n  return(exp((-1*c)*(x.target-inputNodes)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, c){\n  return(weights%*%input.activation(x.target, c))\n}\n\nmean.prediction&lt;-function(x.target, weights, c){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, c,trainVec){\n  #trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, c)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, c)\n  mOver = mean.prediction(xOver, weights, c)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n  \nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){\n  y.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  alm.train\n}\n\ntrainTest.alm&lt;-function(dat, c=0.05, lr=0.5, weights,testVec){\n  print(\"hi\")\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  almPred &lt;- sapply(testVec,mean.prediction,weights,c)\n  examPred &lt;- sapply(testVec,exam.prediction,weights,c,trainVec=c(1,sort(unique(dat$input))))\n  list(almPred=almPred,examPred=examPred)\n}\n\n\nFitting Functions\n\nCodewrap_alm &lt;- function(par,dat, weights,lossFun){\n    c=par[1]; lr=par[2]\n   pred=train.alm(dat, c=c, lr=lr, weights=weights)\n   #sqrt(mean((dat$vx -pred)^2))\n   lossFun(dat$vx,pred)\n}\n\nwrap_optim &lt;- function(dat,wm,lossFun){\n  bounds_lower &lt;- c(.0000001, .00001)\n  bounds_upper &lt;- c(5, 5)\n  parmsLab &lt;- c(\"c\",\"lr\")\n fit=optim(par=c(.1, .2),\n   fn = wrap_alm,\n   dat = dat, weights = wm,lossFun=lossFun,\n   method = \"L-BFGS-B\",\n   lower = bounds_lower,\n   upper = bounds_upper,\n   control = list(maxit = 1e4, pgtol = 0, factr = 0)\n )\n\n l=reduce(list(list(fit),fit$par,fit$value),append)\n names(l)=c(\"Fit\",parmsLab,\"Value\")\n return(l)\n}\n\nRMSE &lt;- function(x,y){\n # print(\"rmseTrial\")\n  sqrt(mean((x-y)^2))\n}\n\nRMSE.blocked &lt;- function(x,y,blocks=6){\n # print(\"rmseBlocked\")\n  data.table(x=x,y=y,t=seq(1,length(x))) %&gt;% \n    .[, `:=`(fitBins = cut(t, breaks = ..blocks, labels = c(1:..blocks)))] %&gt;%\n    .[, .(predMean = mean(x), obsMean = mean(y)), keyby = .(fitBins)] %&gt;%\n    .[, RMSE(predMean,obsMean)] %&gt;% as.numeric()\n}\n\n\n\nCodeinputNodes = seq(1,7,1)  # \noutputNodes = seq(50,1600,50)\nwm=matrix(.00001,nrow=length(outputNodes),ncol=length(inputNodes))\ntestVec=seq(2,7)\n\nlossFunctions &lt;- list(RMSE,RMSE.blocked)\n\n# fit_tbl &lt;- crossing(dat=tv %&gt;% split(.$condit),lossFun=lossFunctions)\n# fitGroups2 &lt;- pmap(fit_tbl,wrap_optim,wm)\n#fit_tbl &lt;- fit_tbl %&gt;% mutate(fit=pmap(list(dat,lossFun),wrap_optim))\n\n\nfitGroups &lt;- tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE.blocked))\nfitGroups2 &lt;- tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE))\n\n\n#ftg &lt;- replicate(2,tv %&gt;% split(.$condit) %&gt;% map(~wrap_optim(.,wm,RMSE)))\n\n\n\nfg=tv %&gt;% group_by(condit) %&gt;% nest() %&gt;% \n  mutate(fit=map(data,~wrap_optim(.,wm,RMSE)),\n         c=map(fit,\"c\"),lr=map(fit,\"lr\"))\n\n\n\n\nfg &lt;- fg %&gt;% mutate(trainPred=pmap(list(data,c,lr),train.alm,wm),\n                    obsv=map(data,\"vx\"),\n                    diff=map2(obsv,trainPred,~.x-.y),\n                    rmse=map(diff,~sqrt((mean(.^2)))),\n                    trainVec=map(data,~c(1,sort(unique(.$input)))),\n                    testVec=list(seq(2,7)),\n                    bandInt=list(sort(unique(dtest$bandInt))),\n                    testPred=pmap(list(data,c,lr,testVec),trainTest.alm,weights=wm),\n                    almTest=map(testPred,\"almPred\"),\n                    examTest=map(testPred,\"examPred\"))\n\n\n\ntestPreds &lt;- fg %&gt;% select(condit,bandInt,c,lr,almTest,examTest,testVec) %&gt;% unnest(cols = c(bandInt,c, lr, almTest, examTest,testVec))\n\n\n\nk &lt;- fg %&gt;%\n  select(condit, almTest, examTest) %&gt;%\n  split(.$condit) %&gt;%\n  map(~ list(d = data.frame(c1 = .$almTest, .$examTest)))\n\n\n\n# call train.alm with the optimized parameters\nvPred &lt;- tv %&gt;% filter(condit==\"Varied\") %&gt;% cbind(., pred=train.alm(., c=fitVaried$par[1], lr=fitVaried$par[2], weights=wm))\n\ncPred &lt;- tv %&gt;% filter(condit==\"Constant\") %&gt;% cbind(., pred=train.alm(., c=fitConstant$par[1], lr=fitConstant$par[2], weights=wm))\n\n# plot the results, showing gt.train on x axis, and separate lines with vx and pred\nvPred %&gt;% ggplot(aes(gt.train,vx)) + geom_line() + \ngeom_line(aes(y=pred),color=\"red\") + facet_wrap(~bandInt, scale=\"free_y\")+ggtitle(\"Varied training and predictions\")\n\ncPred %&gt;% ggplot(aes(gt.train,vx)) + geom_line() +\ngeom_line(aes(y=pred),color=\"red\") + facet_wrap(~bandInt, scale=\"free_y\")+ggtitle(\"Constant training and predictions\")\n\n\n# pred &lt;- train.alm(dat, c = .05, lr = .2, wm)\n# sqrt(mean((dat$vx - pred)^2))\n\n# pred &lt;- train.alm(dat, c = .113, lr = .048, wm)\n# sqrt(mean((dat$vx - pred)^2))\n\n\n\n# dat &lt;- tv %&gt;% filter(condit==\"Varied\")\n# c=.05; lr=.5; y.new=dat$vx[1]; x.new=dat$input[1]; weights=wm; i=1\n\n\ncreate learning models for condit and varied groups.\nWe can model the relation between performance and the number of practice trials as a power law function, or exponential function. Aggregatign over ids in dst. The models predict dist as an exponential decay function of trial number. Band is an additional predictor.\n\\[\nf_p(t) = \\alpha + \\beta t^{r} \\enspace\n\\] \\[\nf_e(t) = \\alpha + \\beta e^{rt} \\enspace\n\\]\n\nCode# fit exponential decay model as a function of trial number\n\nfit_exp &lt;- function(trial,dist,input){\n    # fit exponential decay model as a function of trial number, band is an additional predictor\n    fit &lt;- nls(dist ~ yf + (y0-yf) * exp(-r*trial) + beta2*input, start = list(yf = 300, y0 = 364, beta2=0, r = .1), data = data.frame(trial=trial,dist=dist,input=input))\n\n    # extract parameters\n    alpha &lt;- coef(fit)[1]\n    beta &lt;- coef(fit)[2]\n    beta2 &lt;- coef(fit)[3]\n    r &lt;- coef(fit)[4]\n    sigma_e &lt;- summary(fit)$sigma\n\n    # compute negative log likelihood\n    nllh &lt;- negative_llh_exp(dist, trial, alpha, beta, r, sigma_e)\n\n    # return parameters and negative log likelihood\n    return(list(alpha=alpha,beta=beta,beta2=beta2,r=r,sigma_e=sigma_e,nllh=nllh))\n}\n\n# Compute group averages for dist over trial and band. dst \n\navgTrain &lt;- dst %&gt;% group_by(id,condit,trial,band,input) %&gt;% summarise(dist=mean(dist)) %&gt;% ungroup() %&gt;% group_by(condit,trial,band) %&gt;% summarise(dist=mean(dist)) %&gt;% ungroup()\n \n# plot group averages\nggplot(avgTrain,aes(x=trial,y=dist)) + geom_line(aes(group=band,color=band)) +facet_grid(~condit)\n\navgTrain %&gt;% filter(condit==\"Constant\") %&gt;% nls(dist ~ yf + (y0-yf) * exp(-r*trial), start = list(yf = 120, y0 = 364, r = .1), data = .) %&gt;% summary()\n\n\navgTrain %&gt;% filter(condit==\"Constant\") %&gt;% nls(dist ~ SSasymp(trial, yf, y0, log_alpha),data=.)\n\n# fit exponential decay model for each condit\nfit_condit &lt;- avgTrain %&gt;% group_by(condit) %&gt;% do(fit_exp(trial=.$trial,dist=.$dist,input=.$input))\n\n# fit exponential model"
  },
  {
    "objectID": "Simulations/SimReplications.html",
    "href": "Simulations/SimReplications.html",
    "title": "General Simulations",
    "section": "",
    "text": "Functions\n\nCodepacman::p_load(tidyverse,reshape2)\n\ninput.activation&lt;-function(x.target, association.parameter){\n  return(exp(-1*association.parameter*(x.target-x.plotting)^2))\n}\n\noutput.activation&lt;-function(x.target, weights, association.parameter){\n  return(weights%*%input.activation(x.target, association.parameter))\n}\n\nmean.prediction&lt;-function(x.target, weights, association.parameter){\n  probability&lt;-output.activation(x.target, weights, association.parameter)/sum(output.activation(x.target, weights, association.parameter))\n  return(y.plotting%*%probability) # integer prediction\n}\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, association.parameter){\n  trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, association.parameter)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, association.parameter)\n  mOver = mean.prediction(xOver, weights, association.parameter)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n  \nupdate.weights&lt;-function(x.new, y.new, weights, association.parameter, update.parameter){\n  y.feedback.activation&lt;-exp(-1*association.parameter*(y.new-y.plotting)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, association.parameter)\n  return(weights+update.parameter*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, association.parameter)))\n}\n\nlearn.alm&lt;-function(y.learning, association.parameter=0.05, update.parameter=0.5){\n  weights&lt;-matrix(rep(0.00, length(y.plotting)*length(x.plotting)), nrow=length(y.plotting), ncol=length(x.plotting))\n  for (i in 1:length(y.learning)){\n    weights&lt;-update.weights(x.learning[i], y.learning[i], weights, association.parameter, update.parameter)\n    resp=mean.prediction(x.learning[i],weights,association.parameter)\n    weights[weights&lt;0]=0\n  }\n  alm.predictions&lt;-sapply(x.plotting, mean.prediction, weights=weights, association.parameter=association.parameter)\n  exam.predictions &lt;- sapply(x.plotting, exam.prediction, weights=weights, association.parameter=association.parameter)\n  return(list(alm.predictions=alm.predictions, exam.predictions=exam.predictions))\n}\n\n\nNo noise, 1 training rep\nRed dots are training points - gray lines are individual simulations, black line is average of simulations\n\nCode# | eval: false\n\ntrainRep=1\n\nx.plotting&lt;&lt;-seq(0,100, .5)\ny.plotting&lt;&lt;-seq(0, 210, by=2)\nf.plotting&lt;-as.numeric(x.plotting*2.2+30)\nx.learning&lt;-rep(x.plotting[20*c(4:7)+1])\nf.learning&lt;-rep(f.plotting[20*c(4:7)+1])\n\nparmVec &lt;- expand.grid(assoc=c(.1,0.5),update=c(0.2,1),noise=c(0),trainRep=c(1))\n#parmVec &lt;- expand.grid(assoc=c(.01),update=c(0.5),noise=c(30),trainRep=c(1,2,3,4))\n\nparmVec$sim &lt;- 1:nrow(parmVec)\nnSim=nrow(parmVec)\n\nnRep=5\noutput &lt;- list()\nfor (i in 1:nrow(parmVec)){\n  x.learning&lt;-rep(x.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  f.learning&lt;-rep(f.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  #noise.learning &lt;- rnorm(n_distinct(f.learning),sd=parmVec$noise[i])\n  output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning+rep(rnorm(n_distinct(f.learning),sd=parmVec$noise[i]),times=parmVec$trainRep[i]), \n                                         association.parameter=parmVec$assoc[i], update.parameter=parmVec$update[i])))\n}\n\n\n# convert list of dataframes to a list of lists, each list is a simulation, each element is a dataframe\noutput1 &lt;- lapply(output, function(x) lapply(x, as.data.frame)) # 10 dfs x 9 lists\noutput2 &lt;- lapply(output1, function(x) Reduce(rbind,x))# 1 df x 9 lists\noutput3 &lt;- lapply(output2, function(x) mutate(x, x=rep(x.plotting,nRep),y=rep(f.plotting,nRep),\n                                              repN=rep(seq(1,nRep),each=length(x.plotting))))\no4 &lt;- Reduce(rbind,output3) %&gt;% \n  mutate(sim=rep(seq(1,nrow(parmVec)),each=nRep*length(x.plotting))) %&gt;%\n  left_join(.,parmVec,by=\"sim\") %&gt;%\n  mutate(pvec=paste0(\"c=\",assoc,\"_lr=\",update,\"_noise=\",noise,\"_nrep=\",trainRep),pv=factor(pvec),rn=factor(repN)) \n\noMeans &lt;- o4 %&gt;% group_by(pv,x,y) %&gt;% \n  summarise(alm.predictions=mean(alm.predictions),exam.predictions=mean(exam.predictions),.groups=\"keep\")\n\no4 %&gt;% ggplot(aes(x=x,y=alm.predictions,color=rn))+geom_line(alpha=.7)+\n   scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5, linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=alm.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"ALM predictions\")\n\n\n\nCodeo4 %&gt;% ggplot(aes(x=x,y=exam.predictions,color=rn))+ geom_line()+ #geom_line(color=\"grey\",alpha=.4)+\n  scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5,linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=exam.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"EXAM predictions\")\n\n\n\n\nHigh noise, 1 training rep\nRed dots are training points - gray lines are individual simulations, black line is average of simulations\n\nCodetrainRep=1\n\nx.plotting&lt;&lt;-seq(0,100, .5)\ny.plotting&lt;&lt;-seq(0, 210, by=2)\nf.plotting&lt;-as.numeric(x.plotting*2.2+30)\nx.learning&lt;-rep(x.plotting[20*c(4:7)+1])\nf.learning&lt;-rep(f.plotting[20*c(4:7)+1])\n\n\nparmVec &lt;- expand.grid(assoc=c(.1,0.5),update=c(0.2,1),noise=c(30),trainRep=c(1))\n#parmVec &lt;- expand.grid(assoc=c(.01),update=c(0.5),noise=c(30),trainRep=c(1,2,3,4))\n\nparmVec$sim &lt;- 1:nrow(parmVec)\nnSim=nrow(parmVec)\n\nnRep=10\noutput &lt;- list()\nfor (i in 1:nrow(parmVec)){\n  x.learning&lt;-rep(x.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  f.learning&lt;-rep(f.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  #noise.learning &lt;- rnorm(n_distinct(f.learning),sd=parmVec$noise[i])\n  output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning+rep(rnorm(n_distinct(f.learning),sd=parmVec$noise[i]),times=parmVec$trainRep[i]), \n                                         association.parameter=parmVec$assoc[i], update.parameter=parmVec$update[i])))\n}\n\n# \n# nRep=3\n# output &lt;- list()\n# for (i in 1:nrow(parmVec)){\n#   output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning + rnorm(length(f.learning), sd=10), \n#                                          association.parameter=parmVec$assoc[i], update.parameter=parmVec$update[i])))\n# }\n\n\n\n#output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning + rnorm(length(f.learning), sd=10)\n\n# convert list of dataframes to a list of lists, each list is a simulation, each element is a dataframe\noutput1 &lt;- lapply(output, function(x) lapply(x, as.data.frame)) # 10 dfs x 9 lists\noutput2 &lt;- lapply(output1, function(x) Reduce(rbind,x))# 1 df x 9 lists\noutput3 &lt;- lapply(output2, function(x) mutate(x, x=rep(x.plotting,nRep),y=rep(f.plotting,nRep),\n                                              repN=rep(seq(1,nRep),each=length(x.plotting))))\no4 &lt;- Reduce(rbind,output3) %&gt;% \n  mutate(sim=rep(seq(1,nrow(parmVec)),each=nRep*length(x.plotting))) %&gt;%\n  left_join(.,parmVec,by=\"sim\") %&gt;%\n  mutate(pvec=paste0(\"c=\",assoc,\"_lr=\",update,\"_noise=\",noise,\"_nrep=\",trainRep),pv=factor(pvec),rn=factor(repN)) \n\noMeans &lt;- o4 %&gt;% group_by(pv,x,y) %&gt;% \n  summarise(alm.predictions=mean(alm.predictions),exam.predictions=mean(exam.predictions),.groups=\"keep\")\n\no4 %&gt;% ggplot(aes(x=x,y=alm.predictions,color=rn))+geom_line(alpha=.7)+\n   scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5, linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=alm.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"ALM predictions\")+ylim(0,300)\n\no4 %&gt;% ggplot(aes(x=x,y=exam.predictions,color=rn))+ geom_line()+ #geom_line(color=\"grey\",alpha=.4)+\n  scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5,linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=exam.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"EXAM predictions\")+ylim(0,300)\n\n\nHigh noise, 60 training rep\nRed dots are training points - gray lines are individual simulations, black line is average of simulations\n\nCodetrainRep=1\n\nx.plotting&lt;&lt;-seq(0,100, .5)\ny.plotting&lt;&lt;-seq(0, 210, by=2)\nf.plotting&lt;-as.numeric(x.plotting*2.2+30)\nx.learning&lt;-rep(x.plotting[20*c(4:7)+1])\nf.learning&lt;-rep(f.plotting[20*c(4:7)+1])\n\n\nparmVec &lt;- expand.grid(assoc=c(.1,0.5),update=c(0.2,1),noise=c(30),trainRep=c(60))\n#parmVec &lt;- expand.grid(assoc=c(.01),update=c(0.5),noise=c(30),trainRep=c(1,2,3,4))\n\nparmVec$sim &lt;- 1:nrow(parmVec)\nnSim=nrow(parmVec)\n\nnRep=10\noutput &lt;- list()\nfor (i in 1:nrow(parmVec)){\n  x.learning&lt;-rep(x.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  f.learning&lt;-rep(f.plotting[20*c(4:7)+1],times=parmVec$trainRep[i])\n  #noise.learning &lt;- rnorm(n_distinct(f.learning),sd=parmVec$noise[i])\n  output[[i]] &lt;- replicate(nRep, list(learn.alm(f.learning+rep(rnorm(n_distinct(f.learning),sd=parmVec$noise[i]),times=parmVec$trainRep[i]), \n                                         association.parameter=parmVec$assoc[i], update.parameter=parmVec$update[i])))\n}\n\n\n# convert list of dataframes to a list of lists, each list is a simulation, each element is a dataframe\noutput1 &lt;- lapply(output, function(x) lapply(x, as.data.frame)) # 10 dfs x 9 lists\noutput2 &lt;- lapply(output1, function(x) Reduce(rbind,x))# 1 df x 9 lists\noutput3 &lt;- lapply(output2, function(x) mutate(x, x=rep(x.plotting,nRep),y=rep(f.plotting,nRep),\n                                              repN=rep(seq(1,nRep),each=length(x.plotting))))\no4 &lt;- Reduce(rbind,output3) %&gt;% \n  mutate(sim=rep(seq(1,nrow(parmVec)),each=nRep*length(x.plotting))) %&gt;%\n  left_join(.,parmVec,by=\"sim\") %&gt;%\n  mutate(pvec=paste0(\"c=\",assoc,\"_lr=\",update,\"_noise=\",noise,\"_nrep=\",trainRep),pv=factor(pvec),rn=factor(repN)) \n\noMeans &lt;- o4 %&gt;% group_by(pv,x,y) %&gt;% \n  summarise(alm.predictions=mean(alm.predictions),exam.predictions=mean(exam.predictions),.groups=\"keep\")\n\no4 %&gt;% ggplot(aes(x=x,y=alm.predictions,color=rn))+geom_line(alpha=.7)+\n   scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5, linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=alm.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"ALM predictions\")+ylim(0,300)\n\no4 %&gt;% ggplot(aes(x=x,y=exam.predictions,color=rn))+ geom_line()+ #geom_line(color=\"grey\",alpha=.4)+\n  scale_color_manual(values=rep(\"grey\",nRep))+\n  theme(legend.position=\"none\")+\n  geom_point(data=data.frame(x=x.learning,y=f.learning),aes(x=x,y=y),color=\"red\")+\n  geom_line(data=o4,aes(x=x,y=y),color=\"black\",alpha=.5,linetype=2)+\n  geom_line(data=oMeans,aes(x=x,y=exam.predictions),color=\"black\")+\n  facet_wrap(~pv, scales=\"free_y\")+ggtitle(\"EXAM predictions\")+ylim(0,300)\n\n\nShiny App\n\nCodex.plotting&lt;&lt;-seq(0,90, .5)\ny.plotting&lt;&lt;-seq(0, 210, by=2)\nf.plotting&lt;-as.numeric(x.plotting * 2.2 + 30)\nx.learning&lt;-x.plotting[10*c(3:9)+1]\nf.learning&lt;-f.plotting[10*c(3:9)+1]\n\n# Single Simulation\n# get alm and exam predictions for full range of x.plotting\noutput&lt;-learn.alm(f.learning)\nalm.predictions&lt;-output$alm.predictions\nexam.predictions&lt;-output$exam.predictions\n\n# plot the results\nplot(x.plotting, f.plotting, type=\"l\", col=\"blue\", lwd=.5, xlab=\"x\", ylab=\"f(x)\")\npoints(x.learning, f.learning, col=\"red\", pch=19)\nlines(x.plotting, alm.predictions, col=\"green\", lwd=2)\nlines(x.plotting, exam.predictions, col=\"purple\", lwd=2)\nlegend(\"topright\", legend=c(\"f(x)\", \"training data\", \"ALM\", \"Exam\"), col=c(\"blue\", \"red\", \"green\", \"purple\"), lty=1.5, cex=0.8)\n\n#function to plot in greyscale\nplot.grey&lt;-function(predictions){\n  lines(x.plotting, predictions, col=\"grey\")\n}\n\n\n\nCode# Average of 100 simulations:\n# get alm and exam predictions for full range of x.plotting, averaged over 100 simulations\nnSim&lt;-10\noutput &lt;- replicate(nSim, list(learn.alm(f.learning + rnorm(length(f.learning), sd=0.1), \n                                         association.parameter=0.05, update.parameter=0.5)))\n\n#alm.predictions&lt;-do.call(rbind, lapply(output, function(x) x$alm.predictions))\nalm.predictions &lt;- Reduce(rbind,output %&gt;% map(\"alm.predictions\"))\nexam.predictions &lt;- Reduce(rbind,output %&gt;% map(\"exam.predictions\"))\n\nalm.predictions.avg&lt;-apply(alm.predictions, 2, mean)\nexam.predictions.avg&lt;-apply(exam.predictions, 2, mean)\ndfAvg&lt;-data.frame(x=x.plotting, f=f.plotting, alm=alm.predictions.avg, exam=exam.predictions.avg)\ndfAvg&lt;-reshape2::melt(dfAvg, id.vars=\"x\")\ndfAvg$model&lt;-factor(dfAvg$variable, levels=c(\"f\", \"alm\", \"exam\"))\nggplot(dfAvg, aes(x=x, y=value, color=model)) + geom_line() + geom_point(data=data.frame(x=x.learning, f=f.learning), aes(x=x, y=f), color=\"red\", size=2) + theme_bw() + theme(legend.position=\"topright\")\n\n\nalm.predictions&lt;-as.data.frame(alm.predictions) %&gt;% mutate(sim=seq(1:nSim))\nalm.predictions&lt;-pivot_longer(alm.predictions, cols=1:ncol(alm.predictions)-1, \n                              names_to=c(\"sim\"), values_to=\"alm\",names_repair = \"unique\") \ncolnames(alm.predictions)=c(\"sim\",\"x\",\"pred\")\nalm.predictions &lt;- alm.predictions %&gt;% mutate(stim = as.numeric(gsub(\"V\", \"\", x)),model=\"alm\",x=x.plotting[stim])\n\n\nexam.predictions&lt;-as.data.frame(exam.predictions) %&gt;% mutate(sim=seq(1:nSim))\nexam.predictions&lt;-pivot_longer(exam.predictions, cols=1:ncol(exam.predictions)-1, \n                               names_to=c(\"sim\"), values_to=\"exam\",names_repair = \"unique\")\ncolnames(exam.predictions)=c(\"sim\",\"x\",\"pred\")\nexam.predictions &lt;- exam.predictions %&gt;% mutate(stim = as.numeric(gsub(\"V\", \"\", x)),model=\"exam\",x=x.plotting[stim])\n\n\ndf&lt;- rbind(alm.predictions,exam.predictions)\n\nggplot(df, aes(x=x, y=pred, color=sim)) + geom_line(alpha=.2) + facet_wrap(~model) + theme_bw() + \n  geom_point(data=data.frame(x=x.learning, f=f.learning), aes(x=x, y=f), color=\"red\", size=2)+\n  geom_line(data=data.frame(x=x.plotting, f=f.plotting),aes(x=x,y=f),color=\"black\")+\n  geom_line(data=dfAvg %&gt;% filter(model!=\"f\"),aes(x=x,y=value),color=\"green\")"
  },
  {
    "objectID": "Simulations/simParameterRecovery.html",
    "href": "Simulations/simParameterRecovery.html",
    "title": "Parameter Recovery Simulations",
    "section": "",
    "text": "Parameter Recovery Simulations\nAssessing the parameter recovery of ALM and EXAM on synthetic data.\n\nCodepacman::p_load(tidyverse,data.table)\noptions(dplyr.summarise.inform=FALSE)\n\ninput.activation&lt;-function(x.target, c) { return(exp((-1*c)*(x.target-inputNodes)^2))}\n\noutput.activation&lt;-function(x.target, weights, c){\n  return(weights%*%input.activation(x.target, c))\n}\nmean.prediction&lt;-function(x.target, weights, c){\n  probability&lt;-output.activation(x.target, weights, c)/sum(output.activation(x.target, weights, c))\n  return(outputNodes%*%probability) # integer prediction\n}\n\nupdate.weights&lt;-function(x.new, y.new, weights, c, lr){t\n  yt.feedback.activation&lt;-exp(-1*c*(y.new-outputNodes)^2)\n  x.feedback.activation&lt;-output.activation(x.new, weights, c)\n  return(weights+lr*(y.feedback.activation-x.feedback.activation)%*%t(input.activation(x.new, c)))\n}\n\ntrain.alm&lt;-function(dat, c=0.05, lr=0.5, weights){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  alm.train\n}\n\n# function to generate exam predictions\nexam.prediction&lt;-function(x.target, weights, c,trainVec){\n  #trainVec = sort(unique(x.learning))\n  nearestTrain = trainVec[which.min(abs(trainVec-x.target))]\n  aresp = mean.prediction(nearestTrain, weights, c)\n  xUnder = ifelse(min(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) - 1])\n  xOver = ifelse(max(trainVec) == nearestTrain, nearestTrain, trainVec[which(trainVec == nearestTrain) + 1])\n  mUnder = mean.prediction(xUnder, weights, c)\n  mOver = mean.prediction(xOver, weights, c)\n  exam.output = round(aresp + ((mOver - mUnder) / (xOver - xUnder)) * (x.target - nearestTrain), 3)\n  exam.output\n}\n\n# Modify the sim_data function to accept the dataset as an argument\nsim_data &lt;- function(dat, c=0.5, lr=0.2, inNodes=7, outNodes=32, trainVec=c(5,6,7)) {\n  inputNodes &lt;&lt;- seq(1,7,length.out=inNodes*1)  \n  outputNodes &lt;&lt;- seq(50,1600,length.out=outNodes*1) \n  wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n  tt&lt;-trainTest.alm(dat, c, lr, wm, trainVec)\n}\n\ngen_train &lt;- function(trainVec=c(5,6,7),trainRep=10,noise=0){\n   bandVec=c(0,100,350,600,800,1000,1200)\n   ts &lt;- rep(seq(1,length(trainVec)),trainRep)\n   noiseVec=rnorm(length(ts),mean=0)*noise\n   if(noise==0) {noiseVec=noiseVec*0}\n   tibble(input=trainVec[ts],vx=bandVec[trainVec[ts]]+noiseVec)\n}\n\n\ntrainTest.alm&lt;-function(dat, c=0.05, lr=0.5, weights,testVec){\n   alm.train&lt;-rep(NA,nrow(dat))  \n  for (i in 1:nrow(dat)){\n    weights &lt;- update.weights(dat$input[i], dat$vx[i], weights, c, lr)\n    resp = mean.prediction(dat$input[i], weights, c)\n    alm.train[i]=resp\n    weights[weights&lt;0]=0\n  }\n  almPred &lt;- sapply(testVec,mean.prediction,weights,c)\n  examPred &lt;- sapply(testVec,exam.prediction,weights,c,trainVec=c(1,sort(unique(dat$input))))\n  list(almTrain=alm.train,almPred=almPred,examPred=examPred)\n}\n\nwrap_alm &lt;- function(par,dat, weights,lossFun){\n    c=par[1]; lr=par[2]\n   pred=train.alm(dat, c=c, lr=lr, weights=weights)\n   #sqrt(mean((dat$vx -pred)^2))\n   lossFun(dat$vx,pred)\n}\n\nwrap_optim &lt;- function(dat,lossFun=RMSE){\n  if(class(lossFun)==\"character\"){lossFun=get(lossFun)}\n  inputNodes = seq(1,7,1)  # \n  outputNodes = seq(50,1600,50)\n  wm=matrix(.00001,nrow=length(outputNodes),ncol=length(inputNodes))\n  testVec=seq(2,7)\n  \n  bounds_lower &lt;- c(.0000001, .00001)\n  bounds_upper &lt;- c(10, 10)\n  parmsLab &lt;- c(\"c\",\"lr\")\n  \n fit=optim(par=c(.1, .2),\n   fn = wrap_alm,\n   dat = dat, weights = wm,lossFun=lossFun,\n   method = \"L-BFGS-B\",\n   lower = bounds_lower,\n   upper = bounds_upper,\n   control = list(maxit = 1e5, pgtol = 0, factr = 0)\n )\n\n l=reduce(list(list(fit),fit$par,fit$value),append)\n names(l)=c(\"Fit\",parmsLab,\"Value\")\n return(l)\n}\n\n# implement wrap_optim using grid search\nwrap_grid &lt;- function(dat,lossFun=RMSE){\n  if(class(lossFun)==\"character\"){lossFun=get(lossFun)}  \n  inputNodes = seq(1,7,1)  # \n  outputNodes = seq(50,1600,50)\n  wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n  testVec=seq(2,7)\n  # define grid boundaries\n  cRange &lt;- seq(.000001, 5, length.out = 30)\n  lrRange &lt;- seq(.05, 5, length.out = 20)\n  # create grid\n  grid &lt;- expand.grid(c = cRange, lr = lrRange)\n  grid$Value &lt;- NA\n  # loop through grid\n  for (i in 1:nrow(grid)) {\n    grid$Value[i] &lt;- wrap_alm(par=c(grid[i,c(\"c\") ],grid[i,c(\"lr\") ]), dat=dat, weights=wm,lossFun=lossFun)\n  }\n\n  # find best fit\n  bestFit &lt;- grid[which.min(grid$Value), ]\n  bestFit$Value &lt;- min(grid$Value)\n\n  # return best fit, and best c and lr\n  return(list(Fit = bestFit, c = bestFit$c, lr = bestFit$lr, Value = bestFit$Value))\n \n}\n\n# sim_data &lt;- function(c=0.5, lr=0.2,noise=0,inNodes=7,outNodes=32,\n#                      trainVec=c(5,6,7),trainRep=10,testVec=seq(2,7)) {\n#   \n#   inputNodes &lt;&lt;- seq(1,7,length.out=inNodes*1)  \n#   outputNodes &lt;&lt;- seq(50,1600,length.out=outNodes*1) \n#   wm=matrix(.0000,nrow=length(outputNodes),ncol=length(inputNodes))\n#   dat&lt;-gen_train(trainVec,trainRep,noise)\n#   #trainDat &lt;- train.alm(dat,c,lr,wm)\n#   tt&lt;-trainTest.alm(dat,c,lr,wm,testVec)\n# }\n\n\nLoss Functions\n\nCodeRMSE &lt;- function(x,y){\n # print(\"rmseTrial\")\n  sqrt(mean((x-y)^2))\n}\n\nRMSE.blocked &lt;- function(x,y,blocks=6){\n  #print(\"rmseBlocked\")\n  data.table(x=x,y=y,t=seq(1,length(x))) %&gt;% \n    .[, `:=`(fitBins = cut(t, breaks = ..blocks, labels = c(1:..blocks)))] %&gt;%\n    .[, .(predMean = mean(x), obsMean = mean(y)), keyby = .(fitBins)] %&gt;%\n    .[, RMSE(predMean,obsMean)] %&gt;% as.numeric()\n}\n\nMAE &lt;- function(x, y) {\n  mean(abs(x - y))\n}\n\nMAPE &lt;- function(x, y) {\n  mean(abs((x - y) / y)) * 100\n}\n\nMedAE &lt;- function(x, y) {\n  median(abs(x - y))\n}\n\nHuberLoss &lt;- function(x, y, delta = 1) {\n  error &lt;- x - y\n  abs_error &lt;- abs(error)\n  loss &lt;- ifelse(abs_error &lt;= delta, 0.5 * error^2, delta * (abs_error - 0.5 * delta))\n  mean(loss)\n}\n\n\n\nCodek= parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise))),\n                                                 o=map(td,head,1),vx1=map(o,\"vx\"))\n\nparmVec &lt;- tibble(crossing(c = c(0.1), lr = c(0.1,0.4,1), noise = c(10), trainRep = c(20), lossFun = list(\"RMSE\", \"MAE\",\"MAPE\", \"MedAE\", \"HuberLoss\"), simNum = 1:10))\n\nsdp &lt;- parmVec %&gt;% group_by(simNum,c,lr) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise)))) %&gt;% ungroup() %&gt;%\n  mutate(d = pmap(list(td, c, lr), ~sim_data(dat = .x, c = ..2, lr = ..3)),\n         almTrainDat = map(d, \"almTrain\"),\n         almTestDat = map(d, \"almPred\"),\n         examTestDat = map(d, \"examPred\"),\n         fitO = map2(td, lossFun, ~wrap_optim(.x, .y)),\n         cFitO = map_dbl(fitO, \"c\"),\n         lrFitO = map_dbl(fitO, \"lr\"),\n         optimValO = map_dbl(fitO, \"Value\")) \n\nsdpResults &lt;- sdp %&gt;% \n  mutate(lossFun=as.character(lossFun),cDiff=cFitO-c,lrDiff=lrFitO-lr) %&gt;%\n  relocate(simNum,lossFun, c,lr,cFitO,lrFitO,optimValO,cDiff,lrDiff) %&gt;% \n  arrange(lossFun,lr,c)\n\naveraged_sdp &lt;- sdpResults %&gt;%\n  group_by(lossFun, c, lr) %&gt;%\n  summarise(\n    avg_cFitO = mean(cFitO),\n    avg_lrFitO = mean(lrFitO),\n    avg_optimValO = mean(optimValO),\n    avg_cDiff = mean(cDiff),\n    avg_lrDiff = mean(lrDiff),\n    .groups = \"drop\"\n  )\n\n\naveraged_sdp &lt;- sdpResults %&gt;%\n  group_by(lossFun, c, lr) %&gt;%\n  summarise(\n    avg_cFitO = mean(cFitO),\n    var_cFitO = var(cFitO),\n    avg_lrFitO = mean(lrFitO),\n    var_lrFitO = var(lrFitO),\n    avg_optimValO = mean(optimValO),\n    var_optimValO = var(optimValO),\n    avg_cDiff = mean(cDiff),\n    var_cDiff = var(cDiff),\n    avg_lrDiff = mean(lrDiff),\n    var_lrDiff = var(lrDiff),\n    .groups = \"drop\"\n  )\n\n# averaged_sdp &lt;- sdp %&gt;%\n#   group_by(c, lr, noise, trainRep, lossFun) %&gt;%\n#   summarise(across(starts_with(\"cFit\") | starts_with(\"lrFit\") | starts_with(\"optimVal\"), list(mean = mean), .names = \"mean_{.col}\")) %&gt;% \n#   mutate(lossFun=as.character(lossFun),\n#          diff_cFitO = abs(c - mean_cFitO),\n#          diff_lrFitO = abs(lr - mean_lrFitO)) %&gt;%\n#   relocate(noise, trainRep, lossFun, c, diff_cFitO, lr, diff_lrFitO) %&gt;%\n#   dplyr::arrange(c,lr)\n\n\n#k=parmVec %&gt;% group_by(simNum) %&gt;% mutate(td = list(gen_train(trainRep = first(trainRep), noise = first(noise))))\n#sdp &lt;- parmVec %&gt;% mutate(d = pmap(list(c, lr, noise, trainRep), ~sim_data(c = ..1, lr = ..2, noise = ..3, trainRep = ..4)),\n\n\n\nCodelibrary(plotly)\n\ng2= grid %&gt;% filter(Value&lt;160) %&gt;% arrange(Value)\n#plot_ly() %&gt;% add_trace(data=g2,x=grid$c,y=grid$lr,z=grid$Value,type=\"mesh3d\")\n \nplot_ly(data=g2,x=~c,y=~lr,z=~Value,type = 'mesh3d')\nplot_ly(g2,type = 'surface')\n\n\n\n\n\nfig &lt;- plot_ly(x = grid2$lr, y = grid2$c, z = grid2$Value) %&gt;% add_surface()\n\np &lt;- ggplot(g2, aes(c, lr, z= Value)) +\n  stat_contour(geom=\"polygon\",aes(fill=stat(level))) +\n  scale_fill_distiller(palette = \"Spectral\", direction = 1)\nggplotly(p)\n\n\np &lt;- ggplot(grid, aes(c, lr, z= Value,colour=stat(level))) +\n  geom_contour() \nggplotly(p)\n\n\nplot_ly(g2, x = ~c, y = ~lr, z = ~Value, type = 'scatter3d', mode = 'lines+markers',\n        opacity = 7, \n        line = list(width = 6, colorscale = 'Viridis', reverscale = FALSE)\n        )\n\n\n#install.packages(\"echarts4r\")\nlibrary(echarts4r)\ng2 |&gt; \n  e_charts(c) |&gt; \n  e_surface(lr, Value, wireframe = list(show = FALSE)) |&gt; \n  e_visual_map(Value)\n\n\nc lr noise trainRep lossFun mean_cFitO mean_cFitG mean_lrFitO mean_lrFitG mean_optimValO mean_optimValG  1 0.1 0.4 500 20 RMSE 4.94 5 1.00 1.09 3.30e- 5 0.115\n2 0.1 0.4 500 20 MAE 0.100 5 0.268 1.09 2.56e+ 1 0.0921 3 0.1 0.4 500 20 MAPE 0.101 5 0.268 1.09 2.31e+ 0 0.00954 4 0.1 0.4 500 20 MedAE 0.288 5 0.429 1.09 3.65e- 1 0.124\n5 0.1 0.4 500 20 HuberLoss 8.09 5 1.00 1.09 1.98e-17 0.00662"
  },
  {
    "objectID": "Analysis/e1_test.html#parameter-median-95-ci-pd-rhat-ess",
    "href": "Analysis/e1_test.html#parameter-median-95-ci-pd-rhat-ess",
    "title": "E1 Testing",
    "section": "Parameter | Median | 95% CI | pd | Rhat | ESS",
    "text": "Parameter | Median | 95% CI | pd | Rhat | ESS\n(Intercept) | 267.70 | [ 210.48, 321.87] | 100% | 1.006 | 750.00 conditVaried | 118.88 | [ 36.84, 202.23] | 99.84% | 1.004 | 840.00 vb350M550 | -67.81 | [ -91.67, -42.66] | 100% | 1.003 | 1740.00 vb600M800 | -113.97 | [-151.32, -73.86] | 100% | 1.005 | 908.00 vb800M1000 | -77.54 | [-129.94, -22.20] | 99.64% | 1.005 | 966.00 vb1000M1200 | -29.85 | [ -90.43, 34.84] | 82.97% | 1.006 | 868.00 vb1200M1400 | 24.76 | [ -39.49, 93.41] | 77.78% | 1.006 | 935.00 conditVaried:vb350M550 | -30.71 | [ -67.09, 6.00] | 94.98% | 1.002 | 2037.00 conditVaried:vb600M800 | -36.92 | [ -94.67, 19.62] | 89.95% | 1.003 | 1094.00 conditVaried:vb800M1000 | -85.53 | [-167.66, -6.41] | 98.18% | 1.003 | 1128.00 conditVaried:vb1000M1200 | -148.70 | [-239.29, -59.15] | 99.96% | 1.004 | 1046.00 conditVaried:vb1200M1400 | -169.24 | [-268.34, -75.07] | 99.99% | 1.003 | 1068.00"
  },
  {
    "objectID": "Analysis/e1_test.html#parameter-median-95-ci-pd-rhat-ess-1",
    "href": "Analysis/e1_test.html#parameter-median-95-ci-pd-rhat-ess-1",
    "title": "E1 Testing",
    "section": "Parameter | Median | 95% CI | pd | Rhat | ESS",
    "text": "Parameter | Median | 95% CI | pd | Rhat | ESS\nsigma | 211.87 | [208.91, 214.99] | 100% | 1.000 | 16418.00\n\n\n\nDiscrimination between Velocity Bands\nNEEDS TO BE WRITTEN"
  },
  {
    "objectID": "Misc/e1_brms_test.html#vx",
    "href": "Misc/e1_brms_test.html#vx",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Vx",
    "text": "Vx\n\nCodemodelName &lt;- \"e1_testVxRF\"\ne1_testDistRF &lt;- brm(vx ~ condit + bandInt+ (1 + vb|id),\n                     data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\n\nmodelName &lt;- \"e1_testVxRF2\"\ne1_testVxRF2 &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\n\nmodelName &lt;- \"e1_test_vx_Int\"\ne1_testDist_Int &lt;- brm(vx ~ 0 + Intercept + bandInt + condit + (1|id), \n                       data=test,\n                       iter=2000, chains=4, silent=0,\n                       file=paste0(here::here(\"data/model_cache\",modelName)))\n\n\nmodelName &lt;- \"e1_testVxRF2_02\"\ne1_testDistRF2_02 &lt;- brm(vx ~ 0 + condit:bandInt + (0 + bandInt|id),data=test,file=paste0(here::here(\"data/model_cache\",modelName)))\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDistRF2_02, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\n\n\nmodelName &lt;- \"e1_testConditBand_vx_Gr\"\ne1_testConditBand_vx_Gr &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|gr(id,by=condit)),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\nmodelName &lt;- \"e1_testConditBand0_vx_Gr\"\ne1_testConditBand_vx_Gr &lt;- brm(vx ~ 0 + condit * bandInt + (0 + bandInt|gr(id,by=condit)),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\nmodelName &lt;- \"e1_testConditBand02_vx_Gr\"\ne1_testConditBand_vx_Gr &lt;- brm(vx ~ 1 + condit * bandInt + (0 + bandInt|gr(id,by=condit)),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4,silent=0)\n\n\n\nCodebform1 &lt;- bf(mvbind(vx, dist) ~ condit * vb + (1|p|id)) + set_rescor(TRUE)\nmv1 &lt;- brm(bform1, data = test, chains = 2, cores = 2)\n\nconditional_effects(mv1)\npp_check(mv1,resp=\"dist\")\npp_check(mv1,resp=\"vx\")\n\n\nppc_dens_overlay_grouped(test$vx,posterior_predict(mv1,ndraws=200),test$vb)\nbayes_R2(mv1)\n\n\nbf_dist &lt;- bf(dist|trunc(lb=-70) ~ condit * vb + (0 + vb|id) ) + gaussian()\nbf_vx &lt;- bf(vx|trunc(lb=0) ~ condit * vb + (0 + vb|id)) + gaussian()\n\nmv3 &lt;- brm(bf_dist + bf_vx + set_rescor(FALSE),\n            data = test, chains = 2, cores = 2,iter=1300, silent=0,\n           file=here::here(\"data/model_cache/mv_trunc2\"))\nbayes_R2(mv3)\npp_check(mv3,resp=\"dist\")\n\nppc_dens_overlay_grouped(test$vx,posterior_predict(mv3,ndraws=200),test$vb)\n\n\n\nbform2 &lt;- bf(mvbind(vx, vy) ~ condit * vb + (0+vb|id)) + set_rescor(FALSE)\nmv3 &lt;- brm(bform2, data = test, chains = 2, cores = 2,silent=0)\nbayes_R2(mv3)\npp_check(mv3,resp=\"vx\")\npp_check(mv3,resp=\"vy\")\n\n\n\nfitted(mv3) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(test %&gt;% select(id,condit,vx,vy,vb,bandInt)) %&gt;%\n  ggplot(aes(x = vx, y = Estimate.vx)) +\n  geom_abline(linetype = 2, color = \"grey50\", linewidth = .5) +  \n  geom_point(size = 1.5, aes(color=), alpha = 3/4) +\n  geom_linerange(aes(ymin = Q2.5.vx, ymax = Q97.5.vx),\n                 size = 1/4, color = \"firebrick4\") +\n  facet_wrap(~vb)\n\n\n\nCodeconditRF &lt;- brm(vx ~ vb + condit + (1+vb|condit),data=test,iter=1000,chains=2)\n\nconditRFX &lt;- brm(vx ~ vb * condit + (1+vb|condit),data=test,iter=1000,chains=2,silent=0,\n                 file=here(\"data/model_cache/e1_conditRFX\"))\n\n\nconditRFX %&gt;%\n  spread_draws(b_Intercept, r_condit[condit,]) %&gt;%\n  mutate(condition_mean = b_Intercept + r_condit) %&gt;%\n  ggplot(aes(y = condit, x = condition_mean)) +\n  stat_halfeye() + facet_wrap(~vb)\n\n\n\nCodevx_mm4 &lt;- brm(vx ~ (1+vb|id), data=test,chains=2,silent=0)\n\nmcmc_areas(vx_mm4,prob=.5,regex_pars=c(\"^r_id\\\\[1,.*\\\\]\"),regex=T)\n\n\nvx_mm5&lt;- brm(vx ~ 0 + (0+vb|id), data=test,chains=2,silent=0,iter=1200)\n\n\nplot_subject_fits &lt;- function(model, subject_code) {\n  pattern &lt;- glue(\"^r_id\\\\[{subject_code},.*\\\\]\")\n  plot &lt;- mcmc_areas(model, prob = .5, regex_pars = c(pattern)) +\n            ggtitle(glue(\"fit for subject #{subject_code}:\"))\n  return(plot)\n}\n\nplot_subject_fits(vx_mm5,86)\n\n\nindividual_coefs &lt;- coef(gt_vx)$id %&gt;% as_tibble(rownames = “id”) %&gt;% select(id, starts_with(“Estim”)) %&gt;% pivot_longer(cols = -id, names_to = “variable”, values_to = “value”) %&gt;% separate(variable, c(“type”, “effect”), sep = “\\.”)\ndist_gauss_trunc0 &lt;- brm(dist|trunc(lb=0) ~ 1+vb*condit + (0+vb|id),data=testExtrap,family=gaussian(), iter=2000, control=list(adapt_delta=0.92, max_treedepth=13), chains=3, file=paste0(here::here(“data/model_cache”,“gauss_t_vbCondit_dist_extrap_ml6”)), silent=0);\ngauss_trunc0 &lt;- brm(vx|trunc(lb=0) ~ 1+vb*condit + (0+vb|id),data=testExtrap,family=gaussian(), iter=2000, control=list(adapt_delta=0.92, max_treedepth=13), chains=3, file=paste0(here::here(“data/model_cache”,“gauss_t_vbCondit_vx_extrap_ml6”)), silent=0);\nintercept condit_varied vb350m550 vb600m800 vb800m1000 vb1000m1200 vb1200m1400 condit_varied_vb350m550 condit_varied_vb600m800 condit_varied_vb800m1000 condit_varied_vb1000m1200 condit_varied_vb1200m1400"
  },
  {
    "objectID": "Analysis/e1_test.html#analyses-strategy",
    "href": "Analysis/e1_test.html#analyses-strategy",
    "title": "E1 Testing",
    "section": "",
    "text": "To assess differences between groups, we used Bayesian Mixed Effects Regression. Analyses were done using brms package in R Bürkner (2017) as well as Makowski et al. (2019).\nUnless otherwise stated, we ran each model with 4 chains, 5000 iterations per chain, the first 2500 of which were discarded as warmup chains. Rhat values were generally within an acceptable range, with values &lt;=1.02.\nWe made use of two separate performance metrics, deviation and discrimination. Deviation was quantified as the absolute deviation from the nearest boundary of the velocity band, or set to 0 if the throw velocity fell anywhere inside the target band. Thus, when the target band was 600-800, throws of 400, 650, and 1100 would result in deviation values of 200, 0, and 300, respectively. Discrimination was measured by fitting a linear model to the testing throws of each subjects, with the lower end of the target velocity band as the predicted variable, and the x velocity produced by the participants as the predictor variable. Participants who reliably discriminated between velocity bands tended to have positive slopes with values ~1, while participants who made throws irrespective of the current target band would have slopes ~0."
  },
  {
    "objectID": "Analysis/e1_test.html#results",
    "href": "Analysis/e1_test.html#results",
    "title": "E1 Testing",
    "section": "Results",
    "text": "Results\nTesting Phase - No feedback.\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw.\nDeviation From Target Band\nTo assess differences in accuracy between groups, we used Bayesian mixed effects regression models implemented via brms in R. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (vb), and their interaction, with random intercepts and slopes for each participant (id).\n\nCode#contrasts(test$condit) \n#  Varied\n# Constant      0\n# Varied        1\n\n# contrasts(test$vb)\n#           350-550 600-800 800-1000 1000-1200 1200-1400\n# 100-300         0       0        0         0         0\n# 350-550         1       0        0         0         0\n# 600-800         0       1        0         0         0\n# 800-1000        0       0        1         0         0\n# 1000-1200       0       0        0         1         0\n# 1200-1400       0       0        0         0         1\n\nmodelName &lt;- \"e1_testDistRF2\"\ne1_testDistRF2 &lt;- brm(dist ~ condit * vb + (1 + vb|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nmp1 &lt;- model_parameters(e1_testDistRF2,effects=\"fixed\",diagnostic=c(\"ESS\"),verbose=FALSE)\n\n\nThe model predicting absolute deviation (dist) showed clear effects of both training condition and target velocity band (Table X). Overall, the varied training group showed a larger deviation relative to the constant training group (β = 119, 95% CI [37, 202]). Deviation also depended on target velocity band, with lower bands showing less deviation.\nDiscrimination between Velocity Bands\nIn addition to accuracy/deviation. We also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350).\nNEEDS TO BE WRITTEN\nE1 Results Discussion"
  },
  {
    "objectID": "Analysis/e1_test.html#misc",
    "href": "Analysis/e1_test.html#misc",
    "title": "E1 Testing",
    "section": "Misc",
    "text": "Misc\nCodetext_tbl &lt;- data.frame(\n    'Step'=c(\"Input Activation\",\"Output Activation\",\"Output Probability\",\"Mean Output\",\"Feedback Activation\",\"Update Weights\",\"Extrapolation\",\"\"),\n    'Equation' = c(\"$a_i(X) = \\\\frac{e^{-c \\\\cdot (X-X_i)^2}}{ \\\\sum_{k=1}^Me^{-c \\\\cdot (X-X_i)^2}}$\", \n                   '$O_j(X) = \\\\sum_{k=1}^Mw_{ji} \\\\cdot a_i(X)$',\n                   '$P[Y_j | X] = \\\\frac{O_i(X)}{\\\\sum_{k=1}^Mo_k(X)}$',\n                   \"$m(x) = \\\\sum_{j=1}^LY_j \\\\cdot \\\\bigg[\\\\frac{O_j(X)}{\\\\sum_{k=1}^Lo_k(X)}\\\\bigg]$\",\n                   \"$f_j(Z)=e^{-c\\\\cdot(Z-Y_j)^2}$\",\n                   \"$w_{ji}(t+1)=w_{ji}(t)+\\\\alpha \\\\cdot {f_i(Z(t))-O_j(X(t))} \\\\cdot a_i(X(t))$\",\n                   \"$P[X_i|X] = \\\\frac{a_i(X)}{\\\\sum_{k=1}^Ma_k(X)}$\",\n                   \"$E[Y|X_i]=m(X_i) + \\\\bigg[\\\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\\\bigg] \\\\cdot[X-X_i]$\"),\n    \n    'Description'= c(\n            \"Activation of each input node, $X_i$, is a function of the Gaussian similarity between the node value and stimulus X. \",\n            \"Activation of each Output unit $O_j$ is the weighted sum of the input activations and association weights\",\n            \"Each output node has associated response, $Y_j$. The probability of response $Y_j$ is determined by the ratio of output activations\",\n            \"The response to stimulus x is the weighted average of the response probabilities\",\n            \"After responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response  \",\n            \"Delta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\",\n            \"Novel test stimulus X activates input nodes associated with trained stimuli\",\n            \"Slope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)\")\n)\ntext_tbl$Step=cell_spec(text_tbl$Step,font_size=12)\ntext_tbl$Equation=cell_spec(text_tbl$Equation,font_size=18)\nalmTable=kable(text_tbl, 'html', \n  booktabs=T, escape = F, align='l',\n  caption = '&lt;span style = \"color:black;\"&gt;&lt;center&gt;&lt;strong&gt;Table 1: ALM & EXAM Equations&lt;/strong&gt;&lt;/center&gt;&lt;/span&gt;',\n  col.names=c(\"\",\"Equation\",\"Description\")) %&gt;%\n  kable_styling(position=\"left\",bootstrap_options = c(\"hover\")) %&gt;%\n  column_spec(1, bold = F,border_right=T) %&gt;%\n  column_spec(2, width = '10cm')%&gt;%\n  column_spec(3, width = '15cm') %&gt;%\n  pack_rows(\"ALM Activation & Response\",1,4,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"ALM Learning\",5,6,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"EXAM\",7,8,bold=FALSE,italic=TRUE)\n  #save_kable(file=\"almTable.html\",self_contained=T)\n#almTable\n\n\ncat(almTable)\n\n\n\n\nTable 1: ALM & EXAM Equations\n\n\n\n\n\n\n\nEquation\n\n\nDescription\n\n\n\n\n\nALM Activation & Response\n\n\n\n\nInput Activation\n\n\n\\(a_i(X) = \\frac{e^{-c \\cdot (X-X_i)^2}}{ \\sum_{k=1}^Me^{-c \\cdot (X-X_i)^2}}\\)\n\n\nActivation of each input node, \\(X_i\\), is a function of the Gaussian similarity between the node value and stimulus X.\n\n\n\n\nOutput Activation\n\n\n\\(O_j(X) = \\sum_{k=1}^Mw_{ji} \\cdot a_i(X)\\)\n\n\nActivation of each Output unit \\(O_j\\) is the weighted sum of the input activations and association weights\n\n\n\n\nOutput Probability\n\n\n\\(P[Y_j | X] = \\frac{O_i(X)}{\\sum_{k=1}^Mo_k(X)}\\)\n\n\nEach output node has associated response, \\(Y_j\\). The probability of response \\(Y_j\\) is determined by the ratio of output activations\n\n\n\n\nMean Output\n\n\n\\(m(x) = \\sum_{j=1}^LY_j \\cdot \\bigg[\\frac{O_j(X)}{\\sum_{k=1}^Lo_k(X)}\\bigg]\\)\n\n\nThe response to stimulus x is the weighted average of the response probabilities\n\n\n\n\nALM Learning\n\n\n\n\nFeedback Activation\n\n\n\\(f_j(Z)=e^{-c\\cdot(Z-Y_j)^2}\\)\n\n\nAfter responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response\n\n\n\n\nUpdate Weights\n\n\n\\(w_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t))\\)\n\n\nDelta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\n\n\n\n\nEXAM\n\n\n\n\nExtrapolation\n\n\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^Ma_k(X)}\\)\n\n\nNovel test stimulus X activates input nodes associated with trained stimuli\n\n\n\n\n\n\n\n\\(E[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg] \\cdot[X-X_i]\\)\n\n\nSlope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)\n\n\n\n\nHere is a footnote reference,1 and another.2"
  },
  {
    "objectID": "Analysis/e1_test.html#references",
    "href": "Analysis/e1_test.html#references",
    "title": "E1 Testing",
    "section": "References",
    "text": "References\n\n\nBürkner, P.-C. (2017). Brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80, 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541"
  },
  {
    "objectID": "Misc/alm_table.html",
    "href": "Misc/alm_table.html",
    "title": "HTW Project",
    "section": "",
    "text": "text_tbl &lt;- data.frame(\n    'Step'=c(\"Input Activation\",\"Output Activation\",\"Output Probability\",\"Mean Output\",\"Feedback Activation\",\"Update Weights\",\"Extrapolation\",\"\"),\n    'Equation' = c(\"$a_i(X) = \\\\frac{e^{-c \\\\cdot (X-X_i)^2}}{ \\\\sum_{k=1}^Me^{-c \\\\cdot (X-X_i)^2}}$\", \n                   '$O_j(X) = \\\\sum_{k=1}^Mw_{ji} \\\\cdot a_i(X)$',\n                   '$P[Y_j | X] = \\\\frac{O_i(X)}{\\\\sum_{k=1}^Mo_k(X)}$',\n                   \"$m(x) = \\\\sum_{j=1}^LY_j \\\\cdot \\\\bigg[\\\\frac{O_j(X)}{\\\\sum_{k=1}^Lo_k(X)}\\\\bigg]$\",\n                   \"$f_j(Z)=e^{-c\\\\cdot(Z-Y_j)^2}$\",\n                   \"$w_{ji}(t+1)=w_{ji}(t)+\\\\alpha \\\\cdot {f_i(Z(t))-O_j(X(t))} \\\\cdot a_i(X(t))$\",\n                   \"$P[X_i|X] = \\\\frac{a_i(X)}{\\\\sum_{k=1}^Ma_k(X)}$\",\n                   \"$E[Y|X_i]=m(X_i) + \\\\bigg[\\\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\\\bigg] \\\\cdot[X-X_i]$\"),\n    \n    'Description'= c(\n            \"Activation of each input node, $X_i$, is a function of the Gaussian similarity between the node value and stimulus X. \",\n            \"Activation of each Output unit $O_j$ is the weighted sum of the input activations and association weights\",\n            \"Each output node has associated response, $Y_j$. The probability of response $Y_j$ is determined by the ratio of output activations\",\n            \"The response to stimulus x is the weighted average of the response probabilities\",\n            \"After responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response  \",\n            \"Delta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\",\n            \"Novel test stimulus X activates input nodes associated with trained stimuli\",\n            \"Slope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)\")\n)\ntext_tbl$Step=cell_spec(text_tbl$Step,font_size=12)\ntext_tbl$Equation=cell_spec(text_tbl$Equation,font_size=18)\nalmTable=kable(text_tbl, 'html', \n  booktabs=T, escape = F, align='l',\n  caption = '&lt;span style = \"color:black;\"&gt;&lt;center&gt;&lt;strong&gt;Table 1: ALM & EXAM Equations&lt;/strong&gt;&lt;/center&gt;&lt;/span&gt;',\n  col.names=c(\"\",\"Equation\",\"Description\")) %&gt;%\n  kable_styling(position=\"left\",bootstrap_options = c(\"hover\")) %&gt;%\n  column_spec(1, bold = F,border_right=T) %&gt;%\n  column_spec(2, width = '10cm')%&gt;%\n  column_spec(3, width = '15cm') %&gt;%\n  pack_rows(\"ALM Activation & Response\",1,4,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"ALM Learning\",5,6,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"EXAM\",7,8,bold=FALSE,italic=TRUE)\n  #save_kable(file=\"almTable.html\",self_contained=T)\n#almTable\n\n\ncat(almTable)\n\n\n\n\nTable 1: ALM & EXAM Equations\n\n\n\n\n\n\n\nEquation\n\n\nDescription\n\n\n\n\n\nALM Activation & Response\n\n\n\n\nInput Activation\n\n\n\\(a_i(X) = \\frac{e^{-c \\cdot (X-X_i)^2}}{ \\sum_{k=1}^Me^{-c \\cdot (X-X_i)^2}}\\)\n\n\nActivation of each input node, \\(X_i\\), is a function of the Gaussian similarity between the node value and stimulus X.\n\n\n\n\nOutput Activation\n\n\n\\(O_j(X) = \\sum_{k=1}^Mw_{ji} \\cdot a_i(X)\\)\n\n\nActivation of each Output unit \\(O_j\\) is the weighted sum of the input activations and association weights\n\n\n\n\nOutput Probability\n\n\n\\(P[Y_j | X] = \\frac{O_i(X)}{\\sum_{k=1}^Mo_k(X)}\\)\n\n\nEach output node has associated response, \\(Y_j\\). The probability of response \\(Y_j\\) is determined by the ratio of output activations\n\n\n\n\nMean Output\n\n\n\\(m(x) = \\sum_{j=1}^LY_j \\cdot \\bigg[\\frac{O_j(X)}{\\sum_{k=1}^Lo_k(X)}\\bigg]\\)\n\n\nThe response to stimulus x is the weighted average of the response probabilities\n\n\n\n\nALM Learning\n\n\n\n\nFeedback Activation\n\n\n\\(f_j(Z)=e^{-c\\cdot(Z-Y_j)^2}\\)\n\n\nAfter responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response\n\n\n\n\nUpdate Weights\n\n\n\\(w_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t))\\)\n\n\nDelta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\n\n\n\n\nEXAM\n\n\n\n\nExtrapolation\n\n\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^Ma_k(X)}\\)\n\n\nNovel test stimulus X activates input nodes associated with trained stimuli\n\n\n\n\n\n\n\n\\(E[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg] \\cdot[X-X_i]\\)\n\n\nSlope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)"
  },
  {
    "objectID": "Analysis/e1_test.html#parameter-median-95-ci-pd-ess",
    "href": "Analysis/e1_test.html#parameter-median-95-ci-pd-ess",
    "title": "E1 Testing",
    "section": "Parameter | Median | 95% CI | pd | ESS",
    "text": "Parameter | Median | 95% CI | pd | ESS\nsigma | 211.87 | [208.91, 214.99] | 100% | 16418.00\nGroup-Level Effects: ~id (Number of levels: 166) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 263.32 14.02 237.28 292.55 1.00 1828 3602 sd(vb350M550) 90.85 8.09 75.59 107.34 1.00 4181 5750 sd(vb600M800) 170.08 10.60 150.42 192.36 1.00 2525 4436 sd(vb800M1000) 240.52 14.64 213.47 270.98 1.00 2442 4275 sd(vb1000M1200) 276.84 16.38 246.59 310.44 1.00 2411 4973 sd(vb1200M1400) 298.91 17.54 266.19 335.58 1.00 2549 4184\nThe model predicting absolute deviation (dist) showed clear effects of both training condition and target velocity band (Table X). Overall, the varied training group showed a larger deviation relative to the constant training group (β = 119, 95% CI [37, 202]). Deviation also depended on target velocity band, with lower bands showing less deviation.\nDiscrimination between Velocity Bands\nIn addition to accuracy/deviation. We also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350).\nNEEDS TO BE WRITTEN\nE1 Results Discussion"
  },
  {
    "objectID": "Misc/e1_brms_test.html#random-effects---interaction-condit-x-vb-1",
    "href": "Misc/e1_brms_test.html#random-effects---interaction-condit-x-vb-1",
    "title": "E1 Testing - Bayesian Mixed Models",
    "section": "Random Effects - Interaction condit x vb",
    "text": "Random Effects - Interaction condit x vb\n\nCodemodelName &lt;- \"e1_testDistRF2\"\ne1_testDistRF2 &lt;- brm(dist ~ condit * vb + (1 + vb|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\n\nGetModelStats(e1_testDistRF2)\n\nbrms_eq_tidy &lt;-tidyMCMC(e1_testDistRF2, conf.int = TRUE, conf.level = 0.95,\n           estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nbayes_R2(e1_testDistRF2)\nsjPlot::tab_model(e1_testDistRF2)\n\n\nfixef(e1_testDistRF2)\nnewdat &lt;-data.frame(crossing(condit=c(\"Constant\",\"Varied\"), vb = unique(test$vb)))\npreds &lt;- fitted(e1_testDistRF2, re_formula = NA, newdata = newdat, probs = c(0.025, 0.975))\n\n\n#shinystan::launch_shinystan(e1_testDistRF2)"
  },
  {
    "objectID": "Analysis/e1_Analysis.html#tables",
    "href": "Analysis/e1_Analysis.html#tables",
    "title": "Experiment 1 Analysis",
    "section": "Tables",
    "text": "Tables\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(vxMean=mean(vx),vxSd=sd(vx)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(vxMean,0)),sdLab=paste0(\"Sd=\",round(vxSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 7)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=544 Sd=335\nMean=664 Sd=448\n\n\n350-550\nMean=674 Sd=309\nMean=768 Sd=402\n\n\n600-800\nMean=786 Sd=305\nMean=876 Sd=390\n\n\n800-1000\nMean=1010 Sd=358\nMean=1064 Sd=370\n\n\n1000-1200\nMean=1167 Sd=430\nMean=1180 Sd=372\n\n\n1200-1400\nMean=1283 Sd=483\nMean=1265 Sd=412\n\n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(vxMean=mean(vx),vxSd=sd(vx)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(vxMean,0)),sdLab=paste0(\"Sd=\",round(vxSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=544 Sd=335\nMean=664 Sd=448\n\n\n350-550\nMean=674 Sd=309\nMean=768 Sd=402\n\n\n600-800\nMean=786 Sd=305\nMean=876 Sd=390\n\n\n800-1000\nMean=1010 Sd=358\nMean=1064 Sd=370\n\n\n1000-1200\nMean=1167 Sd=430\nMean=1180 Sd=372\n\n\n1200-1400\nMean=1283 Sd=483\nMean=1265 Sd=412\n\n\n\n\nCodee1 |&gt; filter(expMode %in% c(\"test-Nf\",\"test-train-nf\")) |&gt; group_by(vb,condit) %&gt;% \n  summarise(sdistMean=mean(sdist),sdistSd=sd(sdist)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(sdistMean,0)),sdLab=paste0(\"Sd=\",round(sdistSd,0))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=269 Sd=309\nMean=385 Sd=427\n\n\n350-550\nMean=175 Sd=257\nMean=265 Sd=356\n\n\n600-800\nMean=74 Sd=237\nMean=152 Sd=323\n\n\n800-1000\nMean=95 Sd=291\nMean=136 Sd=303\n\n\n1000-1200\nMean=64 Sd=360\nMean=66 Sd=300\n\n\n1200-1400\nMean=-3 Sd=408\nMean=-25 Sd=336\n\n\n\n\nCodetestAvg |&gt; group_by(vb,condit) %&gt;% \n  summarise(Percent_HitMean=mean(Percent_Hit),Percent_HitSd=sd(Percent_Hit)) %&gt;% \n  mutate(meanLab=paste0(\"Mean=\",round(Percent_HitMean,3)),sdLab=paste0(\"Sd=\",round(Percent_HitSd,2))) %&gt;% \n  mutate(sumStatLab=paste0(meanLab,\"\\n\",sdLab)) %&gt;% \n  select(vb,condit,sumStatLab) %&gt;% \n  spread(condit,sumStatLab) %&gt;% \n  kable(format = \"html\",escape = FALSE) %&gt;% \n  kable_styling(font_size = 10)\n\n\n\nvb\nConstant\nVaried\n\n\n\n100-300\nMean=0.236 Sd=0.26\nMean=0.201 Sd=0.26\n\n\n350-550\nMean=0.24 Sd=0.18\nMean=0.245 Sd=0.21\n\n\n600-800\nMean=0.282 Sd=0.16\nMean=0.206 Sd=0.16\n\n\n800-1000\nMean=0.272 Sd=0.22\nMean=0.19 Sd=0.19\n\n\n1000-1200\nMean=0.21 Sd=0.2\nMean=0.206 Sd=0.2\n\n\n1200-1400\nMean=0.161 Sd=0.17\nMean=0.2 Sd=0.19"
  }
]